<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Retractions ¬∑ GeometricMachineLearning.jl</title><meta name="title" content="Retractions ¬∑ GeometricMachineLearning.jl"/><meta property="og:title" content="Retractions ¬∑ GeometricMachineLearning.jl"/><meta property="twitter:title" content="Retractions ¬∑ GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/manifold_related/retractions/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/manifold_related/retractions/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/manifold_related/retractions/"/><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img class="docs-light-only" src="../../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">Home</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Optimizers</span><ul><li><a class="tocitem" href="../../optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../global_sections/">Global Sections</a></li><li class="is-active"><a class="tocitem" href>Retractions</a><ul class="internal"><li><a class="tocitem" href="#Classical-Retractions"><span>Classical Retractions</span></a></li><li><a class="tocitem" href="#In-GeometricMachineLearning"><span>In <code>GeometricMachineLearning</code></span></a></li><li><a class="tocitem" href="#Retractions-for-Homogeneous-Spaces"><span>Retractions for Homogeneous Spaces</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../../layers/attention_layer/">Attention</a></li><li><a class="tocitem" href="../../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modelling</span><ul><li><a class="tocitem" href="../../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../../reduced_order_modeling/losses/">Network Losses</a></li><li><a class="tocitem" href="../../../reduced_order_modeling/symplectic_autoencoder/">PSD and Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../../reduced_order_modeling/kolmogorov_n_width/">Kolmogorov n-width</a></li><li><a class="tocitem" href="../../../reduced_order_modeling/projection_reduction_errors/">Projection and Reduction Error</a></li></ul></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../../data_loader/data_loader/">Routines</a></li><li><a class="tocitem" href="../../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../../tutorials/sympnet_tutorial/">Sympnets</a></li><li><a class="tocitem" href="../../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../../tutorials/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../../tutorials/grassmann_layer/">Grassmann manifold</a></li><li><a class="tocitem" href="../../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../../references/">References</a></li><li><a class="tocitem" href="../../../library/">Library</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Optimizers</a></li><li class="is-active"><a href>Retractions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Retractions</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/optimizers/manifold_related/retractions.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Retractions"><a class="docs-heading-anchor" href="#Retractions">Retractions</a><a id="Retractions-1"></a><a class="docs-heading-anchor-permalink" href="#Retractions" title="Permalink"></a></h1><p>In practice we usually do not solve the geodesic equation exactly in each optimization step (even though this is possible and computationally feasible), but prefer approximations that are called &quot;retractions&quot; [<a href="../../../references/#absil2008optimization">10</a>] for stability. The definition of a retraction in <code>GeometricMachineLearning</code> is slightly different from how it is usually defined in textbooks [<a href="../../../references/#hairer2006geometric">7</a>, <a href="../../../references/#absil2008optimization">10</a>]. We discuss these differences here.</p><h2 id="Classical-Retractions"><a class="docs-heading-anchor" href="#Classical-Retractions">Classical Retractions</a><a id="Classical-Retractions-1"></a><a class="docs-heading-anchor-permalink" href="#Classical-Retractions" title="Permalink"></a></h2><p>By &quot;classical retraction&quot; we here mean the textbook definition. </p><div class="admonition is-info"><header class="admonition-header">Theorem</header><div class="admonition-body"><p>A <strong>classical retraction</strong> is a smooth map</p><p class="math-container">\[R: T\mathcal{M}\to\mathcal{M}:(x,v)\mapsto{}R_x(v),\]</p><p>such that each curve <span>$c(t) := R_x(tv)$</span> is a local approximation of a geodesic, i.e. the following two conditions hold:</p><ol><li><span>$c(0) = x$</span> and </li><li><span>$c&#39;(0) = v.$</span></li></ol></div></div><p>Perhaps the most common example for matrix manifolds is the <em>Cayley retraction</em>:</p><div class="admonition is-info"><header class="admonition-header">Example</header><div class="admonition-body"><p>The <strong>Cayley retraction</strong> is defined as</p><p class="math-container">\[\mathrm{Cayley}(V_x) = \left(\mathbb{I} - \frac{1}{2}V_x\right)^{-1}\left(\mathbb{I} +\frac{1}{2}V_x\right).\]</p></div></div><p>We should mention that the factor <span>$\frac{1}{2}$</span> is sometimes left out in the definition of the Cayley transform when used in different contexts. But it is necessary for defining a retraction. </p><p>We want to compare the <a href="../../../library/#GeometricMachineLearning.geodesic-Union{Tuple{GrassmannLieAlgHorMatrix{T, ST} where ST&lt;:AbstractMatrix{T}}, Tuple{T}} where T"><code>geodesic</code></a> retraction with the <a href="../../../library/#GeometricMachineLearning.cayley-Union{Tuple{GrassmannLieAlgHorMatrix{T, ST} where ST&lt;:AbstractMatrix{T}}, Tuple{T}} where T"><code>cayley</code></a> retraction for the example we already introduced when talking about the <a href="../../../manifolds/riemannian_manifolds/#Geodesic-Sprays-and-the-Exponential-Map">exponential map</a>:</p><pre><code class="language-julia hljs">Œ∑_increments = 0.1 : 0.1 : 5.5
Œî_increments = [Œî * Œ∑ for Œ∑ in Œ∑_increments]

Y_increments_geodesic = [geodesic(Y, Œî_increment) for Œî_increment in Œî_increments]
Y_increments_cayley = [cayley(Y, Œî_increment) for Œî_increment in Œî_increments]



Y_zeros = zeros(length(Y_increments_geodesic))
Y_geodesic_reshaped = [copy(Y_zeros), copy(Y_zeros), copy(Y_zeros)]
Y_cayley_reshaped = [copy(Y_zeros), copy(Y_zeros), copy(Y_zeros)]

zip_ob = zip(Y_increments_geodesic, Y_increments_cayley, axes(Y_increments_geodesic, 1))

for (Y_increment_geodesic, Y_increment_cayley, i) in zip_ob
    for d in (1, 2, 3)
        Y_geodesic_reshaped[d][i] = Y_increment_geodesic[d]

        Y_cayley_reshaped[d][i] = Y_increment_cayley[d]
    end
end

scatter!(ax, Y_geodesic_reshaped...;
        color = mred, markersize = 5, label = rich(&quot;geodesic retraction&quot;; color = text_color))

scatter!(ax, Y_cayley_reshaped...;
        color = mblue, markersize = 5, label = rich(&quot;Cayley retraction&quot;; color = text_color))</code></pre><object type="image/svg+xml" class="display-light-only" data=../retraction_comparison.png></object><object type="image/svg+xml" class="display-dark-only" data=../retraction_comparison_dark.png></object><p>We see that for small <span>$\Delta$</span> increments the Cayley retraction seems to match the geodesic retraction very well, but for larger values there is a notable discrepancy:</p><pre><code class="language-julia hljs">using LinearAlgebra: norm

discrepancies = [norm(Y_geo_inc - Y_cay_inc) for (Y_geo_inc, Y_cay_inc, _) in zip_ob]</code></pre><object type="image/svg+xml" class="display-light-only" data=../retraction_discrepancy.png></object><object type="image/svg+xml" class="display-dark-only" data=../retraction_discrepancy_dark.png></object><h2 id="In-GeometricMachineLearning"><a class="docs-heading-anchor" href="#In-GeometricMachineLearning">In <code>GeometricMachineLearning</code></a><a id="In-GeometricMachineLearning-1"></a><a class="docs-heading-anchor-permalink" href="#In-GeometricMachineLearning" title="Permalink"></a></h2><p>The way we use <em>retractions</em><sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> in <code>GeometricMachineLearning</code> is slightly different from their classical definition:</p><div class="admonition is-info"><header class="admonition-header">Definition</header><div class="admonition-body"><p>Given a section <span>$\lambda:\mathcal{M}\to{}G$</span> a <strong>retraction</strong> is a map <span>$\mathrm{Retraction}:\mathfrak{g}^\mathrm{hor}\to{}G$</span> such that </p><p class="math-container">\[\Delta \mapsto \lambda(Y)\mathrm{Retraction}(\lambda(Y)^{-1}\Omega(\Delta)\lambda(Y))E,\]</p><p>is a classical retraction.</p></div></div><p>We now discuss how two of these retractions, the geodesic retraction (exponential map) and the Cayley retraction, are implemented in <code>GeometricMachineLearning</code>.</p><h2 id="Retractions-for-Homogeneous-Spaces"><a class="docs-heading-anchor" href="#Retractions-for-Homogeneous-Spaces">Retractions for Homogeneous Spaces</a><a id="Retractions-for-Homogeneous-Spaces-1"></a><a class="docs-heading-anchor-permalink" href="#Retractions-for-Homogeneous-Spaces" title="Permalink"></a></h2><p>Here we harness special properties of homogeneous spaces to obtain computationally efficient retractions for the <a href="../../../manifolds/homogeneous_spaces/#The-Stiefel-Manifold">Stiefel manifold</a> and the <a href="../../../manifolds/homogeneous_spaces/#The-Grassmann-Manifold">Grassmann manifold</a>. This is also discussed in e.g. [<a href="../../../references/#bendokat2020grassmann">11</a>, <a href="../../../references/#bendokat2021real">15</a>].</p><p>The <em>geodesic retraction</em> is a retraction whose associated curve is also the unique geodesic. For many matrix Lie groups (including <span>$SO(N)$</span>) geodesics are obtained by simply evaluating the exponential map [<a href="../../../references/#absil2008optimization">10</a>, <a href="../../../references/#o1983semi">16</a>]:</p><div class="admonition is-info"><header class="admonition-header">Theorem</header><div class="admonition-body"><p>The geodesic on a compact matrix Lie group <span>$G$</span> with bi-invariant metric for <span>$B\in{}T_AG$</span> is simply</p><p class="math-container">\[\gamma(t) = \exp(t\cdot{}BA^{-1})A,\]</p><p>where <span>$\exp:\mathcal{g}\to{}G$</span> is the matrix exponential map.</p></div></div><p>Because <span>$SO(N)$</span> is compact and we furnish it with the canonical metric, i.e. </p><p class="math-container">\[    g:T_AG\times{}T_AG \to \mathbb{R}, (B_1, B_2) \mapsto \mathrm{Tr}(B_1^TB_2) = \mathrm{Tr}((B_1A^{-1})^T(B_2A^{-1})),\]</p><p>its geodesics are thus equivalent to the exponential maps. We now use this observation to obtain expression for the geodesics on the <a href="../../../manifolds/homogeneous_spaces/#The-Stiefel-Manifold">Stiefel manifold</a> <span>$St(n, N)$</span>. We use the following theorem from [<a href="../../../references/#o1983semi">16</a>, Proposition 25.7]:</p><div class="admonition is-info"><header class="admonition-header">Theorem</header><div class="admonition-body"><p>The geodesics for a naturally-reductive homogeneous space <span>$\mathcal{M}$</span> starting at <span>$Y$</span> are given by:</p><p class="math-container">\[\gamma_{\Delta}(t) = \exp(t\cdot\Omega(\Delta))Y,\]</p><p>where the <span>$\exp$</span> is the exponential map for the Lie group <span>$G$</span> corresponding to <span>$\mathcal{M}$</span>.</p></div></div><p>The theorem requires the homogeneous space to be naturally reductive: </p><div class="admonition is-info"><header class="admonition-header">Definition</header><div class="admonition-body"><p>A homogeneous space is called <strong>naturally-reductive</strong> if the following two conditions hold:</p><ol><li><span>$A^{-1}BA\in\mathfrak{g}^\mathrm{hor}$</span> for every <span>$B\in\mathfrak{g}^\mathrm{hor}$</span> and <span>$A\in\exp(\mathfrak{g}^\mathrm{ver}$</span>),</li><li><span>$g([X, Y]^\mathrm{hor}, Z) = g(X, [Y, Z]^\mathrm{hor})$</span> for all <span>$X, Y, Z \in \mathfrak{g}^\mathrm{hor}$</span>,</li></ol><p>where <span>$[X, Y]^\mathrm{hor} = \Omega(XYE - YXE)$</span>. If only the first condition holds the homogeneous space is called <strong>reductive</strong> but not <strong>naturally-reductive</strong>.</p></div></div><p>We state here without proof that the <a href="../../../manifolds/homogeneous_spaces/#The-Stiefel-Manifold">Stiefel manifold</a> and the <a href="../../../manifolds/homogeneous_spaces/#The-Grassmann-Manifold">Grassmann manifold</a> are naturally-reductive. An empirical verification of this is very easy:</p><pre><code class="language-julia hljs">using GeometricMachineLearning

B = rand(SkewSymMatrix, 6) # ‚àà ùî§
A = exp(B - StiefelLieAlgHorMatrix(B, 3)) # ‚àà exp(ùî§·µõ·µâ ≥)

X = rand(StiefelLieAlgHorMatrix, 6, 3) # ‚àà ùî§ ∞·µí ≥
Y = rand(StiefelLieAlgHorMatrix, 6, 3) # ‚àà ùî§ ∞·µí ≥
Z = rand(StiefelLieAlgHorMatrix, 6, 3) # ‚àà ùî§ ∞·µí ≥

A&#39; * X * A # this has to be in ùî§ ∞·µí ≥ for St(3, 6) to be reductive</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6√ó6 Matrix{Float64}:
 0.0       -0.914247  -0.387374  -0.650933  -0.741877  -0.7678
 0.914247   0.0       -0.771916  -0.454759  -0.869684  -0.80972
 0.387374   0.771916   0.0       -0.924238  -0.292897  -0.749455
 0.650933   0.454759   0.924238   0.0        0.0        0.0
 0.741877   0.869684   0.292897   0.0        0.0        0.0
 0.7678     0.80972    0.749455   0.0        0.0        0.0</code></pre><p>verifies the first property and</p><pre><code class="language-julia hljs">using LinearAlgebra: tr
ad ∞·µí ≥(X, Y) = StiefelLieAlgHorMatrix(X * Y - Y * X, 3)

tr(ad ∞·µí ≥(X, Y)&#39; * Z) ‚âà tr(X&#39; * ad ∞·µí ≥(Y, Z))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><p>verifies the second.</p><p>In <code>GeometricMachineLearning</code> we always work with elements in <span>$\mathfrak{g}^\mathrm{hor}$</span> and the Lie group <span>$G$</span> is always <span>$SO(N)$</span>. We hence use:</p><p class="math-container">\[    \gamma_\Delta(t) = \exp(\lambda(Y)\lambda(Y)^{-1}\Omega(\Delta)\lambda(Y)\lambda(Y)^{-1})Y = \lambda(Y)\exp(\lambda(Y)^{-1}\Omega(\Delta)\lambda(Y))E,\]</p><p>where we have used that </p><p class="math-container">\[ \exp(\Lambda{}B\Lambda^{-1}) = \sum_{n = 0}^\infty \frac{1}{n!}(\Lambda{}B\Lambda^{-1})^n = \sum_{n = 0}^\infty \frac{1}{n!}\underbrace{(\Lambda{}B\Lambda^{-1})}_{\text{$n$ times}} = \sum_{n = 0}^\infty \Lambda\frac{1}{n!}B^n\Lambda^{-1}.\]</p><p>Based on this we define the maps: </p><p class="math-container">\[\mathtt{geodesic}: \mathfrak{g}^\mathrm{hor} \to G, B \mapsto \exp(B),\]</p><p>and</p><p class="math-container">\[\mathtt{cayley}: \mathfrak{g}^\mathrm{hor} \to G, B \mapsto \mathrm{Cayley}(B),\]</p><p>where <span>$B = \lambda(Y)^{-1}\Omega(\Delta)\lambda(Y)$</span>. These expressions for <code>geodesic</code> and <code>cayley</code> are the ones that we typically use in <code>GeometricMachineLearning</code> for computational reasons. We show how we can utilize the sparse structure of <span>$\mathfrak{g}^\mathrm{hor}$</span> for computing the geodesic retraction and the Cayley retraction (i.e. the expressions <span>$\exp(B)$</span> and <span>$\mathrm{Cayley}(B)$</span> for <span>$B\in\mathfrak{g}^\mathrm{hor}$</span>). Similar derivations can be found in [<a href="../../../references/#bendokat2021real">15</a>, <a href="../../../references/#celledoni2000approximating">17</a>, <a href="../../../references/#fraikin2007optimization">18</a>].</p><div class="admonition is-info"><header class="admonition-header">Remark</header><div class="admonition-body"><p>Further note that, even though the global section <span>$\lambda:\mathcal{M} \to G$</span> is not unique, the final geodesic <span>$\gamma_\Delta(t) = \lambda(Y)\exp(\lambda(Y)^{-1}\Omega(\Delta)\lambda(Y))E$</span> does not depend on the particular section we choose.</p></div></div><h3 id="The-Geodesic-Retraction"><a class="docs-heading-anchor" href="#The-Geodesic-Retraction">The Geodesic Retraction</a><a id="The-Geodesic-Retraction-1"></a><a class="docs-heading-anchor-permalink" href="#The-Geodesic-Retraction" title="Permalink"></a></h3><p>An element of <span>$\mathfrak{g}^\mathrm{hor}$</span> can be written as:</p><p class="math-container">\[\begin{bmatrix}
    A &amp; -B^T \\ 
    B &amp; \mathbb{O}
\end{bmatrix} = \begin{bmatrix}  \frac{1}{2}A &amp; \mathbb{I} \\ B &amp; \mathbb{O} \end{bmatrix} \begin{bmatrix}  \mathbb{I} &amp; \mathbb{O} \\ \frac{1}{2}A &amp; -B^T  \end{bmatrix} =: B&#39;(B&#39;&#39;)^T,\]</p><p>where we exploit the sparse structure of the array, i.e. it is a multiplication of a <span>$N\times2n$</span> with a <span>$2n\times{}N$</span> matrix.</p><p>We further use the following: </p><p class="math-container">\[    \begin{aligned}
    \exp(B&#39;(B&#39;&#39;)^T) &amp; = \sum_{n=0}^\infty \frac{1}{n!} (B&#39;(B&#39;&#39;)^T)^n = \mathbb{I} + \sum_{n=0}^\infty \frac{1}{n!} B&#39;\sum_{n=1}^\infty B&#39;((B&#39;&#39;)^TB&#39;)(B&#39;&#39;)^T \\
    &amp; = \mathbb{I} + B&#39;\left( \sum_{n=1}^\infty \frac{1}{n!} ((B&#39;&#39;)^TB&#39;)^{n-1} \right)B&#39;&#39; =: \mathbb{I} + B&#39;\mathfrak{A}(B&#39;, B&#39;&#39;)B&#39;&#39;,
    \end{aligned}\]</p><p>where we defined <span>$\mathfrak{A}(B&#39;, B&#39;&#39;) := \sum_{n=1}^\infty \frac{1}{n!} ((B&#39;&#39;)^TB&#39;)^{n-1}.$</span> Note that evaluating <span>$\mathfrak{A}$</span> relies on computing products of <em>small</em> matrices of size <span>$2n\times2n.$</span> We do this by relying on a simple Taylor expansion (see the docstring for <a href="../../../library/#GeometricMachineLearning.ùîÑ-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T"><code>GeometricMachineLearning.ùîÑ</code></a>). </p><p>The final expression we obtain is: </p><p class="math-container">\[\exp(B) = \mathbb{I} + B&#39; \mathfrak{A}(B&#39;, B&#39;&#39;)  (B&#39;&#39;)^T\]</p><h3 id="The-Cayley-Retraction"><a class="docs-heading-anchor" href="#The-Cayley-Retraction">The Cayley Retraction</a><a id="The-Cayley-Retraction-1"></a><a class="docs-heading-anchor-permalink" href="#The-Cayley-Retraction" title="Permalink"></a></h3><p>For the Cayley retraction we leverage the decomposition of <span>$B = B&#39;(B&#39;&#39;)^T\in\mathfrak{g}^\mathrm{hor}$</span> through the <em>Sherman-Morrison-Woodbury formula</em>:</p><p class="math-container">\[(\mathbb{I} - \frac{1}{2}B&#39;(B&#39;&#39;)^T)^{-1} = \mathbb{I} + \frac{1}{2}B&#39;(\mathbb{I} - \frac{1}{2}B&#39;(B&#39;&#39;)^T)^{-1}(B&#39;&#39;)^T\]</p><p>So what we have to compute the inverse of:</p><p class="math-container">\[\mathbb{I} - \frac{1}{2}\begin{bmatrix}  \mathbb{I} &amp; \mathbb{O} \\ \frac{1}{2}A &amp; -B^T  \end{bmatrix}\begin{bmatrix}  \frac{1}{2}A &amp; \mathbb{I} \\ B &amp; \mathbb{O} \end{bmatrix} = 
\begin{bmatrix}  \mathbb{I} - \frac{1}{4}A &amp; - \frac{1}{2}\mathbb{I} \\ \frac{1}{2}B^TB - \frac{1}{8}A^2 &amp; \mathbb{I} - \frac{1}{4}A  \end{bmatrix}.\]</p><p>By leveraging the sparse structure of the matrices in <span>$\mathfrak{g}^\mathrm{hor}$</span> we arrive at the following expression for the Cayley retraction (similar to the case of the geodesic retraction):</p><p class="math-container">\[\mathrm{Cayley}(B) = \mathbb{I} + \frac{1}{2} B&#39; (\mathbb{I}_{2n} - \frac{1}{2} (B&#39;&#39;)^T B&#39;)^{-1} (B&#39;&#39;)^T (\mathbb{I} + \frac{1}{2} B),\]</p><p>where we have abbreviated <span>$\mathbb{I} := \mathbb{I}_N.$</span> We conclude with a remark:</p><div class="admonition is-info"><header class="admonition-header">Remark</header><div class="admonition-body"><p>As mentioned previously the Lie group <span>$SO(N)$</span>, i.e. the one corresponding to the Stiefel manifold and the Grassmann manifold, has a bi-invariant Riemannian metric associated with it: <span>$(B_1,B_2)\mapsto \mathrm{Tr}(B_1^TB_2)$</span>. For other Lie groups (e.g. the symplectic group) the situation is slightly more difficult <a href="@cite">bendokat2021real</a>.</p></div></div><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.geodesic-Tuple{StiefelLieAlgHorMatrix}-optimizers-manifold_related-retractions" href="#GeometricMachineLearning.geodesic-Tuple{StiefelLieAlgHorMatrix}-optimizers-manifold_related-retractions"><code>GeometricMachineLearning.geodesic</code></a> ‚Äî <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">geodesic(Y::Manifold, Œî)</code></pre><p>Take as input an element of a manifold <code>Y</code> and a tangent vector in <code>Œî</code> in the corresponding tangent space and compute the geodesic (exponential map).</p><p>In different notation: take as input an element <span>$x$</span> of <span>$\mathcal{M}$</span> and an element of <span>$T_x\mathcal{M}$</span> and return <span>$\mathtt{geodesic}(x, v_x) = \exp(v_x).$</span> For example: </p><pre><code class="language-julia hljs">Y = rand(StiefelManifold{Float64}, N, n)
Œî = rgrad(Y, rand(N, n))
geodesic(Y, Œî)</code></pre><p>See the docstring for <a href="../../../library/#GeometricMachineLearning.rgrad-Tuple{GrassmannManifold, AbstractMatrix}"><code>rgrad</code></a> for details on this function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/retractions.jl#LL16-L30">source</a></section><section><div><pre><code class="language-julia hljs">geodesic(B::StiefelLieAlgHorMatrix)</code></pre><p>Compute the geodesic of an element in <a href="../../../library/#GeometricMachineLearning.StiefelLieAlgHorMatrix"><code>StiefelLieAlgHorMatrix</code></a>.</p><p><strong>Implementation</strong></p><p>This is using a computationally efficient version of the matrix exponential. See <a href="../../../library/#GeometricMachineLearning.ùîÑ-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T"><code>GeometricMachineLearning.ùîÑ</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/retractions.jl#LL39-L47">source</a></section><section><div><pre><code class="language-julia hljs">geodesic(B::GrassmannLieAlgHorMatrix)</code></pre><p>Compute the geodesic of an element in <a href="../../../library/#GeometricMachineLearning.GrassmannLieAlgHorMatrix"><code>GrassmannLieAlgHorMatrix</code></a>.</p><p>See <a href="../../../library/#GeometricMachineLearning.geodesic-Union{Tuple{GrassmannLieAlgHorMatrix{T, ST} where ST&lt;:AbstractMatrix{T}}, Tuple{T}} where T"><code>geodesic(::StiefelLieAlgHorMatrix)</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/retractions.jl#LL57-L63">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.geodesic-Tuple{GrassmannLieAlgHorMatrix}-optimizers-manifold_related-retractions" href="#GeometricMachineLearning.geodesic-Tuple{GrassmannLieAlgHorMatrix}-optimizers-manifold_related-retractions"><code>GeometricMachineLearning.geodesic</code></a> ‚Äî <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">geodesic(Y::Manifold, Œî)</code></pre><p>Take as input an element of a manifold <code>Y</code> and a tangent vector in <code>Œî</code> in the corresponding tangent space and compute the geodesic (exponential map).</p><p>In different notation: take as input an element <span>$x$</span> of <span>$\mathcal{M}$</span> and an element of <span>$T_x\mathcal{M}$</span> and return <span>$\mathtt{geodesic}(x, v_x) = \exp(v_x).$</span> For example: </p><pre><code class="language-julia hljs">Y = rand(StiefelManifold{Float64}, N, n)
Œî = rgrad(Y, rand(N, n))
geodesic(Y, Œî)</code></pre><p>See the docstring for <a href="../../../library/#GeometricMachineLearning.rgrad-Tuple{GrassmannManifold, AbstractMatrix}"><code>rgrad</code></a> for details on this function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/retractions.jl#LL16-L30">source</a></section><section><div><pre><code class="language-julia hljs">geodesic(B::StiefelLieAlgHorMatrix)</code></pre><p>Compute the geodesic of an element in <a href="../../../library/#GeometricMachineLearning.StiefelLieAlgHorMatrix"><code>StiefelLieAlgHorMatrix</code></a>.</p><p><strong>Implementation</strong></p><p>This is using a computationally efficient version of the matrix exponential. See <a href="../../../library/#GeometricMachineLearning.ùîÑ-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T"><code>GeometricMachineLearning.ùîÑ</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/retractions.jl#LL39-L47">source</a></section><section><div><pre><code class="language-julia hljs">geodesic(B::GrassmannLieAlgHorMatrix)</code></pre><p>Compute the geodesic of an element in <a href="../../../library/#GeometricMachineLearning.GrassmannLieAlgHorMatrix"><code>GrassmannLieAlgHorMatrix</code></a>.</p><p>See <a href="../../../library/#GeometricMachineLearning.geodesic-Union{Tuple{GrassmannLieAlgHorMatrix{T, ST} where ST&lt;:AbstractMatrix{T}}, Tuple{T}} where T"><code>geodesic(::StiefelLieAlgHorMatrix)</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/retractions.jl#LL57-L63">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.cayley-Tuple{StiefelLieAlgHorMatrix}-optimizers-manifold_related-retractions" href="#GeometricMachineLearning.cayley-Tuple{StiefelLieAlgHorMatrix}-optimizers-manifold_related-retractions"><code>GeometricMachineLearning.cayley</code></a> ‚Äî <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cayley(Y::Manifold, Œî)</code></pre><p>Take as input an element of a manifold <code>Y</code> and a tangent vector in <code>Œî</code> in the corresponding tangent space and compute the Cayley retraction.</p><p>In different notation: take as input an element <span>$x$</span> of <span>$\mathcal{M}$</span> and an element of <span>$T_x\mathcal{M}$</span> and return <span>$\mathrm{Cayley}(v_x).$</span> For example: </p><pre><code class="language-julia hljs">Y = rand(StiefelManifold{Float64}, N, n)
Œî = rgrad(Y, rand(N, n))
cayley(Y, Œî)</code></pre><p>See the docstring for <a href="../../../library/#GeometricMachineLearning.rgrad-Tuple{GrassmannManifold, AbstractMatrix}"><code>rgrad</code></a> for details on this function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/retractions.jl#LL75-L89">source</a></section><section><div><pre><code class="language-julia hljs">cayley(B::StiefelLieAlgHorMatrix)</code></pre><p>Compute the Cayley retraction of <code>B</code> and multiply it with <code>E</code> (the distinct element of the Stiefel manifold).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/retractions.jl#LL98-L102">source</a></section><section><div><pre><code class="language-julia hljs">cayley(B::GrassmannLieAlgHorMatrix)</code></pre><p>Compute the Cayley retraction of <code>B</code> and multiply it with <code>E</code> (the distinct element of the Stiefel manifold).</p><p>See <a href="../../../library/#GeometricMachineLearning.cayley-Union{Tuple{GrassmannLieAlgHorMatrix{T, ST} where ST&lt;:AbstractMatrix{T}}, Tuple{T}} where T"><code>cayley(::StiefelLieAlgHorMatrix)</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/retractions.jl#LL116-L122">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.cayley-Tuple{GrassmannLieAlgHorMatrix}-optimizers-manifold_related-retractions" href="#GeometricMachineLearning.cayley-Tuple{GrassmannLieAlgHorMatrix}-optimizers-manifold_related-retractions"><code>GeometricMachineLearning.cayley</code></a> ‚Äî <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cayley(Y::Manifold, Œî)</code></pre><p>Take as input an element of a manifold <code>Y</code> and a tangent vector in <code>Œî</code> in the corresponding tangent space and compute the Cayley retraction.</p><p>In different notation: take as input an element <span>$x$</span> of <span>$\mathcal{M}$</span> and an element of <span>$T_x\mathcal{M}$</span> and return <span>$\mathrm{Cayley}(v_x).$</span> For example: </p><pre><code class="language-julia hljs">Y = rand(StiefelManifold{Float64}, N, n)
Œî = rgrad(Y, rand(N, n))
cayley(Y, Œî)</code></pre><p>See the docstring for <a href="../../../library/#GeometricMachineLearning.rgrad-Tuple{GrassmannManifold, AbstractMatrix}"><code>rgrad</code></a> for details on this function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/retractions.jl#LL75-L89">source</a></section><section><div><pre><code class="language-julia hljs">cayley(B::StiefelLieAlgHorMatrix)</code></pre><p>Compute the Cayley retraction of <code>B</code> and multiply it with <code>E</code> (the distinct element of the Stiefel manifold).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/retractions.jl#LL98-L102">source</a></section><section><div><pre><code class="language-julia hljs">cayley(B::GrassmannLieAlgHorMatrix)</code></pre><p>Compute the Cayley retraction of <code>B</code> and multiply it with <code>E</code> (the distinct element of the Stiefel manifold).</p><p>See <a href="../../../library/#GeometricMachineLearning.cayley-Union{Tuple{GrassmannLieAlgHorMatrix{T, ST} where ST&lt;:AbstractMatrix{T}}, Tuple{T}} where T"><code>cayley(::StiefelLieAlgHorMatrix)</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/retractions.jl#LL116-L122">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.cayley-Union{Tuple{T}, Tuple{Manifold{T}, AbstractMatrix{T}}} where T-optimizers-manifold_related-retractions" href="#GeometricMachineLearning.cayley-Union{Tuple{T}, Tuple{Manifold{T}, AbstractMatrix{T}}} where T-optimizers-manifold_related-retractions"><code>GeometricMachineLearning.cayley</code></a> ‚Äî <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">cayley(Y::Manifold, Œî)</code></pre><p>Take as input an element of a manifold <code>Y</code> and a tangent vector in <code>Œî</code> in the corresponding tangent space and compute the Cayley retraction.</p><p>In different notation: take as input an element <span>$x$</span> of <span>$\mathcal{M}$</span> and an element of <span>$T_x\mathcal{M}$</span> and return <span>$\mathrm{Cayley}(v_x).$</span> For example: </p><pre><code class="language-julia hljs">Y = rand(StiefelManifold{Float64}, N, n)
Œî = rgrad(Y, rand(N, n))
cayley(Y, Œî)</code></pre><p>See the docstring for <a href="../../../library/#GeometricMachineLearning.rgrad-Tuple{GrassmannManifold, AbstractMatrix}"><code>rgrad</code></a> for details on this function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/retractions.jl#LL75-L89">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.ùîÑ-optimizers-manifold_related-retractions" href="#GeometricMachineLearning.ùîÑ-optimizers-manifold_related-retractions"><code>GeometricMachineLearning.ùîÑ</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ùîÑ(A)</code></pre><p>Compute <span>$\mathfrak{A}(A) := \sum_{n=1}^\infty \frac{1}{n!} (A)^{n-1}.$</span></p><p><strong>Implementation</strong></p><p>This uses a Taylor expansion that iteratively adds terms with</p><pre><code class="language-julia hljs">while norm(A‚Åø) &gt; Œµ
    mul!(A_temp, A‚Åø, A)
    A‚Åø .= A_temp
    rmul!(A‚Åø, inv(n))

    ùîÑ += B
    n += 1 
end</code></pre><p>until the norm of <code>A‚Åø</code> becomes smaller than machine precision.  The counter <code>n</code> in the above algorithm is initialized as <code>2</code> The matrices <code>A‚Åø</code> and <code>ùîÑ</code> are initialized as the identity matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/modified_exponential.jl#LL1-L24">source</a></section><section><div><pre><code class="language-julia hljs">ùîÑ(BÃÇ, BÃÑ)</code></pre><p>Compute <span>$\mathfrak{A}(B&#39;, B&#39;&#39;) := \sum_{n=1}^\infty \frac{1}{n!} ((B&#39;&#39;)^TB&#39;)^{n-1}.$</span></p><p>This expression has the property <span>$\mathbb{I} +  B&#39;\mathfrak{A}(B&#39;, B&#39;&#39;)(B&#39;&#39;)^T = \exp(B&#39;(B&#39;&#39;)^T).$</span></p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using GeometricMachineLearning
using GeometricMachineLearning: ùîÑ
import Random
Random.seed!(123)

B = rand(StiefelLieAlgHorMatrix, 10, 2)
BÃÇ = hcat(vcat(.5 * B.A, B.B), vcat(one(B.A), zero(B.B)))
BÃÑ = hcat(vcat(one(B.A), zero(B.B)), vcat(-.5 * B.A, -B.B))

one(BÃÇ * BÃÑ&#39;) + BÃÇ * ùîÑ(BÃÇ, BÃÑ) * BÃÑ&#39; ‚âà exp(Matrix(B))

# output

true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/1f16ae16a050252629705d7134b8985c40fcc0d8/src/optimizers/manifold_related/modified_exponential.jl#LL42-L67">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[10]</dt><dd><div>P.-A.¬†Absil, R.¬†Mahony and R.¬†Sepulchre. <em>Optimization algorithms on matrix manifolds</em> (Princeton University Press, Princeton, New Jersey, 2008).</div></dd><dt>[15]</dt><dd><div>T.¬†Bendokat and R.¬†Zimmermann. <em>The real symplectic Stiefel and Grassmann manifolds: metrics, geodesics and applications</em>, arXiv¬†preprint¬†arXiv:2108.12447 (2021).</div></dd><dt>[16]</dt><dd><div>B.¬†O&#39;neill. <em>Semi-Riemannian geometry with applications to relativity</em> (Academic press, New York City, New York, 1983).</div></dd></dl></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>Classical retractions are also defined in <code>GeometricMachineLearning</code> under the same name, i.e. there is e.g. a method <a href="../../../library/#GeometricMachineLearning.cayley-Union{Tuple{GrassmannLieAlgHorMatrix{T, ST} where ST&lt;:AbstractMatrix{T}}, Tuple{T}} where T"><code>cayley(::StiefelLieAlgHorMatrix)</code></a> and a method <a href="../../../library/#GeometricMachineLearning.cayley-Union{Tuple{GrassmannLieAlgHorMatrix{T, ST} where ST&lt;:AbstractMatrix{T}}, Tuple{T}} where T"><code>cayley(::StiefelManifold, ::AbstractMatrix)</code></a> (the latter being the classical retraction); but the user is <em>strongly discouraged</em> from using classical retractions as these are computational inefficient.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../global_sections/">¬´ Global Sections</a><a class="docs-footer-nextpage" href="../parallel_transport/">Parallel Transport ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Tuesday 2 July 2024 19:08">Tuesday 2 July 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
