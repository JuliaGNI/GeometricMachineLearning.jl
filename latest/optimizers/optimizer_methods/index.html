<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optimizer Methods · GeometricMachineLearning.jl</title><meta name="title" content="Optimizer Methods · GeometricMachineLearning.jl"/><meta property="og:title" content="Optimizer Methods · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Optimizer Methods · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/optimizer_methods/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/optimizer_methods/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/optimizer_methods/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../manifold_related/parallel_transport/">Parallel Transport</a></li><li class="is-active"><a class="tocitem" href>Optimizer Methods</a><ul class="internal"><li><a class="tocitem" href="#The-Gradient-Optimizer"><span>The Gradient Optimizer</span></a></li><li><a class="tocitem" href="#The-Momentum-Optimizer"><span>The Momentum Optimizer</span></a></li><li><a class="tocitem" href="#The-Adam-Optimizer"><span>The Adam Optimizer</span></a></li><li><a class="tocitem" href="#The-Adam-Optimizer-with-Decay"><span>The Adam Optimizer with Decay</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Optimizer</a></li><li class="is-active"><a href>Optimizer Methods</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Optimizer Methods</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/optimizers/optimizer_methods.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Standard-Neural-Network-Optimizers"><a class="docs-heading-anchor" href="#Standard-Neural-Network-Optimizers">Standard Neural Network Optimizers</a><a id="Standard-Neural-Network-Optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Standard-Neural-Network-Optimizers" title="Permalink"></a></h1><p>In this section we discuss optimization methods that are often used in training neural networks. The <a href="../bfgs_optimizer/#The-BFGS-Optimizer">BFGS optimizer</a> may also be viewed as a <em>standard neural network optimizer</em> but is treated in a separate section because of its complexity. From a perspective of manifolds the <em>optimizer methods</em> outlined here operate on <span>$\mathfrak{g}^\mathrm{hor}$</span> only. Each of them has a cache associated with it<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> and this cache is updated with the function <a href="../bfgs_optimizer/#AbstractNeuralNetworks.update!-Tuple{Optimizer{&lt;:BFGSOptimizer}, BFGSCache, AbstractArray}"><code>update!</code></a>. The precise role of this function is described below.</p><h2 id="The-Gradient-Optimizer"><a class="docs-heading-anchor" href="#The-Gradient-Optimizer">The Gradient Optimizer</a><a id="The-Gradient-Optimizer-1"></a><a class="docs-heading-anchor-permalink" href="#The-Gradient-Optimizer" title="Permalink"></a></h2><p>The gradient optimizer is the simplest optimization algorithm used to train neural networks. It was already briefly discussed when we introduced <a href="../../manifolds/riemannian_manifolds/#Gradient-Flows-and-Riemannian-Optimization">Riemannian manifolds</a>.</p><p>It simply does: </p><p class="math-container">\[\mathrm{weight} \leftarrow \mathrm{weight} + (-\eta\cdot\mathrm{gradient}),\]</p><p>where addition has to be replaced with appropriate operations in the manifold case<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>.</p><p>When calling <a href="#GeometricMachineLearning.GradientOptimizer"><code>GradientOptimizer</code></a> we can specify a learning rate <span>$\eta$</span> (or use the default).</p><pre><code class="language-julia hljs">const η = 0.01
method = GradientOptimizer(η)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">GradientOptimizer{Float64}(0.01)</code></pre><p>In order to use the optimizer we need an instance of <a href="../optimizer_framework/#GeometricMachineLearning.Optimizer"><code>Optimizer</code></a> that is called with the method and the weights of the neural network:</p><pre><code class="language-julia hljs">weight = (A = zeros(4, 4), )
o = Optimizer(method, weight)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Optimizer{GradientOptimizer{Float64}, @NamedTuple{A::GradientCache{Float64}}, typeof(cayley)}(GradientOptimizer{Float64}(0.01), (A = GradientCache{Float64}(),), 0, GeometricMachineLearning.cayley)</code></pre><p>If we operate on a derivative with <a href="../bfgs_optimizer/#AbstractNeuralNetworks.update!-Tuple{Optimizer{&lt;:BFGSOptimizer}, BFGSCache, AbstractArray}"><code>update!</code></a> this will compute a <em>final velocity</em> that is then used to compute a retraction (or simply perform addition if we do not deal with a manifold):</p><pre><code class="language-julia hljs">dx = (A = one(weight.A), )
update!(o, o.cache, dx)

dx.A</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4×4 Matrix{Float64}:
 -0.01  -0.0   -0.0   -0.0
 -0.0   -0.01  -0.0   -0.0
 -0.0   -0.0   -0.01  -0.0
 -0.0   -0.0   -0.0   -0.01</code></pre><p>So what has happened here is that the gradient <code>dx</code> was simply multiplied with <span>$-\eta$</span> as the cache of the gradient optimizer is trivial.</p><h2 id="The-Momentum-Optimizer"><a class="docs-heading-anchor" href="#The-Momentum-Optimizer">The Momentum Optimizer</a><a id="The-Momentum-Optimizer-1"></a><a class="docs-heading-anchor-permalink" href="#The-Momentum-Optimizer" title="Permalink"></a></h2><p>The momentum optimizer is similar to the gradient optimizer but further stores past information as <em>first moments</em>. We let these first moments <em>decay</em> with a <em>decay parameter</em> <span>$\alpha$</span>:</p><p class="math-container">\[\mathrm{weights} \leftarrow \mathrm{weights} + (\alpha\cdot\mathrm{moment} - \eta\cdot\mathrm{gradient}),\]</p><p>where addition has to be replaced with appropriate operations in the manifold case.</p><p>In the case of the momentum optimizer the cache is non-trivial:</p><pre><code class="language-julia hljs">const α = 0.5
method = MomentumOptimizer(η, α)
o = Optimizer(method, weight)

o.cache.A # the cache is stored for each array in `weight` (which is a `NamedTuple`)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">`MomentumCache` that currently stores `B`as  ...
4×4 Matrix{Float64}:
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0</code></pre><p>But as the cache is initialized with zeros it will lead to the same result as the gradient optimizer in the first iteration:</p><pre><code class="language-julia hljs">dx = (A = one(weight.A), )

update!(o, o.cache, dx)

dx.A</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4×4 Matrix{Float64}:
 -0.01  -0.0   -0.0   -0.0
 -0.0   -0.01  -0.0   -0.0
 -0.0   -0.0   -0.01  -0.0
 -0.0   -0.0   -0.0   -0.01</code></pre><p>The cache has changed however:</p><pre><code class="language-julia hljs">o.cache.A</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">`MomentumCache` that currently stores `B`as  ...
4×4 Matrix{Float64}:
 1.0  0.0  0.0  0.0
 0.0  1.0  0.0  0.0
 0.0  0.0  1.0  0.0
 0.0  0.0  0.0  1.0</code></pre><p>If we have weights on manifolds calling <a href="../optimizer_framework/#GeometricMachineLearning.Optimizer"><code>Optimizer</code></a> will automatically allocate the correct cache on <span>$\mathfrak{g}^\mathrm{hor}$</span>:</p><pre><code class="language-julia hljs">weight = (Y = rand(StiefelManifold, 5, 3), )

Optimizer(method, weight).cache.Y</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">`MomentumCache` that currently stores `B`as  ...
5×5 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:
 0.0  -0.0  -0.0  -0.0  -0.0
 0.0   0.0  -0.0  -0.0  -0.0
 0.0   0.0   0.0  -0.0  -0.0
 0.0   0.0   0.0   0.0   0.0
 0.0   0.0   0.0   0.0   0.0</code></pre><p>So if the weight is <span>$Y\in{}St(n,N)$</span> the corresponding cache is initialized as the zero element on <span>$\mathfrak{g}^\mathrm{hor}\subset\mathbb{R}^{N\times{}N}$</span> as this is the global tangent space representation corresponding to the StiefelManifold.</p><h2 id="The-Adam-Optimizer"><a class="docs-heading-anchor" href="#The-Adam-Optimizer">The Adam Optimizer</a><a id="The-Adam-Optimizer-1"></a><a class="docs-heading-anchor-permalink" href="#The-Adam-Optimizer" title="Permalink"></a></h2><p>The Adam Optimizer is one of the most widely neural network optimizers. The cache of the Adam optimizer consists of <em>first and second moments</em>. The <em>first moments</em> <span>$B_1$</span>, similar to the momentum optimizer, store linear information about the current and previous gradients, and the <em>second moments</em> <span>$B_2$</span> store quadratic information about current and previous gradients. These second moments can be interpreted as approximating the curvature of the optimization landscape.  </p><p>If all the weights are on a vector space, then we directly compute updates for <span>$B_1$</span> and <span>$B_2$</span>:</p><ol><li><span>$B_1 \gets ((\rho_1 - \rho_1^t)/(1 - \rho_1^t))\cdot{}B_1 + (1 - \rho_1)/(1 - \rho_1^t)\cdot{}\nabla{}L,$</span></li><li><span>$B_2 \gets ((\rho_2 - \rho_1^t)/(1 - \rho_2^t))\cdot{}B_2 + (1 - \rho_2)/(1 - \rho_2^t)\cdot\nabla{}L\odot\nabla{}L,$</span></li></ol><p>where <span>$\odot:\mathbb{R}^n\times\mathbb{R}^n\to\mathbb{R}^n$</span> is the <em>Hadamard product</em>: <span>$[a\odot{}b]_i = a_ib_i.$</span> <span>$\rho_1$</span> and <span>$\rho_2$</span> are hyperparameters. Their defaults, <span>$\rho_1=0.9$</span> and <span>$\rho_2=0.99$</span>, are taken from [<a href="../../references/#goodfellow2016deep">43</a>, page 301]. After having updated the <code>cache</code> (i.e. <span>$B_1$</span> and <span>$B_2$</span>) we compute a <em>velocity</em> with which the parameters of the network are then updated:</p><ul><li><span>$W_t\gets -\eta{}B_1/\sqrt{B_2 + \delta},$</span></li><li><span>$Y^{(t+1)} \gets Y^{(t)} + W^{(t)},$</span></li></ul><p>where the last addition has to be replaced with appropriate operations when dealing with manifolds. Further <span>$\eta$</span> is the <em>learning rate</em> and <span>$\delta$</span> is a small constant that is added for stability. The division, square root and addition in the computation of <span>$W_t$</span> are performed element-wise.</p><p>In the following we show a schematic update that Adam performs for the case when no elements are on manifolds (also compare this figure with the <a href="../optimizer_framework/#Generalization-to-Homogeneous-Spaces">general optimization framework</a>):</p><object type="image/svg+xml" class="display-light-only" data=../../tikz/adam_optimizer.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/adam_optimizer_dark.png></object><p>We demonstrate the Adam cache on the same example from before:</p><pre><code class="language-julia hljs">const ρ₁ = 0.9
const ρ₂ = 0.99
const δ = 1e-8

method = AdamOptimizer(η, ρ₁, ρ₂, δ)
o = Optimizer(method, weight)

o.cache.Y</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">`AdamCache` that currently stores `B₁` as ...
5×5 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:
 0.0  -0.0  -0.0  -0.0  -0.0
 0.0   0.0  -0.0  -0.0  -0.0
 0.0   0.0   0.0  -0.0  -0.0
 0.0   0.0   0.0   0.0   0.0
 0.0   0.0   0.0   0.0   0.0
and `B₂` as ...
5×5 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:
 0.0  -0.0  -0.0  -0.0  -0.0
 0.0   0.0  -0.0  -0.0  -0.0
 0.0   0.0   0.0  -0.0  -0.0
 0.0   0.0   0.0   0.0   0.0
 0.0   0.0   0.0   0.0   0.0</code></pre><h3 id="Weights-on-Manifolds"><a class="docs-heading-anchor" href="#Weights-on-Manifolds">Weights on Manifolds</a><a id="Weights-on-Manifolds-1"></a><a class="docs-heading-anchor-permalink" href="#Weights-on-Manifolds" title="Permalink"></a></h3><p>The problem with generalizing Adam to manifolds is that the Hadamard product <span>$\odot$</span> as well as the other element-wise operations (<span>$/$</span>, <span>$\sqrt{}$</span> and <span>$+$</span>) lack a clear geometric interpretation. In <code>GeometricMachineLearning</code> we get around this issue by utilizing the <a href="../../arrays/global_tangent_spaces/#Global-Tangent-Spaces">global tangent space representation</a>. A similar approach is shown in [<a href="../../references/#kong2023momentum">8</a>].</p><h2 id="The-Adam-Optimizer-with-Decay"><a class="docs-heading-anchor" href="#The-Adam-Optimizer-with-Decay">The Adam Optimizer with Decay</a><a id="The-Adam-Optimizer-with-Decay-1"></a><a class="docs-heading-anchor-permalink" href="#The-Adam-Optimizer-with-Decay" title="Permalink"></a></h2><p>The Adam optimizer with decay is similar to the standard Adam optimizer with the difference that the learning rate <span>$\eta$</span> decays exponentially. We start with a relatively high learning rate <span>$\eta_1$</span> (e.g. <span>$10^{-2}$</span>) and end with a low learning rate <span>$\eta_2$</span> (e.g. <span>$10^{-8}$</span>). If we want to use this optimizer we have to tell it beforehand how many epochs we train for such that it can adjust the learning rate decay accordingly:</p><pre><code class="language-julia hljs">const η₁ = 1e-2
const η₂ = 1e-6
const n_epochs = 1000

method = AdamOptimizerWithDecay(n_epochs, η₁, η₂, ρ₁, ρ₂, δ)
o = Optimizer(method, weight)</code></pre><p>The cache is however exactly the same as for the Adam optimizer:</p><pre><code class="language-julia hljs">o.cache.Y</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">`AdamCache` that currently stores `B₁` as ...
5×5 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:
 0.0  -0.0  -0.0  -0.0  -0.0
 0.0   0.0  -0.0  -0.0  -0.0
 0.0   0.0   0.0  -0.0  -0.0
 0.0   0.0   0.0   0.0   0.0
 0.0   0.0   0.0   0.0   0.0
and `B₂` as ...
5×5 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:
 0.0  -0.0  -0.0  -0.0  -0.0
 0.0   0.0  -0.0  -0.0  -0.0
 0.0   0.0   0.0  -0.0  -0.0
 0.0   0.0   0.0   0.0   0.0
 0.0   0.0   0.0   0.0   0.0</code></pre><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.OptimizerMethod" href="#GeometricMachineLearning.OptimizerMethod"><code>GeometricMachineLearning.OptimizerMethod</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">OptimizerMethod</code></pre><p>Each <code>Optimizer</code> has to be called with an <code>OptimizerMethod</code>. This specifies how the neural network weights are updated in each optimization step.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/optimizers/optimizer_method.jl#LL1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.GradientOptimizer" href="#GeometricMachineLearning.GradientOptimizer"><code>GeometricMachineLearning.GradientOptimizer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GradientOptimizer(η)</code></pre><p>Make an instance of a gradient optimizer. </p><p>This is the simplest neural network optimizer. It has no cache and computes the final velocity as:</p><p class="math-container">\[    \mathrm{velocity} \gets - \eta\nabla_\mathrm{weight}L.\]</p><p><strong>Implementation</strong></p><p>The operations are done as memory efficiently as possible. This means the provided <span>$\nabla_WL$</span> is mutated via:</p><pre><code class="language-julia hljs">rmul!(∇L, -method.η)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/optimizers/gradient_optimizer.jl#LL1-L18">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.MomentumOptimizer" href="#GeometricMachineLearning.MomentumOptimizer"><code>GeometricMachineLearning.MomentumOptimizer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MomentumOptimizer(η, α)</code></pre><p>Make an instance of the momentum optimizer.</p><p>The momentum optimizer is similar to the <a href="#GeometricMachineLearning.GradientOptimizer"><code>GradientOptimizer</code></a>. It however has a nontrivial cache that stores past history (see <a href="#GeometricMachineLearning.MomentumCache"><code>MomentumCache</code></a>). The cache is updated via:</p><p class="math-container">\[    B^{\mathrm{cache}} \gets \alpha{}B^{\mathrm{cache}} + \nabla_\mathrm{weights}L\]</p><p>and then the final velocity is computed as</p><p class="math-container">\[    \mathrm{velocity} \gets  - \eta{}B^{\mathrm{cache}}.\]</p><p><strong>Implementation</strong></p><p>To save memory the <em>velocity</em> is stored in the input <span>$\nabla_WL$</span>. This is similar to the case of the <a href="#GeometricMachineLearning.GradientOptimizer"><code>GradientOptimizer</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/optimizers/momentum_optimizer.jl#LL1-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.AdamOptimizer" href="#GeometricMachineLearning.AdamOptimizer"><code>GeometricMachineLearning.AdamOptimizer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AdamOptimizer(η, ρ₁, ρ₂, δ)</code></pre><p>Make an instance of the Adam Optimizer.</p><p>Here the cache consists of first and second moments that are updated as </p><p class="math-container">\[B_1 \gets ((\rho_1 - \rho_1^t)/(1 - \rho_1^t))\cdot{}B_1 + (1 - \rho_1)/(1 - \rho_1^t)\cdot{}\nabla{}L,\]</p><p>and</p><p class="math-container">\[B_2 \gets ((\rho_2 - \rho_1^t)/(1 - \rho_2^t))\cdot{}B_2 + (1 - \rho_2)/(1 - \rho_2^t)\cdot\nabla{}L\odot\nabla{}L.\]</p><p>The final velocity is computed as:</p><p class="math-container">\[\mathrm{velocity} \gets -\eta{}B_1/\sqrt{B_2 + \delta}.\]</p><p><strong>Implementation</strong></p><p>The <em>velocity</em> is stored in the input to save memory:</p><pre><code class="language-julia hljs">mul!(B, -o.method.η, /ᵉˡᵉ(C.B₁, scalar_add(racᵉˡᵉ(C.B₂), o.method.δ)))</code></pre><p>where <code>B</code> is the input to the [<code>update!</code>] function.</p><p>The algorithm and suggested defaults are taken from [<a href="../../references/#goodfellow2016deep">43</a>, page 301].</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/optimizers/adam_optimizer.jl#LL1-L32">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.AdamOptimizerWithDecay" href="#GeometricMachineLearning.AdamOptimizerWithDecay"><code>GeometricMachineLearning.AdamOptimizerWithDecay</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AdamOptimizerWithDecay(n_epochs, η₁=1f-2, η₂=1f-6, ρ₁=9f-1, ρ₂=9.9f-1, δ=1f-8)</code></pre><p>Make an instance of the Adam Optimizer with weight decay.</p><p>All except the first argument (the number of epochs) have defaults.</p><p>The difference to the standard <a href="#GeometricMachineLearning.AdamOptimizer"><code>AdamOptimizer</code></a> is that we change the learning reate <span>$\eta$</span> in each step. Apart from the <em>time dependency</em> of <span>$\eta$</span> the two algorithms are however equivalent. <span>$\eta(0)$</span> starts with a high value <span>$\eta_1$</span> and then exponentially decrease until it reaches <span>$\eta_2$</span> with</p><p class="math-container">\[ \eta(t) = \gamma^t\eta_1,\]</p><p>where <span>$\gamma = \exp(\log(\eta_1 / \eta_2) / \mathtt{n\_epochs}).$</span></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/optimizers/adam_optimizer_with_learning_rate_decay.jl#LL1-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.AbstractCache" href="#GeometricMachineLearning.AbstractCache"><code>GeometricMachineLearning.AbstractCache</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AbstractCache</code></pre><p><code>AbstractCache</code> has subtypes: <a href="#GeometricMachineLearning.AdamCache"><code>AdamCache</code></a>, <a href="#GeometricMachineLearning.MomentumCache"><code>MomentumCache</code></a>, <a href="#GeometricMachineLearning.GradientCache"><code>GradientCache</code></a> and <a href="../bfgs_optimizer/#GeometricMachineLearning.BFGSCache"><code>BFGSCache</code></a>.</p><p>All of them can be initialized with providing an array (also supporting manifold types).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/optimizers/optimizer_caches.jl#LL1-L7">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.GradientCache" href="#GeometricMachineLearning.GradientCache"><code>GeometricMachineLearning.GradientCache</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GradientCache(Y)</code></pre><p>Do not store anything.</p><p>The cache for the <a href="#GeometricMachineLearning.GradientOptimizer"><code>GradientOptimizer</code></a> does not consider past information.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/optimizers/optimizer_caches.jl#LL79-L85">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.MomentumCache" href="#GeometricMachineLearning.MomentumCache"><code>GeometricMachineLearning.MomentumCache</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MomentumCache(Y)</code></pre><p>Store the moment for <code>Y</code> (initialized as zeros).</p><p>The moment is called <code>B</code>.</p><p>If the cache is called with an instance of a <a href="../../manifolds/manifolds/#GeometricMachineLearning.Manifold"><code>Manifold</code></a> it initializes the moments as elements of <span>$\mathfrak{g}^\mathrm{hor}$</span> (<a href="../../arrays/global_tangent_spaces/#GeometricMachineLearning.AbstractLieAlgHorMatrix"><code>AbstractLieAlgHorMatrix</code></a>).</p><p>See <a href="#GeometricMachineLearning.AdamCache"><code>AdamCache</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/optimizers/optimizer_caches.jl#LL56-L66">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.AdamCache" href="#GeometricMachineLearning.AdamCache"><code>GeometricMachineLearning.AdamCache</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AdamCache(Y)</code></pre><p>Store the first and second moment for <code>Y</code> (initialized as zeros).</p><p>First and second moments are called <code>B₁</code> and <code>B₂</code>.</p><p>If the cache is called with an instance of a homogeneous space, e.g. the <a href="../../manifolds/homogeneous_spaces/#GeometricMachineLearning.StiefelManifold"><code>StiefelManifold</code></a> <span>$St(n,N)$</span> it initializes the moments as elements of <span>$\mathfrak{g}^\mathrm{hor}$</span> (<a href="../../arrays/global_tangent_spaces/#GeometricMachineLearning.StiefelLieAlgHorMatrix"><code>StiefelLieAlgHorMatrix</code></a>).</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using GeometricMachineLearning

Y = rand(StiefelManifold, 5, 3)
AdamCache(Y).B₁

# output

5×5 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:
 0.0  -0.0  -0.0  -0.0  -0.0
 0.0   0.0  -0.0  -0.0  -0.0
 0.0   0.0   0.0  -0.0  -0.0
 0.0   0.0   0.0   0.0   0.0
 0.0   0.0   0.0   0.0   0.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/optimizers/optimizer_caches.jl#LL13-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AbstractNeuralNetworks.update!-Tuple{Optimizer, AbstractCache, AbstractArray}" href="#AbstractNeuralNetworks.update!-Tuple{Optimizer, AbstractCache, AbstractArray}"><code>AbstractNeuralNetworks.update!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">update!(o, cache, B)</code></pre><p>Update the <code>cache</code> and output a final velocity that is stored in <code>B</code>.</p><p>Note that <span>$B\in\mathfrak{g}^\mathrm{hor}$</span> in general.</p><p>In the manifold case the final velocity is the input to a retraction.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/optimizers/optimizer.jl#LL73-L81">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[43]</dt><dd><div>I. Goodfellow, Y. Bengio and A. Courville. <em>Deep learning</em> (MIT press, Cambridge, MA, 2016).</div></dd></dl></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>In the case of the <a href="#The-Gradient-Optimizer">gradient optimizer</a> this cache is trivial.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>In the manifold case the expression <span>$-\eta\cdot\mathrm{gradient}$</span> is an element of the <a href="../../arrays/global_tangent_spaces/#Global-Tangent-Spaces">global tangent space</a> <span>$\mathfrak{g}^\mathrm{hor}$</span> and a retraction maps from <span>$\mathfrak{g}^\mathrm{hor}$</span>. We then still have to compose it with the <a href="../manifold_related/parallel_transport/#Parallel-Transport">updated global section</a> <span>$\Lambda^{(t)}$</span>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../manifold_related/parallel_transport/">« Parallel Transport</a><a class="docs-footer-nextpage" href="../bfgs_optimizer/">BFGS Optimizer »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Friday 15 November 2024 14:51">Friday 15 November 2024</span>. Using Julia version 1.11.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
