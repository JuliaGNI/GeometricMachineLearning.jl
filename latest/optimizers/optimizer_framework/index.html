<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optimizers · GeometricMachineLearning.jl</title><meta name="title" content="Optimizers · GeometricMachineLearning.jl"/><meta property="og:title" content="Optimizers · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Optimizers · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/optimizer_framework/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/optimizer_framework/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/optimizer_framework/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li class="is-active"><a class="tocitem" href>Optimizers</a><ul class="internal"><li><a class="tocitem" href="#Generalization-to-Homogeneous-Spaces"><span>Generalization to Homogeneous Spaces</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Optimizer</a></li><li class="is-active"><a href>Optimizers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Optimizers</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/optimizers/optimizer_framework.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Neural-Network-Optimizers"><a class="docs-heading-anchor" href="#Neural-Network-Optimizers">Neural Network Optimizers</a><a id="Neural-Network-Optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-Network-Optimizers" title="Permalink"></a></h1><p>In this section we present the general Optimizer framework used in <code>GeometricMachineLearning</code>. For more information on the particular steps involved in this consult the documentation on the various optimizer methods such as the gradient optimizer, the momentum optimizer and the <a href="../optimizer_methods/#The-Adam-Optimizer">Adam optimizer</a>, and the documentation on <a href="../manifold_related/retractions/#Retractions">retractions</a>.</p><p>During <em>optimization</em> we aim at changing the neural network parameters in such a way to minimize the loss function. A loss function assigns a scalar value to the weights that <a href="../../structure_preservation/structure_preserving_neural_networks/#Structure-Preserving-Neural-Networks">parametrize the neural network</a>:</p><p class="math-container">\[    L: \mathbb{P}\to\mathbb{R}_{\geq0},\quad \Theta \mapsto L(\Theta),\]</p><p>where <span>$\mathbb{P}$</span> is the parameter space. We can then phrase the optimization task as: </p><div class="admonition is-info"><header class="admonition-header">Definition</header><div class="admonition-body"><p>Given a neural network <span>$\mathcal{NN}$</span> parametrized by <span>$\Theta$</span> and a loss function <span>$L:\mathbb{P}\to\mathbb{R}$</span> we call an algorithm an <strong>iterative optimizer</strong> (or simply <strong>optimizer</strong>) if it performs the following task:</p><p class="math-container">\[\Theta \leftarrow \mathtt{Optimizer}(\Theta, \text{past history}, t),\]</p><p>with the aim of decreasing the value <span>$L(\Theta)$</span> in each optimization step.</p></div></div><p>The past history of the optimization is stored in a cache (<a href="../optimizer_methods/#GeometricMachineLearning.AdamCache"><code>AdamCache</code></a>, <a href="../optimizer_methods/#GeometricMachineLearning.MomentumCache"><code>MomentumCache</code></a>, <a href="../optimizer_methods/#GeometricMachineLearning.GradientCache"><code>GradientCache</code></a>, <span>$\ldots$</span>) in <code>GeometricMachineLearning</code>.</p><p>Optimization for neural networks is (almost always) some variation on gradient descent. The most basic form of gradient descent is a discretization of the <em>gradient flow equation</em>:</p><p class="math-container">\[\dot{\Theta} = -\nabla_\Theta{}L,\]</p><p>by means of an Euler time-stepping scheme: </p><p class="math-container">\[\Theta^{t+1} = \Theta^{t} - h\nabla_{\Theta^{t}}L,\]</p><p>where <span>$\eta$</span> (the time step of the Euler scheme) is referred to as the <em>learning rate</em>. </p><p>This equation can easily be generalized to <a href="../../manifolds/manifolds/#(Matrix)-Manifolds">manifolds</a> with the following two steps:</p><ol><li>modify <span>$-\nabla_{\Theta^{t}}L\implies{}-h\mathrm{grad}_{\Theta^{t}}L,$</span> i.e. replace the Euclidean gradient by a <a href="../../manifolds/riemannian_manifolds/#The-Riemannian-Gradient">Riemannian gradient</a> and</li><li>replace addition with the <a href="../../manifolds/riemannian_manifolds/#Geodesic-Sprays-and-the-Exponential-Map">geodesic map</a>.</li></ol><p>To sum up, we then have:</p><p class="math-container">\[\Theta^{t+1} = \mathrm{geodesic}(\Theta^{t}, -h\mathrm{grad}_{\Theta^{t}}L).\]</p><p>In practice we very often do not use the geodesic map but approximations thereof. These approximations are called <a href="../manifold_related/retractions/#Retractions">retractions</a>.</p><h2 id="Generalization-to-Homogeneous-Spaces"><a class="docs-heading-anchor" href="#Generalization-to-Homogeneous-Spaces">Generalization to Homogeneous Spaces</a><a id="Generalization-to-Homogeneous-Spaces-1"></a><a class="docs-heading-anchor-permalink" href="#Generalization-to-Homogeneous-Spaces" title="Permalink"></a></h2><p>In order to generalize neural network optimizers to <a href="../../manifolds/homogeneous_spaces/#Homogeneous-Spaces">homogeneous spaces</a> we utilize their corresponding <a href="../../arrays/global_tangent_spaces/#Global-Tangent-Spaces">global tangent space representation</a> <span>$\mathfrak{g}^\mathrm{hor}$</span>. </p><p>When introducing the notion of a <a href="../../arrays/global_tangent_spaces/#Global-Tangent-Spaces">global tangent space</a> we discussed how an element of the tangent space <span>$T_Y\mathcal{M}$</span> can be represented in <span>$\mathfrak{g}^\mathrm{hor}$</span> by performing two mappings: </p><ol><li>the first one is the horizontal lift <span>$\Omega$</span> (see the docstring for <a href="../../manifolds/homogeneous_spaces/#GeometricMachineLearning.Ω-Union{Tuple{T}, Tuple{StiefelManifold{T, AT} where AT&lt;:AbstractMatrix{T}, AbstractMatrix{T}}} where T"><code>GeometricMachineLearning.Ω</code></a>) and </li><li>the second one is performing the adjoint operation<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> of <span>$\lambda(Y),$</span> the section of <span>$Y$</span>, on <span>$\Omega(\Delta).$</span> </li></ol><p>The two steps together are performed as <a href="../../arrays/global_tangent_spaces/#GeometricMachineLearning.global_rep"><code>global_rep</code></a> in <code>GeometricMachineLearning.</code> So we lift to <span>$\mathfrak{g}^\mathrm{hor}$</span>:</p><p class="math-container">\[\mathtt{global\_rep}: T_Y\mathcal{M} \to \mathfrak{g}^\mathrm{hor},\]</p><p>and then perform all the steps of the optimizer in <span>$\mathfrak{g}^\mathrm{hor}.$</span> We can visualize all the steps required in the generalization of the optimizers:</p><object type="image/svg+xml" class="display-light-only" data=../../tikz/general_optimization_with_boundary.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/general_optimization_with_boundary_dark.png></object><p>This picture summarizes all steps involved in an optimization step:</p><ol><li>map the Euclidean gradient <span>$\nabla{}L\in\mathbb{R}^{N\times{}n}$</span> that was obtained via <a href="../../pullbacks/computation_of_pullbacks/#Pullbacks-and-Automatic-Differentiation">automatic differentiation</a> to the Riemannian gradient <span>$\mathrm{grad}L\in{}T_Y\mathcal{M}$</span> with the function <a href="../../manifolds/homogeneous_spaces/#GeometricMachineLearning.rgrad-Tuple{StiefelManifold, AbstractMatrix}"><code>rgrad</code></a>,</li><li>obtain the global tangent space representation of <span>$\mathrm{grad}L$</span> in <span>$\mathfrak{g}^\mathrm{hor}$</span> with the function <a href="../../arrays/global_tangent_spaces/#GeometricMachineLearning.global_rep"><code>global_rep</code></a>,</li><li>perform an <a href="../bfgs_optimizer/#AbstractNeuralNetworks.update!-Tuple{Optimizer{&lt;:BFGSOptimizer}, BFGSCache, AbstractArray}"><code>update!</code></a>; this consists of two steps: (i) update the cache and (ii) output a <em>final velocity</em>,</li><li>use this final velocity to update the <a href="../../arrays/global_tangent_spaces/#Global-Sections">global section</a> <span>$\Lambda\in{}G,$</span></li><li>use the updated global section to update the neural network weight <span>$\in\mathcal{M}.$</span> This is done with <a href="../../arrays/global_tangent_spaces/#GeometricMachineLearning.apply_section"><code>apply_section</code></a>.</li></ol><p>The <code>cache</code> stores information about previous optimization steps and is dependent on the optimizer. Typically the cache is represented as one or more elements in <span>$\mathfrak{g}^\mathrm{hor}$</span>. Based on this the optimizer method (represented by <a href="../bfgs_optimizer/#AbstractNeuralNetworks.update!-Tuple{Optimizer{&lt;:BFGSOptimizer}, BFGSCache, AbstractArray}"><code>update!</code></a> in the figure) computes a <em>final velocity</em>. This final velocity is again an element of <span>$\mathfrak{g}^\mathrm{hor}$</span>. The particular form of the cache and the updating rule depends on which <a href="../optimizer_methods/#Standard-Neural-Network-Optimizers">optimizer method we use</a>.</p><p>The final velocity is then fed into a <a href="../manifold_related/retractions/#Retractions">retraction</a><sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>. For computational reasons we split the retraction into two steps, referred to as &quot;Retraction&quot; and <a href="../../arrays/global_tangent_spaces/#GeometricMachineLearning.apply_section"><code>apply_section</code></a> above. These two mappings together are equivalent to: </p><p class="math-container">\[\mathrm{retraction}(\Delta) = \mathrm{retraction}(\lambda(Y)B^\Delta{}E) = \lambda(Y)\mathrm{Retraction}(B^\Delta), \]</p><p>where <span>$\Delta\in{}T_\mathcal{M}$</span> and <span>$B^\Delta$</span> is its representation in <span>$\mathfrak{g}^\mathrm{hor}$</span> as <span>$B^\Delta = \lambda(Y)^{-1}\Omega(\Delta)\lambda(Y).$</span></p><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.Optimizer" href="#GeometricMachineLearning.Optimizer"><code>GeometricMachineLearning.Optimizer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Optimizer(method, cache, step, retraction)</code></pre><p>Store the <code>method</code> (e.g. <a href="../optimizer_methods/#GeometricMachineLearning.AdamOptimizer"><code>AdamOptimizer</code></a> with corresponding hyperparameters), the <code>cache</code> (e.g. <a href="../optimizer_methods/#GeometricMachineLearning.AdamCache"><code>AdamCache</code></a>), the optimization step and the retraction.</p><p>It takes as input an optimization method and the parameters of a network. </p><p>Before one can call <code>Optimizer</code> a <a href="../optimizer_methods/#GeometricMachineLearning.OptimizerMethod"><code>OptimizerMethod</code></a> that stores all the hyperparameters of the optimizer needs to be specified. </p><p><strong>Functor</strong></p><p>For an instance <code>o</code> of <code>Optimizer</code>, we can call the corresponding functor as:</p><pre><code class="language-julia hljs">o(nn, dl, batch, n_epochs, loss)</code></pre><p>The arguments are:</p><ol><li><code>nn::NeuralNetwork</code></li><li><code>dl::</code><a href="../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a></li><li><code>batch::</code><a href="../../data_loader/data_loader/#GeometricMachineLearning.Batch"><code>Batch</code></a></li><li><code>n_epochs::Integer</code></li><li><code>loss::NetworkLoss</code></li></ol><p>The last argument is optional for many neural network architectures. We have the following defaults:</p><ul><li>A <a href="../../architectures/neural_network_integrators/#GeometricMachineLearning.TransformerIntegrator"><code>TransformerIntegrator</code></a> uses <a href="../../reduced_order_modeling/losses/#GeometricMachineLearning.TransformerLoss"><code>TransformerLoss</code></a>.</li><li>A <a href="../../architectures/neural_network_integrators/#GeometricMachineLearning.NeuralNetworkIntegrator"><code>NeuralNetworkIntegrator</code></a> uses <a href="../../reduced_order_modeling/losses/#GeometricMachineLearning.FeedForwardLoss"><code>FeedForwardLoss</code></a>.</li><li>An <a href="../../reduced_order_modeling/pod_autoencoders/#GeometricMachineLearning.AutoEncoder"><code>AutoEncoder</code></a> uses <a href="../../reduced_order_modeling/losses/#GeometricMachineLearning.AutoEncoderLoss"><code>AutoEncoderLoss</code></a>.</li></ul><p>In addition there is an optional keyword argument that can be supplied to the functor:</p><ul><li><code>show_progress=true</code>: This specifies whether a progress bar should be shown during training.</li></ul><p><strong>Implementation</strong></p><p>Internally the functor for <code>Optimizer</code> calls <a href="../../arrays/global_tangent_spaces/#GeometricMachineLearning.GlobalSection"><code>GlobalSection</code></a> once at the start and then <a href="#GeometricMachineLearning.optimize_for_one_epoch!"><code>optimize_for_one_epoch!</code></a> for each epoch.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/cde4d3f7d9f54ca238af798c00c0bf9b15c47fa1/src/optimizers/optimizer.jl#LL1-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.optimize_for_one_epoch!" href="#GeometricMachineLearning.optimize_for_one_epoch!"><code>GeometricMachineLearning.optimize_for_one_epoch!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">optimize_for_one_epoch!(opt, model, ps, dl, batch, loss, λY)</code></pre><p>Sample the data contained in <code>dl</code> according to <code>batch</code> and optimize for these batches.</p><p>This step also performs automatic differentiation on <code>loss</code>.</p><p>The output of <code>optimize_for_one_epoch!</code> is the average loss over all batches of the epoch:</p><p class="math-container">\[output = \frac{1}{\mathtt{steps\_per\_epoch}}\sum_{t=1}^\mathtt{steps\_per\_epoch}\mathtt{loss}(\theta^{(t-1)}).\]</p><p>This is done because any <em>reverse differentiation</em> routine always has two outputs; for <code>Zygote</code>:</p><pre><code class="language-julia hljs">loss_value, pullback = Zygote.pullback(ps -&gt; loss(model, ps, input, output), ps)</code></pre><p>So we get the value for the loss for free whenever we compute the pullback with AD.</p><p><strong>Arguments</strong></p><p>All the arguments are mandatory (there are no defaults): </p><ol><li>an instance of <a href="#GeometricMachineLearning.Optimizer"><code>Optimizer</code></a>.</li><li>the neural network model.</li><li>the neural network parameters <code>ps</code>.</li><li>the data (i.e. an instance of <a href="../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a>).</li><li><code>batch</code>::<a href="../../data_loader/data_loader/#GeometricMachineLearning.Batch"><code>Batch</code></a>: stores <code>batch_size</code> (and optionally <code>seq_length</code> and <code>prediction_window</code>).</li><li><code>loss::NetworkLoss</code>.</li><li>the <em>section</em> <code>λY</code> of the parameters <code>ps</code>.</li></ol><p><strong>Implementation</strong></p><p>Internally this calls <a href="#GeometricMachineLearning.optimization_step!"><code>optimization_step!</code></a> for each minibatch.</p><p>The number of minibatches can be determined with <a href="../../data_loader/data_loader/#GeometricMachineLearning.number_of_batches"><code>number_of_batches</code></a>:</p><pre><code class="language-julia hljs">using GeometricMachineLearning
using GeometricMachineLearning: number_of_batches

data = [1, 2, 3, 4, 5]
batch = Batch(2)
dl = DataLoader(data; suppress_info = true)

number_of_batches(dl, batch)

# output

3</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/cde4d3f7d9f54ca238af798c00c0bf9b15c47fa1/src/data_loader/optimize.jl#LL1-L49">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.optimization_step!" href="#GeometricMachineLearning.optimization_step!"><code>GeometricMachineLearning.optimization_step!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">optimization_step!(o, λY, ps, dx)</code></pre><p>Update the weights <code>ps</code> based on an <a href="#GeometricMachineLearning.Optimizer"><code>Optimizer</code></a>, a <code>cache</code> and first-order derivatives <code>dx</code>.</p><p><code>optimization_step!</code> is calling <a href="../bfgs_optimizer/#AbstractNeuralNetworks.update!-Tuple{Optimizer{&lt;:BFGSOptimizer}, BFGSCache, AbstractArray}"><code>update!</code></a> internally.  <code>update!</code> has to be implemented for every <a href="../optimizer_methods/#GeometricMachineLearning.OptimizerMethod"><code>OptimizerMethod</code></a>.</p><p><strong>Arguments</strong></p><p>All arguments into <code>optimization_step!</code> are mandatory:</p><ol><li><code>o::</code><a href="#GeometricMachineLearning.Optimizer"><code>Optimizer</code></a>,</li><li><code>λY::NamedTuple</code>: this named tuple has the same keys as <code>ps</code>, but contains <a href="../../arrays/global_tangent_spaces/#GeometricMachineLearning.GlobalSection"><code>GlobalSection</code></a>s,</li><li><code>ps::NamedTuple</code>: the neural network parameters,</li><li><code>dx::NamedTuple</code>: the gradients stores as a NamedTuple.</li></ol><p>All the arguments are given as <code>NamedTuple</code>s  as the neural network weights are stores in that format.</p><pre><code class="language-julia hljs">using GeometricMachineLearning

l = StiefelLayer(3, 5)
ps = initialparameters(l, Float32)
cache = apply_toNT(MomentumCache, ps)
o = Optimizer(MomentumOptimizer(), cache, 0, geodesic)
λY = GlobalSection(ps)
dx = (weight = rand(Float32, 5, 3), )

# call the optimizer
optimization_step!(o, λY, ps, dx)

_test_nt(x) = typeof(x) &lt;: NamedTuple

_test_nt(λY) &amp; _test_nt(ps) &amp; _test_nt(cache) &amp; _test_nt(dx)

# output

true</code></pre><p>Note that we used <code>initialparameters</code> here instead of <code>NeuralNetwork</code> (as we do usually).</p><p><strong>Extended help</strong></p><p>The derivatives <code>dx</code> here are usually obtained via an AD routine by differentiating a loss function, i.e. <code>dx</code> is <span>$\nabla_xL$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/cde4d3f7d9f54ca238af798c00c0bf9b15c47fa1/src/optimizers/optimizer.jl#LL110-L154">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[7]</dt><dd><div>B. Brantner. <em>Generalizing Adam To Manifolds For Efficiently Training Transformers</em>, arXiv preprint arXiv:2305.16901 (2023).</div></dd></dl></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>By the <em>adjoint operation</em> <span>$\mathrm{ad}_A:\mathfrak{g}\to\mathfrak{g}$</span> for an element <span>$A\in{}G$</span> we mean <span>$B \mapsto A^{-1}BA$</span>.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>A retraction is an approximation of the <a href="../../manifolds/riemannian_manifolds/#Geodesic-Sprays-and-the-Exponential-Map">geodesic map</a></li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../structure_preservation/structure_preserving_neural_networks/">« Structure-Preserving Neural Networks</a><a class="docs-footer-nextpage" href="../manifold_related/retractions/">Retractions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Thursday 12 December 2024 13:05">Thursday 12 December 2024</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
