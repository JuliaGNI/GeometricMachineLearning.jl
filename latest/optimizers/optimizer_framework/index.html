<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optimizers · GeometricMachineLearning.jl</title><meta name="title" content="Optimizers · GeometricMachineLearning.jl"/><meta property="og:title" content="Optimizers · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Optimizers · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/optimizer_framework/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/optimizer_framework/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/optimizer_framework/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li></ul></li><li><span class="tocitem">Optimizers</span><ul><li class="is-active"><a class="tocitem" href>Optimizers</a><ul class="internal"><li><a class="tocitem" href="#Generalization-to-Homogeneous-Spaces"><span>Generalization to Homogeneous Spaces</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../manifold_related/global_sections/">Global Sections</a></li><li><a class="tocitem" href="../manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modelling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Network Losses</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_autoencoder/">PSD and Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/kolmogorov_n_width/">Kolmogorov n-width</a></li><li><a class="tocitem" href="../../reduced_order_modeling/projection_reduction_errors/">Projection and Reduction Error</a></li></ul></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">Sympnets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../library/">Library</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Optimizers</a></li><li class="is-active"><a href>Optimizers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Optimizers</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/optimizers/optimizer_framework.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Neural-Network-Optimizers"><a class="docs-heading-anchor" href="#Neural-Network-Optimizers">Neural Network Optimizers</a><a id="Neural-Network-Optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-Network-Optimizers" title="Permalink"></a></h1><p>In this section we present the general Optimizer framework used in <code>GeometricMachineLearning</code>. For more information on the particular steps involved in this consult the documentation on the various optimizer methods such as the <a href="../optimizer_methods/#The-Momentum-Optimizer">momentum optimizer</a> and the <a href="../optimizer_methods/#The-Adam-Optimizer">Adam optimizer</a>, and the documentation on <a href="../manifold_related/retractions/#Retractions">retractions</a>.</p><p>During <em>optimization</em> we aim at changing the neural network parameters in such a way to minimize the loss function. So if we express the loss function <span>$L$</span> as a function of the neural network weights <span>$\Theta$</span> in a parameter space <span>$\mathbb{P}$</span> we can phrase the task as: </p><div class="admonition is-info"><header class="admonition-header">Definition</header><div class="admonition-body"><p>Given a neural network <span>$\mathcal{NN}$</span> parametrized by <span>$\Theta$</span> and a loss function <span>$L:\mathbb{P}\to\mathbb{R}$</span> we call an algorithm an <strong>iterative optimizer</strong> (or simply <strong>optimizer</strong>) if it performs the following task:</p><p class="math-container">\[\Theta \leftarrow \mathtt{Optimizer}(\Theta, \text{past history}, t),\]</p><p>with the aim of decreasing the value <span>$L(\Theta)$</span> in each optimization step.</p></div></div><p>The past history of the optimization is stored in a cache (<a href="../../library/#GeometricMachineLearning.AdamCache"><code>AdamCache</code></a>, <a href="../../library/#GeometricMachineLearning.MomentumCache"><code>MomentumCache</code></a>, <a href="../../library/#GeometricMachineLearning.GradientCache"><code>GradientCache</code></a> etc.) in <code>GeometricMachineLearning</code>.</p><p>Optimization for neural networks is (almost always) some variation on gradient descent. The most basic form of gradient descent is a discretization of the <em>gradient flow equation</em>:</p><p class="math-container">\[\dot{\Theta} = -\nabla_\Theta{}L,\]</p><p>by means of an Euler time-stepping scheme: </p><p class="math-container">\[\Theta^{t+1} = \Theta^{t} - h\nabla_{\Theta^{t}}L,\]</p><p>where <span>$\eta$</span> (the time step of the Euler scheme) is referred to as the <em>learning rate</em>. </p><p>This equation can easily be generalized to <a href="../../manifolds/manifolds/#(Matrix)-Manifolds">manifolds</a> by replacing the <em>Euclidean gradient</em> <span>$\nabla_{\Theta^{t}}L$</span> by a <em>Riemannian gradient</em> <span>$-h\mathrm{grad}_{\Theta^{t}}L$</span> and addition by <span>$-h\nabla_{\Theta^{t}}L$</span> with the <a href="../../manifolds/riemannian_manifolds/#Geodesic-Sprays-and-the-Exponential-Map">exponential map</a> of <span>$-h\mathrm{grad}_{\theta^{t}}L$</span>. In practice we often use approximations ot the exponential map however. These are called <a href="../manifold_related/retractions/#Retractions">retractions</a>.</p><h2 id="Generalization-to-Homogeneous-Spaces"><a class="docs-heading-anchor" href="#Generalization-to-Homogeneous-Spaces">Generalization to Homogeneous Spaces</a><a id="Generalization-to-Homogeneous-Spaces-1"></a><a class="docs-heading-anchor-permalink" href="#Generalization-to-Homogeneous-Spaces" title="Permalink"></a></h2><p>In order to generalize neural network optimizers to <a href="../../manifolds/homogeneous_spaces/#Homogeneous-Spaces">homogeneous spaces</a> we utilize their corresponding <a href="../../arrays/global_tangent_spaces/#Global-Tangent-Spaces">global tangent space representation</a> <span>$\mathfrak{g}^\mathrm{hor}$</span>. </p><p>When introducing the notion of a <a href="../../arrays/global_tangent_spaces/#Global-Tangent-Spaces">global tangent space</a> we discussed how an element of the tangent space <span>$T_Y\mathcal{M}$</span> can be represented in <span>$\mathfrak{g}^\mathrm{hor}$</span> by performing two mappings: the first one is the horizontal lift <span>$\Omega$</span> (see the docstring for <a href="../../library/#GeometricMachineLearning.Ω-Union{Tuple{T}, Tuple{GrassmannManifold{T, AT} where AT&lt;:AbstractMatrix{T}, AbstractMatrix{T}}} where T"><code>GeometricMachineLearning.Ω</code></a>) and the second one is the adjoint operation<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> with the lift of <span>$Y$</span> called <span>$\lambda(Y)$</span>. We can visualize the steps required in performing this generalization:</p><object type="image/svg+xml" class="display-light-only" data=../../tikz/general_optimization_with_boundary.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/general_optimization_with_boundary_dark.png></object><p>The <code>cache</code> stores information about previous optimization steps and is dependent on the optimizer. In general the cache is represented as one or more elements in <span>$\mathfrak{g}^\mathrm{hor}$</span>. Based on this the optimizer method (represented by <a href="../../library/#AbstractNeuralNetworks.update!-Tuple{Any, Any, AbstractArray}"><code>update!</code></a> in the figure) computes a <em>final velocity</em>. This final velocity is again an element of <span>$\mathfrak{g}^\mathrm{hor}$</span>.</p><p>The final velocity is then fed into a <a href="../manifold_related/retractions/#Retractions">retraction</a><sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>. For computational reasons we split the retraction into two steps, referred to as &quot;Retraction&quot; and <a href="../../library/#GeometricMachineLearning.apply_section-Union{Tuple{AT}, Tuple{T}, Tuple{GlobalSection{T, AT}, AT}} where {T, AT&lt;:(StiefelManifold{T, AT} where AT&lt;:AbstractMatrix{T})}"><code>apply_section</code></a> above. These two mappings together are equivalent to: </p><p class="math-container">\[\mathrm{retraction}(\Delta) = \mathrm{retraction}(\lambda(Y)B^\Delta{}E) = \lambda(Y)\mathrm{Retraction}(B^\Delta), \]</p><p>where <span>$\Delta\in{}T_\mathcal{M}$</span> and <span>$B^\Delta$</span> is its representation in <span>$\mathfrak{g}^\mathrm{hor}$</span> as <span>$B^\Delta = \lambda(Y)^{-1}\Omega(\Delta)\lambda(Y).$</span></p><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.Optimizer-optimizers-optimizer_framework" href="#GeometricMachineLearning.Optimizer-optimizers-optimizer_framework"><code>GeometricMachineLearning.Optimizer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Optimizer(method, cache, step, retraction)</code></pre><p>Store the <code>method</code> (e.g. <a href="../../library/#GeometricMachineLearning.AdamOptimizer"><code>AdamOptimizer</code></a> with corresponding hyperparameters), the <code>cache</code> (e.g. <a href="../../library/#GeometricMachineLearning.AdamCache"><code>AdamCache</code></a>), the optimization step and the retraction.</p><p>It takes as input an optimization method and the parameters of a network. </p><p>For <em>technical reasons</em> we first specify an <a href="../../library/#GeometricMachineLearning.OptimizerMethod"><code>OptimizerMethod</code></a> that stores all the hyperparameters of the optimizer. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/95b5c47d70aaa2f2e08bca2cfda3a49bcc5b6424/src/optimizers/optimizer.jl#LL1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AbstractNeuralNetworks.update!-optimizers-optimizer_framework" href="#AbstractNeuralNetworks.update!-optimizers-optimizer_framework"><code>AbstractNeuralNetworks.update!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">update!(o, cache, dx::AbstractArray)</code></pre><p>Update the <code>cache</code> based on the gradient information <code>dx</code>, compute the final velocity and store it in <code>dx</code>.</p><p>The optimizer <code>o</code> is needed because some updating schemes (such as <a href="../../library/#GeometricMachineLearning.AdamOptimizer"><code>AdamOptimizer</code></a>) also need information on the current time step.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/95b5c47d70aaa2f2e08bca2cfda3a49bcc5b6424/src/optimizers/optimizer_method.jl#LL13-L19">source</a></section><section><div><pre><code class="language-julia hljs">update!(o, cache, B)</code></pre><p>First update the <code>cache</code> and then update the array <code>B</code> based on the optimizer <code>o</code>. </p><p>Note that <span>$B\in\mathfrak{g}^\mathrm{hor}$</span> in general.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/95b5c47d70aaa2f2e08bca2cfda3a49bcc5b6424/src/optimizers/optimizer.jl#LL47-L53">source</a></section><section><div><pre><code class="language-julia hljs">update!(o::Optimizer{&lt;:BFGSOptimizer}, C, B)</code></pre><p>Peform an update with the BFGS optimizer. </p><p><code>C</code> is the cache, <code>B</code> contains the gradient information (the output of <a href="../../arrays/global_tangent_spaces/#GeometricMachineLearning.global_rep-arrays-global_tangent_spaces"><code>global_rep</code></a> in general).</p><p>First we compute the <em>final velocity</em> with</p><pre><code class="language-julia hljs">    vecS = -o.method.η * C.H * vec(B)</code></pre><p>and then we update <code>H</code></p><pre><code class="language-julia hljs">    C.H .= (𝕀 - ρ * SY) * C.H * (𝕀 - ρ * SY&#39;) + ρ * vecS * vecS&#39;</code></pre><p>where <code>SY</code> is <code>vecS * Y&#39;</code> and <code>𝕀</code> is the idendity. </p><p><strong>Implementation</strong></p><p>For stability we use <code>δ</code> for computing <code>ρ</code>:</p><pre><code class="language-julia hljs">    ρ = 1. / (vecS&#39; * Y + o.method.δ)</code></pre><p>This is similar to the <a href="../../library/#GeometricMachineLearning.AdamOptimizer"><code>AdamOptimizer</code></a></p><p><strong>Extend Help</strong></p><p>If we have weights on a <a href="../../library/#GeometricMachineLearning.Manifold"><code>Manifold</code></a> than the updates are slightly more difficult. In this case the <a href="../../library/#Base.vec-Tuple{GeometricMachineLearning.AbstractTriangular}"><code>vec</code></a> operation has to be generalized to the corresponding <em>global tangent space</em>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/95b5c47d70aaa2f2e08bca2cfda3a49bcc5b6424/src/optimizers/bfgs_optimizer.jl#LL18-L48">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[50]</dt><dd><div>B. Brantner. <em>Generalizing Adam To Manifolds For Efficiently Training Transformers</em>, arXiv preprint arXiv:2305.16901 (2023).</div></dd></dl></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>By the <em>adjoint operation</em> <span>$\mathrm{ad}_A:\mathfrak{g}\to\mathfrak{g}$</span> for an element <span>$A\in{}G$</span> we mean <span>$B \mapsto A^{-1}BA$</span>.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>A retraction is an approximation of the <a href="../../manifolds/riemannian_manifolds/#Geodesic-Sprays-and-the-Exponential-Map">exponential map</a></li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../structure_preservation/volume_preservation/">« Volume-Preservation</a><a class="docs-footer-nextpage" href="../manifold_related/global_sections/">Global Sections »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Thursday 25 July 2024 16:39">Thursday 25 July 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
