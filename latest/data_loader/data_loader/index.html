<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Routines · GeometricMachineLearning.jl</title><meta name="title" content="Routines · GeometricMachineLearning.jl"/><meta property="og:title" content="Routines · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Routines · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/data_loader/data_loader/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/data_loader/data_loader/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/data_loader/data_loader/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li class="is-active"><a class="tocitem" href>Routines</a><ul class="internal"><li><a class="tocitem" href="#Drawing-Batches-with-GeometricMachineLearning"><span>Drawing Batches with <code>GeometricMachineLearning</code></span></a></li><li><a class="tocitem" href="#Sampling-from-a-tensor"><span>Sampling from a tensor</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li></ul></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Data Loader</a></li><li class="is-active"><a href>Routines</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Routines</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/data_loader/data_loader.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="The-Data-Loader"><a class="docs-heading-anchor" href="#The-Data-Loader">The Data Loader</a><a id="The-Data-Loader-1"></a><a class="docs-heading-anchor-permalink" href="#The-Data-Loader" title="Permalink"></a></h1><p>The <code>DataLoader</code> in <code>GeometricMachineLearning</code> is designed to make training convenient. </p><p>The data loader can be called with various types of arrays as input, for example a <a href="../snapshot_matrix/#Snapshot-Matrix">snapshot matrix</a>:</p><pre><code class="language-julia hljs">SnapshotMatrix = [ 1; 2;; 3; 4;; 5; 6;; 7; 8;; 9; 10 ]

dl = DataLoader(SnapshotMatrix; suppress_info = true)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DataLoader{Int64, Array{Int64, 3}, Nothing, :RegularData}([1; 2;;; 3; 4;;; 5; 6;;; 7; 8;;; 9; 10], nothing, 2, 1, 5, nothing, nothing)</code></pre><p>or a snapshot tensor: </p><pre><code class="language-julia hljs">SnapshotTensor = [ 1;  2;; 3; 4;; 5; 6;; 7; 8;; 9; 10 ;;;]

dl = DataLoader(SnapshotTensor; suppress_info = true)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DataLoader{Int64, Array{Int64, 3}, Nothing, :TimeSeries}([1 3 … 7 9; 2 4 … 8 10;;;], nothing, 2, 5, 1, nothing, nothing)</code></pre><p>Here the <code>DataLoader</code> has different properties <code>:RegularData</code> and <code>:TimeSeries</code>. This indicates that in the first case we treat all columns in the input tensor independently; this is mostly used for <a href="../../reduced_order_modeling/pod_autoencoders/#Autoencoders">autoencoder problems</a>. In the second case we have <em>time series-like data</em>, which are mostly used for <a href="../../architectures/neural_network_integrators/#Neural-Network-Integrators">integration problems</a>. As shown above the default when using a matrix is <code>:RegularData</code> and the default when using a tensor is <code>:TimeSeries</code>.</p><p>We can also treat a problem with a matrix as input as a time series-like problem by providing an additional keyword argument: <code>autoencoder=false</code>:</p><pre><code class="language-julia hljs">dl = DataLoader(SnapshotMatrix; autoencoder=false, suppress_info = true)
dl |&gt; typeof</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DataLoader{Int64, Array{Int64, 3}, Nothing, :TimeSeries}</code></pre><p>If we deal with hamiltonian systems we typically split the coordinates into a <span>$q$</span> and a <span>$p$</span> part. Such data can also be used as input arguments for <code>DataLoader</code>:</p><pre><code class="language-julia hljs">SymplecticSnapshotTensor = (q = SnapshotTensor, p = SnapshotTensor)
dl = DataLoader(SymplecticSnapshotTensor)
dl |&gt; typeof</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DataLoader{Int64, @NamedTuple{q::Array{Int64, 3}, p::Array{Int64, 3}}, Nothing, :TimeSeries}</code></pre><p>The dimension of the system is then the sum of the dimensions of the <span>$q$</span> and the <span>$p$</span> component:</p><pre><code class="language-julia hljs">dl.input_dim</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4</code></pre><h2 id="Drawing-Batches-with-GeometricMachineLearning"><a class="docs-heading-anchor" href="#Drawing-Batches-with-GeometricMachineLearning">Drawing Batches with <code>GeometricMachineLearning</code></a><a id="Drawing-Batches-with-GeometricMachineLearning-1"></a><a class="docs-heading-anchor-permalink" href="#Drawing-Batches-with-GeometricMachineLearning" title="Permalink"></a></h2><p>If we want to draw mini batches from a data set, we need to allocate an instance of <a href="#GeometricMachineLearning.Batch"><code>Batch</code></a>. If we call the corresponding functor on an instance of <a href="#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a> we get the following result<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>:</p><pre><code class="language-julia hljs">matrix_data = [ 1 2 3 4  5;
                6 7 8 9 10]
dl = DataLoader(matrix_data; autoencoder = true)

batch = Batch(3)
batches = batch(dl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([(1, 5), (1, 4), (1, 2)], [(1, 3), (1, 1)])</code></pre><p>The output of applying the batch functor is always of the form: </p><p class="math-container">\[([(b_{1,1}^t, b_{1,1}^p), (b_{1,2}^t, b_{1,2}^p), \ldots], [(b_{2,1}^t, b_{2, 1}^p), (b_{2, 2}^t, b_{2, 2}^p), \ldots], [(b_{3, 1}^t, b_{3, 2}^p), \ldots], \ldots),\]</p><p>so it is a tuple of vectors of tuples. One vector represents one batch:</p><pre><code class="language-julia hljs">for (minibatch, i) in zip(batches[1], axes(batches[1], 1))
    println(stdout, minibatch[1], &quot; = bᵗ₁&quot; * join(&#39;₀&#39; + d for d in digits(i)))
    println(stdout, minibatch[2], &quot; = bᵖ₁&quot; * join(&#39;₀&#39; + d for d in digits(i)))
    println()
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1 = bᵗ₁₁
5 = bᵖ₁₁

1 = bᵗ₁₂
4 = bᵖ₁₂

1 = bᵗ₁₃
2 = bᵖ₁₃</code></pre><p>The tuples that make up this vector always have two entries: a <em>time index</em> <span>$b^t_{1i}$</span> and a <em>parameter index</em> <span>$b^p_{1i}$</span> indicated by the superscripts <span>$t$</span> and <span>$p$</span> respectively. Because <code>dl</code> in this example is of <code>autoencoder</code> type, the time index is always one. The parameter index differs. Because the input to <code>DataLoader</code> was a <span>$2\times5$</span> matrix and we specified a batch size of three, there are two batches in total. The second batch is:</p><pre><code class="language-julia hljs">for (minibatch, i) in zip(batches[2], axes(batches[2], 1))
    println(stdout, minibatch[1], &quot; = bᵗ₁&quot; * join(&#39;₀&#39; + d for d in digits(i)))
    println(stdout, minibatch[2], &quot; = bᵖ₁&quot; * join(&#39;₀&#39; + d for d in digits(i)))
    println()
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1 = bᵗ₁₁
3 = bᵖ₁₁

1 = bᵗ₁₂
1 = bᵖ₁₂</code></pre><p>Looking at the first and the second batch together, we see that we sample with replacement, i.e. all indices <span>$b^p_{1i} = 1, 2, 3, 4, 5$</span> appear. This also works if the data are in <span>$(q, p)$</span> form:</p><pre><code class="language-julia hljs">qp_data = (q = rand(Float32, 2, 5), p = rand(Float32, 2, 5))
dl = DataLoader(qp_data; autoencoder = true)

batch = Batch(3)
batch(dl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([(1, 3), (1, 4), (1, 1)], [(1, 2), (1, 5)])</code></pre><p>In those two examples the <code>autoencoder</code> keyword was set to <code>true</code> (the default). This is why the first index was always <code>1</code>. This changes if we set <code>autoencoder = false</code>: </p><pre><code class="language-julia hljs">qp_data = (q = rand(Float32, 2, 5), p = rand(Float32, 2, 5))
dl = DataLoader(qp_data; autoencoder = false) # false is default

batch = Batch(3)
batch(dl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([(4, 1), (3, 1), (2, 1)], [(1, 1)])</code></pre><p>Specifically the sampling routines do the following: </p><ol><li><span>$\mathtt{n\_indices}\leftarrow \mathtt{n\_params}\lor\mathtt{input\_time\_steps},$</span> </li><li><span>$\mathtt{indices} \leftarrow \mathtt{shuffle}(\mathtt{1:\mathtt{n\_indices}}),$</span></li><li><span>$\mathcal{I}_i \leftarrow \mathtt{indices[(i - 1)} \cdot \mathtt{batch\_size} + 1 \mathtt{:} i \cdot \mathtt{batch\_size]}\text{ for }i=1, \ldots, (\mathrm{last} -1),$</span></li><li><span>$\mathcal{I}_\mathtt{last} \leftarrow \mathtt{indices[}(\mathtt{n\_batches} - 1) \cdot \mathtt{batch\_size} + 1\mathtt{:end]}.$</span></li></ol><p>Note that the routines are implemented in such a way that no two indices appear double, i.e. we <em>sample without replacement</em>. </p><h2 id="Sampling-from-a-tensor"><a class="docs-heading-anchor" href="#Sampling-from-a-tensor">Sampling from a tensor</a><a id="Sampling-from-a-tensor-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling-from-a-tensor" title="Permalink"></a></h2><p>We can also sample from a tensor:</p><pre><code class="language-julia hljs">qp_data = (q = rand(Float32, 2, 5, 3), p = rand(Float32, 2, 5, 3))
dl = DataLoader(qp_data)

# also specify sequence length and a prediction window here
batch = Batch(4, 2, 0)
batch(dl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([(1, 3), (3, 3), (2, 3), (4, 3)], [(1, 1), (3, 1), (2, 1), (4, 1)], [(1, 2), (3, 2), (2, 2), (4, 2)])</code></pre><p>Sampling from a tensor is done the following way (<span>$\mathcal{I}_i$</span> again denotes the batch indices for the <span>$i$</span>-th batch): </p><ol><li><span>$\mathtt{time\_indices} \leftarrow \mathtt{shuffle}(\mathtt{1:}(\mathtt{input\_time\_steps} - \mathtt{seq\_length} - \mathtt{prediction\_window}),$</span></li><li><span>$\mathtt{parameter\_indices} \leftarrow \mathtt{shuffle}(\mathtt{1:n\_params}),$</span></li><li><span>$\mathtt{complete\_indices} \leftarrow \mathtt{product}(\mathtt{time\_indices}, \mathtt{parameter\_indices}),$</span></li><li><span>$\mathcal{I}_i \leftarrow \mathtt{complete\_indices[}(i - 1) \cdot \mathtt{batch\_size} + 1 : i \cdot \mathtt{batch\_size]}\text{ for }i=1, \ldots, (\mathrm{last} -1),$</span></li><li><span>$\mathcal{I}_\mathrm{last} \leftarrow \mathtt{complete\_indices[}(\mathrm{last} - 1) \cdot \mathtt{batch\_size} + 1\mathtt{:end]}.$</span></li></ol><p>Note that we supplied two additional arguments to the <a href="#GeometricMachineLearning.Batch"><code>Batch</code></a> constructor here: <code>seq_length</code> and <code>prediction_window</code>. These two arguments specify how many time are considered in one mini batch and how long the prediction runs into the future respectively. These two numbers are explained when we talk about <a href="../../layers/attention_layer/#How-is-Structure-Preserved?">structure on product spaces</a>.</p><p>This algorithm can be visualized the following way (here <code>batch_size = 4</code>):</p><object type="image/svg+xml" class="display-light-only" data=../../tikz/tensor_sampling.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/tensor_sampling_dark.png></object><p>Here the sampling is performed over the second axis (the <em>time step dimension</em>) and the third axis (the <em>parameter dimension</em>). Whereas each block has thickness 1 in the <span>$x$</span> direction (i.e. pertains to a single parameter), its length in the <span>$y$</span> direction is <code>seq_length</code>. In total we sample as many such blocks as the batch size is big. By construction those blocks are never the same throughout a training epoch but may intersect each other!</p><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.DataLoader" href="#GeometricMachineLearning.DataLoader"><code>GeometricMachineLearning.DataLoader</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DataLoader(data)</code></pre><p>Make an instance based on a data set.</p><p>This is designed to make training convenient.</p><p><strong>Fields of <code>DataLoader</code></strong></p><p>The fields of the <code>DataLoader</code> struct are the following: </p><ul><li><code>input</code>: The input data with axes (i) system dimension, (ii) number of time steps and (iii) number of parameters.</li><li><code>output</code>: The tensor that contains the output (supervised learning) - this may be of type <code>Nothing</code> if the constructor is only called with one tensor (unsupervised learning).</li><li><code>input_dim</code>: The <em>dimension</em> of the system, i.e. what is taken as input by a regular neural network.</li><li><code>input_time_steps</code>: The length of the entire time series (length of the second axis).</li><li><code>n_params</code>: The number of parameters that are present in the data set (length of third axis)</li><li><code>output_dim</code>: The dimension of the output tensor (first axis). If <code>output</code> is of type <code>Nothing</code>, then this is also of type <code>Nothing</code>.</li><li><code>output_time_steps</code>: The size of the second axis of the output tensor. If <code>output</code> is of type <code>Nothing</code>, then this is also of type <code>Nothing</code>.</li></ul><p><strong>Implementation</strong></p><p>Even though <code>DataLoader</code> can be called with inputs of various forms, internally it always stores tensors with three axes.</p><pre><code class="language-julia hljs">using GeometricMachineLearning

data = [1 2 3; 4 5 6]
dl = DataLoader(data)
dl.input

# output

[ Info: You have provided a matrix as input. The axes will be interpreted as (i) system dimension and (ii) number of parameters.
2×1×3 Array{Int64, 3}:
[:, :, 1] =
 1
 4

[:, :, 2] =
 2
 5

[:, :, 3] =
 3
 6</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/data_loader/data_loader.jl#LL1-L46">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.Batch" href="#GeometricMachineLearning.Batch"><code>GeometricMachineLearning.Batch</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Batch</code></pre><p><code>Batch</code> is a struct whose functor acts on an instance of <code>DataLoader</code> to produce a sequence of training samples for training for one epoch. </p><p>See <a href="#GeometricMachineLearning.Batch-Tuple{Int64}"><code>Batch(::Int)</code></a> and <a href="#GeometricMachineLearning.Batch-Tuple{Int64, Int64, Int64}"><code>Batch(::Int, ::Int, ::Int)</code></a> for the different constructors.</p><p><strong>The functor</strong></p><p>An instance of <code>Batch</code> can be called on an instance of <code>DataLoader</code> to produce a sequence of samples that contain all the input data, i.e. for training for one epoch. </p><p>The output of applying <code>batch:Batch</code> to <code>dl::DataLoader</code> is a tuple of vectors of integers. Each of these vectors contains two integers: the first is the <em>time index</em> and the second one is the <em>parameter index</em>.</p><p><strong>Examples</strong></p><p>Consider the following example for drawing batches of size 2 for an instance of <code>DataLoader</code> constructed with a vector:</p><pre><code class="language-julia hljs">using GeometricMachineLearning
import Random

rng = Random.TaskLocalRNG()
Random.seed!(rng, 123)

dl = DataLoader(rand(rng, 5))
batch = Batch(2)

batch(dl)

# output

[ Info: You have provided a matrix as input. The axes will be interpreted as (i) system dimension and (ii) number of parameters.
([(1, 5), (1, 3)], [(1, 4), (1, 1)], [(1, 2)])</code></pre><p>Here the first index is always 1 (the time dimension). We get a total number of 3 batches.  The last batch is only of size 1 because we <em>sample without replacement</em>. Also see the docstring for <a href="#GeometricMachineLearning.DataLoader"><code>DataLoader(::AbstractVector)</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/data_loader/batch.jl#LL1-L39">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.Batch-Tuple{Int64}" href="#GeometricMachineLearning.Batch-Tuple{Int64}"><code>GeometricMachineLearning.Batch</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Batch(batch_size)</code></pre><p>Make an instance of <code>Batch</code> for a specific batch size.</p><p>This is, among others, used to train neural networks of <a href="../../architectures/neural_network_integrators/#GeometricMachineLearning.NeuralNetworkIntegrator"><code>NeuralNetworkIntegrator</code></a> type (as opposed to <a href="../../architectures/neural_network_integrators/#GeometricMachineLearning.TransformerIntegrator"><code>TransformerIntegrator</code></a>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/data_loader/batch.jl#LL51-L57">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.Batch-Tuple{Int64, Int64, Int64}" href="#GeometricMachineLearning.Batch-Tuple{Int64, Int64, Int64}"><code>GeometricMachineLearning.Batch</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Batch(batch_size, seq_length)</code></pre><p>Make an instance of <code>Batch</code> for a specific batch size and a sequence length.</p><p>This is used to train neural networks of <a href="../../architectures/neural_network_integrators/#GeometricMachineLearning.TransformerIntegrator"><code>TransformerIntegrator</code></a> type.</p><p>Optionally the prediction window can also be specified by calling:</p><pre><code class="language-julia hljs">using GeometricMachineLearning

batch_size = 2
seq_length = 3
prediction_window = 2

Batch(batch_size, seq_length, prediction_window)

# output

Batch{:Transformer}(2, 3, 2)</code></pre><p>Note that here the batch is of type <code>:Transformer</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/data_loader/batch.jl#LL62-L86">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.number_of_batches" href="#GeometricMachineLearning.number_of_batches"><code>GeometricMachineLearning.number_of_batches</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">number_of_batches(dl, batch)</code></pre><p>Compute the number of batches.</p><p>Here the distinction is between data that are <em>time-series like</em> and data that are <em>autoencoder like</em>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using GeometricMachineLearning
using GeometricMachineLearning: number_of_batches
import Random

Random.seed!(123)

dat = [1, 2, 3, 4, 5]
dl₁ = DataLoader(dat; autoencoder = false, suppress_info = true) # time series-like
dl₂ = DataLoader(dat; autoencoder = true, suppress_info = true) # autoencoder-like
batch = Batch(3)

nob₁ = number_of_batches(dl₁, batch)
nob₂ = number_of_batches(dl₂, batch)
println(stdout, &quot;Number of batches of dl₁: &quot;, nob₁)
println(stdout, &quot;Number of batches of dl₂: &quot;, nob₂)
println(stdout, batch(dl₁), &quot;\n&quot;, batch(dl₂))

# output

Number of batches of dl₁: 2
Number of batches of dl₂: 2
([(1, 1), (4, 1), (2, 1)], [(3, 1)])
([(1, 3), (1, 2), (1, 4)], [(1, 1), (1, 5)])</code></pre><p>Here we see that in the <em>autoencoder case</em> that last minibatch has an additional element.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/data_loader/batch.jl#LL97-L133">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.convert_input_and_batch_indices_to_array" href="#GeometricMachineLearning.convert_input_and_batch_indices_to_array"><code>GeometricMachineLearning.convert_input_and_batch_indices_to_array</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">convert_input_and_batch_indices_to_array(dl, batch, batch_indices)</code></pre><p>Assign batch data based on (i) input and (ii) batch indices.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using GeometricMachineLearning

dl = DataLoader([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]; suppress_info = true)
batch = Batch(3)
batch_indices = [(1, 1), (1, 3), (1, 5)]

GeometricMachineLearning.convert_input_and_batch_indices_to_array(dl, batch, batch_indices)

# output

1×1×3 Array{Float64, 3}:
[:, :, 1] =
 0.1

[:, :, 2] =
 0.3

[:, :, 3] =
 0.5</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/73a940767896739688149ba1591f4d53e2328d0a/src/data_loader/batch.jl#LL204-L232">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>We first demonstrate how to sample data on the example of a matrix. The case of sampling from a tensor is slightly more complicated and is explained below.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../snapshot_matrix/">« Snapshot matrix &amp; tensor</a><a class="docs-footer-nextpage" href="../../tutorials/sympnet_tutorial/">SympNets »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Friday 15 November 2024 14:51">Friday 15 November 2024</span>. Using Julia version 1.11.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
