<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Routines · GeometricMachineLearning.jl</title><meta name="title" content="Routines · GeometricMachineLearning.jl"/><meta property="og:title" content="Routines · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Routines · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/data_loader/data_loader/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/data_loader/data_loader/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/data_loader/data_loader/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li></ul></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">The Inverse Function Theorem</a></li><li><a class="tocitem" href="../../manifolds/submersion_theorem/">The Submersion Theorem</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li><li><a class="tocitem" href="../../manifolds/stiefel_manifold/">Stiefel</a></li><li><a class="tocitem" href="../../manifolds/grassmann_manifold/">Grassmann</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li></ul></li><li><span class="tocitem">Arrays</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/stiefel_lie_alg_horizontal/">Stiefel Global Tangent Space</a></li><li><a class="tocitem" href="../../arrays/grassmann_lie_alg_hor_matrix/">Grassmann Global Tangent Space</a></li></ul></li><li><span class="tocitem">Optimizer Framework</span><ul><li><a class="tocitem" href="../../Optimizer/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/general_optimization/">General Optimization</a></li></ul></li><li><span class="tocitem">Optimizer Functions</span><ul><li><a class="tocitem" href="../../optimizers/manifold_related/horizontal_lift/">Horizontal Lift</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/global_sections/">Global Sections</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/geodesic/">Geodesic Retraction</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/cayley/">Cayley Retraction</a></li><li><a class="tocitem" href="../../optimizers/adam_optimizer/">Adam Optimizer</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li class="is-active"><a class="tocitem" href>Routines</a><ul class="internal"><li><a class="tocitem" href="#Convenience-functions"><span>Convenience functions</span></a></li><li><a class="tocitem" href="#Sampling-from-a-tensor"><span>Sampling from a tensor</span></a></li></ul></li><li><a class="tocitem" href="../snapshot_matrix/">Snapshot matrix &amp; tensor</a></li></ul></li><li><span class="tocitem">Reduced Order Modelling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/autoencoder/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_autoencoder/">PSD and Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/kolmogorov_n_width/">Kolmogorov n-width</a></li><li><a class="tocitem" href="../../reduced_order_modeling/projection_reduction_errors/">Projection and Reduction Error</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">Sympnets</a></li><li><a class="tocitem" href="../../tutorials/linear_wave_equation/">Linear Wave Equation</a></li><li><a class="tocitem" href="../../tutorials/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann manifold</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../library/">Library</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Data Loader</a></li><li class="is-active"><a href>Routines</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Routines</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/data_loader/data_loader.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Data-Loader"><a class="docs-heading-anchor" href="#Data-Loader">Data Loader</a><a id="Data-Loader-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Loader" title="Permalink"></a></h1><p>Data Loader is a struct that creates an instance based on a tensor (or different input format) and is designed to make training convenient. </p><p>The fields of the struct are the following: </p><ul><li><code>data</code>: The input data with axes (i) system dimension, (ii) number of parameters and (iii) number of time steps.</li><li><code>output</code>: The tensor that contains the output (supervised learning) - this may be of type Nothing if the constructor is only called with one tensor (unsupervised learning).</li><li><code>input_dim</code>: The <em>dimension</em> of the system, i.e. what is taken as input by a regular neural network.</li><li><code>input_time_steps</code>: The length of the entire time series of the data</li><li><code>n_params</code>: The number of parameters that are present in the data set (length of third axis)</li><li><code>output_dim</code>: The dimension of the output tensor (first axis). </li><li><code>output_time_steps</code>: The size of the second axis of the output tensor (also called <code>prediction_window</code>, <code>output_time_steps=1</code> in most cases)</li></ul><p>If for the output we have a tensor whose second axis has length 1, we still store it as a tensor and not a matrix for consistency. </p><p>The data loader can be called with various types of arrays as input, for example a <a href="../snapshot_matrix/">snapshot matrix</a>:</p><pre><code class="language-julia hljs">using GeometricMachineLearning # hide

SnapshotMatrix = rand(Float32, 10, 100)

dl = DataLoader(SnapshotMatrix)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DataLoader{Float32, Matrix{Float32}, Nothing, GeometricMachineLearning.RegularData}(Float32[0.9779337 0.07830292 … 0.6618728 0.38199872; 0.09247178 0.1143558 … 0.5278867 0.16788745; … ; 0.3931883 0.73388684 … 0.63851935 0.8693268; 0.51607597 0.45531034 … 0.4806878 0.5148129], nothing, 10, nothing, 100, nothing, nothing)</code></pre><p>or a snapshot tensor: </p><pre><code class="language-julia hljs">using GeometricMachineLearning # hide

SnapshotTensor = rand(Float32, 10, 100, 5)

dl = DataLoader(SnapshotTensor)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DataLoader{Float32, Array{Float32, 3}, Nothing, GeometricMachineLearning.TimeSteps}(Float32[0.5001291 0.23855036 … 0.66662776 0.6833274; 0.764154 0.29181898 … 0.06501728 0.8571023; … ; 0.5724136 0.6293258 … 0.20578182 0.58523875; 0.57791847 0.78559095 … 0.359542 0.10863912;;; 0.64574474 0.33082682 … 0.61867523 0.071002245; 0.09926456 0.25627935 … 0.71336704 0.006448984; … ; 0.6549929 0.007443249 … 0.07244152 0.39152133; 0.71690917 0.18454391 … 0.6465034 0.31819105;;; 0.112075925 0.794732 … 0.6938315 0.7852037; 0.94227725 0.76313996 … 0.1287229 0.5957718; … ; 0.09597188 0.3632937 … 0.3986807 0.3635401; 0.72414637 0.4862957 … 0.5959367 0.99161386;;; 0.45418698 0.2141062 … 0.7948153 0.66540295; 0.49608803 0.33208847 … 0.33555382 0.49670368; … ; 0.48317862 0.9632071 … 0.85714614 0.30235445; 0.059553325 0.28946847 … 0.3951097 0.15759242;;; 0.37406796 0.8083878 … 0.15743786 0.10528195; 0.96830106 0.8393653 … 0.58008766 0.23664087; … ; 0.21937609 0.7847966 … 0.42664963 0.5533724; 0.62052584 0.20001686 … 0.30000806 0.4529788], nothing, 10, 100, 5, nothing, nothing)</code></pre><p>The constructor for the data loader, when called on a matrix, also takes an optional argument <code>autoencoder</code>. If set to true than the data loader assumes we are dealing with an <em>autoencoder problem</em> and the field <code>n_params</code> in the <code>DataLoader</code> object will be set to the number of columns of our input matrix.  If <code>autoencoder=false</code>, then the field <code>input_time_steps</code> of the <code>DataLoader</code> object will be set to the <em>number of columns minus 1</em>. This is because in this case the data are used to train a neural network integrator and we need to leave at least one time step after the last one free in order to have something that we can compare the prediction to.  So we have that for an input of form <span>$(z^{(0)}, \ldots, z^{(T)})$</span> <code>input_time_steps</code> is <span>$T$</span>. </p><pre><code class="language-julia hljs">SnapshotMatrix = rand(Float32, 10, 100)

dl = DataLoader(SnapshotMatrix; autoencoder=false)
dl.input_time_steps</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">99</code></pre><p><code>DataLoader</code> can also be called with a <code>NamedTuple</code> that has <code>q</code> and <code>p</code> as keys.</p><p>In this case the field <code>input_dim</code> of <code>DataLoader</code> is interpreted as the sum of the <span>$q$</span>- and <span>$p$</span>-dimensions, i.e. if <span>$q$</span> and <span>$p$</span> both evolve on <span>$\mathbb{R}^n$</span>, then <code>input_dim</code> is <span>$2n$</span>.</p><pre><code class="language-julia hljs">SymplecticSnapshotTensor = (q = rand(Float32, 10, 100, 5), p = rand(Float32, 10, 100, 5))

dl = DataLoader(SymplecticSnapshotTensor)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DataLoader{Float32, @NamedTuple{q::Array{Float32, 3}, p::Array{Float32, 3}}, Nothing, GeometricMachineLearning.TimeSteps}((q = Float32[0.61651754 0.33966738 … 0.6392323 0.5456334; 0.5981444 0.5970181 … 0.96797776 0.8403585; … ; 0.89983445 0.43871957 … 0.30641758 0.29141223; 0.10775399 0.79606867 … 0.89556134 0.3749941;;; 0.44778532 0.90133244 … 0.0663054 0.44321138; 0.27171272 0.12375605 … 0.6419465 0.8056908; … ; 0.73892254 0.61626023 … 0.81701225 0.50683373; 0.33217126 0.95048106 … 0.9758505 0.44567752;;; 0.846135 0.6325421 … 0.7956503 0.52732503; 0.79275244 0.33794641 … 0.25767893 0.64267665; … ; 0.46134531 0.48821455 … 0.44488078 0.5874201; 0.16276652 0.34497607 … 0.01117295 0.95569515;;; 0.14319658 0.33274692 … 0.35389006 0.58551437; 0.7524452 0.18047333 … 0.37579745 0.15737325; … ; 0.7649576 0.72973317 … 0.6063513 0.13179296; 0.83569795 0.064750314 … 0.2168014 0.27308297;;; 0.023539066 0.27077156 … 0.78382635 0.12442535; 0.23863077 0.17819452 … 0.93705857 0.9277537; … ; 0.6876977 0.782966 … 0.3280642 0.24358022; 0.9060148 0.9385935 … 0.89361024 0.026619375], p = Float32[0.113008976 0.31949276 … 0.55535525 0.6035735; 0.6554986 0.36502492 … 0.21763027 0.65031207; … ; 0.84834063 0.97556156 … 0.20635742 0.54359496; 0.6304222 0.6922173 … 0.2500555 0.96646744;;; 0.24049622 0.0101364255 … 0.18718177 0.24717116; 0.910632 0.4046411 … 0.7318425 0.622719; … ; 0.160582 0.73337346 … 0.022364676 0.35973644; 0.49063122 0.13014758 … 0.9479225 0.7332934;;; 0.84461325 0.090987206 … 0.518456 0.6738852; 0.6771102 0.12998801 … 0.26951647 0.18702221; … ; 0.5721582 0.39901507 … 0.6351893 0.034969747; 0.5741035 0.3159814 … 0.42176783 0.11013442;;; 0.19464326 0.93788725 … 0.1871345 0.036281347; 0.6817219 0.78198797 … 0.17133069 0.2902723; … ; 0.91562927 0.27746385 … 0.09422517 0.34960818; 0.07019454 0.33147 … 0.8873733 0.6058082;;; 0.18336391 0.59123695 … 0.9548497 0.5216776; 0.034000278 0.9129574 … 0.34805977 0.39054918; … ; 0.59399426 0.10354239 … 0.5028308 0.85422564; 0.2803514 0.29981053 … 0.4862396 0.38271272]), nothing, 20, 99, 5, nothing, 1)</code></pre><h2 id="Convenience-functions"><a class="docs-heading-anchor" href="#Convenience-functions">Convenience functions</a><a id="Convenience-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Convenience-functions" title="Permalink"></a></h2><p><code>Batch</code> is a struct with an associated functor that acts on an instance of <code>DataLoader</code>. </p><p>The constructor of <code>Batch</code> takes <code>batch_size</code> (an integer) as input argument. Optionally we can provide <code>seq_length</code> if we deal with time series data and want to draw batches of a certain <em>length</em> (i.e. a range contained in the second dimension of the input array).</p><p>For a snapshot matrix (or a <code>NamedTuple</code> of the form <code>(q=A, p=B)</code> where <code>A</code> and <code>B</code> are matrices), the functor for <code>Batch</code> is called on an instance of <code>DataLoader</code>. It then returns a tuple of batch indices: </p><ul><li>for <code>autoencoder=true</code>: <span>$(\mathcal{I}_1, \ldots, \mathcal{I}_{\lceil\mathtt{n\_params/batch\_size}\rceil})$</span>, where the index runs from 1 to the number of batches, which is the number of columns in the snapshot matrix divided by the batch size (rounded up).</li><li>for <code>autoencoder=false</code>: <span>$(\mathcal{I}_1, \ldots, \mathcal{I}_{\lceil\mathtt{dl.input\_time\_steps/batch\_size}\rceil})$</span>, where the index runs from 1 to the number of batches, which is the number of columns in the snapshot matrix (minus one) divided by the batch size (rounded up).</li></ul><pre><code class="language-julia hljs">matrix_data = rand(Float32, 2, 10)
dl = DataLoader(matrix_data)

batch = Batch(3)
batch(dl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([7, 6, 10], [5, 9, 2], [4, 3, 8], [1])</code></pre><p>This also works if the data are in <span>$qp$</span> form: </p><pre><code class="language-julia hljs">using GeometricMachineLearning # hide

qp_data = (q = rand(Float32, 2, 10), p = rand(Float32, 2, 10))
dl = DataLoader(qp_data; autoencoder=true)

batch = Batch(3)
batch(dl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([4, 8, 10], [3, 2, 1], [7, 6, 5], [9])</code></pre><pre><code class="language-julia hljs">using GeometricMachineLearning # hide

qp_data = (q = rand(Float32, 2, 10), p = rand(Float32, 2, 10))
dl = DataLoader(qp_data; autoencoder=false) # false is default

batch = Batch(3)
batch(dl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([4, 9, 6], [7, 3, 2], [5, 1, 8])</code></pre><p>Specifically the routines do the following: </p><ol><li><span>$\mathtt{n\_indices}\leftarrow \mathtt{n\_params}\lor\mathtt{input\_time\_steps}$</span> </li><li><span>$\mathtt{indices} \leftarrow \mathtt{shuffle}(\mathtt{1:\mathtt{n\_indices}})$</span></li><li><span>$\mathcal{I}_i \leftarrow \mathtt{indices[(i - 1)} \cdot \mathtt{batch\_size} + 1 \mathtt{:} i \cdot \mathtt{batch\_size]}\text{ for }i=1, \ldots, (\mathrm{last} -1)$</span></li><li><span>$\mathcal{I}_\mathtt{last} \leftarrow \mathtt{indices[}(\mathtt{n\_batches} - 1) \cdot \mathtt{batch\_size} + 1\mathtt{:end]}$</span></li></ol><p>Note that the routines are implemented in such a way that no two indices appear double. </p><h2 id="Sampling-from-a-tensor"><a class="docs-heading-anchor" href="#Sampling-from-a-tensor">Sampling from a tensor</a><a id="Sampling-from-a-tensor-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling-from-a-tensor" title="Permalink"></a></h2><p>We can also sample tensor data.</p><pre><code class="language-julia hljs">using GeometricMachineLearning # hide

qp_data = (q = rand(Float32, 2, 8, 3), p = rand(Float32, 2, 8, 3))
dl = DataLoader(qp_data)

# also specify sequence length here
batch = Batch(4, 5)
batch(dl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([(2, 3), (1, 3), (2, 2), (1, 2)], [(2, 1), (1, 1)])</code></pre><p>Sampling from a tensor is done the following way (<span>$\mathcal{I}_i$</span> again denotes the batch indices for the <span>$i$</span>-th batch): </p><ol><li><span>$\mathtt{time\_indices} \leftarrow \mathtt{shuffle}(\mathtt{1:}(\mathtt{input\_time\_steps} - \mathtt{seq\_length})$</span></li><li><span>$\mathtt{parameter\_indices} \leftarrow \mathtt{shuffle}(\mathtt{1:n\_params})$</span></li><li><span>$\mathtt{complete\_indices} \leftarrow \mathtt{Iterators.product}(\mathtt{time\_indices}, \mathtt{parameter\_indices}) \mathtt{|&gt; collect |&gt; vec}$</span></li><li><span>$\mathcal{I}_i \leftarrow \mathtt{complete\_indices[}(i - 1) \cdot \mathtt{batch\_size} + 1 : i \cdot \mathtt{batch\_size]}\text{ for }i=1, \ldots, (\mathrm{last} -1)$</span></li><li><span>$\mathcal{I}_\mathrm{last} \leftarrow \mathtt{complete\_indices[}(\mathrm{last} - 1) \cdot \mathtt{batch\_size} + 1\mathtt{:end]}$</span></li></ol><p>This algorithm can be visualized the following way (here <code>batch_size = 4</code>):</p><object type="image/svg+xml" class="display-light-only" data=../../tikz/tensor_sampling.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/tensor_sampling_dark.png></object><p>Here the sampling is performed over the second axis (the <em>time step dimension</em>) and the third axis (the <em>parameter dimension</em>). Whereas each block has thickness 1 in the <span>$x$</span> direction (i.e. pertains to a single parameter), its length in the <span>$y$</span> direction is <code>seq_length</code>. In total we sample as many such blocks as the batch size is big. By construction those blocks are never the same throughout a training epoch but may intersect each other!</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../layers/multihead_attention_layer/">« Multihead Attention</a><a class="docs-footer-nextpage" href="../snapshot_matrix/">Snapshot matrix &amp; tensor »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Wednesday 10 April 2024 16:20">Wednesday 10 April 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
