<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Routines · GeometricMachineLearning.jl</title><meta name="title" content="Routines · GeometricMachineLearning.jl"/><meta property="og:title" content="Routines · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Routines · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/data_loader/data_loader/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/data_loader/data_loader/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/data_loader/data_loader/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li></ul></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">The Inverse Function Theorem</a></li><li><a class="tocitem" href="../../manifolds/submersion_theorem/">The Submersion Theorem</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li><li><a class="tocitem" href="../../manifolds/stiefel_manifold/">Stiefel</a></li><li><a class="tocitem" href="../../manifolds/grassmann_manifold/">Grassmann</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li></ul></li><li><span class="tocitem">Arrays</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/stiefel_lie_alg_horizontal/">Stiefel Global Tangent Space</a></li><li><a class="tocitem" href="../../arrays/grassmann_lie_alg_hor_matrix/">Grassmann Global Tangent Space</a></li></ul></li><li><span class="tocitem">Optimizer Framework</span><ul><li><a class="tocitem" href="../../Optimizer/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/general_optimization/">General Optimization</a></li></ul></li><li><span class="tocitem">Optimizer Functions</span><ul><li><a class="tocitem" href="../../optimizers/manifold_related/horizontal_lift/">Horizontal Lift</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/global_sections/">Global Sections</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/geodesic/">Geodesic Retraction</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/cayley/">Cayley Retraction</a></li><li><a class="tocitem" href="../../optimizers/adam_optimizer/">Adam Optimizer</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li class="is-active"><a class="tocitem" href>Routines</a><ul class="internal"><li><a class="tocitem" href="#The-Batch-struct"><span>The <code>Batch</code> struct</span></a></li><li><a class="tocitem" href="#Sampling-from-a-tensor"><span>Sampling from a tensor</span></a></li></ul></li><li><a class="tocitem" href="../snapshot_matrix/">Snapshot matrix &amp; tensor</a></li></ul></li><li><span class="tocitem">Reduced Order Modelling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/autoencoder/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_autoencoder/">PSD and Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/kolmogorov_n_width/">Kolmogorov n-width</a></li><li><a class="tocitem" href="../../reduced_order_modeling/projection_reduction_errors/">Projection and Reduction Error</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">Sympnets</a></li><li><a class="tocitem" href="../../tutorials/linear_wave_equation/">Linear Wave Equation</a></li><li><a class="tocitem" href="../../tutorials/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann manifold</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../library/">Library</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Data Loader</a></li><li class="is-active"><a href>Routines</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Routines</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/data_loader/data_loader.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Data-Loader"><a class="docs-heading-anchor" href="#Data-Loader">Data Loader</a><a id="Data-Loader-1"></a><a class="docs-heading-anchor-permalink" href="#Data-Loader" title="Permalink"></a></h1><p>Data Loader is a struct that creates an instance based on a tensor (or different input format) and is designed to make training convenient. </p><h2>Constructor</h2><p>The data loader can be called with various inputs:</p><ul><li><strong>A single vector</strong>: If the data loader is called with a single vector (and no other arguments are given), then this is interpreted as an autoencoder problem, i.e. the second axis indicates parameter values and/or time steps and the system has a single degree of freedom (i.e. the system dimension is one).</li><li><strong>A single matrix</strong>: If the data loader is called with a single matrix (and no other arguments are given), then this is interpreted as an autoencoder problem, i.e. the first axis is assumed to indicate the degrees of freedom of the system and the second axis indicates parameter values and/or time steps. </li><li><strong>A single tensor</strong>: If the data loader is called with a single tensor, then this is interpreted as an <em>integration problem</em> with the second axis indicating the time step and the third one indicating the parameters.</li><li><strong>A tensor and a vector</strong>: This is a special case (MNIST classification problem). For the MNIST problem for example the input are <span>$n_p$</span> matrices (first input argument) and <span>$n_p$</span> integers (second input argument).</li><li><strong>A <code>NamedTuple</code> with fields <code>q</code> and <code>p</code></strong>: The <code>NamedTuple</code> contains (i) two matrices or (ii) two tensors. </li><li><strong>An <code>EnsembleSolution</code></strong>: The <code>EnsembleSolution</code> typically comes from <code>GeometricProblems</code>.</li></ul><p>When we supply a single vector or a single matrix as input to <code>DataLoader</code> and further set <code>autoencoder = false</code> (keyword argument), then the data are stored as an <em>integration problem</em> and the second axis is assumed to indicate time steps.</p><p>The data loader can be called with various types of arrays as input, for example a <a href="../snapshot_matrix/">snapshot matrix</a>:</p><pre><code class="language-julia hljs">SnapshotMatrix = rand(Float32, 10, 100)

dl = DataLoader(SnapshotMatrix)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DataLoader{Float32, Array{Float32, 3}, Nothing, :RegularData}(Float32[0.22285312; 0.24307334; … ; 0.6759369; 0.5519217;;; 0.20352274; 0.28339177; … ; 0.9727076; 0.7658595;;; 0.968267; 0.7961354; … ; 0.12518084; 0.76060724;;; … ;;; 0.9591879; 0.09975815; … ; 0.7455645; 0.8058178;;; 0.08448684; 0.6794417; … ; 0.67843664; 0.2641533;;; 0.7456144; 0.14503914; … ; 0.46142083; 0.4525925], nothing, 10, 1, 100, nothing, nothing)</code></pre><p>or a snapshot tensor: </p><pre><code class="language-julia hljs">SnapshotTensor = rand(Float32, 10, 100, 5)

dl = DataLoader(SnapshotTensor)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DataLoader{Float32, Array{Float32, 3}, Nothing, :TimeSeries}(Float32[0.7372873 0.5297716 … 0.30026257 0.34512484; 0.18515235 0.62638575 … 0.33359766 0.76147586; … ; 0.3167023 0.13998479 … 0.6190827 0.43921083; 0.8222499 0.9680691 … 0.3314203 0.038860917;;; 0.02583152 0.2619208 … 0.6834038 0.70247525; 0.7046808 0.49961072 … 0.78303885 0.78600234; … ; 0.009305418 0.8941876 … 0.9503127 0.24760056; 0.3240661 0.687296 … 0.9268558 0.79006183;;; 0.8235485 0.68454385 … 0.578376 0.29106402; 0.83938086 0.6670284 … 0.46135998 0.31197536; … ; 0.76915234 0.56561184 … 0.01469028 0.97325367; 0.11667901 0.3636588 … 0.24557573 0.20570123;;; 0.58321995 0.6568781 … 0.18913174 0.8958822; 0.9195444 0.70537513 … 0.10124564 0.31622803; … ; 0.019436717 0.9826356 … 0.43633884 0.0022076368; 0.25794375 0.0050197244 … 0.86520654 0.47125947;;; 0.03387499 0.5816525 … 0.72205895 0.12548625; 0.48261827 0.26107526 … 0.48584896 0.019511104; … ; 0.8815814 0.34440374 … 0.3523628 0.77762336; 0.74658465 0.07134575 … 0.39682704 0.6709598], nothing, 10, 100, 5, nothing, nothing)</code></pre><p>Here the <code>DataLoader</code> has different properties <code>:RegularData</code> and <code>:TimeSeries</code>. This indicates that in the first case we treat all columns in the input tensor independently (this is mostly used for autoencoder problems), whereas in the second case we have <em>time series-like data</em>, which are mostly used for integration problems.  We can also treat a problem with a matrix as input as a time series-like problem by providing an additional keyword argument: <code>autoencoder=false</code>:</p><pre><code class="language-julia hljs">SnapshotMatrix = rand(Float32, 10, 100)

dl = DataLoader(SnapshotMatrix; autoencoder=false)
dl.input_time_steps</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">100</code></pre><p><code>DataLoader</code> can also be called with a <code>NamedTuple</code> that has <code>q</code> and <code>p</code> as keys.</p><p>In this case the field <code>input_dim</code> of <code>DataLoader</code> is interpreted as the sum of the <span>$q$</span>- and <span>$p$</span>-dimensions, i.e. if <span>$q$</span> and <span>$p$</span> both evolve on <span>$\mathbb{R}^n$</span>, then <code>input_dim</code> is <span>$2n$</span>.</p><pre><code class="language-julia hljs">SymplecticSnapshotTensor = (q = rand(Float32, 10, 100, 5), p = rand(Float32, 10, 100, 5))

dl = DataLoader(SymplecticSnapshotTensor)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">DataLoader{Float32, @NamedTuple{q::Array{Float32, 3}, p::Array{Float32, 3}}, Nothing, :TimeSeries}((q = Float32[0.63194525 0.48117453 … 0.42750108 0.7052233; 0.29683727 0.7666657 … 0.93470424 0.65966356; … ; 0.76579696 0.77377367 … 0.58042306 0.53537774; 0.68570423 0.34214318 … 0.6517937 0.7268162;;; 0.56836736 0.32606965 … 0.40225732 0.5918043; 0.5771421 0.98329556 … 0.6552729 0.6097527; … ; 0.18114781 0.8483613 … 0.091011465 0.7567567; 0.25187266 0.6789831 … 0.64611125 0.43449926;;; 0.50327224 0.97427225 … 0.34510094 0.21600366; 0.24288523 0.8388147 … 0.37709677 0.54414016; … ; 0.75977355 0.3065204 … 0.53313583 0.47730428; 0.09038824 0.93326914 … 0.020009875 0.61045855;;; 0.24906754 0.4860415 … 0.7999047 0.0420987; 0.25769067 0.12035012 … 0.80716753 0.10296035; … ; 0.14724338 0.53261405 … 0.06628728 0.56460816; 0.43888104 0.96188486 … 0.44307256 0.8584739;;; 0.93536085 0.6686914 … 0.78082573 0.5877933; 0.17444223 0.73573333 … 0.18575513 0.91286063; … ; 0.9622858 0.8713324 … 0.5640003 0.47448134; 0.38630593 0.19958311 … 0.1085515 0.7231936], p = Float32[0.42188752 0.5476018 … 0.27022403 0.042162895; 0.94916725 0.47921842 … 0.92821187 0.10728997; … ; 0.40955967 0.47208154 … 0.43827945 0.70578724; 0.9111329 0.14379501 … 0.9947228 0.13129443;;; 0.505744 0.47397435 … 0.32164508 0.62912714; 0.80585307 0.6837222 … 0.28863782 0.8620921; … ; 0.43336862 0.36984855 … 0.032850742 0.65122026; 0.7694597 0.05448234 … 0.47539407 0.61601883;;; 0.598147 0.0035421848 … 0.1798827 0.0069471; 0.96715087 0.4422326 … 0.33253896 0.7255499; … ; 0.5145125 0.021776497 … 0.7848872 0.5934332; 0.6444791 0.34651232 … 0.8601005 0.11753702;;; 0.9952162 0.53902924 … 0.37067795 0.4272086; 0.11934304 0.80591553 … 0.7042335 0.24562919; … ; 0.44097763 0.32638228 … 0.1363405 0.5713097; 0.5102197 0.77721447 … 0.77541935 0.33808726;;; 0.990427 0.022998571 … 0.87408257 0.016198039; 0.61876625 0.97801495 … 0.45116526 0.58369213; … ; 0.5273809 0.38570845 … 0.84813946 0.4014138; 0.8545336 0.84762865 … 0.8272887 0.62784296]), nothing, 20, 100, 5, nothing, nothing)</code></pre><pre><code class="language-julia hljs">dl.input_dim</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">20</code></pre><h2 id="The-Batch-struct"><a class="docs-heading-anchor" href="#The-Batch-struct">The <code>Batch</code> struct</a><a id="The-Batch-struct-1"></a><a class="docs-heading-anchor-permalink" href="#The-Batch-struct" title="Permalink"></a></h2><p><code>Batch</code> is a struct whose functor acts on an instance of <code>DataLoader</code> to produce a sequence of training samples for training for one epoch. </p><h2>The Constructor</h2><p>The constructor for <code>Batch</code> is called with: </p><ul><li><code>batch_size::Int</code></li><li><code>seq_length::Int</code> (optional)</li><li><code>prediction_window::Int</code> (optional)</li></ul><p>The first one of these arguments is required; it indicates the number of training samples in a batch. If we deal with time series data then we can additionaly supply a <em>sequence length</em> and a <em>prediction window</em> as input arguments to <code>Batch</code>. These indicate the number of input vectors and the number of output vectors.</p><h2>The functor</h2><p>An instance of <code>Batch</code> can be called on an instance of <code>DataLoader</code> to produce a sequence of samples that contain all the input data, i.e. for training for one epoch. The output of applying <code>batch:Batch</code> to <code>dl::DataLoader</code> is a tuple of vectors of integers. Each of these vectors contains two integers: the first is the <em>time index</em> and the second one is the <em>parameter index</em>.</p><pre><code class="language-julia hljs">matrix_data = rand(Float32, 2, 10)
dl = DataLoader(matrix_data; autoencoder = true)

batch = Batch(3)
batch(dl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([(1, 7), (1, 9), (1, 5)], [(1, 10), (1, 1), (1, 8)], [(1, 6), (1, 2), (1, 3)], [(1, 4)])</code></pre><p>This also works if the data are in <span>$qp$</span> form: </p><pre><code class="language-julia hljs">qp_data = (q = rand(Float32, 2, 10), p = rand(Float32, 2, 10))
dl = DataLoader(qp_data; autoencoder = true)

batch = Batch(3)
batch(dl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([(1, 5), (1, 1), (1, 9)], [(1, 10), (1, 8), (1, 4)], [(1, 7), (1, 2), (1, 3)], [(1, 6)])</code></pre><p>In those two examples the <code>autoencoder</code> keyword was set to <code>true</code> (the default). This is why the first index was always <code>1</code>. This changes if we set <code>autoencoder = false</code>: </p><pre><code class="language-julia hljs">qp_data = (q = rand(Float32, 2, 10), p = rand(Float32, 2, 10))
dl = DataLoader(qp_data; autoencoder = false) # false is default

batch = Batch(3)
batch(dl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([(8, 1), (5, 1), (7, 1)], [(2, 1), (9, 1), (3, 1)], [(6, 1), (1, 1), (4, 1)])</code></pre><p>Specifically the routines do the following: </p><ol><li><span>$\mathtt{n\_indices}\leftarrow \mathtt{n\_params}\lor\mathtt{input\_time\_steps},$</span> </li><li><span>$\mathtt{indices} \leftarrow \mathtt{shuffle}(\mathtt{1:\mathtt{n\_indices}}),$</span></li><li><span>$\mathcal{I}_i \leftarrow \mathtt{indices[(i - 1)} \cdot \mathtt{batch\_size} + 1 \mathtt{:} i \cdot \mathtt{batch\_size]}\text{ for }i=1, \ldots, (\mathrm{last} -1),$</span></li><li><span>$\mathcal{I}_\mathtt{last} \leftarrow \mathtt{indices[}(\mathtt{n\_batches} - 1) \cdot \mathtt{batch\_size} + 1\mathtt{:end]}.$</span></li></ol><p>Note that the routines are implemented in such a way that no two indices appear double. </p><h2 id="Sampling-from-a-tensor"><a class="docs-heading-anchor" href="#Sampling-from-a-tensor">Sampling from a tensor</a><a id="Sampling-from-a-tensor-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling-from-a-tensor" title="Permalink"></a></h2><p>We can also sample tensor data.</p><pre><code class="language-julia hljs">qp_data = (q = rand(Float32, 2, 20, 3), p = rand(Float32, 2, 20, 3))
dl = DataLoader(qp_data)

# also specify sequence length here
batch = Batch(4, 5)
batch(dl)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">([(10, 3), (2, 3), (6, 3), (9, 3)], [(7, 3), (8, 3), (4, 3), (11, 3)], [(5, 3), (1, 3), (3, 3), (10, 1)], [(2, 1), (6, 1), (9, 1), (7, 1)], [(8, 1), (4, 1), (11, 1), (5, 1)], [(1, 1), (3, 1), (10, 2), (2, 2)], [(6, 2), (9, 2), (7, 2), (8, 2)], [(4, 2), (11, 2), (5, 2), (1, 2)], [(3, 2)])</code></pre><p>Sampling from a tensor is done the following way (<span>$\mathcal{I}_i$</span> again denotes the batch indices for the <span>$i$</span>-th batch): </p><ol><li><span>$\mathtt{time\_indices} \leftarrow \mathtt{shuffle}(\mathtt{1:}(\mathtt{input\_time\_steps} - \mathtt{seq\_length} - \mathtt{prediction_window}),$</span></li><li><span>$\mathtt{parameter\_indices} \leftarrow \mathtt{shuffle}(\mathtt{1:n\_params}),$</span></li><li><span>$\mathtt{complete\_indices} \leftarrow \mathtt{product}(\mathtt{time\_indices}, \mathtt{parameter\_indices}),$</span></li><li><span>$\mathcal{I}_i \leftarrow \mathtt{complete\_indices[}(i - 1) \cdot \mathtt{batch\_size} + 1 : i \cdot \mathtt{batch\_size]}\text{ for }i=1, \ldots, (\mathrm{last} -1),$</span></li><li><span>$\mathcal{I}_\mathrm{last} \leftarrow \mathtt{complete\_indices[}(\mathrm{last} - 1) \cdot \mathtt{batch\_size} + 1\mathtt{:end]}.$</span></li></ol><p>This algorithm can be visualized the following way (here <code>batch_size = 4</code>):</p><object type="image/svg+xml" class="display-light-only" data=../../tikz/tensor_sampling.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/tensor_sampling_dark.png></object><p>Here the sampling is performed over the second axis (the <em>time step dimension</em>) and the third axis (the <em>parameter dimension</em>). Whereas each block has thickness 1 in the <span>$x$</span> direction (i.e. pertains to a single parameter), its length in the <span>$y$</span> direction is <code>seq_length</code>. In total we sample as many such blocks as the batch size is big. By construction those blocks are never the same throughout a training epoch but may intersect each other!</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../layers/multihead_attention_layer/">« Multihead Attention</a><a class="docs-footer-nextpage" href="../snapshot_matrix/">Snapshot matrix &amp; tensor »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.0 on <span class="colophon-date" title="Monday 15 April 2024 15:39">Monday 15 April 2024</span>. Using Julia version 1.10.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
