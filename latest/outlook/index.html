<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Conclusion · GeometricMachineLearning.jl</title><meta name="title" content="Conclusion · GeometricMachineLearning.jl"/><meta property="og:title" content="Conclusion · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Conclusion · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/outlook/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/outlook/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/outlook/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../architectures/hamiltonian_neural_network/">Hamiltonian Neural Network</a></li><li><a class="tocitem" href="../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../architectures/symplectic_transformer/">Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../tutorials/hamiltonian_neural_network/">Hamiltonian Neural Network</a></li><li><a class="tocitem" href="../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../tutorials/matrix_softmax/">Matrix Attention</a></li><li><a class="tocitem" href="../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../tutorials/symplectic_transformer/">Symplectic Transformer</a></li><li><a class="tocitem" href="../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../references/">References</a></li><li><a class="tocitem" href="../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Conclusion</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Conclusion</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/outlook.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Conclusion"><a class="docs-heading-anchor" href="#Conclusion">Conclusion</a><a id="Conclusion-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusion" title="Permalink"></a></h1><p>In this dissertation it was shown how neural networks can be imbued with structure to improve their approximation capabilities when applied to physical systems. In the following we summarize the novelties of this work and give an outlook for how it can be expanded in the future.</p><h2 id="Reduced-Order-Modeling-as-Motivation"><a class="docs-heading-anchor" href="#Reduced-Order-Modeling-as-Motivation">Reduced Order Modeling as Motivation</a><a id="Reduced-Order-Modeling-as-Motivation-1"></a><a class="docs-heading-anchor-permalink" href="#Reduced-Order-Modeling-as-Motivation" title="Permalink"></a></h2><p>Most of the work presented in this dissertation is motivated by <a href="../reduced_order_modeling/reduced_order_modeling/#Basic-Concepts-of-Reduced-Order-Modeling">data-driven reduced order modeling</a>. This is the discipline of building low-dimensional surrogate models from data that come from high-dimensional full order models. Both the low-dimensional surrogate model and the high-dimensional full order model are described by differential equations. When we talk about <em>structure-preserving reduced order modeling</em> we mean that the equation on the low-dimensional space <a href="../reduced_order_modeling/symplectic_mor/#Hamiltonian-Model-Order-Reduction">shares features with the equation on the high-dimensional space</a>. In this work these properties were mainly for the vector field to be <a href="../structure_preservation/symplecticity/#Symplectic-Systems">symplectic</a> or <a href="../structure_preservation/volume_preservation/#Divergence-Free-Vector-Fields">divergence-free</a>. A typical reduced order modeling framework is further divided into two phases:</p><ol><li>in the <em>offline phase</em> we find the low-dimensional surrogate model (reduced representation) and</li><li>in the <em>online phase</em> we solve the equations in the reduced space.</li></ol><p>For the offline phase we proposed <a href="../architectures/symplectic_autoencoder/#The-Symplectic-Autoencoder">symplectic autoencoders</a>, and for the online phase we proposed <a href="../architectures/volume_preserving_transformer/#Volume-Preserving-Transformer">volume-preserving transformers</a> and <a href="../architectures/linear_symplectic_transformer/#Linear-Symplectic-Transformer">linear symplectic transformers</a>. In the following we summarize the three main methods that were developed in the course of this dissertation and constitute its main results: symplectic autoencoders, structure-preserving transformers and structure-preserving optimizers.</p><h2 id="Structure-Preserving-Reduced-Order-Modeling-of-Hamiltonian-Systems-The-Offline-Phase"><a class="docs-heading-anchor" href="#Structure-Preserving-Reduced-Order-Modeling-of-Hamiltonian-Systems-The-Offline-Phase">Structure-Preserving Reduced Order Modeling of Hamiltonian Systems - The Offline Phase</a><a id="Structure-Preserving-Reduced-Order-Modeling-of-Hamiltonian-Systems-The-Offline-Phase-1"></a><a class="docs-heading-anchor-permalink" href="#Structure-Preserving-Reduced-Order-Modeling-of-Hamiltonian-Systems-The-Offline-Phase" title="Permalink"></a></h2><p>A central part of this dissertation was the development of <a href="../architectures/symplectic_autoencoder/#The-Symplectic-Autoencoder">symplectic autoencoders</a> [<a href="../references/#brantner2023symplectic">3</a>]. Symplectic autoencoders build on existing approaches of <em>symplectic neural networks</em> (SympNets) [<a href="../references/#jin2020sympnets">5</a>] and <em>proper symplectic decomposition</em> (PSD) [<a href="../references/#peng2016symplectic">68</a>], both of which <em>preserve symplecticity.</em> SympNets can approximate arbitrary canonical symplectic maps in <span>$\mathbb{R}^{2n},$</span> i.e.</p><p class="math-container">\[    \mathrm{SympNet}: \mathbb{R}^{2n} \to \mathbb{R}^{2n},\]</p><p>but the input has necessarily the same dimension as the output. PSD can change dimension, i.e.<sup class="footnote-reference"><a id="citeref-0" href="#footnote-0">[0]</a></sup></p><p class="math-container">\[    \mathrm{PSD}^\mathrm{enc}: \mathbb{R}^{2N} \to \mathbb{R}^{2n},\]</p><p>but is strictly linear. Symplectic autoencoders offer a way of (i) constructing nonlinear symplectic maps that (ii) can change dimension. We used these to reduce a 400-dimensional Hamiltonian system to a two-dimensional one<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>:</p><p class="math-container">\[(\mathbb{R}^{400}, H) \xRightarrow{\mathrm{SAE}^\mathrm{enc}} (\mathbb{R}^2, \bar{H}).\]</p><p>For this case we observed speed-ups of up to a factor 1000 when a <a href="../tutorials/symplectic_autoencoder/#Symplectic-Autoencoders-and-the-Toda-Lattice">symplectic autoencoder was combined with a transformer in the online phase</a>. We also compared the symplectic autoencoder to a PSD, and showed that the PSD was unable to learn a useful two-dimensional representation.</p><p>Like PSD, symplectic autoencoders have the property that they induce a Hamiltonian system on the reduced space. This distinguishes them from &quot;weakly symplectic autoencoders&quot; [<a href="../references/#buchfink2023symplectic">70</a>, <a href="../references/#yildiz2024data">72</a>] that only approximately obtain a Hamiltonian system on a restricted domain by using a &quot;physics-informed neural networks&quot; [<a href="../references/#raissi2019physics">71</a>] approach.</p><p>We also mention that the development of symplectic autoencoders required generalizing existing neural network optimizers to manifolds<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>. This is further discussed below.</p><h2 id="Structure-Preserving-Neural-Network-Based-Integrators-The-Online-Phase"><a class="docs-heading-anchor" href="#Structure-Preserving-Neural-Network-Based-Integrators-The-Online-Phase">Structure-Preserving Neural Network-Based Integrators - The Online Phase</a><a id="Structure-Preserving-Neural-Network-Based-Integrators-The-Online-Phase-1"></a><a class="docs-heading-anchor-permalink" href="#Structure-Preserving-Neural-Network-Based-Integrators-The-Online-Phase" title="Permalink"></a></h2><p>For the online phase of reduced order modeling we developed new neural network architectures based on the <a href="../architectures/transformer/#Standard-Transformer">transformer</a> [<a href="../references/#vaswani2017attention">54</a>] which is a neural network architecture that is extensively used in other fields of neural network research such as natural language processing<sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup>. We used transformers to build an equivalent of structure-preserving multi-step methods [<a href="../references/#hairer2006geometric">1</a>].</p><p>The transformer consists of a composition of standard neural network layers and attention layers:</p><p class="math-container">\[    \mathrm{Transformer}(Z) = \mathcal{NN}_n\circ\mathrm{AttentionLayer}_n\circ\cdots\circ\mathcal{NN}_1\circ\mathrm{AttentionLayer}_1(Z),\]</p><p>where <span>$\mathcal{NN}$</span> indicates a standard neural network layer (e.g. a multilayer perceptron). The attention layer makes it possible for a transformer to process time series data by acting on a whole series of vectors at once:</p><p class="math-container">\[     \mathrm{AttentionLayer}(Z) = \mathrm{AttentionLayer}(z^{(1)}, \ldots, z^{(T)}) = [f^1(z^{(1)}, \ldots, z^{(T)}), \ldots, f^T(z^{(1)}, \ldots, z^{(T)})].\]</p><p>The attention layer thus <em>performs a preprocessing step</em> after which the standard neural network layer <span>$\mathcal{NN}$</span> is applied.</p><p>In this dissertation we presented two modifications of the standard transformer: the <a href="../architectures/volume_preserving_transformer/#Volume-Preserving-Transformer">volume-preserving transformer</a> [<a href="../references/#brantner2024volume">4</a>] and the <a href="../architectures/linear_symplectic_transformer/#Linear-Symplectic-Transformer">linear symplectic transformer</a>. In both cases we modified the attention mechanism so that it is either volume-preserving (in the first case) or symplectic (in the second case). The standard neural network layer <span>$\mathcal{NN}$</span> was replaced by a <a href="../architectures/volume_preserving_feedforward/#Volume-Preserving-Feedforward-Neural-Network">volume-preserving feedforward neural network</a> or a <a href="../architectures/sympnet/#SympNet-Architecture">symplectic neural network</a> [<a href="../references/#jin2020sympnets">5</a>] respectively.</p><p>In this dissertation we applied the volume-preserving transformer for <a href="../tutorials/volume_preserving_transformer_rigid_body/#The-Volume-Preserving-Transformer-for-the-Rigid-Body">learning the trajectory of a rigid body</a> and the linear symplectic transformer for <a href="../tutorials/linear_symplectic_transformer/#linear_symplectic_transformer_tutorial">learning the trajectory of a coupled harmonic oscillator</a>. In both cases our new transformer architecture significantly outperformed the standard transformer. The trajectory modeled with the volume-preserving transformer for instance stays very close to a submanifold which is a level set of the quadratic invariant <span>$I(z_1, z_2, z_3) = z^2_1 + z^2_2 + z^2_3.$</span> This is not the case for the standard transformer: it moves away from this submanifold after a few time steps.</p><h2 id="Structure-Preserving-Optimizers"><a class="docs-heading-anchor" href="#Structure-Preserving-Optimizers">Structure-Preserving Optimizers</a><a id="Structure-Preserving-Optimizers-1"></a><a class="docs-heading-anchor-permalink" href="#Structure-Preserving-Optimizers" title="Permalink"></a></h2><p>Training a symplectic autoencoder requires optimization on manifolds<sup class="footnote-reference"><a id="citeref-4" href="#footnote-4">[4]</a></sup>. The particular manifolds we need in this case are &quot;homogeneous spaces&quot; [<a href="../references/#frankel2011geometry">109</a>]. In this dissertation we proposed a new optimizer framework that manages to <a href="../optimizers/optimizer_framework/#Neural-Network-Optimizers">generalize existing neural network optimizers to manifolds</a>. This is done by identifying a <a href="../arrays/global_tangent_spaces/#Global-Tangent-Spaces">global tangent space representation</a> and dispenses with the need for a <em>projection step</em> as is necessary in other approaches [<a href="../references/#kong2023momentum">8</a>, <a href="../references/#li2020efficient">110</a>].</p><p>As was already observed by others [<a href="../references/#kong2023momentum">8</a>, <a href="../references/#zhang2021orthogonality">9</a>, <a href="../references/#huang2018orthogonal">111</a>] putting weights on manifolds can improve training significantly in contexts other than scientific computing. Motivated by this we show an example of training a vision transformer [<a href="../references/#dosovitskiy2020image">88</a>] on the MNIST data set [<a href="../references/#deng2012mnist">90</a>] to demonstrate the efficacy of the new optimizers. Contrary to other applications of the transformer we do not have to rely on layer normalization [<a href="../references/#xiong2020layer">112</a>] or add connections to <a href="../tutorials/mnist/mnist_tutorial/#MNIST-Tutorial">achieve convergent training for relatively big neural networks</a>. We also applied the new optimizers to a neural network that contains weights on the <a href="../manifolds/homogeneous_spaces/#The-Grassmann-Manifold">Grassmann manifold</a> to be <a href="../tutorials/grassmann_layer/#Example-of-a-Neural-Network-with-a-Grassmann-Layer">able to sample from a nonlinear space</a>.</p><h2 id="Outlook"><a class="docs-heading-anchor" href="#Outlook">Outlook</a><a id="Outlook-1"></a><a class="docs-heading-anchor-permalink" href="#Outlook" title="Permalink"></a></h2><p>We believe that the topics <em>structure-preserving autoencoders</em>, <em>structure-preserving transformers,</em> <em>structure-preserving optimizers</em> and <em>structure-preserving machine learning</em> in general offer great potential for future research. </p><p>Symplectic autoencoders could be used for model reduction of higher-dimensional systems [<a href="../references/#fresca2022pod">113</a>] as well as using them for treating systems that are more general than canonical Hamiltonian ones; these include port-Hamiltonian [<a href="../references/#van2014port">73</a>] and metriplectic [<a href="../references/#morrison1986paradigm">114</a>] systems. Structure-preserving model order reductions for such systems have been proposed [<a href="../references/#moser2023structure">79</a>, <a href="../references/#gruber2023energetically">115</a>–<a href="../references/#mamunuzzaman2022structure">117</a>] but without using neural networks. In the appendix we sketch how symplectic autoencoders could be used for <a href="../port_hamiltonian_systems/#Using-Symplectic-Autoencoders-for-Port-Hamiltonian-Systems">structure-preserving model reduction of port-Hamiltonian systems</a>.</p><p>Structure-preserving transformers have shown great potential for learning dynamical systems, but their application should not be limited to that area. Structure-preserving machine learning techniques such as <em>Hamilton Monte Carlo</em> [<a href="../references/#duane1987hybrid">118</a>] has been used in various fields such as image classification [<a href="../references/#cobb2021scaling">119</a>] and inverse problems [<a href="../references/#fichtner2018hamiltonian">120</a>] and we believe that the structure-preserving transformers introduced in this work can also find applications in these fields, by replacing the activation function in the attention layers of a vision transformer for example.</p><p>Lastly structure-preserving optimization is an exciting field, especially with regards to neural networks. The manifold optimizers introduced in this work can speed up neural network training significantly and are suitable for modern hardware (i.e. GPUs). They are however based on existing neural network optimizers such as Adam [<a href="../references/#kingma2014adam">108</a>] and thus still lack a clear geometric interpretation. By utilizing a more geometric representation, as presented in this work, we hope to be able to find a differential equation describing Adam and other neural network optimizer, perhaps through a variational principle [<a href="../references/#wibisono2016variational">121</a>, <a href="../references/#duruisseaux2022accelerated">122</a>]. One could also build on the existing optimization framework and use retractions other than the <em>geodesic retraction</em> and the <em>Cayley retraction</em> <a href="../optimizers/manifold_related/retractions/#Retractions">presented here</a>; an example would be a <em>QR-based retraction</em> [<a href="../references/#sato2019cholesky">123</a>, <a href="../references/#gao2024optimization">124</a>]. This will be left for future work.</p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-0"><a class="tag is-link" href="#citeref-0">0</a>Here we only show the <em>PSD encoder</em> <span>$\mathrm{PSD}^\mathrm{enc}.$</span> A complete reduced order modeling framework also need a decoder <span>$\mathrm{PSD}^\mathrm{dec}$</span> in addition to the encoder. When we use PSD both of these maps are linear, i.e. can be represented as <span>$(\mathrm{PSD}^\mathrm{enc})^T, \mathrm{PSD}^\mathrm{dec}\in\mathbb{R}^{2N\times{}2n}.$</span></li><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a><span>$\bar{H} = H\circ\Psi^\mathrm{dec}_{\theta_2}:\mathbb{R}^2\to\mathbb{R}$</span> here refers to the <em>induced Hamiltonian on the reduced space</em>. <em>SAE</em> is short for <em>symplectic autoencoder</em>. </li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>We also refer to optimizers that preserve manifold structure as <em>structure-preserving optimizers</em>.</li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>The T in ChatGPT [<a href="../references/#achiam2023gpt">95</a>] stands for <em>transformer</em>.</li><li class="footnote" id="footnote-4"><a class="tag is-link" href="#citeref-4">4</a>This is necessary to preserve the symplectic structure of the neural network.</li></ul></section></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.4 on <span class="colophon-date" title="Wednesday 4 June 2025 16:01">Wednesday 4 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
