<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Symplectic Model Order Reduction · GeometricMachineLearning.jl</title><meta name="title" content="Symplectic Model Order Reduction · GeometricMachineLearning.jl"/><meta property="og:title" content="Symplectic Model Order Reduction · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Symplectic Model Order Reduction · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/reduced_order_modeling/symplectic_mor/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/reduced_order_modeling/symplectic_mor/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/reduced_order_modeling/symplectic_mor/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li></ul></li><li><span class="tocitem">Optimizers</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../losses/">Losses and Errors</a></li><li class="is-active"><a class="tocitem" href>Symplectic Model Order Reduction</a><ul class="internal"><li><a class="tocitem" href="#The-Symplectic-Solution-Manifold"><span>The Symplectic Solution Manifold</span></a></li><li><a class="tocitem" href="#Proper-Symplectic-Decomposition"><span>Proper Symplectic Decomposition</span></a></li><li><a class="tocitem" href="#Symplectic-Autoencoders"><span>Symplectic Autoencoders</span></a></li><li><a class="tocitem" href="#Workflow-for-Symplectic-ROM"><span>Workflow for Symplectic ROM</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li></ul></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">Sympnets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reduced Order Modeling</a></li><li class="is-active"><a href>Symplectic Model Order Reduction</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Symplectic Model Order Reduction</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/reduced_order_modeling/symplectic_mor.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Hamiltonian-Model-Order-Reduction"><a class="docs-heading-anchor" href="#Hamiltonian-Model-Order-Reduction">Hamiltonian Model Order Reduction</a><a id="Hamiltonian-Model-Order-Reduction-1"></a><a class="docs-heading-anchor-permalink" href="#Hamiltonian-Model-Order-Reduction" title="Permalink"></a></h1><p>Hamiltonian PDEs are partial differential equations that, like its ODE counterpart, have a Hamiltonian associated with it. The linear wave equation can be written as such a Hamiltonian PDE with </p><p class="math-container">\[\mathcal{H}(q, p; \mu) := \frac{1}{2}\int_\Omega\mu^2(\partial_\xi{}q(t,\xi;\mu))^2 + p(t,\xi;\mu)^2d\xi.\]</p><p>Note that in contrast to the ODE case where the Hamiltonian is a function <span>$H(\cdot; \mu):\mathbb{R}^{2d}\to\mathbb{R},$</span> we now have a functional <span>$\mathcal{H}(\cdot, \cdot; \mu):\mathcal{C}^\infty(\mathcal{D})\times\mathcal{C}^\infty(\mathcal{D})\to\mathbb{R}.$</span> The PDE for to this Hamiltonian can be obtained similarly as in the ODE case:</p><p class="math-container">\[\partial_t{}q(t,\xi;\mu) = \frac{\delta{}\mathcal{H}}{\delta{}p} = p(t,\xi;\mu), \quad \partial_t{}p(t,\xi;\mu) = -\frac{\delta{}\mathcal{H}}{\delta{}q} = \mu^2\partial_{\xi{}\xi}q(t,\xi;\mu)\]</p><p>Neglecting the Hamiltonian structure of a system can have grave consequences on the performance of the reduced order model [<a href="../../references/#peng2016symplectic">46</a>–<a href="../../references/#buchfink2023symplectic">48</a>] which is why all algorithms in <code>GeometricMachineLearning</code> designed for producing reduced order models respect the structure of the system.</p><h2 id="The-Symplectic-Solution-Manifold"><a class="docs-heading-anchor" href="#The-Symplectic-Solution-Manifold">The Symplectic Solution Manifold</a><a id="The-Symplectic-Solution-Manifold-1"></a><a class="docs-heading-anchor-permalink" href="#The-Symplectic-Solution-Manifold" title="Permalink"></a></h2><p>As with regular parametric PDEs, we also associate a solution manifold with Hamiltonian PDEs. This is a finite-dimensional manifold, on which the dynamics can be described through a Hamiltonian ODE. The reduced system, with which we approximate this symplectic solution manifold, is a low dimensional symplectic vector space <span>$\mathbb{R}^{2n}$</span> together with a reduction <span>$\mathcal{P}$</span> and a reconstruction <span>$\mathcal{R}.$</span> If we now take an initial condition on the solution manifold <span>$\hat{u}_0\in\mathcal{M} \approx \mathcal{R}(\mathbb{R}^{2n})$</span> and project it to the reduced space with <span>$\mathcal{P}$</span>, we get <span>$u = \mathcal{P}(\hat{u}_0).$</span> We can now integrate it on the reduced space via the induced differential equation, which is of canonical Hamiltonian form, and obtain an orbit <span>$u(t)$</span> which can then be mapped back to an orbit on the solution manifold<sup class="footnote-reference"><a id="citeref-4" href="#footnote-4">[4]</a></sup> via <span>$\mathcal{R}.$</span> The resulting orbit <span>$\mathcal{R}(u(t))$</span> is ideally the unique orbit on the full order model <span>$\hat{u}(t)\in\mathcal{M}$</span>.</p><p>For Hamiltonian model order reduction we additionally require that the reduction <span>$\mathcal{P}$</span> satisfies</p><p class="math-container">\[    \nabla_z\mathcal{P}\mathbb{J}_{2N}(\nabla_z\mathcal{P})^T = \mathbb{J}_{2N} \text{ for $z\in\mathbb{R}^{2N}$}\]</p><p>and the reconstruction <span>$\mathcal{R}$</span> satisfies<sup class="footnote-reference"><a id="citeref-5" href="#footnote-5">[5]</a></sup></p><p class="math-container">\[    (\nabla_z\mathcal{R})^T\mathbb{J}_{2N}\nabla_z\mathcal{R} = \mathbb{J}_{2n}.\]</p><p>With this we have</p><div class="admonition is-info"><header class="admonition-header">Theorem</header><div class="admonition-body"><p>A Hamiltonian system on the reduced space <span>$(\mathbb{R}^{2n}, \mathbb{J}_{2n}^T)$</span> is equivalent to a <em>non-canonical symplectic system</em> on <span>$(\mathcal{M}, \mathbb{J}_{2N}^T|_\mathcal{M})$</span> where </p><p class="math-container">\[    \mathcal{M} = \mathcal{R}(\mathbb{R}^{2n})\]</p><p>is an approximation to the solution manifold.</p></div></div><p>For the proof we use the fact that <span>$\mathcal{M} = \mathcal{R}(\mathbb{R}^{2n})$</span> is a manifold whose coordinate chart is the <a href="../../manifolds/manifolds/#The-Immersion-Theorem"><em>local inverse</em> of <span>$\mathcal{R}$</span></a> which we will call <span>$\psi$</span>, i.e. around a point <span>$y\in\mathcal{M}$</span> we have <span>$\mathcal{R}\circ\psi(y) = y$</span>.</p><details class="admonition is-details"><summary class="admonition-header">Proof</summary><div class="admonition-body"><p>Note that the tangent space at <span>$y = \mathcal{R}(z)$</span> to <span>$\mathcal{M}$</span> is:</p><p class="math-container">\[    T_y\mathcal{M} = \{(\nabla_z\mathcal{R})v: v\in\mathbb{R}^{2n}\}.\]</p><p>The mapping </p><p class="math-container">\[    \mathcal{M} \to T\mathcal{M}, y \mapsto (\nabla_z\mathcal{R})\mathbb{J}_{2n}\nabla_zH\]</p><p>is clearly a vector field. We now prove that it is symplectic and equal to <span>$\mathbb{J}_{2N}\nabla_y(H\circ\psi).$</span> For this first note that we have <span>$\mathbb{I} = (\nabla_z\mathcal{R})^+\nabla_z\mathcal{R} = (\nabla_{\mathcal{R}(z)}\psi)\nabla_z\mathcal{R}$</span> and that the pseudoinverse is unique. We then have:</p><p class="math-container">\[    \mathbb{J}_{2N}\nabla_yH\circ\psi = \mathbb{J}_{2N}(\nabla_y\psi)^T\nabla_{\psi(y)}H = \mathbb{J}_{2N}\left((\nabla_{\psi(y)}\mathcal{R})^+\right)^T\nabla_{\psi(y)}H = (\nabla_{\psi(y)}\mathcal{R})\mathbb{J}_{2n}\nabla_{\psi(y)}H,\]</p><p>which proves that every Hamiltonian system on <span>$\mathbb{R}^{2n}$</span> induces a Hamiltonian system on <span>$\mathcal{M}$</span>. Conversely we built the manifold <span>$\mathcal{M}$</span> based on the flow of a high-dimensional Hamiltonian system. We can thus approximate the high-dimensional Hamiltonian system with a low-dimensional Hamiltonian system on <span>$\mathbb{R}^{2n}$</span>.</p></div></details><p>This theorem serves as the basis for Hamiltonian model order reduction via <a href="#Proper-Symplectic-Decomposition">proper symplectic decomposition</a> and <a href="#Symplectic-Autoencoders">symplectic autoencoders</a>.</p><h2 id="Proper-Symplectic-Decomposition"><a class="docs-heading-anchor" href="#Proper-Symplectic-Decomposition">Proper Symplectic Decomposition</a><a id="Proper-Symplectic-Decomposition-1"></a><a class="docs-heading-anchor-permalink" href="#Proper-Symplectic-Decomposition" title="Permalink"></a></h2><p>For proper symplectic decomposition (PSD) the reduction <span>$\mathcal{P}$</span> and the reconstruction <span>$\mathcal{R}$</span> are constrained to be linear, orthonormal and symplectic. Note that these first two properties are shared with <a href="../pod_autoencoders/#Proper-Orthogonal-Decomposition">POD</a>. The easiest way<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> to enforce this is through the so-called &quot;cotangent lift&quot; [<a href="../../references/#peng2016symplectic">46</a>]: </p><p class="math-container">\[\mathcal{R} \equiv \Psi_\mathrm{CL} = \begin{bmatrix} \Phi &amp; \mathbb{O} \\ \mathbb{O} &amp; \Phi \end{bmatrix} \text{ where $\Psi_\mathrm{CL}\in{}St(n,N)\subset\mathbb{R}^{N\times{}n}$},\]</p><p>i.e. is an element of the <a href="../../manifolds/homogeneous_spaces/#The-Stiefel-Manifold">Stiefel manifold</a>. If the <a href="../../data_loader/snapshot_matrix/#Snapshot-Matrix">snapshot matrix</a> is of the form: </p><p class="math-container">\[M = \left[\begin{array}{c:c:c:c}
\hat{q}_1(t_0) &amp;  \hat{q}_1(t_1) &amp; \quad\ldots\quad &amp; \hat{q}_1(t_f) \\
\hat{q}_2(t_0) &amp;  \hat{q}_2(t_1) &amp; \ldots &amp; \hat{q}_2(t_f) \\
\ldots &amp; \ldots &amp; \ldots &amp; \ldots \\
\hat{q}_N(t_0) &amp;  \hat{q}_N(t_1) &amp; \ldots &amp; \hat{q}_N(t_f) \\
\hat{p}_1(t_0) &amp; \hat{p}_1(t_1) &amp; \ldots &amp; \hat{p}_1(t_f) \\
\hat{p}_2(t_0) &amp;  \hat{p}_2(t_1) &amp; \ldots &amp; \hat{p}_2(t_f) \\
\ldots &amp;  \ldots &amp; \ldots &amp; \ldots \\
\hat{p}_{N}(t_0) &amp;  \hat{p}_{N}(t_1) &amp; \ldots &amp; \hat{p}_{N}(t_f) \\
\end{array}\right],\]</p><p>then <span>$\Phi_\mathrm{CL}$</span> can be computed in a very straight-forward manner:</p><div class="admonition is-info"><header class="admonition-header">Theorem</header><div class="admonition-body"><p>The ideal cotangent lift <span>$\Psi_\mathrm{CL}$</span> for the snapshot matrix of form</p><p class="math-container">\[    M = \begin{bmatrix} M_q \\ M_p \end{bmatrix},\]</p><p>i.e. the cotangent lift that minimizes the projection error, can be obtained the following way:</p><ol><li>Rearrange the rows of the matrix <span>$M$</span> such that we end up with a <span>$N\times(2\mathtt{nts})$</span> matrix: <span>$\hat{M} := [M_q, M_p]$</span>.</li><li>Perform SVD: <span>$\hat{M} = V\Sigma{}U^T.$</span> </li><li>Set <span>$\Phi\gets{}U\mathtt{[:,1:n]}.$</span></li></ol><p><span>$\Psi_\mathrm{CL}$</span> is then built based on this <span>$\Phi$</span>.</p></div></div><p>For details on the cotangent lift (and other methods for linear symplectic model reduction) consult [<a href="../../references/#peng2016symplectic">46</a>]. In <code>GeometricMachineLearning</code> we use the function <a href="#GeometricMachineLearning.solve!"><code>solve!</code></a> for this task.</p><h2 id="Symplectic-Autoencoders"><a class="docs-heading-anchor" href="#Symplectic-Autoencoders">Symplectic Autoencoders</a><a id="Symplectic-Autoencoders-1"></a><a class="docs-heading-anchor-permalink" href="#Symplectic-Autoencoders" title="Permalink"></a></h2><p>Symplectic Autoencoders are a type of neural network suitable for treating Hamiltonian parametrized PDEs with slowly decaying <a href="../reduced_order_modeling/#Kolmogorov-n-width">Kolmogorov <span>$n$</span>-width</a>. It is based on PSD and <a href="../../architectures/sympnet/#SympNet-Architecture">symplectic neural networks (SympNets)</a>.</p><p>PSD suffers from the similar shortcomings as regular POD: it is a linear map and the approximation space <span>$\tilde{\mathcal{M}}= \{\Psi^\mathrm{dec}(z_r)\in\mathbb{R}^{2N}:z_r\in\mathrm{R}^{2n}\}$</span> is therefore also linear. For problems with slowly-decaying <a href="../reduced_order_modeling/#Kolmogorov-n-width">Kolmogorov <span>$n$</span>-width</a> this leads to very poor approximations.  </p><p>In order to overcome this difficulty we use neural networks, more specifically <a href="../../architectures/sympnet/#SympNet-Architecture">SympNets</a>, together with cotangent lift-like matrices. The resulting architecture, symplectic autoencoders, are discussed in the <a href="../../architectures/symplectic_autoencoder/#The-Symplectic-Autoencoder">dedicated section on neural network architectures</a>.</p><h2 id="Workflow-for-Symplectic-ROM"><a class="docs-heading-anchor" href="#Workflow-for-Symplectic-ROM">Workflow for Symplectic ROM</a><a id="Workflow-for-Symplectic-ROM-1"></a><a class="docs-heading-anchor-permalink" href="#Workflow-for-Symplectic-ROM" title="Permalink"></a></h2><p>As with any other <a href="../reduced_order_modeling/#General-Workflow">reduced order modeling technique</a> we first discretize the PDE. This should be done with a structure-preserving scheme, thus yielding a (high-dimensional) Hamiltonian ODE as a result. Going back to the example of the linear wave equation, we can discretize this equation with finite differences to obtain a Hamiltonian ODE: </p><p class="math-container">\[\mathcal{H}_\mathrm{discr}(z(t;\mu);\mu) := \frac{1}{2}x(t;\mu)^T\begin{bmatrix}  -\mu^2D_{\xi{}\xi} &amp; \mathbb{O} \\ \mathbb{O} &amp; \mathbb{I}  \end{bmatrix} x(t;\mu).\]</p><p>In Hamiltonian reduced order modelling we try to find a symplectic submanifold in the solution space<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup> that captures the dynamics of the full system as well as possible.</p><p>Similar to the regular PDE case we again build an encoder <span>$\mathcal{P} \equiv \Psi^\mathrm{enc}$</span> and a decoder <span>$\mathcal{R} \equiv \Psi^\mathrm{dec}$</span>; but now both these mappings are required to be symplectic!</p><p>Concretely this means: </p><ol><li>The encoder is a mapping from a high-dimensional symplectic space to a low-dimensional symplectic space, i.e. <span>$\Psi^\mathrm{enc}:\mathbb{R}^{2N}\to\mathbb{R}^{2n}$</span> such that <span>$\nabla\Psi^\mathrm{enc}\mathbb{J}_{2N}(\nabla\Psi^\mathrm{enc})^T = \mathbb{J}_{2n}$</span>.</li><li>The decoder is a mapping from a low-dimensional symplectic space to a high-dimensional symplectic space, i.e. <span>$\Psi^\mathrm{dec}:\mathbb{R}^{2n}\to\mathbb{R}^{2N}$</span> such that <span>$(\nabla\Psi^\mathrm{dec})^T\mathbb{J}_{2N}\nabla\Psi^\mathrm{dec} = \mathbb{J}_{2n}$</span>.</li></ol><p>If these two maps are constrained to linear maps this amounts to PSD.</p><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.SymplecticEncoder" href="#GeometricMachineLearning.SymplecticEncoder"><code>GeometricMachineLearning.SymplecticEncoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Abstract <code>SymplecticEncoder</code> type. </p><p>See <a href="../pod_autoencoders/#GeometricMachineLearning.Encoder"><code>Encoder</code></a> for the super type and <a href="#GeometricMachineLearning.NonLinearSymplecticEncoder"><code>NonLinearSymplecticEncoder</code></a> for a derived <code>struct</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/20bddd67c359aa3a3d1417eb8d0060a91e0bb283/src/architectures/autoencoder.jl#LL40-L44">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.SymplecticDecoder" href="#GeometricMachineLearning.SymplecticDecoder"><code>GeometricMachineLearning.SymplecticDecoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Abstract <code>SymplecticDecoder</code> type.</p><p>See <a href="../pod_autoencoders/#GeometricMachineLearning.Decoder"><code>Decoder</code></a> for the super type and <a href="#GeometricMachineLearning.NonLinearSymplecticDecoder"><code>NonLinearSymplecticDecoder</code></a> for a derived <code>struct</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/20bddd67c359aa3a3d1417eb8d0060a91e0bb283/src/architectures/autoencoder.jl#LL47-L51">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.NonLinearSymplecticEncoder" href="#GeometricMachineLearning.NonLinearSymplecticEncoder"><code>GeometricMachineLearning.NonLinearSymplecticEncoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>The <code>NonLinearSymplecticEncoder</code>. </p><p>This is typically generated by calling <a href="../pod_autoencoders/#GeometricMachineLearning.encoder"><code>encoder</code></a> on an instance of <a href="../../architectures/symplectic_autoencoder/#GeometricMachineLearning.SymplecticAutoencoder"><code>SymplecticAutoencoder</code></a>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/20bddd67c359aa3a3d1417eb8d0060a91e0bb283/src/architectures/symplectic_autoencoder.jl#LL48-L52">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.NonLinearSymplecticDecoder" href="#GeometricMachineLearning.NonLinearSymplecticDecoder"><code>GeometricMachineLearning.NonLinearSymplecticDecoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>The <code>NonLinearSymplecticDecoder</code>. </p><p>This is typically generated by calling <a href="../pod_autoencoders/#GeometricMachineLearning.decoder"><code>decoder</code></a> on an instance of <a href="../../architectures/symplectic_autoencoder/#GeometricMachineLearning.SymplecticAutoencoder"><code>SymplecticAutoencoder</code></a>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/20bddd67c359aa3a3d1417eb8d0060a91e0bb283/src/architectures/symplectic_autoencoder.jl#LL62-L66">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.HRedSys" href="#GeometricMachineLearning.HRedSys"><code>GeometricMachineLearning.HRedSys</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p><code>HRedSys</code> computes the reconstructed dynamics in the full system based on the reduced one. Optionally it can be compared to the FOM solution.</p><p>It can be called using the following constructor: <code>HRedSys(N, n; encoder, decoder, v_full, f_full, v_reduced, f_reduced, parameters, tspan, tstep, ics, projection_error)</code> where </p><ul><li><code>encoder</code>: a function <span>$\mathbb{R}^{2N}\mapsto{}\mathbb{R}^{2n}$</span></li><li><code>decoder</code>: a (differentiable) function <span>$\mathbb{R}^{2n}\mapsto\mathbb{R}^{2N}$</span></li><li><code>v_full</code>: a (differentiable) mapping defined the same way as in GeometricIntegrators.</li><li><code>f_full</code>: a (differentiable) mapping defined the same way as in GeometricIntegrators.</li><li><code>v_reduced</code>: a (differentiable) mapping defined the same way as in GeometricIntegrators.</li><li><code>f_reduced</code>: a (differentiable) mapping defined the same way as in GeometricIntegrators.</li><li><code>parameters</code>: a NamedTuple that parametrizes the vector fields (the same for full<em>vector</em>field and reduced<em>vector</em>field)</li><li><code>tspan</code>: a tuple <code>(t₀, tₗ)</code> that specifies start and end point of the time interval over which integration is performed. </li><li><code>tstep</code>: the time step </li><li><code>ics</code>: the initial condition for the big system.</li><li><code>projection_error</code>: the error <span>$||M - \mathcal{R}\circ\mathcal{P}(M)||$</span> where <span>$M$</span> is the snapshot matrix; <span>$\mathcal{P}$ and $\mathcal{R}$</span> are the reduction and reconstruction respectively.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/20bddd67c359aa3a3d1417eb8d0060a91e0bb283/src/reduced_system/reduced_system.jl#LL1-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.build_v_reduced" href="#GeometricMachineLearning.build_v_reduced"><code>GeometricMachineLearning.build_v_reduced</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">build_v_reduced(v_full, f_full, decoder)</code></pre><p>Builds the reduced vector field (<span>$q$</span> part) based on the full vector field for a Hamiltonian system. </p><p>We derive the reduced vector field via the reduced Hamiltonian: <span>$\tilde{H} := H\circ\Psi^\mathrm{dec}$</span>. </p><p>We then get </p><p class="math-container">\[\mathbb{J}_{2n}\nabla_\xi\tilde{H} = \mathbb{J}_{2n}(\nabla\Psi^\mathrm{dec})^T\mathbb{J}_{2N}^T\mathbb{J}_{2N}\nabla_z{}H = \mathbb{J}_{2n}(\nabla\Psi^\mathrm{dec})^T\mathbb{J}_{2N}^T \begin{pmatrix} v(z) \\ f(z) \end{pmatrix} = \begin{pmatrix} - (\nabla_p\Psi_q)^Tf(z) + (\nabla_p\Psi_p)^Tv(z) \\ (\nabla_q\Psi_q)^Tf(z) - (\nabla_q\Psi_p)^Tv(z) \end{pmatrix}.\]</p><p><code>build_v_reduced</code> outputs the first half of the entries of this vector field.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/20bddd67c359aa3a3d1417eb8d0060a91e0bb283/src/reduced_system/reduced_system.jl#LL90-L103">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.build_f_reduced" href="#GeometricMachineLearning.build_f_reduced"><code>GeometricMachineLearning.build_f_reduced</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">build_f_reduced(v_full, f_full, decoder)</code></pre><p>Builds the reduced vector field (<span>$p$</span> part) based on the full vector field for a Hamiltonian system. </p><p><code>build_f_reduced</code> outputs the second half of the entries of this vector field.</p><p>See <a href="#GeometricMachineLearning.build_v_reduced"><code>build_v_reduced</code></a> for more information.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/20bddd67c359aa3a3d1417eb8d0060a91e0bb283/src/reduced_system/reduced_system.jl#LL118-L126">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.PSDLayer" href="#GeometricMachineLearning.PSDLayer"><code>GeometricMachineLearning.PSDLayer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>This is a PSD-like layer used for symplectic autoencoders.  One layer has the following shape:</p><p class="math-container">\[A = \begin{bmatrix} \Phi &amp; \mathbb{O} \\ \mathbb{O} &amp; \Phi \end{bmatrix},\]</p><p>where <span>$\Phi$</span> is an element of the Stiefel manifold <span>$St(n, N)$</span>.</p><p>The constructor of PSDLayer is called by <code>PSDLayer(M, N; retraction=retraction)</code>: </p><ul><li><code>M</code> is the input dimension.</li><li><code>N</code> is the output dimension. </li><li><code>retraction</code> is an instance of a struct with supertype <code>AbstractRetraction</code>. The only options at the moment are <code>Geodesic()</code> and <code>Cayley()</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/20bddd67c359aa3a3d1417eb8d0060a91e0bb283/src/layers/psd_like_layer.jl#LL1-L14">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.PSDArch" href="#GeometricMachineLearning.PSDArch"><code>GeometricMachineLearning.PSDArch</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p><strong>The architecture</strong></p><p>Proper symplectic decomposition (PSD) can be seen as a <a href="../../architectures/symplectic_autoencoder/#GeometricMachineLearning.SymplecticAutoencoder">SymplecticAutoencoder</a> for which the decoder and the encoder are both PSD-like matrices (see the docs for <a href="#GeometricMachineLearning.PSDLayer">PSDLayer</a>. </p><p><strong>Training</strong></p><p>For optimizing the parameters in this architecture no neural network training is necessary (see the docs for <a href="#GeometricMachineLearning.solve!">solve!</a>).</p><p><strong>The constructor</strong></p><p>The constructor only takes two arguments as input:</p><ul><li><code>full_dim::Integer</code></li><li><code>reduced_dim::Integer</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/20bddd67c359aa3a3d1417eb8d0060a91e0bb283/src/architectures/psd.jl#LL1-L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.solve!" href="#GeometricMachineLearning.solve!"><code>GeometricMachineLearning.solve!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p><a href="#GeometricMachineLearning.PSDArch">PSDArch</a> does not require neural network training since it is a strictly linear operation that can be solved with singular value decomposition (SVD).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/20bddd67c359aa3a3d1417eb8d0060a91e0bb283/src/architectures/psd.jl#LL45-L47">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[48]</dt><dd><div>P. Buchfink, S. Glas and B. Haasdonk. <em>Symplectic model reduction of Hamiltonian systems on nonlinear manifolds and approximation with weakly symplectic autoencoder</em>. SIAM Journal on Scientific Computing <strong>45</strong>, A289–A311 (2023).</div></dd><dt>[46]</dt><dd><div>L. Peng and K. Mohseni. <em>Symplectic model reduction of Hamiltonian systems</em>. SIAM Journal on Scientific Computing <strong>38</strong>, A1–A27 (2016).</div></dd></dl></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-4"><a class="tag is-link" href="#citeref-4">4</a>To be precise, an <em>approximation of the solution manifold</em> <span>$\mathcal{R}(\mathbb{R}^{2n})$</span>, because we are not able to find the solution manifold exactly in practice. </li><li class="footnote" id="footnote-5"><a class="tag is-link" href="#citeref-5">5</a>We should note that satisfying this <em>symplecticity condition</em> is much more important for the reconstruction than for the reduction. We are not entirely sure if the condition on the reduction is really necessary. In [<a href="../../references/#buchfink2023symplectic">48</a>] it is ignored.</li><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>The original PSD paper [<a href="../../references/#peng2016symplectic">46</a>] proposes another approach to build linear reductions and reconstructions with the so-called &quot;complex SVD.&quot; In practice this only brings minor advantages over the cotangent lift however [<a href="../../references/#tyranowski2023symplectic">47</a>].</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>The submanifold, that approximates the solution manifold, is <span>$\tilde{\mathcal{M}} = \{\Psi^\mathrm{dec}(z_r)\in\mathbb{R}^{2N}:u_r\in\mathrm{R}^{2n}\}$</span> where <span>$z_r$</span> is the reduced state of the system. By a slight abuse of notation we also denote <span>$\mathcal{M}$</span> by <span>$\mathcal{M}$</span>. </li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../losses/">« Losses and Errors</a><a class="docs-footer-nextpage" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.6.0 on <span class="colophon-date" title="Wednesday 28 August 2024 05:15">Wednesday 28 August 2024</span>. Using Julia version 1.10.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
