<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>General Framework · GeometricMachineLearning.jl</title><meta name="title" content="General Framework · GeometricMachineLearning.jl"/><meta property="og:title" content="General Framework · GeometricMachineLearning.jl"/><meta property="twitter:title" content="General Framework · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/reduced_order_modeling/reduced_order_modeling/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/reduced_order_modeling/reduced_order_modeling/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/reduced_order_modeling/reduced_order_modeling/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li class="is-active"><a class="tocitem" href>General Framework</a><ul class="internal"><li><a class="tocitem" href="#The-Solution-Manifold"><span>The Solution Manifold</span></a></li><li><a class="tocitem" href="#General-Workflow"><span>General Workflow</span></a></li><li><a class="tocitem" href="#Obtaining-the-Reduced-System-via-Galerkin-Projection"><span>Obtaining the Reduced System via Galerkin Projection</span></a></li><li><a class="tocitem" href="#Kolmogorov-n-width"><span>Kolmogorov <span>$n$</span>-width</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../losses/">Losses and Errors</a></li><li><a class="tocitem" href="../symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reduced Order Modeling</a></li><li class="is-active"><a href>General Framework</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>General Framework</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/reduced_order_modeling/reduced_order_modeling.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Basic-Concepts-of-Reduced-Order-Modeling"><a class="docs-heading-anchor" href="#Basic-Concepts-of-Reduced-Order-Modeling">Basic Concepts of Reduced Order Modeling</a><a id="Basic-Concepts-of-Reduced-Order-Modeling-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-Concepts-of-Reduced-Order-Modeling" title="Permalink"></a></h1><p>Reduced order modeling is a data-driven technique that exploits the structure of parametric partial differential equations (PPDEs) to make repeated simulations of this PPDE much cheaper.</p><p>For this consider a PPDE written in the form: <span>$F(z(\mu);\mu)=0$</span> where <span>$z(\mu)$</span> evolves on an infinite-dimensional Hilbert space <span>$V$</span>. </p><p>In modeling any PDE we have to choose a discretization (particle discretization, finite element method, ...) of <span>$V$</span> which will be denoted by <span>$V_h \simeq \mathbb{R}^N$</span>. The space <span>$V_h$</span> is not infinite-dimensional but its dimension <span>$N$</span> is still very large. Solving a discretized PDE in this space is typically very expensive. In reduced order modeling we utilize the fact that slightly different choices of parameters <span>$\mu$</span> will give qualitatively similar solutions. We can therefore perform a few simulations in the full space <span>$V_h$</span> and then make successive simulations cheaper by <em>learning</em> from the past simulations:</p><object type="image/svg+xml" class="display-light-only" data=../../tikz/reduced_order_modeling_idea.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/reduced_order_modeling_idea_dark.png></object><p>In the figure above we refer to the discretized PDE as the <em>full order model</em> (FOM) and to the cheaper representation (that we construct in a data-driven manner) as the <em>reduced order model</em> (ROM). We now introduce the <em>solution manifold</em>, which is a crucial concept in reduced order modeling.</p><h2 id="The-Solution-Manifold"><a class="docs-heading-anchor" href="#The-Solution-Manifold">The Solution Manifold</a><a id="The-Solution-Manifold-1"></a><a class="docs-heading-anchor-permalink" href="#The-Solution-Manifold" title="Permalink"></a></h2><p>To any PPDE and a certain parameter set <span>$\mathbb{P}$</span> we associate a <em>solution manifold</em>: </p><p class="math-container">\[\mathcal{M} = \{z(\mu):F(z(\mu);\mu)=0, \mu\in\mathbb{P}\}.\]</p><p>A motivation for reduced order modeling is that even though the space <span>$V_h$</span> is of very high-dimension, the solution manifold will typically be a very small space. The image here shows a two-dimensional solution manifold<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> embedded in <span>$V_h\equiv\mathbb{R}^3$</span>.</p><p><img src="../../tikz/solution_manifold_2.png" alt="A representation of a two-dimensional solution manifold embedded in three-dimensional Euclidean space."/></p><p>As an actual example of a solution manifold consider the one-dimensional wave equation [<a href="../../references/#blickhan2023registration">60</a>]: </p><p class="math-container">\[\partial_{tt}^2q(t,\omega;\mu) = \mu^2\partial_{\omega\omega}^2q(t,\omega;\mu)\text{ on }I\times\Omega,\]</p><p>where <span>$I = (0,1)$</span> and <span>$\Omega=(-1/2,1/2)$</span>. As initial condition for the first derivative we have <span>$\partial_tq(0,\omega;\mu) = -\mu\partial_\omega{}q_0(\xi;\mu)$</span> and furthermore <span>$q(t,\omega;\mu)=0$</span> on the boundary (i.e. <span>$\omega\in\{-1/2,1/2\}$</span>).</p><p>The solution manifold is a two-dimensional submanifold of an infinite-dimensional function space: </p><p class="math-container">\[\mathcal{M} = \{(t, \omega)\mapsto{}q(t,\omega;\mu)=q_0(\omega-\mu{}t;\mu):\mu\in\mathbb{P}\subset\mathbb{R}\}.\]</p><p>We can plot some of the <em>points</em> on <span>$\mathcal{M}$</span> (each curve at a specific time corresponds to one point on the solution manifold): </p><object type="image/svg+xml" class="display-light-only" data=../wave_equation_different_parameters.png></object><object type="image/svg+xml" class="display-dark-only" data=../wave_equation_different_parameters_dark.png></object><p>Here we plotted the curves for the time steps (0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0) and the parameter values (0.416, 0.508, 0.6). We see that, depending on the parameter value <span>$\mu$</span>, the wave travels at different speeds. In reduced order modeling we try to find an approximation to the solution manifolds, i.e. model the evolution of the curve in a cheap way for different parameter values <span>$\mu$</span>. Neural networks offer a way of doing so efficiently!</p><h2 id="General-Workflow"><a class="docs-heading-anchor" href="#General-Workflow">General Workflow</a><a id="General-Workflow-1"></a><a class="docs-heading-anchor-permalink" href="#General-Workflow" title="Permalink"></a></h2><p>In reduced order modeling we aim to construct an approximation to the solution manifold and that is ideally of a dimension not much greater than that of the solution manifold and then (approximately) solve the so-called <em>reduced equations</em> in the small space. Constructing this approximation to the solution manifold can be divided into three steps<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>: </p><ol><li>Discretize the PDE, i.e. find <span>$V\to{}V_h$</span>.</li><li>Solve the discretized PDE on <span>$V_h$</span> for a set of parameter instances <span>$\mu\in\mathbb{P}$</span>.</li><li>Build a reduced basis with the data obtained from having solved the discretized PDE. This step consists of finding two mappings: the <em>reduction</em> <span>$\mathcal{P}$</span> and the <em>reconstruction</em> <span>$\mathcal{R}$</span>.</li></ol><p>The third step can be done with various machine learning (ML) techniques. Traditionally the most popular of these has been <em>proper orthogonal decomposition</em> (POD), but in recent years <em>autoencoders</em> have become a widely-used alternative [<a href="../../references/#fresca2021comprehensive">61</a>, <a href="../../references/#lee2020model">62</a>].</p><p>After having obtained <span>$\mathcal{P}$</span> and <span>$\mathcal{R}$</span> we still need to solve the <em>reduced system</em>. Solving the reduced system is typically referred to as the <em>online phase</em> in reduced order modeling. This is sketched below: </p><object type="image/svg+xml" class="display-light-only" data=../../tikz/offline_online.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/offline_online_dark.png></object><p>In this figure the online phase consists of applying the mapping <span>$\mathcal{NN}$</span> in the low-dimensional space in order to predict the next time step; this can either be done with a standard integrator [<a href="../../references/#Kraus:2020:GeometricIntegrators">2</a>] or, as is indicated here, <a href="../../architectures/neural_network_integrators/#Neural-Network-Integrators">with a neural network</a>. Crucially this step can be made very cheap when compared to the full-order model<sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup>. In the following we discuss how an equation for the reduced model can be found classically, without relying on a neural network for the online phase.</p><h2 id="Obtaining-the-Reduced-System-via-Galerkin-Projection"><a class="docs-heading-anchor" href="#Obtaining-the-Reduced-System-via-Galerkin-Projection">Obtaining the Reduced System via Galerkin Projection</a><a id="Obtaining-the-Reduced-System-via-Galerkin-Projection-1"></a><a class="docs-heading-anchor-permalink" href="#Obtaining-the-Reduced-System-via-Galerkin-Projection" title="Permalink"></a></h2><p><em>Galerkin projection</em> [<a href="../../references/#gander2012euler">63</a>] offers a way of constructing an ODE on the reduced space once the reconstruction <span>$\mathcal{R}$</span> has been found. </p><div class="admonition is-info"><header class="admonition-header">Definition</header><div class="admonition-body"><p>Given a full-order model described by a differential equation <span>$\hat{F}(\cdot; \mu):V\to{}V$</span>, where <span>$V$</span> may be an infinite-dimensional Hilbert space (PDE case) or a finite-dimensional vector space <span>$\mathbb{R}^N$</span> (ODE case), and a reconstruction <span>$\mathcal{R}:\mathbb{R}^n\to{}V$</span>, we can find an equation on the reduced space <span>$\mathbb{R}^n.$</span> For this we first take as possible solutions for the equation</p><p class="math-container">\[    \hat{F}(\hat{u}(t); \mu) - \hat{u}&#39;(t) =: F(\hat{u}(t); \mu) = 0\]</p><p>the ones that are the <em>result of a reconstruction</em>:</p><p class="math-container">\[    \hat{F}(\mathcal{R}(u(t)); \mu) - d\mathcal{R}u&#39;(t) = 0,\]</p><p>where <span>$u:[0, T]\to\mathbb{R}^n$</span> is an orbit on the reduced space and <span>$d\mathcal{R}$</span> is the differential of the reconstruction; this is <span>$\nabla{}\mathcal{R}$</span> if <span>$V$</span> is finite-dimensional. Typically we test this expression with a set of basis functions or vectors <span>$\{\tilde{\psi}_1, \ldots, \tilde{\psi}_n \}$</span> and hence obtain <span>$n$</span> scalar equations:</p><p class="math-container">\[    \langle \hat{F}(\mathcal{R}(u(t)); \mu) - d\mathcal{R}u&#39;(t), \psi_i \rangle_V \text{ for $1\leq{}i\leq{}n$}.\]</p><p>Such a procedure to obtain a reduced equation is known as <strong>Galerkin projection</strong>.</p></div></div><p>We give specific examples of reduced systems obtained with a Galerkin projection when introducing <a href="../pod_autoencoders/#Proper-Orthogonal-Decomposition">proper orthogonal decomposition</a>, <a href="../pod_autoencoders/#Autoencoders">autoencoders</a>, proper symplectic decomposition and <a href="../symplectic_mor/#Symplectic-Autoencoders">symplectic auteoncoders</a>.</p><h2 id="Kolmogorov-n-width"><a class="docs-heading-anchor" href="#Kolmogorov-n-width">Kolmogorov <span>$n$</span>-width</a><a id="Kolmogorov-n-width-1"></a><a class="docs-heading-anchor-permalink" href="#Kolmogorov-n-width" title="Permalink"></a></h2><p>The Kolmogorov <span>$n$</span>-width [<a href="../../references/#arbes2023kolmogorov">64</a>] measures how well some set <span>$\mathcal{M}$</span> (typically the solution manifold) can be approximated with a linear subspace:</p><p class="math-container">\[d_n(\mathcal{M}) := \mathrm{inf}_{V_n\subset{}V;\mathrm{dim}V_n=n}\mathrm{sup}(u\in\mathcal{M})\mathrm{inf}_{v_n\in{}V_n}|| u - v_n ||_V,\]</p><p>with <span>$\mathcal{M}\subset{}V$</span> and <span>$V$</span> is a (typically infinite-dimensional) Banach space. For advection-dominated problems (among others) the <em>decay of the Kolmogorov <span>$n$</span>-width is very slow</em>, i.e. one has to pick <span>$n$</span> very high in order to obtain useful approximations (see [<a href="../../references/#greif2019decay">65</a>] and [<a href="../../references/#blickhan2023registration">60</a>]). As <a href="../pod_autoencoders/#Proper-Orthogonal-Decomposition">proper orthogonal decomposition</a> is a linear approximation to the solution manifold, this does not work very well if the decay of the Kolmogorov <span>$n$</span>-width is slow.</p><p>In order to overcome this, techniques based on neural networks [<a href="../../references/#lee2020model">62</a>] and optimal transport [<a href="../../references/#blickhan2023registration">60</a>] have been used. </p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[61]</dt><dd><div>S. Fresca, L. Dede’ and A. Manzoni. <em>A comprehensive deep learning-based approach to reduced order modeling of nonlinear time-dependent parametrized PDEs</em>. Journal of Scientific Computing <strong>87</strong>, 1–36 (2021).</div></dd><dt>[62]</dt><dd><div>K. Lee and K. T. Carlberg. <em>Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders</em>. Journal of Computational Physics <strong>404</strong>, 108973 (2020).</div></dd><dt>[60]</dt><dd><div>T. Blickhan. <em>A registration method for reduced basis problems using linear optimal transport</em>, arXiv preprint arXiv:2304.14884 (2023).</div></dd></dl></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>The systems we deal with usually have much greater dimension of course. The dimension of <span>$V_h$</span> will be in the thousands and the dimension of the solution manifold will be a few orders of magnitudes smaller. Because this cannot be easily visualized, we resort to showing a two-dimensional manifold in a three-dimensional space here. </li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>Approximating the solution manifold is referred to as the <em>offline phase</em> of reduced order modeling.</li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>Solving the reduced system is typically faster by a factor of <span>$10^2$</span> <a href="../../tutorials/symplectic_autoencoder/#The-online-stage-with-a-neural-network">or more</a>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../layers/linear_symplectic_attention/">« Linear Symplectic Attention</a><a class="docs-footer-nextpage" href="../pod_autoencoders/">POD and Autoencoders »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Thursday 12 December 2024 13:05">Thursday 12 December 2024</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
