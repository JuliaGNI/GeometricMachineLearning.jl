<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear Symplectic Transformer · GeometricMachineLearning.jl</title><meta name="title" content="Linear Symplectic Transformer · GeometricMachineLearning.jl"/><meta property="og:title" content="Linear Symplectic Transformer · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Linear Symplectic Transformer · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/architectures/linear_symplectic_transformer/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/architectures/linear_symplectic_transformer/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/architectures/linear_symplectic_transformer/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../sympnet/">SympNet</a></li><li><a class="tocitem" href="../volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li class="is-active"><a class="tocitem" href>Linear Symplectic Transformer</a><ul class="internal"><li><a class="tocitem" href="#Why-use-Transformers-for-Model-Order-Reduction"><span>Why use Transformers for Model Order Reduction</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li class="toplevel"><a class="tocitem" href="#References"><span>References</span></a></li></ul></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Architectures</a></li><li class="is-active"><a href>Linear Symplectic Transformer</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear Symplectic Transformer</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/architectures/linear_symplectic_transformer.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Linear-Symplectic-Transformer"><a class="docs-heading-anchor" href="#Linear-Symplectic-Transformer">Linear Symplectic Transformer</a><a id="Linear-Symplectic-Transformer-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Symplectic-Transformer" title="Permalink"></a></h1><p>The linear symplectic transformer consists of a combination of <a href="../../layers/linear_symplectic_attention/#Linear-Symplectic-Attention">linear symplectic attention</a> and <a href="../../layers/sympnet_gradient/#SympNet-Gradient-Layer">gradient layers</a> and is visualized below:</p><object type="image/svg+xml" class="display-light-only" data=../../tikz/linear_symplectic_transformer.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/linear_symplectic_transformer_dark.png></object><p>In this picture we also visualize the keywords <code>n_sympnet</code> and <span>$L$</span> for <a href="#GeometricMachineLearning.LinearSymplecticTransformer"><code>LinearSymplecticTransformer</code></a>.</p><p>What we discussed for the <a href="../volume_preserving_transformer/#Volume-Preserving-Transformer">volume-preserving transformer</a> also applies here: the attention mechanism acts on all the input vectors at once and is designed such that it preserves the product structure (here this is the symplectic product structure). The attention mechanism serves as a <em>preprocessing step</em> after which we apply a regular feedforward neural network; here this is a <a href="../sympnet/#SympNet-Architecture">SympNet</a>.</p><h2 id="Why-use-Transformers-for-Model-Order-Reduction"><a class="docs-heading-anchor" href="#Why-use-Transformers-for-Model-Order-Reduction">Why use Transformers for Model Order Reduction</a><a id="Why-use-Transformers-for-Model-Order-Reduction-1"></a><a class="docs-heading-anchor-permalink" href="#Why-use-Transformers-for-Model-Order-Reduction" title="Permalink"></a></h2><p>The <a href="../transformer/#Standard-Transformer">standard transformer</a>, the <a href="../volume_preserving_transformer/#Volume-Preserving-Transformer">volume-preserving transformer</a> and the linear symplectic transformer are suitable for model order reduction for a number of reasons. Besides their improved accuracy [<a href="../../references/#solera2023beta">85</a>] their ability to resolve time series data also makes it possible to deal with data that come from multiple parameters. For this consider the following two trajectories:</p><object type="image/svg+xml" class="display-light-only" data=../../tikz/multiple_parameters.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/multiple_parameters_dark.png></object><p>The trajectories come from a parameter-dependent <a href="../../manifolds/existence_and_uniqueness_theorem/#The-Existence-And-Uniqueness-Theorem">ODE</a> in two dimensions. As initial condition we take <span>$A\in\mathbb{R}^2$</span> and we look at two different parameter instances: <span>$\mu_1$</span> and <span>$\mu_2$</span>. As we can see the curves <span>$\tilde{z}_{\mu_1}$</span> and <span>$\tilde{z}_{\mu_2}$</span> both start out at <span>$A,$</span> then go into different directions but cross again at <span>$D.$</span> If we used a standard feedforward neural network to treat this system it would not be able to resolve those training data as the information would be ambiguous at points <span>$A$</span> and <span>$D,$</span> i.e. the network would not know what it should predict. If we however consider the information coming from points three points, either <span>$(A, B, D)$</span> or <span>$(A, C, D),$</span> then the network can learn to predict the next time step. We will elaborate more on this in the <a href="../../tutorials/volume_preserving_attention/#Comparing-Different-VolumePreservingAttention-Mechanisms">tutorial section</a>.</p><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.LinearSymplecticTransformer" href="#GeometricMachineLearning.LinearSymplecticTransformer"><code>GeometricMachineLearning.LinearSymplecticTransformer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">LinearSymplecticTransformer(sys_dim, seq_length)</code></pre><p>Make an instance of <code>LinearSymplecticTransformer</code> for a specific system dimension and sequence length.</p><p><strong>Arguments</strong></p><p>You can provide the additional optional keyword arguments:</p><ul><li><code>n_sympnet::Int = (2)</code>: The number of sympnet layers in the transformer.</li><li><code>upscaling_dimension::Int = 2*dim</code>: The upscaling that is done by the gradient layer. </li><li><code>L::Int = 1</code>: The number of transformer units. </li><li><code>activation = tanh</code>: The activation function for the SympNet layers. </li><li><code>init_upper::Bool=true</code>: Specifies if the first layer is a <span>$q$</span>-type layer (<code>init_upper=true</code>) or if it is a <span>$p$</span>-type layer (<code>init_upper=false</code>).</li></ul><p>The number of SympNet layers in the network is <code>2n_sympnet</code>, i.e. for <code>n_sympnet = 1</code> we have one <a href="../../layers/sympnet_gradient/#GeometricMachineLearning.GradientLayerQ"><code>GradientLayerQ</code></a> and one <a href="../../layers/sympnet_gradient/#GeometricMachineLearning.GradientLayerP"><code>GradientLayerP</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/0545a139376ef2772e4c7de3244727395aaeae32/src/architectures/linear_symplectic_transformer.jl#LL6-L21">source</a></section></article><!--<h1 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h1><div class="citation noncanonical"><dl><dt>[3]</dt><dd><div>B. Brantner and M. Kraus. <em>Symplectic autoencoders for Model Reduction of Hamiltonian Systems</em>, arXiv preprint arXiv:2312.10004 (2023).</div></dd><dt>[2]</dt><dd><div>M. Kraus. <a href="https://doi.org/10.5281/zenodo.3648325"><em>GeometricIntegrators.jl: Geometric Numerical Integration in Julia</em></a>, <a href="https://github.com/JuliaGNI/GeometricIntegrators.jl"><code>https://github.com/JuliaGNI/GeometricIntegrators.jl</code></a> (2020).</div></dd><dt>[56]</dt><dd><div>K. Feng. <em>The step-transition operators for multi-step methods of ODE&#39;s</em>. Journal of Computational Mathematics, 193–202 (1998).</div></dd><dt>[4]</dt><dd><div>B. Brantner, G. de Romemont, M. Kraus and Z. Li. <em>Volume-Preserving Transformers for Learning Time Series Data with Structure</em>, arXiv preprint arXiv:2312:11166v2 (2024).</div></dd></dl></div>--></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../volume_preserving_transformer/">« Volume-Preserving Transformer</a><a class="docs-footer-nextpage" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Thursday 13 February 2025 14:43">Thursday 13 February 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
