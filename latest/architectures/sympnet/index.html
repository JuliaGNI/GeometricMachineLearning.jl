<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>SympNet · GeometricMachineLearning.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/architectures/sympnet/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">GeometricMachineLearning.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Architectures</span><ul><li class="is-active"><a class="tocitem" href>SympNet</a><ul class="internal"><li><a class="tocitem" href="#Quick-overview-of-the-theory-of-SympNet"><span>Quick overview of the theory of SympNet</span></a></li><li><a class="tocitem" href="#SympNet-with-GeometricMachineLearning.jl"><span>SympNet with <code>GeometricMachineLearning.jl</code></span></a></li><li><a class="tocitem" href="#Examples"><span>Examples</span></a></li></ul></li></ul></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li><li><a class="tocitem" href="../../manifolds/stiefel_manifold/">Stiefel</a></li><li><a class="tocitem" href="../../manifolds/grassmann_manifold/">Grassmann</a></li></ul></li><li><span class="tocitem">Arrays</span><ul><li><a class="tocitem" href="../../arrays/stiefel_lie_alg_horizontal/">Global Tangent Space</a></li></ul></li><li><a class="tocitem" href="../../Optimizer/">Optimizer Framework</a></li><li><span class="tocitem">Optimizer Functions</span><ul><li><a class="tocitem" href="../../optimizers/manifold_related/horizontal_lift/">Horizontal Lift</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/global_sections/">Global Sections</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/geodesic/">Geodesic Retraction</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/cayley/">Cayley Retraction</a></li><li><a class="tocitem" href="../../optimizers/adam_optimizer/">Adam Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/attention_layer/">Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix</a></li></ul></li><li><span class="tocitem">Reduced Order Modelling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/autoencoder/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_autoencoder/">PSD and Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/kolmogorov_n_width/">Kolmogorov n-width</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/mnist_tutorial/">MNIST</a></li></ul></li><li><a class="tocitem" href="../../library/">Library</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Architectures</a></li><li class="is-active"><a href>SympNet</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>SympNet</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/architectures/sympnet.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="SympNet"><a class="docs-heading-anchor" href="#SympNet">SympNet</a><a id="SympNet-1"></a><a class="docs-heading-anchor-permalink" href="#SympNet" title="Permalink"></a></h1><p>This page documents the SympNet architecture and its implementation in <code>GeometricMachineLearning.jl</code>.</p><h2 id="Quick-overview-of-the-theory-of-SympNet"><a class="docs-heading-anchor" href="#Quick-overview-of-the-theory-of-SympNet">Quick overview of the theory of SympNet</a><a id="Quick-overview-of-the-theory-of-SympNet-1"></a><a class="docs-heading-anchor-permalink" href="#Quick-overview-of-the-theory-of-SympNet" title="Permalink"></a></h2><h3 id="Principle"><a class="docs-heading-anchor" href="#Principle">Principle</a><a id="Principle-1"></a><a class="docs-heading-anchor-permalink" href="#Principle" title="Permalink"></a></h3><p>SympNets is a new type of neural network proposing a new approach to compute the trajectory of an Hamiltonian system in phase space. Let us denote by <span>$(q,p)=(q_1,...,q_d,p_1,....p_d)\in \mathbb{R}^{2d}$</span> the phase space with <span>$q\in \mathbb{R}^{d}$</span> the generalized position and  <span>$p\in \mathbb{R}^{d}$</span> the generalized momentum. Given a physical problem, SympNets takes a phase space element <span>$(q,p)$</span> and aims to compute the next position <span>$(q&#39;,p&#39;)$</span> of the trajectory in phase space a time step later while preserving the well known symplectic structure of Hamiltonian systems. The way SympNet preserve the symplectic structure is really specific and characterizes it as this preserving is intrinsic of the neural network. Indeed, SympNet is not made with traditional layers but with symplectic layers (described later) modifying the traditional universal approximation theorem into a symplectic one : SympNet is able to approach any symplectic function providing conditions on an activation function.</p><p>SympNet (noted <span>$\Phi$</span> in the following) is so an integrator from <span>$\mathbb{R}^{d} \times \mathbb{R}^{d}$</span> to <span>$\mathbb{R}^{d} \times \mathbb{R}^{d}$</span> preserving symplecticity which can compute, from an initial condition <span>$(q_0,p_0)$</span>, a sequence of phase space elements of a trajectory <span>$(q_n,p_n)=\Phi(q_{n-1},p_{n-1})=...=\Phi^n(q_0,p_0)$</span>. The time step between predictions is not a parameter we can choose but is related to the temporal frequency of the training data. SympNet can handle both  temporally regular data, i.e with a fix time step between data, and temporally irregular data, i.e with variable time step. </p><h3 id="Architecture-of-SympNets"><a class="docs-heading-anchor" href="#Architecture-of-SympNets">Architecture of SympNets</a><a id="Architecture-of-SympNets-1"></a><a class="docs-heading-anchor-permalink" href="#Architecture-of-SympNets" title="Permalink"></a></h3><p>With <code>GeometricMachineLearning.jl</code>, it is possible to implement two types of architecture which are LA-SympNet and G-SympNet. </p><h4 id="LA-SympNet"><a class="docs-heading-anchor" href="#LA-SympNet">LA-SympNet</a><a id="LA-SympNet-1"></a><a class="docs-heading-anchor-permalink" href="#LA-SympNet" title="Permalink"></a></h4><p><img src="../images/sympnet_pendulum_architecture.png" alt/></p><p>LA-SympNets are made of the alternation of two types of layers, symplectic linear layers and symplectic activation layers.  For a given integer <span>$n$</span>, a symplectic linear layer is defined by</p><p class="math-container">\[\mathcal{L}^{n,up}
\begin{pmatrix}
 q \\
 p \\
\end{pmatrix}
 =  
\begin{pmatrix} 
 I &amp; S^n/0 \\
 0/S^n &amp; I \\
\end{pmatrix}
 \cdots 
\begin{pmatrix} 
 I &amp; 0 \\
 S^2 &amp; I \\
\end{pmatrix}
\begin{pmatrix} 
 I &amp; S^1 \\
 0 &amp; I \\
\end{pmatrix}
\begin{pmatrix}
 q \\
 p \\
\end{pmatrix}
+ b ,\]</p><p>or </p><p class="math-container">\[\mathcal{L}^{n,low}
\begin{pmatrix}  q  \\  
 p  \end{pmatrix} =  
  \begin{pmatrix} 
 I &amp; 0/S^n  \\ 
 S^n/0 &amp; I
 \end{pmatrix} \cdots 
  \begin{pmatrix} 
 I &amp; S^2  \\ 
 0 &amp; I
 \end{pmatrix}
 \begin{pmatrix} 
 I &amp; 0  \\ 
 S^1 &amp; I
 \end{pmatrix}
 \begin{pmatrix}  q  \\  
 p  \end{pmatrix}
  + b . \]</p><p>The parameters to learn are the symmetric matrices <span>$S^i\in\mathbb{R}^{d\times d}$</span> and the bias <span>$b\in\mathbb{R}^{2d}$</span>. The integer <span>$n$</span> is the width of the symplectic linear layer. If <span>$n\geq9$</span>, we know that the symplectic linear layers represent any linear symplectic map so that <span>$n$</span> need not be larger than 9. We note the set of symplectic linear layers <span>$\mathcal{M}^L$</span>. This type of layers plays the role of standard linear layers. </p><p>For a given activation function <span>$\sigma$</span>, a symplectic activation layer is defined by</p><p class="math-container">\[ \mathcal{A}^{up}  \begin{pmatrix}  q  \\  
 p  \end{pmatrix} =  
  \begin{bmatrix} 
 I&amp;\hat{\sigma}^{a}  \\ 
 0&amp;I
 \end{bmatrix} \begin{pmatrix}  q  \\  
 p  \end{pmatrix} :=
 \begin{pmatrix} 
  \mathrm{diag}(a)\sigma(p)+q \\ 
  p
 \end{pmatrix},\]</p><p>or</p><p class="math-container">\[ \mathcal{A}^{low}  \begin{pmatrix}  q  \\  
 p  \end{pmatrix} =  
  \begin{bmatrix} 
 I&amp;0  \\ 
 \hat{\sigma}^{a}&amp;I
 \end{bmatrix} \begin{pmatrix}  q  \\  
 p  \end{pmatrix}
 :=
 \begin{pmatrix} 
 q \\ 
 \mathrm{diag}(a)\sigma(q)+p
 \end{pmatrix}.\]</p><p>The parameters to learn are the weights <span>$a\in\mathbb{R^{d}}$</span>. This type of layers plays the role of standard activation layers layers. We note the set of symplectic activation layers <span>$\mathcal{M}^A$</span>. </p><p>A LA-SympNet is a function of the form <span>$\Psi=l_{k+1} \circ a_{k} \circ v_{k} \circ \cdots \circ a_1 \circ l_1$</span> where <span>$(l_i)_{1\leq i\leq k+1} \subset (\mathcal{M}^L)^{k+1}$</span> and  </p><p class="math-container">\[(a_i)_{1\leq i\leq k} \subset (\mathcal{M}^A)^{k}\]</p><p>.</p><h4 id="G-SympNet"><a class="docs-heading-anchor" href="#G-SympNet">G-SympNet</a><a id="G-SympNet-1"></a><a class="docs-heading-anchor-permalink" href="#G-SympNet" title="Permalink"></a></h4><p>G-SympNets are an alternative to LA-SympNet. They are constituated with only one kind of layers called gradient layers. For a given activation function <span>$\sigma$</span> and an integer <span>$n\geq d$</span>, a gradient layers is a symplectic map from <span>$\mathbb{R}^{2d}$</span> to <span>$\mathbb{R}^{2d}$</span> defined by</p><p class="math-container">\[ \mathcal{G}^{up}  \begin{pmatrix}  q  \\  
 p  \end{pmatrix} =  
  \begin{bmatrix} 
 I&amp;\hat{\sigma}^{K,a,b}  \\ 
 0&amp;I
 \end{bmatrix} \begin{pmatrix}  q  \\  
 p  \end{pmatrix} :=
 \begin{pmatrix} 
  K^T \mathrm{diag}(a)\sigma(Kp+b)+q \\ 
  p
 \end{pmatrix},\]</p><p>or</p><p class="math-container">\[ \mathcal{G}^{low}  \begin{pmatrix}  q  \\  
 p  \end{pmatrix} =  
  \begin{bmatrix} 
 I&amp;0  \\ 
 \hat{\sigma}^{K,a,b}&amp;I
 \end{bmatrix} \begin{pmatrix}  q  \\  
 p  \end{pmatrix}
 :=
 \begin{pmatrix} 
 q \\ 
 K^T \mathrm{diag}(a)\sigma(Kq+b)+p
 \end{pmatrix}.\]</p><p>The parameters of this layer are the scale matrix <span>$K\in\mathbb{R}^{n\times d}$</span>, the bias <span>$b\in\mathbb{R}^{n}$</span> and the vector of weights <span>$a\in\mathbb{R}^{n}$</span>. The idea is that <span>$\hat{\sigma}^{K,a,b}$</span> can approximate any function of the form <span>$\nabla V$</span>, hence the name of this layer. The integer <span>$n$</span> is called the width of the gradient layer.</p><p>If we note by <span>$\mathcal{M}^G$</span> the set of gradient layers, a G-SympNet is a function of the form <span>$\Psi=g_k \circ g_{k-1} \circ \cdots \circ g_1$</span> where <span>$(g_i)_{1\leq i\leq k} \subset (\mathcal{M}^G)^k$</span>.</p><h3 id="Universal-approximation-theorems"><a class="docs-heading-anchor" href="#Universal-approximation-theorems">Universal approximation theorems</a><a id="Universal-approximation-theorems-1"></a><a class="docs-heading-anchor-permalink" href="#Universal-approximation-theorems" title="Permalink"></a></h3><p>We give now properly the universal approximation for both architectures. But let us give few definitions before. </p><p>Let <span>$U$</span> be an open set of <span>$\mathbb{R}^{2d}$</span>, and let us note by <span>$SP^r(U)$</span> the set of <span>$C^r$</span> smooth symplectic maps on <span>$U$</span>. Let us give a topology on the  set of <span>$C^r$</span> smooth maps from a compact K of <span>$\mathbb{R}^{n}$</span> to <span>$\mathbb{R}^{n}$</span> for any positive integers <span>$n$</span> through the norm</p><p class="math-container">\[||f||_{C^r(K,\mathbb{R}^{n})} = \underset{|\alpha|\leq r}{\sum} \underset{1\leq i \leq n}{\max}\underset{x\in K}{\sup} |D^\alpha f_i(x)|\]</p><p>where the differential operator <span>$D^\alpha$</span> is defined for any map of <span>$C^r(\mathbb{R}^{n},\mathbb{R})$</span> by </p><p class="math-container">\[D^\alpha f = \frac{\partial^{|\alpha|} f}{\partial x_1^{\alpha_1}...x_n^{\alpha_n}}\]</p><p>with <span>$|\alpha| = \alpha_1 +...+ \alpha_n$</span>. </p><p><strong>Definition</strong> Let <span>$\sigma$</span> a real map and <span>$r\in \mathbb{N}$</span>. <span>$\sigma$</span> is r-finite if <span>$\sigma\in C^r(\mathbb{R},\mathbb{R})$</span> and <span>$\int |D^r\sigma(x)|dx &lt;+\infty$</span>.</p><p><strong>Definition</strong> Let <span>$m,n,r\in \mathbb{N}$</span> with <span>$m,n&gt;0$</span> be given, <span>$U$</span> an open set of <span>$\mathbb{R}^m$</span>, and <span>$I,J\subset C^r(U,\mathbb{R}^n$</span>. We say <span>$J$</span> is r-uniformly dense on compacta in <span>$I$</span> if <span>$J \subset I$</span> and for any <span>$f\in I$</span>, <span>$\epsilon&gt;0$</span>, and any compact <span>$K\subset U$</span>, there exists <span>$g\in J$</span> such that <span>$||f-g||_{C^r(K,\mathbb{R}^{n})} &lt; \epsilon$</span>.</p><p>We can now gives the theorems.</p><p><strong>Theorem (Approximation theorem for LA-SympNet)</strong> For any positive integer <span>$r&gt;0$</span> and open set <span>$U\in \mathbb{R}^{2d}$</span>, the set of LA-SympNet is r-uniformly dense on compacta in <span>$SP^r(U)$</span> if the activation function <span>$\sigma$</span> is r-finite.</p><p><strong>Theorem (Approximation theorem for G-SympNet)</strong> For any positive integer <span>$r&gt;0$</span> and open set <span>$U\in \mathbb{R}^{2d}$</span>, the set of G-SympNet is r-uniformly dense on compacta in <span>$SP^r(U)$</span> if the activation function <span>$\sigma$</span> is r-finite.</p><p>These two theorems are at odds with the well-foundedness of the SympNets. </p><p><strong>Example of r-finite functions</strong></p><ul><li>sigmoid <span>$\sigma(x)=\frac{1}{1+e^{-x}}$</span> for any positive integer <span>$r$</span>, </li><li>tanh <span>$\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$</span> for any positive integer <span>$r$</span>. </li></ul><h2 id="SympNet-with-GeometricMachineLearning.jl"><a class="docs-heading-anchor" href="#SympNet-with-GeometricMachineLearning.jl">SympNet with <code>GeometricMachineLearning.jl</code></a><a id="SympNet-with-GeometricMachineLearning.jl-1"></a><a class="docs-heading-anchor-permalink" href="#SympNet-with-GeometricMachineLearning.jl" title="Permalink"></a></h2><p>With <code>GeometricMachineLearning.jl</code>, it is really easy to implement and train a SympNet. The steps are the following :</p><ul><li><strong>Create the architecture</strong> in one line with the function <code>GSympNet</code> or <code>LASympNet</code>,</li><li><strong>Create the neural networks</strong> depending a backend (e.g. with Lux),</li><li><strong>Create an optimizer</strong> for the training step,</li><li><strong>Train</strong> the neural networks with the <code>train!</code>function.</li></ul><p>Both LA-SympNet and G-SympNet architectures can be generated in one line with <code>GeometricMachineLearning.jl</code>.</p><h3 id="LA-SympNet-2"><a class="docs-heading-anchor" href="#LA-SympNet-2">LA-SympNet</a><a class="docs-heading-anchor-permalink" href="#LA-SympNet-2" title="Permalink"></a></h3><p>To create a LA-SympNet, one needs to write</p><pre><code class="language-julia hljs">lasympnet = LASympNet(dim; width=9, nhidden=1, activation=tanh, init_uplow_linear=[true,false], 
            init_uplow_act=[true,false],init_sym_matrices=Lux.glorot_uniform, init_bias=Lux.zeros32, 
            init_weight=Lux.glorot_uniform) </code></pre><p><code>LASympNet</code> takes one obligatory argument:</p><ul><li><strong>dim</strong> : the dimension of the phase space,</li></ul><p>and several keywords argument :</p><ul><li><strong>width</strong> : the width for all the symplectic linear layers with default value set to 9 (if width&gt;9, width is set to 9),</li><li><strong>nhidden</strong> : the number of pairs of symplectic linear and activation layers with default value set to 0 (i.e LA-SympNet is a single symplectic linear layer),</li><li><strong>activation</strong> : the activation function for all the symplectic activations layers with default value set to tanh,</li><li><strong>init<em>uplow</em>linear</strong> : a vector of boolean whose the ith coordinate is true only if all the symplectic linear layers in (i mod <code>length(init_uplow_linear)</code>)-th position is up (for example the default value is [true,false] which represents an alternation of up and low symplectic linear layers),</li><li><strong>init<em>uplow</em>act</strong> : a vector of boolean whose the ith coordinate is true only if all the symplectic activation layers in (i mod <code>length(init_uplow_act)</code>)-th position is up (for example the default value is [true,false] which represents an alternation of up and low symplectic activation layers),</li><li><strong>init<em>sym</em>matrices</strong>: the function which gives the way to initialize the symmetric matrices <span>$S^i$</span> of symplectic linear layers,</li><li><strong>init_bias</strong>: the function which gives the way to initialize the vector of bias <span>$b$</span>,</li><li><strong>init_weight</strong>: the function which gives the way to initialize the weight <span>$a$</span>.</li></ul><p>The default value of the last three keyword arguments uses Lux functions.</p><h3 id="G-SympNet-2"><a class="docs-heading-anchor" href="#G-SympNet-2">G-SympNet</a><a class="docs-heading-anchor-permalink" href="#G-SympNet-2" title="Permalink"></a></h3><p>To create a G-SympNet, one needs to write</p><pre><code class="language-julia hljs">gsympnet = GSympNet(dim; width=dim, nhidden=1, activation=tanh, init_uplow=[true,false], init_weight=Lux.glorot_uniform, 
init_bias=Lux.zeros32, init_scale=Lux.glorot_uniform) </code></pre><p><code>GSympNet</code> takes one obligatory argument:</p><ul><li><strong>dim</strong> : the dimension of the phase space,</li></ul><p>and severals keywords argument :</p><ul><li><strong>width</strong> : the width for all the gradients layers with default value set to dim to have width<span>$\geq$</span>dim,</li><li><strong>nhidden</strong> : the number of gradient layers with default value set to 1,</li><li><strong>activation</strong> : the activation function for all the gradients layers with default value set to tanh,</li><li><strong>init_uplow</strong>: a vector of boolean whose the ith coordinate is true only if all the gradient layers in (i mod <code>length(init_uplow)</code>)-th position is up (for example the default value is [true,false] which represents an alternation of up and low gradient layers),</li><li><strong>init_weight</strong>: the function which gives the way to initialize the vector of weights <span>$a$</span>,</li><li><strong>init_bias</strong>: the function which gives the way to initialize the vector of bias <span>$b$</span>,</li><li><strong>init_scale</strong>: the function which gives the way to initialize the scale matrix <span>$K$</span>.</li></ul><p>The default value of the last three keyword arguments uses Lux functions.</p><h3 id="Loss-function"><a class="docs-heading-anchor" href="#Loss-function">Loss function</a><a id="Loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function" title="Permalink"></a></h3><p>To train the SympNet, one need data along a trajectory such that the model is trained to perform an integration. These data are <span>$(Q,P)$</span> where <span>$Q[i,j]$</span> (respectively <span>$P[i,j]$</span>) is the real number <span>$q_j(t_i)$</span> (respectively <span>$p[i,j]$</span>) which is the j-th coordinates of the generalized position (respectively momentum) at the i-th time step. One also need a loss function defined as :</p><p class="math-container">\[Loss(Q,P) = \underset{i}{\sum} d(\Phi(Q[i,-],P[i,-]), [Q[i,-] P[i,-]]^T)\]</p><p>where <span>$d$</span> is a distance on <span>$\mathbb{R}^d$</span>.</p><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><p>Let us see how to use it on several examples.</p><h3 id="Example-of-a-pendulum-with-G-SympNet"><a class="docs-heading-anchor" href="#Example-of-a-pendulum-with-G-SympNet">Example of a pendulum with G-SympNet</a><a id="Example-of-a-pendulum-with-G-SympNet-1"></a><a class="docs-heading-anchor-permalink" href="#Example-of-a-pendulum-with-G-SympNet" title="Permalink"></a></h3><p>Let us begin with a simple example, the pendulum system, the Hamiltonian of which is </p><p class="math-container">\[H:(q,p)\in\mathbb{R}^2 \mapsto \frac{1}{2}p^2-cos(q) \in \mathbb{R}.\]</p><p>The first thing to do is to create an architecture, in this example a G-SympNet.</p><pre><code class="language-julia hljs"># number of inputs/dimension of system
const ninput = 2
# layer dimension for gradient module 
const ld = 10 
# hidden layers
const ln = 4
# activation function
const act = tanh

# Creation of a G-SympNet architecture 
gsympnet = GSympNet(ninput, width=ld, nhidden=ln, activation=act)

# Creation of a LA-SympNet architecture 
lasympnet = LASympNet(ninput, nhidden=ln, activation=act)</code></pre><p>Then we can create the neural networks depending on the backend. Here we will use Lux:</p><pre><code class="language-julia hljs"># create Lux network
nn = NeuralNetwork(gsympnet, LuxBackend())</code></pre><p>We have to define an optimizer which will be use in the training of the SympNet. For more details on optimizer, please see the corresponding documentation <a href="../../Optimizer/">Optimizer.md</a>. For example, let us use a momentum optimizer :</p><pre><code class="language-julia hljs"># Optimiser
opt = MomentumOptimizer(1e-2, 0.5)</code></pre><p>We can now perform the training of the neural networks. The syntax is the following :</p><pre><code class="language-julia hljs"># number of training runs
const nruns = 10000
# Batchsize used to compute the gradient of the loss function with respect to the parameters of the neural networks.
const nbatch = 10

# perform training (returns array that contains the total loss for each training step)
total_loss = train!(nn, opt, data_q, data_p; ntraining = nruns, batch_size = nbatch)</code></pre><p>The train function will change the parameters of the neural networks and gives an a vector containing the evolution of the value of the loss function during the training. Default values for the arguments <code>ntraining</code> and <code>batch_size</code> are respectively <span>$1000$</span> and <span>$10$</span>.</p><p>The trainings data <code>data_q</code> and <code>data_p</code> must be matrices of <span>$\mathbb{R}^{n\times d}$</span> where <span>$n$</span> is the length of data and <span>$d$</span> is the half of the dimension of the system, i.e <code>data_q[i,j]</code> is <span>$q_j(t_i)$</span> where <span>$(t_1,...,t_n)$</span> are the corresponding time of the training data.</p><p>Then we can make prediction. Let&#39;s compare the initial data with a prediction starting from the same phase space point using the provided function Iterate_Sympnet:</p><pre><code class="language-julia hljs">#predictions
q_learned, p_learned = Iterate_Sympnet(nn, q0, p0; n_points = size(data_q,1))</code></pre><p><img src="../../images/sympnet_pendulum_test.png" alt/></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« Home</a><a class="docs-footer-nextpage" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Thursday 28 September 2023 04:48">Thursday 28 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
