<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Neural Network Integrators · GeometricMachineLearning.jl</title><meta name="title" content="Neural Network Integrators · GeometricMachineLearning.jl"/><meta property="og:title" content="Neural Network Integrators · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Neural Network Integrators · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/architectures/neural_network_integrators/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/architectures/neural_network_integrators/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/architectures/neural_network_integrators/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../symplectic_autoencoder/">Symplectic Autoencoders</a></li><li class="is-active"><a class="tocitem" href>Neural Network Integrators</a><ul class="internal"><li><a class="tocitem" href="#Multi-step-methods"><span>Multi-step methods</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../sympnet/">SympNet</a></li><li><a class="tocitem" href="../volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../symplectic_transformer/">Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/matrix_softmax/">Matrix Attention</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/symplectic_transformer/">Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Architectures</a></li><li class="is-active"><a href>Neural Network Integrators</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Neural Network Integrators</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/architectures/neural_network_integrators.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Neural-Network-Integrators"><a class="docs-heading-anchor" href="#Neural-Network-Integrators">Neural Network Integrators</a><a id="Neural-Network-Integrators-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-Network-Integrators" title="Permalink"></a></h1><p>In <code>GeometricMachineLearning</code> we can divide most neural network architectures (that are used for applications to physical systems) into two categories: autoencoders and integrators. This is also closely related to the application of reduced order modeling where <em>autoencoders are used in the offline phase</em> and <em>integrators are used in the online phase</em>.</p><p>The term <em>integrator</em> in its most general form refers to an approximation of the <a href="../../manifolds/existence_and_uniqueness_theorem/#The-Existence-And-Uniqueness-Theorem">flow of an ODE</a> by a numerical scheme. Traditionally, for so called <em>one-step methods</em>, these numerical schemes are constructed by defining certain relationships between a known time step <span>$z^{(t)}$</span> and a future unknown one <span>$z^{(t+1)}$</span> [<a href="../../references/#hairer2006geometric">1</a>, <a href="../../references/#leimkuhler2004simulating">82</a>]: </p><p class="math-container">\[    f(z^{(t)}, z^{(t+1)}) = 0.\]</p><p>One usually refers to such a relationship as an <em>integration scheme</em>. If this relationship can be reformulated as </p><p class="math-container">\[    z^{(t+1)} = g(z^{(t)}),\]</p><p>then we refer to the scheme as <em>explicit</em>, if it cannot be reformulated in such a way then we refer to it as <em>implicit</em>. Implicit schemes are typically more expensive to solve than explicit ones. The <code>Julia</code> library <code>GeometricIntegrators</code> [<a href="../../references/#Kraus:2020:GeometricIntegrators">2</a>] offers a wide variety of integration schemes both implicit and explicit. </p><p>The neural network integrators in <code>GeometricMachineLearning</code> (the corresponding type is <a href="#GeometricMachineLearning.NeuralNetworkIntegrator"><code>NeuralNetworkIntegrator</code></a>) are all explicit integration schemes where the function <span>$g$</span> above is modeled with a neural network.</p><p>Neural networks, as an alternative to traditional methods, are employed because of (i) potentially superior performance and (ii) an ability to learn unknown dynamics from data. </p><p>The simplest of such a neural network for modeling an explicit integrator is the <a href="#GeometricMachineLearning.ResNet"><code>ResNet</code></a>. <a href="../sympnet/#SympNet-Architecture">SympNets</a> can be seen as the <a href="../../structure_preservation/symplecticity/#Symplectic-Systems">symplectic</a> version of the ResNet. There is an example <a href="../../tutorials/sympnet_tutorial/#SympNets-with-GeometricMachineLearning">demonstrating the performance of SympNets</a>. This example demonstrates the advantages of symplectic neural networks.</p><h2 id="Multi-step-methods"><a class="docs-heading-anchor" href="#Multi-step-methods">Multi-step methods</a><a id="Multi-step-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Multi-step-methods" title="Permalink"></a></h2><p><em>Multi-step method</em> [<a href="../../references/#feng1987symplectic">58</a>, <a href="../../references/#ge1988approximation">59</a>] refers to schemes that are of the form<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>: </p><p class="math-container">\[    f(z^{(t - \mathtt{sl} + 1)}, z^{(t - \mathtt{sl} + 2)}, \ldots, z^{(t)}, z^{(t + 1)}, \ldots, z^{(\mathtt{pw} + 1)}) = 0,\]</p><p>where <code>sl</code> is short for <em>sequence length</em> and <code>pw</code> is short for <em>prediction window</em>. Note that we can recover traditional one-step methods by setting <code>sl</code> and <code>pw</code> equal to 1. We can also formulate explicit mulit-step methods. They are of the form: </p><p class="math-container">\[[z^{(t+1)}, \ldots, z^{(t+\mathtt{pw})}] = g(z^{(t - \mathtt{sl} + 1)}, \ldots, z^{(t)}).\]</p><p>In <code>GeometricMachineLearning</code> all multi-step methods, as is the case with one-step methods, are explicit. There are essentially two ways to construct multi-step methods with neural networks: the older one is using recurrent neural networks such as long short-term memory cells (LSTMs) [<a href="../../references/#hochreiter1997long">83</a>] and the newer one is using transformer neural networks [<a href="../../references/#vaswani2017attention">54</a>]. Both of these approaches have been successfully employed to learn multi-step methods (see [<a href="../../references/#fresca2021comprehensive">61</a>, <a href="../../references/#lee2020model">62</a>] for the former and [<a href="../../references/#brantner2024volume">4</a>, <a href="../../references/#hemmasian2023reduced">84</a>, <a href="../../references/#solera2023beta">85</a>] for the latter), but because the transformer architecture exhibits superior performance on modern hardware and can be imbued with geometric properties we almost always use a transformer-derived architecture when dealing with time series<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>.</p><p>Explicit multi-step methods derived from the transformer are always subtypes of the type <a href="#GeometricMachineLearning.TransformerIntegrator"><code>TransformerIntegrator</code></a> in <code>GeometricMachineLearning</code>. In <code>GeometricMachineLearning</code> the <a href="../transformer/#Standard-Transformer">standard transformer</a>, the <a href="../volume_preserving_transformer/#Volume-Preserving-Transformer">volume-preserving transformer</a> and the <a href="../linear_symplectic_transformer/#Linear-Symplectic-Transformer">linear symplectic transformer</a> are implemented. </p><div class="admonition is-success"><header class="admonition-header">Remark</header><div class="admonition-body"><p>For standard multi-step methods (that are not neural network-based) <code>sl</code> is generally a number greater than one whereas <code>pw = 1</code> in most cases.  For the <code>TransformerIntegrator</code>s in <code>GeometricMachineLearning</code> however we usually have:</p><p class="math-container">\[    \mathtt{pw} = \mathtt{sl},\]</p><p>so the number of vectors in the input sequence is equal to the number of vectors in the output sequence. This makes it easier to define structure-preservation for these architectures and improves stability.</p></div></div><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.NeuralNetworkIntegrator" href="#GeometricMachineLearning.NeuralNetworkIntegrator"><code>GeometricMachineLearning.NeuralNetworkIntegrator</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p><code>NeuralNetworkIntegrator</code> is a super type of various neural network architectures such as <a href="../sympnet/#GeometricMachineLearning.SympNet"><code>SympNet</code></a> and <a href="#GeometricMachineLearning.ResNet"><code>ResNet</code></a>.</p><p>The purpose of such neural networks is to approximate the flow of an ordinary differential equation (ODE).</p><p><code>NeuralNetworkIntegrator</code>s can be seen as modeling traditional one-step methods with neural networks, i.e. for a fixed time step they perform:</p><p class="math-container">\[    \mathtt{NeuralNetworkIntegrator}: z^{(t)} \mapsto z^{(t+1)},\]</p><p>to try to approximate the flow of some ODE:</p><p class="math-container">\[    || \mathtt{Integrator}(z^{(t)}) - \varphi^h(z^{(t)}) || \approx \mathcal{O}(h),\]</p><p>where <span>$\varphi^h$</span> is the flow map of the ODE for a time step <span>$h$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/fe8658c704325b22d15669f965908b0025b82705/src/architectures/neural_network_integrator.jl#LL1-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.ResNet" href="#GeometricMachineLearning.ResNet"><code>GeometricMachineLearning.ResNet</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ResNet(dim, n_blocks, activation)</code></pre><p>Make an instance of a <code>ResNet</code>.</p><p>A ResNet is a neural network that realizes a mapping of the form: </p><p class="math-container">\[    x = \mathcal{NN}(x) + x,\]</p><p>so the input is again added to the output (a so-called add connection).  In <code>GeometricMachineLearning</code> the specific ResNet that we use consists of a series of simple <a href="#GeometricMachineLearning.ResNetLayer"><code>ResNetLayer</code></a>s.</p><p><strong>Constructor</strong></p><p><code>ResNet</code> can also be called with the constructor:</p><pre><code class="language-julia hljs">ResNet(dl, n_blocks)</code></pre><p>where <code>dl</code> is an instance of <code>DataLoader</code>.</p><p>See <a href="#Base.iterate-Union{Tuple{BT}, Tuple{AT}, Tuple{T}, Tuple{NeuralNetwork{&lt;:NeuralNetworkIntegrator}, BT}} where {T, AT&lt;:AbstractVector{T}, BT&lt;:@NamedTuple{q::AT, p::AT}}"><code>iterate</code></a> for an example of this.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/fe8658c704325b22d15669f965908b0025b82705/src/architectures/resnet.jl#LL1-L24">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.ResNetLayer" href="#GeometricMachineLearning.ResNetLayer"><code>GeometricMachineLearning.ResNetLayer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ResNetLayer(dim)</code></pre><p>Make an instance of the resnet layer.</p><p>The <code>ResNetLayer</code> is a simple feedforward neural network to which we add the input after applying it, i.e. it realizes <span>$x \mapsto x + \sigma(Ax + b)$</span>.</p><p><strong>Arguments</strong></p><p>The ResNet layer takes the following arguments:</p><ol><li><code>dim::Integer</code>: the system dimension.</li><li><code>activation = identity</code>: The activation function.</li></ol><p>The following is a keyword argument:</p><ul><li><code>use_bias::Bool = true</code>: This determines whether a bias <span>$b$</span> is used.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/fe8658c704325b22d15669f965908b0025b82705/src/layers/resnet.jl#LL1-L16">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Base.iterate-Union{Tuple{BT}, Tuple{AT}, Tuple{T}, Tuple{NeuralNetwork{&lt;:NeuralNetworkIntegrator}, BT}} where {T, AT&lt;:AbstractVector{T}, BT&lt;:@NamedTuple{q::AT, p::AT}}" href="#Base.iterate-Union{Tuple{BT}, Tuple{AT}, Tuple{T}, Tuple{NeuralNetwork{&lt;:NeuralNetworkIntegrator}, BT}} where {T, AT&lt;:AbstractVector{T}, BT&lt;:@NamedTuple{q::AT, p::AT}}"><code>Base.iterate</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">iterate(nn, ics)</code></pre><p>This function computes a trajectory for a <a href="#GeometricMachineLearning.NeuralNetworkIntegrator"><code>NeuralNetworkIntegrator</code></a> that has already been trained for valuation purposes.</p><p>It takes as input: </p><ol><li><code>nn</code>: a <code>NeuralNetwork</code> (that has been trained).</li><li><code>ics</code>: initial conditions (a <code>NamedTuple</code> of two vectors)</li></ol><p><strong>Examples</strong></p><p>To demonstrate <code>iterate</code> we use a simple ResNet that does:</p><p class="math-container">\[\mathrm{ResNet}: x \mapsto \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 1\end{pmatrix}x + \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}\]</p><p>and we iterate three times with</p><p class="math-container">\[    \mathtt{ics} = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}.\]</p><pre><code class="language-julia hljs">using GeometricMachineLearning

model = ResNet(3, 0, identity)
weight = [1 0 0; 0 2 0; 0 0 1]
bias = [0, 0, 1]
ps = NeuralNetworkParameters((L1 = (weight = weight, bias = bias), ))
nn = NeuralNetwork(model, Chain(model), ps, CPU())

ics = [1, 1, 1]
iterate(nn, ics; n_points = 4)

# output

3×4 Matrix{Int64}:
 1  2  4   8
 1  3  9  27
 1  3  7  15</code></pre><p><strong>Arguments</strong></p><p>The optional keyword argument is </p><ul><li><code>n_points = 100</code></li></ul><p>The number of integration steps that should be performed.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/fe8658c704325b22d15669f965908b0025b82705/src/architectures/neural_network_integrator.jl#LL47-L96">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.TransformerIntegrator" href="#GeometricMachineLearning.TransformerIntegrator"><code>GeometricMachineLearning.TransformerIntegrator</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TransformerIntegrator &lt;: Architecture</code></pre><p>Encompasses various transformer architectures, such as the <a href="../volume_preserving_transformer/#GeometricMachineLearning.VolumePreservingTransformer"><code>VolumePreservingTransformer</code></a> and the <a href="../linear_symplectic_transformer/#GeometricMachineLearning.LinearSymplecticTransformer"><code>LinearSymplecticTransformer</code></a>. </p><p>The central idea behind this is to construct an explicit multi-step integrator:</p><p class="math-container">\[    \mathtt{Integrator}: [ z^{(t - \mathtt{sl} + 1)}, z^{(t - \mathtt{sl} + 2)}, \ldots, z^{(t)} ] \mapsto [ z^{(t + 1)}, z^{(t + 2)}, \ldots, z^{(t + \mathtt{pw})} ],\]</p><p>where <code>sl</code> stands for <em>sequence length</em> and <code>pw</code> stands for <em>prediction window</em>, so the numbers of input and output vectors respectively.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/fe8658c704325b22d15669f965908b0025b82705/src/architectures/transformer_integrator.jl#LL1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Base.iterate-Union{Tuple{AT}, Tuple{T}, Tuple{NeuralNetwork{&lt;:TransformerIntegrator}, @NamedTuple{q::AT, p::AT}}} where {T, AT&lt;:AbstractMatrix{T}}" href="#Base.iterate-Union{Tuple{AT}, Tuple{T}, Tuple{NeuralNetwork{&lt;:TransformerIntegrator}, @NamedTuple{q::AT, p::AT}}} where {T, AT&lt;:AbstractMatrix{T}}"><code>Base.iterate</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">iterate(nn, ics)</code></pre><p>Iterate the neural network of type <a href="#GeometricMachineLearning.TransformerIntegrator"><code>TransformerIntegrator</code></a> for initial conditions <code>ics</code>.</p><p>The initial condition is a matrix <span>$\in\mathbb{R}^{n\times\mathtt{seq\_length}}$</span> or <code>NamedTuple</code> of two matrices).</p><p>This function computes a trajectory for a Transformer that has already been trained for valuation purposes.</p><p><strong>Parameters</strong></p><p>The following are optional keyword arguments:</p><ul><li><code>n_points::Int=100</code>: The number of time steps for which we run the prediction.</li><li><code>prediction_window::Int=size(ics.q, 2)</code>: The prediction window (i.e. the number of steps we predict into the future) is equal to the sequence length (i.e. the number of input time steps) by default.  </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/fe8658c704325b22d15669f965908b0025b82705/src/architectures/transformer_integrator.jl#LL26-L40">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[1]</dt><dd><div>E. Hairer, C. Lubich and G. Wanner. <em>Geometric Numerical integration: structure-preserving algorithms for ordinary differential equations</em> (Springer, Heidelberg, 2006).</div></dd><dt>[82]</dt><dd><div>B. Leimkuhler and S. Reich. <em>Simulating hamiltonian dynamics</em>. No. 14 (Cambridge university press, 2004).</div></dd><dt>[2]</dt><dd><div>M. Kraus. <a href="https://doi.org/10.5281/zenodo.3648325"><em>GeometricIntegrators.jl: Geometric Numerical Integration in Julia</em></a>, <a href="https://github.com/JuliaGNI/GeometricIntegrators.jl"><code>https://github.com/JuliaGNI/GeometricIntegrators.jl</code></a> (2020).</div></dd><dt>[56]</dt><dd><div>K. Feng. <em>The step-transition operators for multi-step methods of ODE&#39;s</em>. Journal of Computational Mathematics, 193–202 (1998).</div></dd><dt>[54]</dt><dd><div>A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and I. Polosukhin. <em>Attention is all you need</em>. Advances in neural information processing systems <strong>30</strong> (2017).</div></dd><dt>[84]</dt><dd><div>A. Hemmasian and A. Barati Farimani. <em>Reduced-order modeling of fluid flows with transformers</em>. Physics of Fluids <strong>35</strong> (2023).</div></dd><dt>[85]</dt><dd><div>A. Solera-Rico, C. S. Vila, M. Gómez, Y. Wang, A. Almashjary, S. Dawson and R. Vinuesa, <em><span>$\beta$</span>-Variational autoencoders and transformers for reduced-order modelling of fluid flows</em>, arXiv preprint arXiv:2304.03571 (2023).</div></dd><dt>[4]</dt><dd><div>B. Brantner, G. de Romemont, M. Kraus and Z. Li. <em>Volume-Preserving Transformers for Learning Time Series Data with Structure</em>, arXiv preprint arXiv:2312:11166v2 (2024).</div></dd></dl></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>We again assume that all the steps up to and including <span>$t$</span> are known.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a><code>GeometricMachineLearning</code> also has an LSTM implementation, but this may be deprecated in the future. </li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../symplectic_autoencoder/">« Symplectic Autoencoders</a><a class="docs-footer-nextpage" href="../sympnet/">SympNet »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Monday 17 February 2025 13:52">Monday 17 February 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
