<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Symplectic Autoencoders · GeometricMachineLearning.jl</title><meta name="title" content="Symplectic Autoencoders · GeometricMachineLearning.jl"/><meta property="og:title" content="Symplectic Autoencoders · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Symplectic Autoencoders · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/architectures/symplectic_autoencoder/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/architectures/symplectic_autoencoder/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/architectures/symplectic_autoencoder/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li class="is-active"><a class="tocitem" href>Symplectic Autoencoders</a><ul class="internal"><li><a class="tocitem" href="#Intermediate-Dimensions"><span>Intermediate Dimensions</span></a></li><li><a class="tocitem" href="#Example"><span>Example</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../sympnet/">SympNet</a></li><li><a class="tocitem" href="../volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Architectures</a></li><li class="is-active"><a href>Symplectic Autoencoders</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Symplectic Autoencoders</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/architectures/symplectic_autoencoder.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="The-Symplectic-Autoencoder"><a class="docs-heading-anchor" href="#The-Symplectic-Autoencoder">The Symplectic Autoencoder</a><a id="The-Symplectic-Autoencoder-1"></a><a class="docs-heading-anchor-permalink" href="#The-Symplectic-Autoencoder" title="Permalink"></a></h1><p>Symplectic autoencoders offer a structure-preserving way of mapping a high-dimensional system to a low-dimensional system. Concretely this means that if we obtain a reduced system by means of a symplectic autoencoder, this system will again be symplectic; we can thus model a symplectic FOM with a <a href="../../reduced_order_modeling/symplectic_mor/#The-Symplectic-Solution-Manifold">symplectic ROM</a>. </p><p>The architecture is represented by the figure below<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>:</p><object type="image/svg+xml" class="display-light-only" data=../../tikz/symplectic_autoencoder.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/symplectic_autoencoder_dark.png></object><p>It is a composition of <a href="../../layers/sympnet_gradient/#SympNet-Gradient-Layer">SympNet gradient layers</a> and <a href="../../reduced_order_modeling/symplectic_mor/#Proper-Symplectic-Decomposition">PSD-like matrices</a>, so a matrix <span>$A_i$</span> (respectively <span>$A_i^+$</span>) is of the form</p><p class="math-container">\[    A_i^{(+)} = \begin{bmatrix} \Phi_i &amp; \mathbb{O} \\ \mathbb{O} &amp; \Phi_i \end{bmatrix} \text{ where }\begin{cases} \Phi_i\in{}St(d_{i},d_{i+1})\subset\mathbb{R}^{d_{i+1}\times{}d_i} &amp; \text{if $d_{i+1} &gt; d_i$}
    \\
    \Phi_i\in{}St(d_{i+1},d_{i})\subset\mathbb{R}^{d{i}\times{}d_{i+1}} &amp; \text{if $d_i &gt; d_{i+1}$},
    \end{cases}\]</p><p>where <span>$A_i^{(+)} = A_i$</span> if <span>$d_{i+1} &gt; d_i$</span> and <span>$A_i^{(+)} = A_i^+$</span> if <span>$d_{i+1} &lt; d_i.$</span> Also note that for cotangent lift-like matrices we have</p><p class="math-container">\[\begin{aligned}
    A_i^+ = \mathbb{J}_{2N} A_i^T \mathbb{J}_{2n}^T &amp; = \begin{bmatrix} \mathbb{O}_{n\times{}n} &amp; \mathbb{I}_n \\ -\mathbb{I}_n &amp; \mathbb{O}_{n\times{}n} \end{bmatrix} \begin{bmatrix} \Phi_i^T &amp; \mathbb{O}_{n\times{}N} \\ \mathbb{O}_{n\times{}N} &amp; \Phi_i^T \end{bmatrix} \begin{bmatrix} \mathbb{O}_{N\times{}N} &amp; - \mathbb{I}_N \\ \mathbb{I}_N &amp; \mathbb{O}_{N\times{}N} \end{bmatrix} \\ &amp; = \begin{bmatrix} \Phi_i^T &amp; \mathbb{O}_{n\times{}N} \\ \mathbb{O}_{n\times{}N} &amp; \Phi_i^T \end{bmatrix} = A_i^T,
\end{aligned}\]</p><p>so the symplectic inverse is equivalent to a matrix transpose in this case. In the symplectic autoencoder we use SympNets as a form of <em>symplectic preprocessing</em> before the linear symplectic reduction (i.e. the PSD layer) is employed. The resulting neural network has some of its weights on manifolds, which is why we cannot use standard neural network optimizers, but have to resort to <a href="../../optimizers/optimizer_framework/#Generalization-to-Homogeneous-Spaces">manifold optimizers</a>. Note that manifold optimization is not necessary for the weights corresponding to the SympNet layers, these are still updated with standard neural network optimizers during training. Also note that SympNets are nonlinear and preserve symplecticity, but they cannot change the dimension of a system while PSD layers can change the dimension of a system and preserve symplecticity, but are strictly linear. Symplectic autoencoders have all three properties: they preserve symplecticity, can change dimension and are nonlinear mappings. We can visualize this in a Venn diagram:</p><object type="image/svg+xml" class="display-light-only" data=../../tikz/sae_venn.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/sae_venn_dark.png></object><p>We now show the proof that shows <span>$\nabla_{\mathcal{R}(z)}\psi = (\nabla_{z}\mathcal{R})^+$</span> which was used when <a href="../../reduced_order_modeling/symplectic_mor/#The-Symplectic-Solution-Manifold">showing the equivalence between Hamiltonian systems on the full and the reduced space</a>:</p><details class="admonition is-details"><summary class="admonition-header">Proof</summary><div class="admonition-body"><p>The symplectic autoencoder is a composition of <span>$G$</span>-SympNet layers and PSD-like matrices:</p><p class="math-container">\[\Psi^d = A_n\circ\psi_n\circ\cdots\circ{}A_1\circ\psi_1.\]</p><p>It&#39;s local inverse is</p><p class="math-container">\[(\Psi^d)^{-1} = \psi_1^{-1}\circ{}A_1^+\circ\ldots\circ\psi_n^{-1}\circ{}A_n^+.\]</p><p>The jacobian of <span>$\Psi^d$</span> is:</p><p class="math-container">\[\nabla_z\Psi^d = A_n\nabla_{A_{n-1}\cdots{}A_1\psi_1(z)}\psi_n\cdots{}A_1\nabla_z\psi_1,\]</p><p>and thus</p><p class="math-container">\[(\nabla_z\Psi^d)^+ = (\nabla\psi)^+A_1^+\cdots(\nabla\psi_n)^+A_n^+,\]</p><p>where we dropped the argument in the derivative of the nonlinear parts. We further have</p><p class="math-container">\[A^+ = A^T\]</p><p>for PSD-like matrices and</p><p class="math-container">\[(\nabla_z\psi)^+ = \begin{pmatrix} \mathbb{O} &amp; \mathbb{I} \\ -\mathbb{I} &amp; \mathbb{O}  \end{pmatrix} \begin{pmatrix} \mathbb{I} &amp; \nabla_pf \\ \mathbb{O} &amp; \mathbb{I} \end{pmatrix}^T \begin{pmatrix} \mathbb{O} &amp; -\mathbb{I} \\ \mathbb{I} &amp; \mathbb{O}  \end{pmatrix} = \begin{pmatrix} \mathbb{I} &amp; -\nabla_pf \\ \mathbb{O} &amp; \mathbb{I} \end{pmatrix},\]</p><p>for the <span>$G$</span>-SympNet layers, where we assumed that <span>$\psi$</span> only changes the <span>$q$</span> component. Because these matrices are square, the inverse <span>$(\nabla\psi)^+ = (\nabla\psi)^{-1}$</span> is unique.</p></div></details><p>The SympNet layers in the symplectic autoencoder operate in <em>intermediate dimensions</em> (as well as the input and output dimensions). In the following we explain how <code>GeometricMachineLearning</code> computes those intermediate dimensions. </p><h2 id="Intermediate-Dimensions"><a class="docs-heading-anchor" href="#Intermediate-Dimensions">Intermediate Dimensions</a><a id="Intermediate-Dimensions-1"></a><a class="docs-heading-anchor-permalink" href="#Intermediate-Dimensions" title="Permalink"></a></h2><p>For a high-fidelity system of dimension <span>$2N$</span> and a reduced system of dimension <span>$2n$</span>, the intermediate dimensions in the symplectic encoder and the decoder are computed according to: </p><pre><code class="language-julia hljs">iterations = Vector{Int}(n : (N - n) ÷ (n_blocks - 1) : N)
iterations[end] = full_dim2
iterations * 2</code></pre><p>So for e.g. <span>$2N = 100,$</span> <span>$2n = 10$</span> and <span>$\mathtt{n\_blocks} = 3$</span> we get </p><p class="math-container">\[\mathrm{iterations} = 5\mathtt{:}(45 \div 2)\mathtt{:}50 = 5\mathtt{:}22\mathtt{:}50 = (5, 27, 49).\]</p><p>We still have to perform the two other modifications in the algorithm above:</p><ol><li><code>iterations[end] = full_dim2</code> <span>$\ldots$</span> assign <code>full_dim2</code> to the last entry,</li><li><code>iterations * 2</code> <span>$\ldots$</span> multiply all the intermediate dimensions by two.</li></ol><p>The resulting dimensions are:</p><p class="math-container">\[(10, 54, 100).\]</p><p>The second step (the multiplication by two) is needed to arrive at intermediate dimensions that are even. This is necessary to preserve the <a href="../../structure_preservation/symplecticity/#Symplectic-Systems">canonical symplectic structure of the system</a>.</p><h2 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h2><p>A visualization of an instance of <a href="#GeometricMachineLearning.SymplecticAutoencoder"><code>SymplecticAutoencoder</code></a> is shown below: </p><object type="image/svg+xml" class="display-light-only" data=../../tikz/symplectic_autoencoder_architecture.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/symplectic_autoencoder_architecture_dark.png></object><p>In this figure we have the following configuration: <code>n_encoder_blocks</code> is two, <code>n_encoder_layers</code> is four, <code>n_decoder_blocks</code> is three and <code>n_decoder_layers</code> is two. For a full dimension of 100 and a reduced dimension of ten we can build such an instance of a symplectic autoencoder by calling:</p><pre><code class="language-julia hljs">using GeometricMachineLearning

const full_dim = 100
const reduced_dim = 10

model = SymplecticAutoencoder(full_dim, reduced_dim;
                                                    n_encoder_blocks = 2,
                                                    n_encoder_layers = 4,
                                                    n_decoder_blocks = 3,
                                                    n_decoder_layers = 2)

for layer in Chain(model)
    println(stdout, layer)
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">GradientLayerQ{100, 100, typeof(tanh)}(500, tanh)
GradientLayerP{100, 100, typeof(tanh)}(500, tanh)
GradientLayerQ{100, 100, typeof(tanh)}(500, tanh)
GradientLayerP{100, 100, typeof(tanh)}(500, tanh)
PSDLayer{100, 10}()
GradientLayerQ{10, 10, typeof(tanh)}(50, tanh)
GradientLayerP{10, 10, typeof(tanh)}(50, tanh)
PSDLayer{10, 54}()
GradientLayerQ{54, 54, typeof(tanh)}(270, tanh)
GradientLayerP{54, 54, typeof(tanh)}(270, tanh)
PSDLayer{54, 100}()</code></pre><p>We also see that the intermediate dimension in the decoder is <code>54</code>  for the specified dimensions and <code>n_decoder_blocks = 3</code> as was outlined before.</p><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.SymplecticAutoencoder" href="#GeometricMachineLearning.SymplecticAutoencoder"><code>GeometricMachineLearning.SymplecticAutoencoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SymplecticAutoencoder(full_dim, reduced_dim)</code></pre><p>Make an instance of <code>SymplecticAutoencoder</code> for dimensions <code>full_dim</code> and <code>reduced_dim</code>.</p><p><strong>The architecture</strong></p><p>The symplectic autoencoder architecture was introduced in [<a href="../../references/#brantner2023symplectic">3</a>]. Like any other autoencoder it consists of an <em>encoder</em> <span>$\Psi^e:\mathbb{R}^{2N}\to\mathbb{R}^{2n}$</span> and a <em>decoder</em> <span>$\Psi^d:\mathbb{R}^{2n}\to\mathbb{R}^{2N}$</span> with <span>$n\ll{}N$</span>. These satisfy the following properties: </p><p class="math-container">\[\begin{aligned}
    \nabla_z\Psi^e\mathbb{J}_{2N}(\nabla_z\Psi^e\mathbb{J}_{2N})^T = \mathbb{J}_{2n} &amp; \quad\text{and} \\
    (\nabla_\xi\Psi^d)^T\mathbb{J}_{2N}\nabla_\xi\Psi^d = \mathbb{J}_{2n}. &amp; 
\end{aligned}\]</p><p>Because the decoder has this particular property, the reduced system can be described by the Hamiltonian <span>$H\circ\Psi^d$</span>: </p><p class="math-container">\[\mathbb{J}_{2n}\nabla_\xi(H\circ\Psi^d) = \mathbb{J}_{2n}(\nabla_\xi\Psi^d)^T\nabla_{\Psi^d(\xi)}H = \mathbb{J}_{2n}(\nabla_\xi\Psi^d)^T\mathbb{J}_{2N}^T\mathbb{J}_{2N}\nabla_{\Psi^d(\xi)}H = (\nabla_\xi\Psi^d)^+X_H(\Psi^d(\xi)),\]</p><p>where <span>$(\nabla_\xi\Psi^d)^+$</span> is the <em>symplectic inverse</em> of <span>$\nabla_\xi\Psi^d$</span> (for more details see the docs on the <a href="../../reduced_order_modeling/pod_autoencoders/#GeometricMachineLearning.AutoEncoder"><code>AutoEncoder</code></a> type).</p><p><strong>Arguments</strong></p><p>Besides the required arguments <code>full_dim</code> and <code>reduced_dim</code> you can provide the following keyword arguments:</p><ul><li><code>n_encoder_layers::Integer = 4</code>: The number of layers in one encoder block.</li><li><code>n_encoder_blocks::Integer = 2</code>: The number of encoder blocks.</li><li><code>n_decoder_layers::Integer = 1</code>: The number of layers in one decoder block.</li><li><code>n_decoder_blocks::Integer = 3</code>: The number of decoder blocks.</li><li><code>sympnet_upscale::Integer = 5</code>: The <em>upscaling dimension</em> of the GSympNet. See <a href="../../layers/sympnet_gradient/#GeometricMachineLearning.GradientLayerQ"><code>GradientLayerQ</code></a> and <a href="../../layers/sympnet_gradient/#GeometricMachineLearning.GradientLayerP"><code>GradientLayerP</code></a>.</li><li><code>activation = tanh</code>: The activation in the gradient layers.</li><li><code>encoder_init_q::Bool = true</code>: Specifies if the first layer in each encoder block should be of <span>$q$</span> type.</li><li><code>decoder_init_q::Bool = true</code>: Specifies if the first layer in each decoder block should be of <span>$p$</span> type.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/a6ea34f779896229a31f8aa008dd4ce2809b82b8/src/architectures/symplectic_autoencoder.jl#LL1-L36">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[3]</dt><dd><div>B. Brantner and M. Kraus. <em>Symplectic autoencoders for Model Reduction of Hamiltonian Systems</em>, arXiv preprint arXiv:2312.10004 (2023).</div></dd></dl></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>For the symplectic autoencoder we only use <a href="../../layers/sympnet_gradient/#SympNet-Gradient-Layer">SympNet gradient layers</a> because they seem to outperform <span>$LA$</span>-SympNets in many cases and are easier to interpret: their nonlinear part is the gradient of a function that only depends on half the coordinates.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../abstract_neural_networks/">« Using Architectures with <code>NeuralNetwork</code></a><a class="docs-footer-nextpage" href="../neural_network_integrators/">Neural Network Integrators »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Tuesday 3 December 2024 11:51">Tuesday 3 December 2024</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
