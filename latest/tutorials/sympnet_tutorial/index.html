<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Sympnets · GeometricMachineLearning.jl</title><meta name="title" content="Sympnets · GeometricMachineLearning.jl"/><meta property="og:title" content="Sympnets · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Sympnets · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/sympnet_tutorial/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/sympnet_tutorial/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/sympnet_tutorial/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li></ul></li><li><span class="tocitem">Optimizers</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href>Sympnets</a><ul class="internal"><li><a class="tocitem" href="#Loss-function"><span>Loss function</span></a></li><li><a class="tocitem" href="#Training-a-Harmonic-Oscillator"><span>Training a Harmonic Oscillator</span></a></li></ul></li><li><a class="tocitem" href="../symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../grassmann_layer/">Grassmann manifold</a></li><li><a class="tocitem" href="../volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Sympnets</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Sympnets</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/tutorials/sympnet_tutorial.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="SympNets-with-GeometricMachineLearning"><a class="docs-heading-anchor" href="#SympNets-with-GeometricMachineLearning">SympNets with <code>GeometricMachineLearning</code></a><a id="SympNets-with-GeometricMachineLearning-1"></a><a class="docs-heading-anchor-permalink" href="#SympNets-with-GeometricMachineLearning" title="Permalink"></a></h1><p>This page serves as a short introduction into using <a href="../../architectures/sympnet/#SympNet-Architecture">SympNets</a> with <code>GeometricMachineLearning</code>. </p><div class="admonition is-success"><header class="admonition-header">Remark</header><div class="admonition-body"><p>As with any neural network we have to make the following choices:</p><ol><li>specify the <em>architecture</em>,</li><li>specify the <em>type</em> and <em>backend</em>,</li><li>pick an <em>optimizer</em> for training the network,</li><li>specify how you want to perform <em>batching</em>,</li><li>choose a number of epochs,</li></ol><p>where points 1 and 3 depend on a variable number of hyperparameters.</p></div></div><p>For the SympNet point 1 is done by calling <a href="../../architectures/sympnet/#GeometricMachineLearning.LASympNet"><code>LASympNet</code></a> or <a href="../../architectures/sympnet/#GeometricMachineLearning.GSympNet"><code>GSympNet</code></a>, point 2 is done by calling <code>NeuralNetwork</code>, point 3 is done by calling <a href="../../optimizers/optimizer_framework/#GeometricMachineLearning.Optimizer"><code>Optimizer</code></a> and point 4 is done by calling <a href="../../data_loader/data_loader/#GeometricMachineLearning.Batch"><code>Batch</code></a>.</p><h2 id="Loss-function"><a class="docs-heading-anchor" href="#Loss-function">Loss function</a><a id="Loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function" title="Permalink"></a></h2><p>The <a href="../../reduced_order_modeling/losses/#GeometricMachineLearning.FeedForwardLoss"><code>FeedForwardLoss</code></a> is the default choice used in <code>GeometricMachineLearning</code> for training SympNets, this <a href="../adjusting_the_loss_function/#Adjusting-the-Loss-Function">can however by altered</a>.</p><h2 id="Training-a-Harmonic-Oscillator"><a class="docs-heading-anchor" href="#Training-a-Harmonic-Oscillator">Training a Harmonic Oscillator</a><a id="Training-a-Harmonic-Oscillator-1"></a><a class="docs-heading-anchor-permalink" href="#Training-a-Harmonic-Oscillator" title="Permalink"></a></h2><p>Let us begin with a simple example, the pendulum system, the Hamiltonian of which is </p><p class="math-container">\[H:(q,p)\in\mathbb{R}^2 \mapsto \frac{1}{2}p^2-cos(q) \in \mathbb{R}.\]</p><p>Here we take the ODE from <a href="https://github.com/JuliaGNI/GeometricProblems.jl"><code>GeometricProblems</code></a> and integrate it with <code>GeometricIntegrators</code> [<a href="../../references/#Kraus:2020:GeometricIntegrators">17</a>]:</p><pre><code class="language-julia hljs">import GeometricProblems.HarmonicOscillator as ho
using GeometricIntegrators: ImplicitMidpoint, integrate


# the problem is the ODE of the harmonic oscillator
problem = ho.hodeproblem(; tspan = 500)

# integrate the system
solution = integrate(problem, ImplicitMidpoint())</code></pre><p>We can then conveniently handle the data with <a href="../../data_loader/data_loader/#The-Data-Loader">the data loader</a>:</p><pre><code class="language-julia hljs">using GeometricMachineLearning
# we can conveniently handle the data with
dl_raw = DataLoader(solution)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">WARNING: using GeometricMachineLearning.problem in module Main conflicts with an existing identifier.
<span class="sgr36"><span class="sgr1">[ Info: </span></span>You have provided a NamedTuple with keys q and p; the data are tensors. This is interpreted as *symplectic data*.</code></pre><p>Note that we have not yet specified the type and backend that we want to use<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>. We do this now:</p><pre><code class="language-julia hljs"># specify the data type and the backend
type = Float16
backend = CPU()

dl = DataLoader(dl_raw, backend, type)</code></pre><p>Next we specify the architectures. <code>GeometricMachineLearning</code> provides useful defaults for all parameters although they can be specified manually (which is done in the following):</p><pre><code class="language-julia hljs"># layer dimension for gradient module
const upscaling_dimension = 2

# hidden layers
const nhidden = 1

# activation function
const activation = tanh

# number of layers for the G-SympNet
const n_layers = 4

# number of linear layers in each &quot;linear block&quot; in the LA-SympNet
const depth = 4

# calling G-SympNet architecture
gsympnet = GSympNet(dl, upscaling_dimension=upscaling_dimension, n_layers=n_layers, activation=activation)

# calling LA-SympNet architecture
lasympnet = LASympNet(dl, nhidden=nhidden, activation=activation, depth = depth)

# initialize the networks
la_nn = NeuralNetwork(lasympnet, backend, type)
g_nn = NeuralNetwork(gsympnet, backend, type)</code></pre><p>If we want to obtain information on the number of parameters in a neural network, we can do that with the function <code>parameterlength</code>. For the <a href="../../architectures/sympnet/#GeometricMachineLearning.LASympNet"><code>LASympNet</code></a>:</p><pre><code class="language-julia hljs">parameterlength(la_nn.model)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">12</code></pre><p>And for the <a href="../../architectures/sympnet/#GeometricMachineLearning.GSympNet"><code>GSympNet</code></a>:</p><pre><code class="language-julia hljs">parameterlength(g_nn.model)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">12</code></pre><div class="admonition is-success"><header class="admonition-header">Remark</header><div class="admonition-body"><p>We can also specify whether we would like to start with a layer that changes the <span>$q$</span>-component or one that changes the <span>$p$</span>-component. This can be done via the keywords <code>init_upper</code> for <code>GSympNet</code>, and <code>init_upper_linear</code> and <code>init_upper_act</code> for <code>LASympNet</code>.</p></div></div><p>We have to define an optimizer which will be use in the training of the SympNet. For more details on optimizer, please see the <a href="../../optimizers/optimizer_framework/#Neural-Network-Optimizers">corresponding documentation</a>. In this example we use <a href="../../optimizers/optimizer_methods/#The-Adam-Optimizer">Adam</a>:</p><pre><code class="language-julia hljs"># set up optimizer; for this we first need to specify the optimization method
opt_method = AdamOptimizer(type)
# we then call the optimizer struct which allocates the cache
la_opt = Optimizer(opt_method, la_nn)
g_opt = Optimizer(opt_method, g_nn)</code></pre><p>We can now perform the training of the neural networks:</p><pre><code class="language-julia hljs"># determine the batch size (the number of samples in one batch)
const batch_size = 16

batch = Batch(batch_size)

# number of training epochs
const nepochs = 100

# perform training (returns array that contains the total loss for each training step)
g_loss_array = g_opt(g_nn, dl, batch, nepochs; show_progress = false)
la_loss_array = la_opt(la_nn, dl, batch, nepochs; show_progress = false)</code></pre><p>We can also plot the training errors against the epoch (here the <span>$y$</span>-axis is in log-scale):</p><pre><code class="language-julia hljs">using CairoMakie
using LaTeXStrings


fig = Figure(; backgroundcolor = :transparent)
ax = Axis(fig[1, 1];
    backgroundcolor = :transparent,
    bottomspinecolor = textcolor,
    topspinecolor = textcolor,
    leftspinecolor = textcolor,
    rightspinecolor = textcolor,
    xtickcolor = textcolor,
    ytickcolor = textcolor,
    xticklabelcolor = textcolor,
    yticklabelcolor = textcolor,
    xlabel=&quot;Epoch&quot;,
    ylabel=&quot;Training loss&quot;,
    xlabelcolor = textcolor,
    ylabelcolor = textcolor,
    yscale = log10
    )

lines!(ax, g_loss_array, label=L&quot;$G$-SympNet&quot;, color=morange)
lines!(ax, la_loss_array, label=L&quot;$LA$-SympNet&quot;, color=mpurple)</code></pre><object type="image/svg+xml" class="display-light-only" data=../sympnet_training_loss.png></object><object type="image/svg+xml" class="display-dark-only" data=../sympnet_training_loss_dark.png></object><p>Now we can make a prediction. Let&#39;s compare the initial data with a prediction starting from the same phase space point using the function <a href="../../architectures/neural_network_integrators/#Base.iterate-Union{Tuple{BT}, Tuple{AT}, Tuple{T}, Tuple{NeuralNetwork{&lt;:NeuralNetworkIntegrator}, BT}} where {T, AT&lt;:AbstractVector{T}, BT&lt;:@NamedTuple{q::AT, p::AT}}"><code>GeometricMachineLearning.iterate</code></a>:</p><pre><code class="language-julia hljs">ics = (q=dl.input.q[:, 1, 1], p=dl.input.p[:, 1, 1])

steps_to_plot = 200

#predictions
la_trajectory = iterate(la_nn, ics; n_points = steps_to_plot)
g_trajectory =  iterate(g_nn, ics; n_points = steps_to_plot)

fig = Figure(; backgroundcolor = :transparent)

ax = Axis(fig[1, 1];
    backgroundcolor = :transparent,
    bottomspinecolor = textcolor,
    topspinecolor = textcolor,
    leftspinecolor = textcolor,
    rightspinecolor = textcolor,
    xtickcolor = textcolor,
    ytickcolor = textcolor,
    xticklabelcolor = textcolor,
    yticklabelcolor = textcolor,
    xlabel=L&quot;q&quot;,
    ylabel=L&quot;p&quot;,
    xlabelcolor = textcolor,
    ylabelcolor = textcolor
    )

lines!(ax, dl.input.q[1, 1:steps_to_plot, 1], dl.input.p[1, 1:steps_to_plot, 1], label=&quot;training data&quot;, color = mblue)
lines!(ax, la_trajectory.q[1, :], la_trajectory.p[1, :], label=L&quot;$LA$-Sympnet&quot;, color = mpurple)
lines!(ax, g_trajectory.q[1, :], g_trajectory.p[1, :], label=L&quot;$G$-Sympnet&quot;, color = morange)</code></pre><object type="image/svg+xml" class="display-light-only" data=../sympnet_prediction.png></object><object type="image/svg+xml" class="display-dark-only" data=../sympnet_prediction_dark.png></object><p>We see that <a href="../../architectures/sympnet/#GeometricMachineLearning.GSympNet"><code>GSympNet</code></a> outperforms <a href="../../architectures/sympnet/#GeometricMachineLearning.LASympNet"><code>LASympNet</code></a> on this problem.</p><div class="admonition is-success"><header class="admonition-header">Remark</header><div class="admonition-body"><p>We have actually never observed a scenario in which the <span>$LA$</span>-SympNet can outperform the <span>$G$</span>-SympNet. The <span>$G$</span>-SympNet seems to train faster, be more accurate and less sensitive to the chosen hyperparameters and initialization of the weights. They are also more straightforward to interpret. We therefore use the <span>$G$</span>-SympNet as a basis for the <em>linear symplectic transformer.</em></p></div></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>Note that we also have to reallocate the data for <a href="../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a> in this case to conform with the neural network parameters. </li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../data_loader/snapshot_matrix/">« Snapshot matrix &amp; tensor</a><a class="docs-footer-nextpage" href="../symplectic_autoencoder/">Symplectic Autoencoders »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.6.0 on <span class="colophon-date" title="Wednesday 28 August 2024 05:15">Wednesday 28 August 2024</span>. Using Julia version 1.10.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
