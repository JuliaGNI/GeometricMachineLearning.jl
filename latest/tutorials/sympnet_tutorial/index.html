<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>SympNets · GeometricMachineLearning.jl</title><meta name="title" content="SympNets · GeometricMachineLearning.jl"/><meta property="og:title" content="SympNets · GeometricMachineLearning.jl"/><meta property="twitter:title" content="SympNets · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/sympnet_tutorial/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/sympnet_tutorial/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/sympnet_tutorial/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href>SympNets</a><ul class="internal"><li><a class="tocitem" href="#Loss-function"><span>Loss function</span></a></li><li><a class="tocitem" href="#Training-a-Harmonic-Oscillator"><span>Training a Harmonic Oscillator</span></a></li><li><a class="tocitem" href="#Comparison-with-a-ResNet"><span>Comparison with a ResNet</span></a></li></ul></li><li><a class="tocitem" href="../symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>SympNets</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>SympNets</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/tutorials/sympnet_tutorial.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="SympNets-with-GeometricMachineLearning"><a class="docs-heading-anchor" href="#SympNets-with-GeometricMachineLearning">SympNets with <code>GeometricMachineLearning</code></a><a id="SympNets-with-GeometricMachineLearning-1"></a><a class="docs-heading-anchor-permalink" href="#SympNets-with-GeometricMachineLearning" title="Permalink"></a></h1><p>This page serves as a short introduction into using <a href="../../architectures/sympnet/#SympNet-Architecture">SympNets</a> with <code>GeometricMachineLearning</code>. </p><h2 id="Loss-function"><a class="docs-heading-anchor" href="#Loss-function">Loss function</a><a id="Loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function" title="Permalink"></a></h2><p>The <a href="../../reduced_order_modeling/losses/#GeometricMachineLearning.FeedForwardLoss"><code>FeedForwardLoss</code></a> is the default choice used in <code>GeometricMachineLearning</code> for training SympNets, this can however be changed or <a href="../adjusting_the_loss_function/#Adjusting-the-Loss-Function">tweaked</a>.</p><h2 id="Training-a-Harmonic-Oscillator"><a class="docs-heading-anchor" href="#Training-a-Harmonic-Oscillator">Training a Harmonic Oscillator</a><a id="Training-a-Harmonic-Oscillator-1"></a><a class="docs-heading-anchor-permalink" href="#Training-a-Harmonic-Oscillator" title="Permalink"></a></h2><p>Here we begin with a simple example, the harmonic oscillator, the Hamiltonian of which is </p><p class="math-container">\[H:(q,p)\in\mathbb{R}^2 \mapsto \frac{1}{2}p^2 + \frac{1}{2}q^2 \in \mathbb{R}.\]</p><p>Here we take the ODE from <a href="https://github.com/JuliaGNI/GeometricProblems.jl"><code>GeometricProblems</code></a> and integrate it with <code>GeometricIntegrators</code> [<a href="../../references/#Kraus:2020:GeometricIntegrators">2</a>]:</p><pre><code class="language-julia hljs">import GeometricProblems.HarmonicOscillator as ho
using GeometricIntegrators: ImplicitMidpoint, integrate

# the problem is the ODE of the harmonic oscillator
ho_problem = ho.hodeproblem(; tspan = 500)

# integrate the system
solution = integrate(ho_problem, ImplicitMidpoint())</code></pre><p>We call <a href="../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a> in order to conveniently handle the data:</p><pre><code class="language-julia hljs">dl_raw = DataLoader(solution; suppress_info = true)</code></pre><p>We have not yet specified the type and backend that we want to use. We do this now:</p><pre><code class="language-julia hljs"># specify the data type and the backend
type = Float16
backend = CPU()

# we can then make a new instance of `DataLoader` with this backend and type.
dl = DataLoader(dl_raw, backend, type)</code></pre><p>Next we specify the architectures<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>: </p><pre><code class="language-julia hljs">const upscaling_dimension = 2
const nhidden = 1
const activation = tanh
const n_layers = 4 # number of layers for the G-SympNet
const depth = 4 # number of layers in each linear block in the LA-SympNet

# calling G-SympNet architecture
gsympnet = GSympNet(dl; upscaling_dimension = upscaling_dimension,
                        n_layers = n_layers,
                        activation = activation)

# calling LA-SympNet architecture
lasympnet = LASympNet(dl;   nhidden = nhidden,
                            activation = activation,
                            depth = depth)

# initialize the networks
la_nn = NeuralNetwork(lasympnet, backend, type)
g_nn = NeuralNetwork(gsympnet, backend, type)</code></pre><p>If we want to obtain information on the number of parameters in a neural network, we can do that with the function <code>parameterlength</code>. For the <a href="../../architectures/sympnet/#GeometricMachineLearning.LASympNet"><code>LASympNet</code></a>:</p><pre><code class="language-julia hljs">parameterlength(la_nn.model)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">14</code></pre><p>And for the <a href="../../architectures/sympnet/#GeometricMachineLearning.GSympNet"><code>GSympNet</code></a>:</p><pre><code class="language-julia hljs">parameterlength(g_nn.model)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">12</code></pre><div class="admonition is-success"><header class="admonition-header">Remark</header><div class="admonition-body"><p>We can also specify whether we would like to start with a layer that changes the <span>$q$</span>-component or one that changes the <span>$p$</span>-component. This can be done via the keywords <code>init_upper</code> for the <code>GSympNet</code>, and <code>init_upper_linear</code> and <code>init_upper_act</code> for the <code>LASympNet</code>.</p></div></div><p>We have to define an <a href="../../optimizers/optimizer_methods/#Standard-Neural-Network-Optimizers">optimizer</a> which will be used in training of the SympNet. In this example we use <a href="../../optimizers/optimizer_methods/#The-Adam-Optimizer">Adam</a>:</p><pre><code class="language-julia hljs"># set up optimizer; for this we first need to specify the optimization method
opt_method = AdamOptimizer(type)
# we then call the optimizer struct which allocates the cache
la_opt = Optimizer(opt_method, la_nn)
g_opt = Optimizer(opt_method, g_nn)</code></pre><p>We can now perform the training of the neural networks:</p><pre><code class="language-julia hljs"># determine the batch size (the number of samples in one batch)
const batch_size = 16

batch = Batch(batch_size)

# number of training epochs
const nepochs = 100

# perform training (returns array that contains the total loss for each training step)
g_loss_array = g_opt(g_nn, dl, batch, nepochs; show_progress = false)
la_loss_array = la_opt(la_nn, dl, batch, nepochs; show_progress = false)</code></pre><p>We plot the training errors against the epoch (here the <span>$y$</span>-axis is in log-scale):</p><object type="image/svg+xml" class="display-light-only" data=../sympnet_training_loss.png></object><object type="image/svg+xml" class="display-dark-only" data=../sympnet_training_loss_dark.png></object><p>Now we can make a prediction. We compare the initial data with a prediction starting from the same phase space point using the function <a href="../../architectures/neural_network_integrators/#Base.iterate-Union{Tuple{BT}, Tuple{AT}, Tuple{T}, Tuple{NeuralNetwork{&lt;:NeuralNetworkIntegrator}, BT}} where {T, AT&lt;:AbstractVector{T}, BT&lt;:@NamedTuple{q::AT, p::AT}}"><code>GeometricMachineLearning.iterate</code></a>:</p><pre><code class="language-julia hljs">ics = (q=dl.input.q[:, 1, 1], p=dl.input.p[:, 1, 1])

steps_to_plot = 200

#predictions
la_trajectory = iterate(la_nn, ics; n_points = steps_to_plot)
g_trajectory =  iterate(g_nn, ics; n_points = steps_to_plot)</code></pre><p>We now plot the result:</p><object type="image/svg+xml" class="display-light-only" data=../sympnet_prediction.png></object><object type="image/svg+xml" class="display-dark-only" data=../sympnet_prediction_dark.png></object><p>We see that <a href="../../architectures/sympnet/#GeometricMachineLearning.GSympNet"><code>GSympNet</code></a> outperforms <a href="../../architectures/sympnet/#GeometricMachineLearning.LASympNet"><code>LASympNet</code></a> on this problem; the blue line (reference) and the orange line (<span>$G$</span>-SympNet) are in fact almost indistinguishable.</p><div class="admonition is-success"><header class="admonition-header">Remark</header><div class="admonition-body"><p>We have actually never observed a scenario in which the <span>$LA$</span>-SympNet can outperform the <span>$G$</span>-SympNet. The <span>$G$</span>-SympNet seems usually trains faster, is more accurate and less sensitive to the chosen hyperparameters and initialization of the weights. They are also more straightforward to interpret. We therefore use the <span>$G$</span>-SympNet as a basis for the <em>linear symplectic transformer.</em></p></div></div><h2 id="Comparison-with-a-ResNet"><a class="docs-heading-anchor" href="#Comparison-with-a-ResNet">Comparison with a ResNet</a><a id="Comparison-with-a-ResNet-1"></a><a class="docs-heading-anchor-permalink" href="#Comparison-with-a-ResNet" title="Permalink"></a></h2><p>We want to show the advantages of using a SympNet over a standard ResNet that is not symplectic. For this we make a ResNet with a similar size of parameters as the two SympNets have:</p><pre><code class="language-julia hljs">resnet = ResNet(dl, n_layers ÷ 2; activation = activation)

rn_nn = NeuralNetwork(resnet, backend, type)

parameterlength(rn_nn)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">18</code></pre><p>We now train the network <span>$\ldots$</span></p><pre><code class="language-julia hljs">rn_opt = Optimizer(opt_method, rn_nn)

rn_loss_array = rn_opt(rn_nn, dl, batch, nepochs; show_progress = false)</code></pre><p>and plot the loss:</p><object type="image/svg+xml" class="display-light-only" data=../sympnet_resnet_training_loss.png></object><object type="image/svg+xml" class="display-dark-only" data=../sympnet_resnet_training_loss_dark.png></object><p>And we see that the loss is significantly lower than for the <span>$LA$</span>-SympNet, but slightly higher than for the <span>$G$</span>-SympNet. We can also plot the prediction:</p><pre><code class="language-julia hljs">rn_trajectory = iterate(rn_nn, ics; n_points = steps_to_plot)</code></pre><object type="image/svg+xml" class="display-light-only" data=../resnet_sympnet_prediction.png></object><object type="image/svg+xml" class="display-dark-only" data=../resnet_sympnet_prediction_dark.png></object><p>We see that the ResNet is slowly gaining energy which consitutes unphysical behaviour. If we let this simulation run for even longer, this effect gets more pronounced:</p><pre><code class="language-julia hljs">steps_to_plot = 800

#predictions
la_trajectory = iterate(la_nn, ics; n_points = steps_to_plot)
g_trajectory =  iterate(g_nn, ics; n_points = steps_to_plot)
rn_trajectory = iterate(rn_nn, ics; n_points = steps_to_plot)</code></pre><object type="image/svg+xml" class="display-light-only" data=../resnet_sympnet_prediction_long.png></object><object type="image/svg+xml" class="display-dark-only" data=../resnet_sympnet_prediction_long_dark.png></object><p>The behavior the ResNet exhibits is characteristic of integration schemes that do not preserve structure: the error in a single time step can be made very small, but for long-time simulations one typically has to consider symplecticity or other properties. Also note that the curves produced by the <span>$LA$</span>-SympNet and the <span>$G$</span>-SympNet are closed (or nearly closed). This is a property of symplectic maps in two dimensions that is preserved by construction [<a href="../../references/#hairer2006geometric">1</a>].</p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a><code>GeometricMachineLearning</code> provides useful defaults for all parameters, but they can still be specified manually; which is what we are doing here. Details on these parameters can be found in the docstrings for <a href="../../architectures/sympnet/#GeometricMachineLearning.GSympNet"><code>GSympNet</code></a> and <a href="../../architectures/sympnet/#GeometricMachineLearning.LASympNet"><code>LASympNet</code></a>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../data_loader/data_loader/">« Routines</a><a class="docs-footer-nextpage" href="../symplectic_autoencoder/">Symplectic Autoencoders »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Monday 17 February 2025 13:17">Monday 17 February 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
