<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>MNIST · GeometricMachineLearning.jl</title><meta name="title" content="MNIST · GeometricMachineLearning.jl"/><meta property="og:title" content="MNIST · GeometricMachineLearning.jl"/><meta property="twitter:title" content="MNIST · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/mnist/mnist_tutorial/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/mnist/mnist_tutorial/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/mnist/mnist_tutorial/"/><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img class="docs-light-only" src="../../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../../../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../../symplectic_autoencoder/">Symplectic Autoencoders</a></li><li class="is-active"><a class="tocitem" href>MNIST</a><ul class="internal"><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../../grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../../volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../../linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../../references/">References</a></li><li><a class="tocitem" href="../../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>MNIST</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>MNIST</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/tutorials/mnist/mnist_tutorial.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="MNIST-Tutorial"><a class="docs-heading-anchor" href="#MNIST-Tutorial">MNIST Tutorial</a><a id="MNIST-Tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#MNIST-Tutorial" title="Permalink"></a></h1><p>In this tutorial we show how we can use <code>GeometricMachineLearning</code> to build a vision transformer and apply it for MNIST [<a href="../../../references/#deng2012mnist">90</a>], while also putting some of the weights on a manifold. This is also the result presented in [<a href="../../../references/#brantner2023generalizing">7</a>].</p><p>We get the dataset from <a href="https://github.com/JuliaML/MLDatasets.jl"><code>MLDatasets</code></a>. Before we use it we allocate it on gpu with <code>cu</code> from <code>CUDA.jl</code> [<a href="../../../references/#besard2018juliagpu">11</a>]:</p><pre><code class="language-julia hljs">using MLDatasets
using CUDA
train_x, train_y = MLDatasets.MNIST(split=:train)[:]
test_x, test_y = MLDatasets.MNIST(split=:test)[:]

train_x = train_x |&gt; cu
train_y = train_y |&gt; cu
test_x = test_x |&gt; cu
test_y = test_y |&gt; cu</code></pre><p>Next we call <a href="../../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a> on these data. For this we first need to specify a <em>patch length</em><sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>.</p><div class="admonition is-success"><header class="admonition-header">Remark</header><div class="admonition-body"><p>In order to apply the transformer to a data set we should typically cast these data into a <em>time series format</em>. MNIST images are pictures with <span>$28\times28$</span> pixels. Here we cast these images into <em>time series</em> of length 16, so one image is represented by a matrix <span>$\in\mathbb{R}^{49\times{}16}$</span>.</p></div></div><pre><code class="language-julia hljs">const patch_length = 7
dl = DataLoader(train_x, train_y, patch_length = patch_length; suppress_info = true)</code></pre><p>Here we called <a href="../../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a> on a tensor and a vector of integers (targets) as input. <a href="../../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a> automatically converts the data to the correct input format for easy handling. This is visualized below:</p><object type="image/svg+xml" class="display-light-only" data=../mnist_visualization.png></object><object type="image/svg+xml" class="display-dark-only" data=../mnist_visualization_dark.png></object><p>Internally <a href="../../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a> calls <a href="#GeometricMachineLearning.split_and_flatten"><code>split_and_flatten</code></a> which splits each image into a number of <em>patches</em> according to the keyword arguments <code>patch_length</code> and <code>number_of_patches</code>. We also load the test data with <a href="../../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a>:</p><pre><code class="language-julia hljs">dl_test = DataLoader(test_x, test_y, patch_length=patch_length)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"><span class="sgr36"><span class="sgr1">[ Info: </span></span>You provided a tensor and a vector as input. This will be treated as a classification problem (MNIST). Tensor axes: (i) &amp; (ii) image axes and (iii) parameter dimension.</code></pre><p>We next define the model with which we want to train:</p><pre><code class="language-julia hljs">const n_heads = 7
const L = 16
const add_connection = false

model1 = ClassificationTransformer(dl;
                                    n_heads = n_heads,
                                    L = L,
                                    add_connection = add_connection,
                                    Stiefel = false)
model2 = ClassificationTransformer(dl;
                                    n_heads = n_heads,
                                    L = L,
                                    add_connection = add_connection,
                                    Stiefel = true)</code></pre><p>Here we have chosen a <a href="../../../architectures/transformer/#GeometricMachineLearning.ClassificationTransformer"><code>ClassificationTransformer</code></a>, i.e. a composition of a specific number of transformer layers composed with a classification layer. We also set the <em>Stiefel option</em> to <code>true</code>, i.e. we are optimizing on the Stiefel manifold.</p><p>We now have to initialize the neural network weights. This is done with the constructor for <code>NeuralNetwork</code>:</p><pre><code class="language-julia hljs">backend = GeometricMachineLearning.get_backend(dl)
T = eltype(dl)
nn1 = NeuralNetwork(model1, backend, T)
nn2 = NeuralNetwork(model2, backend, T)</code></pre><p>We still have to initialize the optimizers:</p><pre><code class="language-julia hljs">const batch_size = 2048
const n_epochs = 500
# an instance of batch is needed for the optimizer
batch = Batch(batch_size, dl)

opt1 = Optimizer(AdamOptimizer(T), nn1)
opt2 = Optimizer(AdamOptimizer(T), nn2)</code></pre><p>And with this we can finally perform the training:</p><pre><code class="language-julia hljs">loss_array1 = opt1(nn1, dl, batch, n_epochs, FeedForwardLoss())
loss_array2 = opt2(nn2, dl, batch, n_epochs, FeedForwardLoss())</code></pre><p>We furthermore optimize the second neural network (with weights on the manifold) with the <a href="../../../optimizers/optimizer_methods/#GeometricMachineLearning.GradientOptimizer"><code>GradientOptimizer</code></a> and the <a href="../../../optimizers/optimizer_methods/#GeometricMachineLearning.MomentumOptimizer"><code>MomentumOptimizer</code></a>:</p><pre><code class="language-julia hljs">nn3 = NeuralNetwork(model2, backend, T)
nn4 = NeuralNetwork(model2, backend, T)

opt3 = Optimizer(GradientOptimizer(T(0.001)), nn3)
opt4 = Optimizer(MomentumOptimizer(T(0.001), T(0.5)), nn4)</code></pre><p>For training we use the same data, the same batch and the same number of epochs:</p><pre><code class="language-julia hljs">loss_array3 = opt3(nn3, dl, batch, n_epochs, FeedForwardLoss())
loss_array4 = opt4(nn4, dl, batch, n_epochs, FeedForwardLoss())</code></pre><p>And we get the following result:</p><object type="image/svg+xml" class="display-light-only" data=../mnist_training_loss.png></object><object type="image/svg+xml" class="display-dark-only" data=../mnist_training_loss_dark.png></object><div class="admonition is-success"><header class="admonition-header">Remark</header><div class="admonition-body"><p>We see that the loss value for the Adam optimizer without parameters on the Stiefel manifold is stuck at around 1.34 which means that it <em>always predicts the same value</em>. So in 1 out of ten cases we have error 0 and in 9 out of ten cases we have error <span>$\sqrt{2}$</span>, giving</p><p class="math-container">\[    \sqrt{2\frac{9}{10}} = 1.342,\]</p><p>which is what we see in the error plot.</p></div></div><p>We can also call <a href="#GeometricMachineLearning.accuracy"><code>GeometricMachineLearning.accuracy</code></a> to obtain the test accuracy instead of the training error:</p><pre><code class="language-julia hljs">(accuracy(nn1, dl_test), accuracy(nn2, dl_test), accuracy(nn3, dl_test), accuracy(nn4, dl_test))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(0.0974, 0.8613, 0.5518, 0.6351)</code></pre><div class="admonition is-success"><header class="admonition-header">Remark</header><div class="admonition-body"><p>We note here that conventional convolutional neural networks and other vision transformers achieve much better accuracy on MNIST in a training time that is often shorter than what we presented here. Our aim here is not to outperform existing neural networks in terms of accuracy on image classification problems, but to demonstrate two things: (i) in many cases putting weights on the Stiefel manifold (which is a compact space) can enable training that would otherwise not be possible and (ii) as is the case with standard Adam, the manifold version also seems to achieve similar performance gain over the gradient and momentum optimizer. Both of these observations are demonstrated figure above.</p></div></div><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.split_and_flatten" href="#GeometricMachineLearning.split_and_flatten"><code>GeometricMachineLearning.split_and_flatten</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">split_and_flatten(input::AbstractArray)::AbstractArray</code></pre><p>Perform a preprocessing of an image into <em>flattened patches</em>.</p><p>This rearranges the input data so that it can easily be processed with a transformer.</p><p><strong>Examples</strong></p><p>Consider a matrix of size <span>$6\times6$</span> which we want to divide into patches of size <span>$3\times3$</span>.</p><pre><code class="language-julia hljs">using GeometricMachineLearning

input = [ 1  2  3  4  5  6; 
          7  8  9 10 11 12; 
         13 14 15 16 17 18;
         19 20 21 22 23 24; 
         25 26 27 28 29 30; 
         31 32 33 34 35 36]

split_and_flatten(input; patch_length = 3, number_of_patches = 4)

# output

9×4 Matrix{Int64}:
  1  19   4  22
  7  25  10  28
 13  31  16  34
  2  20   5  23
  8  26  11  29
 14  32  17  35
  3  21   6  24
  9  27  12  30
 15  33  18  36</code></pre><p>Here we see that <code>split_and_flatten</code>:</p><ol><li><em>splits</em> the original matrix into four <span>$3\times3$</span> matrices and then </li><li><em>flattens</em> each matrix into a column vector of size <span>$9.$</span></li></ol><p>After this all the vectors are put together again to yield a <span>$9\times4$</span> matrix.</p><p><strong>Arguments</strong></p><p>The optional keyword arguments are: </p><ul><li><code>patch_length</code>: by default this is 7. </li><li><code>number_of_patches</code>: by default this is 16.</li></ul><p>The sizes of the first and second axis of the output of <code>split_and_flatten</code> are </p><ol><li><span>$\mathtt{path\_length}^2$</span> and </li><li><code>number_of_patches</code>.</li></ol></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/543c16821f7d5cfab3295ed490c83d5132e2735b/src/data_loader/mnist_utils.jl#LL80-L131">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.accuracy" href="#GeometricMachineLearning.accuracy"><code>GeometricMachineLearning.accuracy</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">accuracy(model, ps, dl)</code></pre><p>Compute the accuracy of a neural network classifier.</p><p>This needs an instance of <a href="../../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a> that stores the <em>test data</em>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/543c16821f7d5cfab3295ed490c83d5132e2735b/src/data_loader/data_loader.jl#LL479-L485">source</a></section><section><div><pre><code class="language-julia hljs">accuracy(nn, dl)</code></pre><p>Compute the accuracy of a neural network classifier.</p><p>This is like <a href="#GeometricMachineLearning.accuracy"><code>accuracy(::Chain, ::Tuple, ::DataLoader)</code></a>, but for a <code>NeuralNetwork</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/543c16821f7d5cfab3295ed490c83d5132e2735b/src/data_loader/data_loader.jl#LL497-L503">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.onehotbatch" href="#GeometricMachineLearning.onehotbatch"><code>GeometricMachineLearning.onehotbatch</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">onehotbatch(target)</code></pre><p>Performs a one-hot-batch encoding of a vector of integers: <span>$input\in\{0,1,\ldots,9\}^\ell$</span>. </p><p>The output is a tensor of shape <span>$10\times1\times\ell$</span>.</p><p>If the input is <span>$0$</span>, this function produces:</p><p class="math-container">\[0 \mapsto \begin{bmatrix} 1 &amp; 0 &amp; \ldots &amp; 0 \end{bmatrix}^T.\]</p><p>In more abstract terms: <span>$i \mapsto e_i$</span>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using GeometricMachineLearning: onehotbatch

target = [0]
onehotbatch(target)

# output

10×1×1 Array{Int64, 3}:
[:, :, 1] =
 1
 0
 0
 0
 0
 0
 0
 0
 0
 0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/543c16821f7d5cfab3295ed490c83d5132e2735b/src/data_loader/mnist_utils.jl#LL6-L42">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.ClassificationLayer" href="#GeometricMachineLearning.ClassificationLayer"><code>GeometricMachineLearning.ClassificationLayer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ClassificationLayer(input_dim, output_dim, activation)</code></pre><p>Make an instance of <code>ClassificationLayer</code>.</p><p><code>ClassificationLayer</code> takes a matrix as an input and returns a vector that is used for classification. </p><p>It does:</p><p class="math-container">\[    X \mapsto \sigma(\mathtt{compute\_vector}(AX)),\]</p><p>where <span>$X$</span> is a matrix and <span>$\mathtt{compute\_vector}$</span> specifices how this matrix is turned into a vector. </p><p><span>$\mathtt{compute\_vector}$</span> can be specified with the keyword <code>average</code>.</p><p><strong>Arguments</strong></p><p><code>ClassificationLayer</code> has the following optional keyword argument: </p><ul><li><code>average:Bool=false</code>.</li></ul><p>If this keyword argument is set to <code>true</code>, then the output is computed as </p><p class="math-container">\[    input \mapsto \frac{1}{N}\sum_{i=1}^N[\mathcal{NN}(input)]_{\bullet{}i}.\]</p><p>If set to <code>false</code> (the default) it picks the last column of the input. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using GeometricMachineLearning

l = ClassificationLayer(2, 2, identity; average = true)
ps = (weight = [1 0; 0 1], )

input = [1 2 3; 1 1 1]

l(input, ps)

# output

2×1 Matrix{Float64}:
 2.0
 1.0</code></pre><pre><code class="language-julia hljs">using GeometricMachineLearning

l = ClassificationLayer(2, 2, identity; average = false)
ps = (weight = [1 0; 0 1], )

input = [1 2 3; 1 1 1]

l(input, ps)

# output

2×1 Matrix{Int64}:
 3
 1</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/543c16821f7d5cfab3295ed490c83d5132e2735b/src/layers/classification.jl#LL1-L64">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[7]</dt><dd><div>B. Brantner. <em>Generalizing Adam To Manifolds For Efficiently Training Transformers</em>, arXiv preprint arXiv:2305.16901 (2023).</div></dd></dl></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>When <a href="../../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a> is called this way it uses <a href="#GeometricMachineLearning.split_and_flatten"><code>split_and_flatten</code></a> internally.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../symplectic_autoencoder/">« Symplectic Autoencoders</a><a class="docs-footer-nextpage" href="../../grassmann_layer/">Grassmann Manifold »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Thursday 28 November 2024 16:20">Thursday 28 November 2024</span>. Using Julia version 1.11.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
