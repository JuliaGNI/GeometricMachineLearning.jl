<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Symplectic Autoencoders · GeometricMachineLearning.jl</title><meta name="title" content="Symplectic Autoencoders · GeometricMachineLearning.jl"/><meta property="og:title" content="Symplectic Autoencoders · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Symplectic Autoencoders · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/symplectic_autoencoder/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/symplectic_autoencoder/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/symplectic_autoencoder/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li></ul></li><li><span class="tocitem">Optimizer Framework</span><ul><li><a class="tocitem" href="../../Optimizer/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/general_optimization/">General Optimization</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Optimizer Functions</span><ul><li><a class="tocitem" href="../../optimizers/manifold_related/horizontal_lift/">Horizontal Lift</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/global_sections/">Global Sections</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/geodesic/">Geodesic Retraction</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/cayley/">Cayley Retraction</a></li><li><a class="tocitem" href="../../optimizers/adam_optimizer/">Adam Optimizer</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Gradient Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li></ul></li><li><span class="tocitem">Reduced Order Modelling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/autoencoder/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_autoencoder/">PSD and Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/kolmogorov_n_width/">Kolmogorov n-width</a></li><li><a class="tocitem" href="../../reduced_order_modeling/projection_reduction_errors/">Projection and Reduction Error</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../sympnet_tutorial/">Sympnets</a></li><li class="is-active"><a class="tocitem" href>Symplectic Autoencoders</a><ul class="internal"><li><a class="tocitem" href="#The-system"><span>The system</span></a></li><li><a class="tocitem" href="#Get-the-data"><span>Get the data</span></a></li><li><a class="tocitem" href="#Train-the-network"><span>Train the network</span></a></li><li><a class="tocitem" href="#The-online-stage"><span>The online stage</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../grassmann_layer/">Grassmann manifold</a></li><li><a class="tocitem" href="../volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../library/">Library</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Symplectic Autoencoders</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Symplectic Autoencoders</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/tutorials/symplectic_autoencoder.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Symplectic-Autoencoders-and-the-Toda-Lattice"><a class="docs-heading-anchor" href="#Symplectic-Autoencoders-and-the-Toda-Lattice">Symplectic Autoencoders and the Toda Lattice</a><a id="Symplectic-Autoencoders-and-the-Toda-Lattice-1"></a><a class="docs-heading-anchor-permalink" href="#Symplectic-Autoencoders-and-the-Toda-Lattice" title="Permalink"></a></h1><p>In this tutorial we use a <a href="../../library/#GeometricMachineLearning.SymplecticAutoencoder">SymplecticAutoencoder</a> to approximate the linear wave equation with a lower-dimensional Hamiltonian model and compare it with standard proper symplectic decomposition (PSD).</p><h2 id="The-system"><a class="docs-heading-anchor" href="#The-system">The system</a><a id="The-system-1"></a><a class="docs-heading-anchor-permalink" href="#The-system" title="Permalink"></a></h2><p>The <a href="https://juliagni.github.io/GeometricProblems.jl/latest/toda_lattice">Toda lattice</a> is a prototypical example of a Hamiltonian PDE. It is described by </p><p class="math-container">\[    H(q, p) = \sum_{n\in\mathbb{Z}}\left(  \frac{p_n^2}{2} + \alpha e^{q_n - q_{n+1}} \right).\]</p><p>We further assume a finite number of particles <span>$N$</span> and impose periodic boundary conditions: </p><p class="math-container">\[\begin{aligned}
    q_{n+N} &amp;  \equiv q_n \\ 
    p_{n+N} &amp;   \equiv p_n.
\end{aligned}\]</p><p>In this tutorial we want to reduce the dimension of the big system by a significant factor with (i) proper symplectic decomposition (PSD) and (ii) symplectic autoencoders. The first approach is strictly linear whereas the second one allows for more general mappings. </p><h3 id="Using-the-Toda-lattice-in-numerical-experiments"><a class="docs-heading-anchor" href="#Using-the-Toda-lattice-in-numerical-experiments">Using the Toda lattice in numerical experiments</a><a id="Using-the-Toda-lattice-in-numerical-experiments-1"></a><a class="docs-heading-anchor-permalink" href="#Using-the-Toda-lattice-in-numerical-experiments" title="Permalink"></a></h3><p>In order to use the Toda lattice in numerical experiments we have to pick suitable initial conditions. For this, consider the <a href="https://juliagni.github.io/GeometricProblems.jl/latest/initial_condition">third-degree spline</a>: </p><p class="math-container">\[h(s)  = \begin{cases}
        1 - \frac{3}{2}s^2 + \frac{3}{4}s^3 &amp; \text{if } 0 \leq s \leq 1 \\ 
        \frac{1}{4}(2 - s)^3 &amp; \text{if } 1 &lt; s \leq 2 \\ 
        0 &amp; \text{else.} 
\end{cases}\]</p><p>Plotted on the relevant domain it looks like this: </p><object type="image/svg+xml" class="display-light-only" data=../../tikz/third_degree_spline.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/third_degree_spline_dark.png></object><p>We end up with the following choice of parametrized initial conditions: </p><p class="math-container">\[u_0(\mu)(\omega) = h(s(\omega, \mu)), \quad s(\omega, \mu) =  20 \mu  |\omega + \frac{\mu}{2}|.\]</p><p>For the purposes of this tutorial we will use the default value for <span>$\mu$</span> provided in <code>GeometricMachineLearning</code>:</p><pre><code class="language-julia hljs">using GeometricProblems.TodaLattice: μ

μ</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.3</code></pre><h2 id="Get-the-data"><a class="docs-heading-anchor" href="#Get-the-data">Get the data</a><a id="Get-the-data-1"></a><a class="docs-heading-anchor-permalink" href="#Get-the-data" title="Permalink"></a></h2><p>The training data can very easily be obtained by using the packages <a href="https://github.com/JuliaGNI/GeometricProblems.jl"><code>GeometricProblems</code></a> and <a href="https://github.com/JuliaGNI/GeometricIntegrators.jl"><code>GeometricIntegrators</code></a>:</p><pre><code class="language-julia hljs">using GeometricProblems.TodaLattice: hodeproblem
using GeometricIntegrators: integrate, ImplicitMidpoint
using GeometricMachineLearning
using Plots
import Random

pr = hodeproblem(; tspan = (0.0, 100.))
sol = integrate(pr, ImplicitMidpoint())
dl = DataLoader(sol; autoencoder = true)

dl.input_dim</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">400</code></pre><p>Here we first integrate the system with implicit midpoint and then put the training data into the right format by calling <code>DataLoader</code>. We can get the dimension of the system by calling <code>dl.input_dim</code>. Also note that the keyword <code>autoencoder</code> was set to true.</p><h2 id="Train-the-network"><a class="docs-heading-anchor" href="#Train-the-network">Train the network</a><a id="Train-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Train-the-network" title="Permalink"></a></h2><p>We now want to compare two different approaches: <a href="../../library/#GeometricMachineLearning.PSDArch">PSDArch</a> and <a href="../../library/#GeometricMachineLearning.SymplecticAutoencoder">SymplecticAutoencoder</a>. For this we first have to set up the networks: </p><pre><code class="language-julia hljs">const reduced_dim = 2

psd_arch = PSDArch(dl.input_dim, reduced_dim)
sae_arch = SymplecticAutoencoder(dl.input_dim, reduced_dim; n_encoder_blocks = 4, n_decoder_blocks = 4, n_encoder_layers = 4, n_decoder_layers = 1)

Random.seed!(123)
psd_nn = NeuralNetwork(psd_arch)
sae_nn = NeuralNetwork(sae_arch)</code></pre><p>Training a neural network is usually done by calling an instance of <a href="../../Optimizer/#Optimizer">Optimizer</a> in <code>GeometricMachineLearning</code>. <a href="../../library/#GeometricMachineLearning.PSDArch">PSDArch</a> however can be solved directly by using singular value decomposition and this is done by calling <a href="../../library/#GeometricMachineLearning.solve!-Tuple{NeuralNetwork{&lt;:PSDArch}, AbstractMatrix}">solve!</a>. The <code>SymplecticAutoencoder</code> we train with the <a href="../../library/#GeometricMachineLearning.AdamOptimizer">AdamOptimizer</a> however: </p><pre><code class="language-julia hljs">const n_epochs = 8
const batch_size = 16

o = Optimizer(sae_nn, AdamOptimizer(Float64))

psd_error = solve!(psd_nn, dl)
sae_error = o(sae_nn, dl, Batch(batch_size), n_epochs)

hline([psd_error]; color = 2, label = &quot;PSD error&quot;)
plot!(sae_error; color = 3, label = &quot;SAE error&quot;, xlabel = &quot;epoch&quot;, ylabel = &quot;training error&quot;)</code></pre><img src="e80b6398.svg" alt="Example block output"/><h2 id="The-online-stage"><a class="docs-heading-anchor" href="#The-online-stage">The online stage</a><a id="The-online-stage-1"></a><a class="docs-heading-anchor-permalink" href="#The-online-stage" title="Permalink"></a></h2><p>After having trained our neural network we can now evaluate it in the online stage of reduced complexity modeling: </p><pre><code class="language-julia hljs">psd_rs = HRedSys(pr, encoder(psd_nn), decoder(psd_nn); integrator = ImplicitMidpoint())
sae_rs = HRedSys(pr, encoder(sae_nn), decoder(sae_nn); integrator = ImplicitMidpoint())

projection_error(psd_rs)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.6172687774821377</code></pre><pre><code class="language-julia hljs">projection_error(sae_rs)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.11220703644622755</code></pre><p>Next we plot a comparison between the PSD prediction and the symplectic autoencoder prediction: </p><pre><code class="language-julia hljs">sol_full = integrate_full_system(psd_rs)
sol_psd_reduced = integrate_reduced_system(psd_rs)
sol_sae_reduced = integrate_reduced_system(sae_rs)

const t_step = 100
plot(sol_full.s.q[t_step], label = &quot;Implicit Midpoint&quot;)
plot!(psd_rs.decoder((q = sol_psd_reduced.s.q[t_step], p = sol_psd_reduced.s.p[t_step])).q, label = &quot;PSD&quot;)
plot!(sae_rs.decoder((q = sol_sae_reduced.s.q[t_step], p = sol_sae_reduced.s.p[t_step])).q, label = &quot;SAE&quot;)</code></pre><img src="25132818.svg" alt="Example block output"/><p>We can see that the autoencoder approach has much more approximation capabilities than the psd approach. The jiggly lines are due to the fact that training was done for only 8 epochs. </p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[34]</dt><dd><div>P. Buchfink, S. Glas and B. Haasdonk. <em>Symplectic model reduction of Hamiltonian systems on nonlinear manifolds and approximation with weakly symplectic autoencoder</em>. SIAM Journal on Scientific Computing <strong>45</strong>, A289–A311 (2023).</div></dd><dt>[35]</dt><dd><div>L. Peng and K. Mohseni. <em>Symplectic model reduction of Hamiltonian systems</em>. SIAM Journal on Scientific Computing <strong>38</strong>, A1–A27 (2016).</div></dd><dt>[36]</dt><dd><div>C. Greif and K. Urban. <em>Decay of the Kolmogorov N-width for wave problems</em>. Applied Mathematics Letters <strong>96</strong>, 216–222 (2019).</div></dd></dl></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../sympnet_tutorial/">« Sympnets</a><a class="docs-footer-nextpage" href="../mnist_tutorial/">MNIST »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.4.1 on <span class="colophon-date" title="Tuesday 11 June 2024 13:34">Tuesday 11 June 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
