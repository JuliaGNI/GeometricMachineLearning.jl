<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>(Volume-Preserving) Attention · GeometricMachineLearning.jl</title><meta name="title" content="(Volume-Preserving) Attention · GeometricMachineLearning.jl"/><meta property="og:title" content="(Volume-Preserving) Attention · GeometricMachineLearning.jl"/><meta property="twitter:title" content="(Volume-Preserving) Attention · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/layers/attention_layer/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/layers/attention_layer/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/layers/attention_layer/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li class="is-active"><a class="tocitem" href>(Volume-Preserving) Attention</a><ul class="internal"><li><a class="tocitem" href="#Reweighting-of-the-Input-Sequence"><span>Reweighting of the Input Sequence</span></a></li><li><a class="tocitem" href="#Volume-Preserving-Attention"><span>Volume-Preserving Attention</span></a></li><li><a class="tocitem" href="#How-is-Structure-Preserved?"><span>How is Structure Preserved?</span></a></li><li><a class="tocitem" href="#Historical-Note"><span>Historical Note</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Special Neural Network Layers</a></li><li class="is-active"><a href>(Volume-Preserving) Attention</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>(Volume-Preserving) Attention</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/layers/attention_layer.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="The-Attention-Layer"><a class="docs-heading-anchor" href="#The-Attention-Layer">The Attention Layer</a><a id="The-Attention-Layer-1"></a><a class="docs-heading-anchor-permalink" href="#The-Attention-Layer" title="Permalink"></a></h1><p>The <em>attention</em> mechanism was originally developed for image recognition and natural language processing (NLP) tasks. It is motivated by the need to handle time series data in an efficient way<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>. Its essential idea is to compute correlations between vectors in input sequences. So given two sequences</p><p class="math-container">\[(z_q^{(1)}, z_q^{(2)}, \ldots, z_q^{(T)}) \text{ and } (z_k^{(1)}, z_k^{(2)}, \ldots, z_k^{(T)}),\]</p><p>an attention mechanism computes pair-wise correlations between all combinations of two input vectors from these sequences. In [<a href="../../references/#bahdanau2014neural">53</a>] &quot;additive&quot; attention is used to compute such correlations: </p><p class="math-container">\[(z_q, z_k) \mapsto v^T\sigma(Wz_q + Uz_k), \]</p><p>where <span>$z_q, z_k \in \mathbb{R}^d$</span> are elements of the input sequences. The learnable parameters are <span>$W, U \in \mathbb{R}^{n\times{}d}$</span> and <span>$v \in \mathbb{R}^n$</span>.</p><p>However <em>multiplicative attention</em> [<a href="../../references/#vaswani2017attention">54</a>] is more straightforward to interpret and cheaper to handle computationally: </p><p class="math-container">\[(z_q, z_k) \mapsto z_q^TWz_k,\]</p><p>where <span>$W \in \mathbb{R}^{d\times{}d}$</span> is a learnable weight matrix with respect to which correlations are computed as scalar products. Regardless of the type of attention used, they all try to compute correlations among input sequences on whose basis further computation is performed. Given two input sequences <span>$Z_q = (z_q^{(1)}, \ldots, z_q^{(T)})$</span> and <span>$Z_k = (z_k^{(1)}, \ldots, z_k^{(T)})$</span>, we can arrange the various correlations into a <em>correlation matrix</em> <span>$C\in\mathbb{R}^{T\times{}T}$</span> with entries <span>$[C]_{ij} = \mathtt{attention}(z_q^{(i)}, z_k^{(j)})$</span>. In the case of multiplicative attention this matrix is just <span>$C = Z^TWZ$</span>.</p><div class="admonition is-success"><header class="admonition-header">Remark</header><div class="admonition-body"><p>The notation with designating different vectors with a <span>$q$</span> and <span>$k$</span> label comes from natural language processing. These labels stand for <em>queries</em> and <em>keys</em></p></div></div><p>In the section on <a href="../multihead_attention_layer/#Multihead-Attention">multihead attention</a> this will be explained further.</p><h2 id="Reweighting-of-the-Input-Sequence"><a class="docs-heading-anchor" href="#Reweighting-of-the-Input-Sequence">Reweighting of the Input Sequence</a><a id="Reweighting-of-the-Input-Sequence-1"></a><a class="docs-heading-anchor-permalink" href="#Reweighting-of-the-Input-Sequence" title="Permalink"></a></h2><p>In <code>GeometricMachineLearning</code> we always compute <em>self-attention</em>, meaning that the two input sequences <span>$Z_q$</span> and <span>$Z_k$</span> are the same, i.e. <span>$Z = Z_q = Z_k$</span>.<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup></p><p>This is then used to reweight the columns in the input sequence <span>$Z$</span>. For this we first apply a nonlinearity <span>$\sigma$</span> onto <span>$C$</span> and then multiply <span>$\sigma(C)$</span> onto <span>$Z$</span> from the right, i.e. the output of the attention layer is <span>$Z\sigma(C)$</span>. So we perform the following mappings:</p><p class="math-container">\[Z \xrightarrow{\mathrm{correlations}} C(Z) =: C \xrightarrow{\sigma} \sigma(C) \xrightarrow{\text{right multiplication}} Z \sigma(C).\]</p><p>After the right multiplication the outputs is of the following form: </p><p class="math-container">\[    [\sum_{i=1}^Tp^{(1)}_iz^{(i)}, \ldots, \sum_{i=1}^Tp^{(T)}_iz^{(i)}],\]</p><p>for <span>$p^{(i)} = [\sigma(C)]_{\bullet{}i}$</span>. What is <em>learned</em> during training are <span>$T$</span> different linear combinations of the input vectors, where the coefficients <span>$p^{(i)}_j$</span> in these linear combinations depend on the input <span>$Z$</span> nonlinearly. </p><h2 id="Volume-Preserving-Attention"><a class="docs-heading-anchor" href="#Volume-Preserving-Attention">Volume-Preserving Attention</a><a id="Volume-Preserving-Attention-1"></a><a class="docs-heading-anchor-permalink" href="#Volume-Preserving-Attention" title="Permalink"></a></h2><p>The <a href="#GeometricMachineLearning.VolumePreservingAttention"><code>VolumePreservingAttention</code></a> layer (and the activation function <span>$\sigma$</span> defined for it) in <code>GeometricMachineLearning</code> was specifically designed to apply it to data coming from physical systems that can be described through a divergence-free or a symplectic vector field.  Traditionally the nonlinearity in the attention mechanism is a softmax<sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup> [<a href="../../references/#vaswani2017attention">54</a>] and the self-attention layer performs the following mapping: </p><p class="math-container">\[Z := [z^{(1)}, \ldots, z^{(T)}] \mapsto Z\mathrm{softmax}(Z^TWZ).\]</p><p>The softmax activation acts vector-wise, i.e. if we supply it with a matrix <span>$C$</span> as input it returns: </p><p class="math-container">\[\mathrm{softmax}(C) = [\mathrm{softmax}(c_{\bullet{}1}), \ldots, \mathrm{softmax}(c_{\bullet{}T})].\]</p><p>The output of a softmax is a <em>probability vector</em> (also called <em>stochastic vector</em>) and the matrix <span>$Y = [y^{(1)}, \ldots, y^{(T)}]$</span>, where each column is a probability vector, is sometimes referred to as a &quot;stochastic matrix&quot; [<a href="../../references/#jacobs1992discrete">55</a>]. This attention mechanism finds application in <em>transformer neural networks</em> [<a href="../../references/#vaswani2017attention">54</a>]. The problem with this matrix from a geometric point of view is that all the columns are independent of each other and the nonlinear transformation could in theory produce a stochastic matrix for which all columns are identical and thus lead to a loss of information. So the softmax activation function is inherently non-geometric. We visualize this with the figure below: </p><object type="image/svg+xml" class="display-light-only" data=../../tikz/convex_recombination.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/convex_recombination_dark.png></object><p>So the <span>$y$</span> coefficients responsible for producing the first output vector are independent from those producing the second output vector etc., they have the condition <span>$\sum_{i=1}^Ty^{(j)}_iz_\mu^{(i)}$</span> for each column <span>$j$</span> imposed on them, but the coefficients for two different columns are independent of each other.</p><div class="admonition is-success"><header class="admonition-header">Remark</header><div class="admonition-body"><p>We said that the coefficients are <em>independent of each other</em>, which means that in theory, they could be the same or ill-conditioned, i.e. we could have</p><p class="math-container">\[\mathrm{det}(\mathrm{softmax}(C)) \approx 0.\]</p><p>With <em>volume-preserving attention</em> we make the coefficients dependent of each other, such that the columns of <span>$\sigma(C)$</span> are *independent of each other:</p><p class="math-container">\[\sigma(C)^T\sigma(C) = \mathbb{I},\]</p><p>which further implies <span>$\mathrm{det}(\sigma(C)) = 1$</span>.</p></div></div><p>Besides the traditional attention mechanism <code>GeometricMachineLearning</code> therefore also has a <em>volume-preserving transformation</em> that fulfills a similar role, but imposes additional structure on the <span>$y$</span> coefficients. There are two approaches implemented to realize this <em>volume-preserving transformation</em>. Both of them however utilize the <em>Cayley transform</em> to produce orthogonal matrices instead of stochastic matrices. For an orthogonal matrix <span>$\Sigma$</span> we have <span>$\Sigma^T\Sigma = \mathbb{I}$</span>, so all the columns are linearly independent, which is not necessarily true for a stochastic matrix <span>$P$</span>. In the following we explain how this new activation function is implemented. First we need to briefly discuss the <em>Cayley transform</em>. </p><h3 id="The-Cayley-Transform"><a class="docs-heading-anchor" href="#The-Cayley-Transform">The Cayley Transform</a><a id="The-Cayley-Transform-1"></a><a class="docs-heading-anchor-permalink" href="#The-Cayley-Transform" title="Permalink"></a></h3><p>The Cayley transform maps from skew-symmetric matrices to orthonormal matrices<sup class="footnote-reference"><a id="citeref-4" href="#footnote-4">[4]</a></sup>. It takes the form<sup class="footnote-reference"><a id="citeref-4" href="#footnote-4">[4]</a></sup>:</p><p class="math-container">\[\mathrm{Cayley}: A \mapsto (\mathbb{I} - A)(\mathbb{I} + A)^{-1}.\]</p><p>Analogously to when we used the Cayley transform <a href="../../optimizers/manifold_related/retractions/#Classical-Retractions">as a retraction</a>, we can easily check that <span>$\mathrm{Cayley}(A)$</span> is orthogonal if <span>$A$</span> is skew-symmetric. For this consider <span>$\varepsilon \mapsto A(\varepsilon)\in\mathcal{S}_\mathrm{skew}$</span> with <span>$A(0) = \mathbb{I}$</span> and <span>$A&#39;(0) = B$</span>. Then we have: </p><p class="math-container">\[\frac{\delta(\mathrm{Cayley}(A)^T\mathrm{Cayley}(A))}{\delta{}A} = \frac{d}{d\varepsilon}|_{\varepsilon=0} \mathrm{Cayley}(A(\varepsilon))^T \mathrm{Cayley}(A(\varepsilon)) = A&#39;(0)^T + A&#39;(0) = \mathbb{O},\]</p><p>So <span>$\mathrm{Cayley}(A)^T\mathrm{Cayley}(A)$</span> remains unchanged among <span>$\varepsilon$</span>. In order to use the Cayley transform as an activation function we further need a mapping from the input <span>$Z$</span> to a skew-symmetric matrix. This is realized in two ways in <code>GeometricMachineLearning</code>: via a scalar-product with a skew-symmetric weighting and via a scalar-product with an arbitrary weighting.</p><h3 id="First-approach:-scalar-products-with-a-skew-symmetric-weighting"><a class="docs-heading-anchor" href="#First-approach:-scalar-products-with-a-skew-symmetric-weighting">First approach: scalar products with a skew-symmetric weighting</a><a id="First-approach:-scalar-products-with-a-skew-symmetric-weighting-1"></a><a class="docs-heading-anchor-permalink" href="#First-approach:-scalar-products-with-a-skew-symmetric-weighting" title="Permalink"></a></h3><p>For this the attention layer is modified in the following way: </p><p class="math-container">\[Z := [z^{(1)}, \ldots, z^{(T)}] \mapsto Z\sigma(Z^TAZ),\]</p><p>where <span>$\sigma(C)=\mathrm{Cayley}(C)$</span> and <span>$A$</span> is a matrix of type <a href="../../arrays/skew_symmetric_matrix/#GeometricMachineLearning.SkewSymMatrix"><code>SkewSymMatrix</code></a> that is learnable, i.e. the parameters of the attention layer are stored in <span>$A$</span>.</p><h3 id="Second-approach:-scalar-products-with-an-arbitrary-weighting"><a class="docs-heading-anchor" href="#Second-approach:-scalar-products-with-an-arbitrary-weighting">Second approach: scalar products with an arbitrary weighting</a><a id="Second-approach:-scalar-products-with-an-arbitrary-weighting-1"></a><a class="docs-heading-anchor-permalink" href="#Second-approach:-scalar-products-with-an-arbitrary-weighting" title="Permalink"></a></h3><p>For this approach we compute correlations between the input vectors based on scalar product with an arbitrary weighting. This arbitrary <span>$T\times{}T$</span> matrix <span>$A$</span> constitutes the learnable parameters of the attention layer. The correlations we consider here are based on: </p><p class="math-container">\[(z^{(2)})^TAz^{(1)}, (z^{(3)})^TAz^{(1)}, \ldots, (z^{(d)})^TAz^{(1)}, (z^{(3)})^TAz^{(2)}, \ldots, (z^{(d)})^TAz^{(2)}, \ldots, (z^{(d)})^TAz^{(d-1)}.\]</p><p>So we consider correlations <span>$(z^{(i)})^Tz^{(j)}$</span> for which <span>$i &gt; j$</span>. We now arrange these correlations into a skew-symmetric matrix: </p><p class="math-container">\[C = \begin{bmatrix}
        0               &amp; -(z^{(2)})^TAz^{(1)} &amp; -(z^{(3)})^TAz^{(1)} &amp;     \ldots &amp; -(z^{(d)})^TAz^{(1)} \\
    (z^{(2)})^TAz^{(1)} &amp;       0              &amp; -(z^{(3)})^TAz^{(2)} &amp;     \ldots &amp; -(z^{(d)})^TAz^{(2)} \\
    \ldots              &amp;       \ldots         &amp;        \ldots        &amp;     \ldots &amp;    \ldots             \\
    (z^{(d)})^TAz^{(1)} &amp; (z^{(d)})^TAz^{(2)}  &amp; (z^{(d)})^TAz^{(3)}  &amp;     \ldots &amp;        0               
\end{bmatrix}.\]</p><p>This correlation matrix can now again be used as an input for the Cayley transform to produce an orthogonal matrix. Mathematically this is also equivalent to first computing all correlations <span>$Z^TAZ$</span> and then mapping the lower triangular to the upper triangular and negating these elements. This is visualized below: </p><object type="image/svg+xml" class="display-light-only" data=../../tikz/skew_sym_mapping.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/skew_sym_mapping_dark.png></object><p>Internally <code>GeometricMachineLearning</code> computes this more efficiently with the function <a href="#GeometricMachineLearning.tensor_mat_skew_sym_assign"><code>GeometricMachineLearning.tensor_mat_skew_sym_assign</code></a>. We show a comparison of the two approaches in the <a href="../../tutorials/volume_preserving_attention/#Comparing-Different-VolumePreservingAttention-Mechanisms">examples section</a>.</p><h2 id="How-is-Structure-Preserved?"><a class="docs-heading-anchor" href="#How-is-Structure-Preserved?">How is Structure Preserved?</a><a id="How-is-Structure-Preserved?-1"></a><a class="docs-heading-anchor-permalink" href="#How-is-Structure-Preserved?" title="Permalink"></a></h2><p>In order to discuss <em>how structure is preserved</em> we first have to define what <em>structure</em> we mean precisely. This structure is strongly inspired by traditional <em>multi-step methods</em> [<a href="../../references/#feng1998step">56</a>]. </p><div class="admonition is-success"><header class="admonition-header">Remark</header><div class="admonition-body"><p>We define what volume preservation means for the product space <span>$\mathbb{R}^{d}\times\cdots\times\mathbb{R}^{d}\equiv\times_\text{$T$ times}\mathbb{R}^{d}$</span>, i.e. <em>neural network multi-step methods</em> in our case are mappings whose domain and image are of the same dimension:</p><p class="math-container">\[\varphi: \times_\text{$T$ times}\mathbb{R}^{d} \to \times_\text{$T$ times}\mathbb{R}^{d}.\]</p><p>This is not the case for <em>traditional multi-step methods</em>; there one usually has a number <span>$T&gt;1$</span> of input vectors, but only one output vector. There are however empirical and theoretical advantages of having equally-sized inputs and outputs.</p></div></div><p>Consider an isomorphism <span>$\hat{}: \times_\text{($T$ times)}\mathbb{R}^{d}\stackrel{\approx}{\longrightarrow}\mathbb{R}^{dT}$</span>. Specifically, we use:</p><p class="math-container">\[Z =  \left[\begin{array}{cccc}
            z_1^{(1)} &amp;  z_1^{(2)} &amp; \quad\cdots\quad &amp; z_1^{(T)} \\
            z_2^{(1)} &amp;  z_2^{(2)} &amp; \cdots &amp; z_2^{(T)} \\
            \cdots &amp;  \cdots &amp; \cdots &amp; \cdots \\
            z_d^{(1)} &amp; z_d^{(2)} &amp; \cdots &amp; z_d^{(T)}
            \end{array}\right] \mapsto 
            \left[\begin{array}{c}  z_1^{(1)} \\ z_1^{(2)} \\ \cdots \\ z_1^{(T)} \\ z_2^{(1)} \\ \cdots \\ z_d^{(T)} \end{array}\right] =: Z_\mathrm{vec},\]</p><p>so we arrange the rows consecutively into a vector. The inverse of <span>$Z \mapsto \hat{Z}$</span> we refer to as <span>$Y \mapsto \tilde{Y}$</span>. In the following we also write <span>$\hat{\varphi}$</span> for the mapping <span>$\,\hat{}\circ\varphi\circ\tilde{}\,$</span>.</p><div class="admonition is-info"><header class="admonition-header">Definition</header><div class="admonition-body"><p>We say that a mapping <span>$\varphi: \times_\text{$T$ times}\mathbb{R}^{d} \to \times_\text{$T$ times}\mathbb{R}^{d}$</span> is <strong>volume-preserving</strong> if the associated <span>$\hat{\varphi}$</span> is volume-preserving.</p></div></div><p>In the transformed coordinate system (in terms of the vector <span>$Z_\mathrm{vec}$</span> defined above) this is equivalent to multiplication by a sparse matrix <span>$\tilde\Lambda(Z)$</span> from the left:</p><p class="math-container">\[    \tilde{\Lambda}(Z) Z_\mathrm{vec} := (\Lambda(Z) \otimes \mathbb{I}_T) Z_\mathrm{vec} = 
    \begin{pmatrix}
    \Lambda(Z) &amp; \mathbb{O} &amp; \cdots  &amp; \mathbb{O} \\
    \mathbb{O} &amp; \Lambda(Z) &amp; \cdots &amp; \mathbb{O} \\
    \cdots &amp; \cdots &amp; \ddots &amp; \cdots \\ 
    \mathbb{O} &amp; \mathbb{O} &amp; \cdots &amp; \Lambda(Z) \\
    \end{pmatrix}
    \left[\begin{array}{c}  z_1^{(1)} \\ z_1^{(2)} \\ \ldots \\ z_1^{(T)} \\ z_2^{(1)} \\ \ldots \\ z_d^{(T)} \end{array}\right],\]</p><p>where <span>$\otimes:\mathbb{R}^{n\times{}n}\times\mathbb{R}^{p\times{}q} \to \mathbb{R}^{pm\times{}qn}$</span> is the <em>Kronecker product</em>. <span>$\tilde{\Lambda}(Z)$</span> is easily shown to be an orthogonal matrix and a symplectic matrix, i.e. it satisfies</p><p class="math-container">\[\tilde{\Lambda}(Z)^T\tilde{\Lambda}(Z) = \mathbb{I}\]</p><p>and</p><p class="math-container">\[\tilde{\Lambda}(Z)^T\mathbb{J}\tilde{\Lambda}(Z) = \mathbb{J}.\]</p><p>So <span>$\tilde{\Lambda}(Z)$</span> is both orthogonal and symplectic.</p><h2 id="Historical-Note"><a class="docs-heading-anchor" href="#Historical-Note">Historical Note</a><a id="Historical-Note-1"></a><a class="docs-heading-anchor-permalink" href="#Historical-Note" title="Permalink"></a></h2><p>Attention was used before <a href="../../architectures/transformer/#Standard-Transformer">the transformer</a> was introduced, but mostly in connection with <em>recurrent neural networks</em> see [<a href="../../references/#bahdanau2014neural">53</a>, <a href="../../references/#luong2015effective">57</a>]. </p><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.tensor_mat_skew_sym_assign" href="#GeometricMachineLearning.tensor_mat_skew_sym_assign"><code>GeometricMachineLearning.tensor_mat_skew_sym_assign</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">tensor_mat_skew_sym_assign(Z::AbstractArray{&lt;:Number, 3}, A::AbstractMatrix)</code></pre><p>Compute scalar products of columns of <span>$Z$</span> along the second axis.</p><p>The scalar products are weighted by <span>$A$</span>.</p><p>Scalar products are computed for any two vectors of the form <code>Z[:, i, k]</code> and <code>Z[:, j, k]</code>, i.e.</p><p class="math-container">\[    (z^{(i)}, z^{(j)}) \mapsto (z^{(i)})^TAz^{(j)} \text{ for } i &gt; j.\]</p><p>The result of this are <span>$n(n-2)\div2$</span> scalar products for each index <code>k</code> from the third axis. </p><p>These scalar products are written into a lower-triangular matrix and the final output of the function is a tensor of these lower-triangular matrices. </p><p>This is used in <a href="#GeometricMachineLearning.VolumePreservingAttention"><code>VolumePreservingAttention</code></a> when <code>skew_sym</code> is set to <code>false</code>.</p><p><strong>Examples</strong></p><p>Here we consider a weighting</p><p class="math-container">\[A = \begin{pmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 3\end{pmatrix}\]</p><p>and three sequences:</p><p class="math-container">\[Z_1 = \begin{pmatrix} 1 &amp; 1 \\ 0 &amp; 1 \\ 0 &amp; 1 \end{pmatrix},\quad Z_2 = \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}, \quad Z_3 = \begin{pmatrix} 0 &amp; 1 \\ 0 &amp; 1 \\ 1 &amp; 1 \end{pmatrix}.\]</p><p>The result of applying <code>tensor_mat_skew_sym_assign</code> is a tensor <span>$\in\mathbb{R}^{2\times2\times3}:$</span></p><pre><code class="language-julia hljs">using GeometricMachineLearning: tensor_mat_skew_sym_assign

A = [1 0 0; 0 2 0; 0 0 3]
Z = [1; 0; 0;; 1; 1; 1;;; 0; 1; 0;; 1; 1; 1;;; 0; 0; 1;; 1; 1; 1]

tensor_mat_skew_sym_assign(Z, A)

# output

2×2×3 Array{Int64, 3}:
[:, :, 1] =
 0  0
 1  0

[:, :, 2] =
 0  0
 2  0

[:, :, 3] =
 0  0
 3  0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/0545a139376ef2772e4c7de3244727395aaeae32/src/kernels/inverses/tensor_mat_skew_sym_assign.jl#LL27-L84">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.VolumePreservingAttention" href="#GeometricMachineLearning.VolumePreservingAttention"><code>GeometricMachineLearning.VolumePreservingAttention</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">VolumePreservingAttention(dim, seq_length)</code></pre><p>Make an instance of <code>VolumePreservingAttention</code> for a specific dimension and sequence length.</p><p>The sequence length is <code>0</code> by default. </p><p>Setting <code>seq_length</code> to <code>0</code> for all sequence lengths but does not apply the fast Cayley activation.</p><p><strong>Arguments</strong></p><p>The constructor can be called with an optional keyword argument: </p><ul><li><code>skew_sym::Bool = false</code>: specifies if the weight matrix is skew symmetric (<code>true</code>) or arbitrary (<code>false</code>).</li></ul><p><strong>Functor</strong></p><p>Applying a layer of type <code>VolumePreservingAttention</code> does the following: </p><ul><li>First we perform the operation <span>$Z \mapsto Z^T A Z =: C$</span>, where <span>$Z\in\mathbb{R}^{N\times\mathtt{seq\_length}}$</span> is a vector containing time series data and <span>$A$</span> is the skew symmetric matrix associated with the layer (if <code>skew_sym = true</code>). </li><li>In a second step we compute the Cayley transform of <span>$C$</span>; <span>$\Lambda = \mathrm{Cayley}(C)$</span>.</li><li>The output of the layer is then <span>$Z\Lambda$</span>.</li></ul><p><strong>Implementation</strong></p><p>The fast activation is only implemented for sequence lengths of 2, 3, 4 and 5.  Other sequence lengths only work on CPU (for now).</p><p>The fast Cayley activation is using inverses that have been computed symbolically.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/0545a139376ef2772e4c7de3244727395aaeae32/src/layers/volume_preserving_attention.jl#LL1-L27">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[53]</dt><dd><div>D. Bahdanau, K. Cho and Y. Bengio. <em>Neural machine translation by jointly learning to align and translate</em>, arXiv preprint arXiv:1409.0473 (2014).</div></dd><dt>[57]</dt><dd><div>M.-T. Luong, H. Pham and C. D. Manning. <em>Effective approaches to attention-based neural machine translation</em>, arXiv preprint arXiv:1508.04025 (2015).</div></dd></dl></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a><em>Recurrent neural networks</em> [<a href="../../references/#cardot2011recurrent">52</a>] have the same motivation. The have however been replaced by <a href="../../architectures/transformer/#Standard-Transformer">transformers</a>, of which <em>attention</em> is the most important component, in many applications.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a><a href="../multihead_attention_layer/#Multihead-Attention">Multihead attention</a> also falls into this category. Here the input <span>$Z$</span> is multiplied from the left with several <em>projection matrices</em> <span>$P^Q_i$</span> and <span>$P^K_i$</span>, where <span>$i$</span> indicates the <em>head</em>. For each head we then compute a correlation matrix <span>$(P^Q_i Z)^T(P^K Z)$</span>. </li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>The softmax acts on the matrix <span>$C$</span> in a vector-wise manner, i.e. it operates on each column of the input matrix <span>$C = [\mathrm{softmax}(c_{\bullet{}1}), \ldots, \mathrm{softmax}(c_{\bullet{}T})] \equiv [c^{(1)}, \ldots, c^{(T)}]$</span>. The result is a sequence of probability vectors <span>$[y^{(1)}, \ldots, y^{(T)}] = [\mathrm{softmax}(y^{(1)}), \ldots, \mathrm{softmax}(y^{(T)})]$</span> for which <span>$\sum_{i=1}^Ty^{(j)}_i=1\quad\forall{}j\in\{1,\dots,T\}.$</span></li><li class="footnote" id="footnote-4"><a class="tag is-link" href="#citeref-4">4</a>The Cayley transform here does not have the factor <span>$1/2$</span> hat we used when talking about the <a href="../../optimizers/manifold_related/retractions/#Classical-Retractions">Cayley retraction</a>. This is because now we do not need the retraction property <span>$d/dt\mathrm{Cayley}(tV)|_{t=0} = V$</span>, but only a map <span>$\mathfrak{g}\to{}G=SO(N).$</span></li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../volume_preserving_feedforward/">« Volume-Preserving Layers</a><a class="docs-footer-nextpage" href="../multihead_attention_layer/">Multihead Attention »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Thursday 13 February 2025 14:43">Thursday 13 February 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
