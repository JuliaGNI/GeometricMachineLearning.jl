var documenterSearchIndex = {"docs":
[{"location":"architectures/neural_network_integrators/#Neural-Network-Integrators","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"","category":"section"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"In GeometricMachineLearning we can divide most neural network architectures (that are used for applications to physical systems) into two categories: autoencoders and integrators. This is also closely related to the application of reduced order modeling where autoencoders are used in the offline phase and integrators are used in the online phase.","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"The term integrator in its most general form refers to an approximation of the flow of an ODE by a numerical scheme. Traditionally, for so called one-step methods, these numerical schemes are constructed by defining certain relationships between a known time step z^(t) and a future unknown one z^(t+1) [1, 82]: ","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"    f(z^(t) z^(t+1)) = 0","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"One usually refers to such a relationship as an integration scheme. If this relationship can be reformulated as ","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"    z^(t+1) = g(z^(t))","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"then we refer to the scheme as explicit, if it cannot be reformulated in such a way then we refer to it as implicit. Implicit schemes are typically more expensive to solve than explicit ones. The Julia library GeometricIntegrators [2] offers a wide variety of integration schemes both implicit and explicit. ","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"The neural network integrators in GeometricMachineLearning (the corresponding type is NeuralNetworkIntegrator) are all explicit integration schemes where the function g above is modeled with a neural network.","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"Neural networks, as an alternative to traditional methods, are employed because of (i) potentially superior performance and (ii) an ability to learn unknown dynamics from data. ","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"The simplest of such a neural network for modeling an explicit integrator is the ResNet. SympNets can be seen as the symplectic version of the ResNet. There is an example demonstrating the performance of SympNets. This example demonstrates the advantages of symplectic neural networks.","category":"page"},{"location":"architectures/neural_network_integrators/#Multi-step-methods","page":"Neural Network Integrators","title":"Multi-step methods","text":"","category":"section"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"Multi-step method [58, 59] refers to schemes that are of the form[1]: ","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"[1]: We again assume that all the steps up to and including t are known.","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"    f(z^(t - mathttsl + 1) z^(t - mathttsl + 2) ldots z^(t) z^(t + 1) ldots z^(mathttpw + 1)) = 0","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"where sl is short for sequence length and pw is short for prediction window. Note that we can recover traditional one-step methods by setting sl and pw equal to 1. We can also formulate explicit mulit-step methods. They are of the form: ","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"z^(t+1) ldots z^(t+mathttpw) = g(z^(t - mathttsl + 1) ldots z^(t))","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"In GeometricMachineLearning all multi-step methods, as is the case with one-step methods, are explicit. There are essentially two ways to construct multi-step methods with neural networks: the older one is using recurrent neural networks such as long short-term memory cells (LSTMs) [83] and the newer one is using transformer neural networks [54]. Both of these approaches have been successfully employed to learn multi-step methods (see [61, 62] for the former and [4, 84, 85] for the latter), but because the transformer architecture exhibits superior performance on modern hardware and can be imbued with geometric properties we almost always use a transformer-derived architecture when dealing with time series[2].","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"[2]: GeometricMachineLearning also has an LSTM implementation, but this may be deprecated in the future. ","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"Explicit multi-step methods derived from the transformer are always subtypes of the type TransformerIntegrator in GeometricMachineLearning. In GeometricMachineLearning the standard transformer, the volume-preserving transformer and the linear symplectic transformer are implemented. ","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"Main.remark(raw\"For standard multi-step methods (that are not neural network-based) `sl` is generally a number greater than one whereas `pw = 1` in most cases. \n\" * Main.indentation * raw\"For the `TransformerIntegrator`s in `GeometricMachineLearning` however we usually have:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\mathtt{pw} = \\mathtt{sl},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"so the number of vectors in the input sequence is equal to the number of vectors in the output sequence. This makes it easier to define structure-preservation for these architectures and improves stability.\")","category":"page"},{"location":"architectures/neural_network_integrators/#Library-Functions","page":"Neural Network Integrators","title":"Library Functions","text":"","category":"section"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"NeuralNetworkIntegrator\nResNet\nGeometricMachineLearning.ResNetLayer\niterate(::NeuralNetwork{<:NeuralNetworkIntegrator}, ::BT) where {T, AT<:AbstractVector{T}, BT<:NamedTuple{(:q, :p), Tuple{AT, AT}}}\nTransformerIntegrator\niterate(::NeuralNetwork{<:TransformerIntegrator}, ::NamedTuple{(:q, :p), Tuple{AT, AT}}) where {T, AT<:AbstractMatrix{T}}","category":"page"},{"location":"architectures/neural_network_integrators/#GeometricMachineLearning.NeuralNetworkIntegrator","page":"Neural Network Integrators","title":"GeometricMachineLearning.NeuralNetworkIntegrator","text":"NeuralNetworkIntegrator is a super type of various neural network architectures such as SympNet and ResNet.\n\nThe purpose of such neural networks is to approximate the flow of an ordinary differential equation (ODE).\n\nNeuralNetworkIntegrators can be seen as modeling traditional one-step methods with neural networks, i.e. for a fixed time step they perform:\n\n    mathttNeuralNetworkIntegrator z^(t) mapsto z^(t+1)\n\nto try to approximate the flow of some ODE:\n\n     mathttIntegrator(z^(t)) - varphi^h(z^(t))  approx mathcalO(h)\n\nwhere varphi^h is the flow map of the ODE for a time step h.\n\n\n\n\n\n","category":"type"},{"location":"architectures/neural_network_integrators/#GeometricMachineLearning.ResNet","page":"Neural Network Integrators","title":"GeometricMachineLearning.ResNet","text":"ResNet(dim, n_blocks, activation)\n\nMake an instance of a ResNet.\n\nA ResNet is a neural network that realizes a mapping of the form: \n\n    x = mathcalNN(x) + x\n\nso the input is again added to the output (a so-called add connection).  In GeometricMachineLearning the specific ResNet that we use consists of a series of simple ResNetLayers.\n\nConstructor\n\nResNet can also be called with the constructor:\n\nResNet(dl, n_blocks)\n\nwhere dl is an instance of DataLoader.\n\nSee iterate for an example of this.\n\n\n\n\n\n","category":"type"},{"location":"architectures/neural_network_integrators/#GeometricMachineLearning.ResNetLayer","page":"Neural Network Integrators","title":"GeometricMachineLearning.ResNetLayer","text":"ResNetLayer(dim)\n\nMake an instance of the resnet layer.\n\nThe ResNetLayer is a simple feedforward neural network to which we add the input after applying it, i.e. it realizes x mapsto x + sigma(Ax + b).\n\nArguments\n\nThe ResNet layer takes the following arguments:\n\ndim::Integer: the system dimension.\nactivation = identity: The activation function.\n\nThe following is a keyword argument:\n\nuse_bias::Bool = true: This determines whether a bias b is used.\n\n\n\n\n\n","category":"type"},{"location":"architectures/neural_network_integrators/#Base.iterate-Union{Tuple{BT}, Tuple{AT}, Tuple{T}, Tuple{NeuralNetwork{<:NeuralNetworkIntegrator}, BT}} where {T, AT<:AbstractVector{T}, BT<:@NamedTuple{q::AT, p::AT}}","page":"Neural Network Integrators","title":"Base.iterate","text":"iterate(nn, ics)\n\nThis function computes a trajectory for a NeuralNetworkIntegrator that has already been trained for valuation purposes.\n\nIt takes as input: \n\nnn: a NeuralNetwork (that has been trained).\nics: initial conditions (a NamedTuple of two vectors)\n\nExamples\n\nTo demonstrate iterate we use a simple ResNet that does:\n\nmathrmResNet x mapsto beginpmatrix 1  0  0  0  2  0  0  0  1endpmatrixx + beginpmatrix 0  0  1 endpmatrix\n\nand we iterate three times with\n\n    mathttics = beginpmatrix 1  1  1 endpmatrix\n\nusing GeometricMachineLearning\n\nmodel = ResNet(3, 0, identity)\nweight = [1 0 0; 0 2 0; 0 0 1]\nbias = [0, 0, 1]\nps = NeuralNetworkParameters((L1 = (weight = weight, bias = bias), ))\nnn = NeuralNetwork(model, Chain(model), ps, CPU())\n\nics = [1, 1, 1]\niterate(nn, ics; n_points = 4)\n\n# output\n\n3×4 Matrix{Int64}:\n 1  2  4   8\n 1  3  9  27\n 1  3  7  15\n\nArguments\n\nThe optional keyword argument is \n\nn_points = 100\n\nThe number of integration steps that should be performed.\n\n\n\n\n\n","category":"method"},{"location":"architectures/neural_network_integrators/#GeometricMachineLearning.TransformerIntegrator","page":"Neural Network Integrators","title":"GeometricMachineLearning.TransformerIntegrator","text":"TransformerIntegrator <: Architecture\n\nEncompasses various transformer architectures, such as the VolumePreservingTransformer and the LinearSymplecticTransformer. \n\nThe central idea behind this is to construct an explicit multi-step integrator:\n\n    mathttIntegrator  z^(t - mathttsl + 1) z^(t - mathttsl + 2) ldots z^(t)  mapsto  z^(t + 1) z^(t + 2) ldots z^(t + mathttpw) \n\nwhere sl stands for sequence length and pw stands for prediction window, so the numbers of input and output vectors respectively.\n\n\n\n\n\n","category":"type"},{"location":"architectures/neural_network_integrators/#Base.iterate-Union{Tuple{AT}, Tuple{T}, Tuple{NeuralNetwork{<:TransformerIntegrator}, @NamedTuple{q::AT, p::AT}}} where {T, AT<:AbstractMatrix{T}}","page":"Neural Network Integrators","title":"Base.iterate","text":"iterate(nn, ics)\n\nIterate the neural network of type TransformerIntegrator for initial conditions ics.\n\nThe initial condition is a matrix inmathbbR^ntimesmathttseq_length or NamedTuple of two matrices).\n\nThis function computes a trajectory for a Transformer that has already been trained for valuation purposes.\n\nParameters\n\nThe following are optional keyword arguments:\n\nn_points::Int=100: The number of time steps for which we run the prediction.\nprediction_window::Int=size(ics.q, 2): The prediction window (i.e. the number of steps we predict into the future) is equal to the sequence length (i.e. the number of input time steps) by default.  \n\n\n\n\n\n","category":"method"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"\\begin{comment}","category":"page"},{"location":"architectures/neural_network_integrators/#References","page":"Neural Network Integrators","title":"References","text":"","category":"section"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"E. Hairer, C. Lubich and G. Wanner. Geometric Numerical integration: structure-preserving algorithms for ordinary differential equations (Springer, Heidelberg, 2006).\n\n\n\nB. Leimkuhler and S. Reich. Simulating hamiltonian dynamics. No. 14 (Cambridge university press, 2004).\n\n\n\nM. Kraus. GeometricIntegrators.jl: Geometric Numerical Integration in Julia, https://github.com/JuliaGNI/GeometricIntegrators.jl (2020).\n\n\n\nK. Feng. The step-transition operators for multi-step methods of ODE's. Journal of Computational Mathematics, 193–202 (1998).\n\n\n\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and I. Polosukhin. Attention is all you need. Advances in neural information processing systems 30 (2017).\n\n\n\nA. Hemmasian and A. Barati Farimani. Reduced-order modeling of fluid flows with transformers. Physics of Fluids 35 (2023).\n\n\n\nA. Solera-Rico, C. S. Vila, M. Gómez, Y. Wang, A. Almashjary, S. Dawson and R. Vinuesa, beta-Variational autoencoders and transformers for reduced-order modelling of fluid flows, arXiv preprint arXiv:2304.03571 (2023).\n\n\n\nB. Brantner, G. de Romemont, M. Kraus and Z. Li. Volume-Preserving Transformers for Learning Time Series Data with Structure, arXiv preprint arXiv:2312:11166v2 (2024).\n\n\n\n","category":"page"},{"location":"architectures/neural_network_integrators/","page":"Neural Network Integrators","title":"Neural Network Integrators","text":"\\end{comment}","category":"page"},{"location":"tutorials/matrix_softmax/#Matrix-Softmax-v-Vector-Softmax","page":"Matrix Attention","title":"Matrix Softmax v Vector Softmax","text":"","category":"section"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"In this section we compare the VectorSoftmax to the MatrixSoftmax. What is usually meant by softmax is the vector softmax, i.e. one that does:","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"mathrmsoftmax(a)_i = frace^a_isum_i=1^de^a_i ","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"So each column of a matrix is normalized to sum up to one. With this softmax, the linear recombination that is performed by the attention layer becomes a convex recombination. This is not the case for the MatrixSoftmax, where the normalization is computed over all matrix entries:","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"mathrmsoftmax(A)_ij = frace^A_ijsum_i=1 j=1^dbarde^A_ij ","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"We want to compare those two approaches on the example of the coupled harmonic oscillator. It is a Hamiltonian system with ","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"H(q_1 q_2 p_1 p_2) = fracq_1^22m_1 + fracq_2^22m_2 + k_1fracq_1^22 + k_2fracq_2^22 +  ksigma(q_1)frac(q_2 - q_1)^22","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"where sigma(x) = 1  (1 + e^-x) is the sigmoid activation function. The system parameters are:","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"k_1: spring constant belonging to m_1,\nk_2: spring constant belonging to m_2,\nm_1: mass 1,\nm_2: mass 2,\nk: coupling strength between the two masses. ","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"(Image: Visualization of the coupled harmonic oscillator.) (Image: Visualization of the coupled harmonic oscillator.)","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"We will leave the parameters fixed but alter the initial conditions[1]:","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"[1]: We here use the implementation of the coupled harmonic oscillator from GeometricProblems.","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"using GeometricMachineLearning # hide\nusing GeometricProblems.CoupledHarmonicOscillator: hodeensemble, default_parameters\nusing GeometricIntegrators: ImplicitMidpoint, integrate # hide\nusing LaTeXStrings # hide\nusing CairoMakie  # hide\nCairoMakie.activate!() # hide\nimport Random # hide\nRandom.seed!(123) # hide\n\nmorange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256) # hide\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256) # hide\nmgreen = RGBf(44 / 256, 160 / 256, 44 / 256) # hide\n\nconst tstep = .3\nconst n_init_con = 5\n\n# ensemble problem\nep = hodeensemble([rand(2) for _ in 1:n_init_con], [rand(2) for _ in 1:n_init_con]; tstep = tstep)\ndl = DataLoader(integrate(ep, ImplicitMidpoint()); suppress_info = true)\n# dl = DataLoader(vcat(dl_nt.input.q, dl_nt.input.p))  # hide\n\nnothing # hide","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"We now use the same architecture, a TransformerIntegrator, twice, but alter its activation function:","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"const seq_length = 4\nconst batch_size = 1024\nconst n_epochs = 1000\n\nact1 = GeometricMachineLearning.VectorSoftmax()\nact2 = GeometricMachineLearning.MatrixSoftmax()\n\narch1 = StandardTransformerIntegrator(dl.input_dim; transformer_dim = 20,\n                                                    n_heads = 4, \n                                                    L = 1, \n                                                    n_blocks = 2,\n                                                    attention_activation = act1)\n\narch2 = StandardTransformerIntegrator(dl.input_dim; transformer_dim = 20,\n                                                    n_heads = 4,\n                                                    L = 1,\n                                                    n_blocks = 2,\n                                                    attention_activation = act2)\n\nnn1 = NeuralNetwork(arch1)\nnn2 = NeuralNetwork(arch2)","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"Training is done with the AdamOptimizer:","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"o_method = AdamOptimizer()\n\no1 = Optimizer(o_method, nn1)\no2 = Optimizer(o_method, nn2)\n\nbatch = Batch(batch_size, seq_length)\n\nloss_array1 = o1(nn1, dl, batch, n_epochs; show_progress = false)\nloss_array2 = o2(nn2, dl, batch, n_epochs; show_progress = false)","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"function make_training_error_plot(; theme = :dark)\n    textcolor = theme == :dark ? :white : :black\n    fig = Figure(; backgroundcolor = :transparent)\n    ax = Axis(fig[1, 1]; \n        backgroundcolor = :transparent,\n        bottomspinecolor = textcolor, \n        topspinecolor = textcolor,\n        leftspinecolor = textcolor,\n        rightspinecolor = textcolor,\n        xtickcolor = textcolor, \n        ytickcolor = textcolor,\n        xticklabelcolor = textcolor,\n        yticklabelcolor = textcolor,\n        xlabel=L\"t\", \n        ylabel=L\"q_1\",\n        xlabelcolor = textcolor,\n        ylabelcolor = textcolor,\n    )\n\n    # we use linewidth  = 2\n    lines!(ax, loss_array1; color = mpurple, label = \"VecSoftM\", linewidth = 2)\n    lines!(ax, loss_array2; color = mred, label = \"MatSoftM\", linewidth = 2)\n    axislegend(; position = (.55, .75), backgroundcolor = :transparent, labelcolor = textcolor)\n\n    fig, ax\nend\n\ntraining_fig_light, training_ax_light = make_training_error_plot(; theme = :light)\ntraining_fig_dark, training_ax_dark = make_training_error_plot(; theme = :dark)\nsave(\"attention_training_dark.png\", training_fig_dark; px_per_unit = 1.2)\nsave(\"attention_training_light.png\", training_fig_light; px_per_unit = 1.2)\n\nnothing","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"(Image: Training loss for the different networks.) (Image: Training loss for the different networks.)","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"const index = 1\ninit_con = (q = dl.input.q[:, 1:seq_length, index], p = dl.input.p[:, 1:seq_length, index])\n\nconst n_steps = 300\n\nfunction make_validation_plot(n_steps = n_steps; theme = :dark)\n    textcolor = theme == :dark ? :white : :black\n    fig = Figure(; backgroundcolor = :transparent)\n    ax = Axis(fig[1, 1]; \n        backgroundcolor = :transparent,\n        bottomspinecolor = textcolor, \n        topspinecolor = textcolor,\n        leftspinecolor = textcolor,\n        rightspinecolor = textcolor,\n        xtickcolor = textcolor, \n        ytickcolor = textcolor,\n        xticklabelcolor = textcolor,\n        yticklabelcolor = textcolor,\n        xlabel=L\"t\", \n        ylabel=L\"q_1\",\n        xlabelcolor = textcolor,\n        ylabelcolor = textcolor,\n    )\n    prediction_vector = iterate(nn1, init_con; n_points = n_steps, prediction_window = seq_length)\n    prediction_matrix = iterate(nn2, init_con; n_points = n_steps, prediction_window = seq_length)\n\n    # we use linewidth  = 2\n    lines!(ax, dl.input.q[1, 1:n_steps, index]; color = mblue, label = \"Implicit midpoint\", linewidth = 2)\n    lines!(ax, prediction_vector.q[1, :]; color = mpurple, label = \"VecSoftM\", linewidth = 2)\n    lines!(ax, prediction_matrix.q[1, :]; color = mred, label = \"MatSoftM\", linewidth = 2)\n    axislegend(; position = (.55, .75), backgroundcolor = :transparent, labelcolor = textcolor)\n\n    fig, ax\nend\n\nfig_light, ax_light = make_validation_plot(n_steps; theme = :light)\nfig_dark, ax_dark = make_validation_plot(n_steps; theme = :dark)\nsave(\"validation_dark.png\", fig_dark; px_per_unit = 1.2)\nsave(\"validation_light.png\", fig_light; px_per_unit = 1.2)\n\nnothing","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"(Image: Predicting trajectories with transformers based on the vector softmax and the matrix softmax.) (Image: Predicting trajectories with transformers based on the vector softmax and the matrix softmax.)","category":"page"},{"location":"tutorials/matrix_softmax/","page":"Matrix Attention","title":"Matrix Attention","text":"A similar page can be found here.","category":"page"},{"location":"architectures/symplectic_transformer/#The-Symplectic-Transformer","page":"Symplectic Transformer","title":"The Symplectic Transformer","text":"","category":"section"},{"location":"architectures/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"The symplectic transformer is, like the standard transformer, a combination of attention layers and feedforward layers. The difference is that the attention layers are not multihead attention layers and the feedforward layers are not standard GeometricMachineLearning.ResNetLayers, but symplectic attention layers and sympnet layers.","category":"page"},{"location":"architectures/symplectic_transformer/#Library-Functions","page":"Symplectic Transformer","title":"Library Functions","text":"","category":"section"},{"location":"architectures/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"SymplecticTransformer","category":"page"},{"location":"architectures/symplectic_transformer/#GeometricMachineLearning.SymplecticTransformer","page":"Symplectic Transformer","title":"GeometricMachineLearning.SymplecticTransformer","text":"SymplecticTransformer <: TransformerIntegrator\n\nConstructors\n\nSymplecticTransformer(sys_dim)\n\nMake an instance of SymplecticTransformer for a specific system dimension and sequence length. Also see LinearSymplecticTransformer.\n\nArguments\n\nYou can provide the additional optional keyword arguments:\n\nn_sympnet::Int = (2): The number of sympnet layers in the transformer.\nupscaling_dimension::Int = 2*dim: The upscaling that is done by the gradient layer. \nL::Int = 1: The number of transformer units. \nsympnet_activation = tanh: The activation function for the SympNet layers. \nattention_activation = GeometricMachineLearning.MatrixSoftmax(): The activation function for the Attention layers.\ninit_upper::Bool=true: Specifies if the first layer is a q-type layer (init_upper=true) or if it is a p-type layer (init_upper=false).\nsymmetric::Bool=false:\n\nThe number of SympNet layers in the network is 2n_sympnet, i.e. for n_sympnet = 1 we have one GradientLayerQ and one GradientLayerP.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/volume_preserving_attention/#Comparing-Different-VolumePreservingAttention-Mechanisms","page":"Volume-Preserving Attention","title":"Comparing Different VolumePreservingAttention Mechanisms","text":"","category":"section"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"In the section on volume-preserving attention we mentioned two ways of computing volume-preserving attention: one where we compute the correlations with a skew-symmetric matrix and one where we compute the correlations with an arbitrary matrix. Here we compare the two approaches. When calling the VolumePreservingAttention layer we can specify whether we want to use the skew-symmetric or the arbitrary weighting by setting the keyword skew_sym = true and skew_sym = false respectively. ","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"In here we demonstrate the differences between the two approaches for computing correlations. For this we first generate a training set consisting of two collections of curves: (i) sine curves and (ii) cosine curve. ","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"using GeometricMachineLearning # hide\nusing GeometricMachineLearning: FeedForwardLoss, TransformerLoss, params # hide\nimport Random # hide\nRandom.seed!(123) # hide\nsine_cosine = zeros(1, 1000, 2)\nsine_cosine[1, :, 1] .= sin.(0.:.1:99.9)\nsine_cosine[1, :, 2] .= cos.(0.:.1:99.9)\n\nconst T = Float16\nconst dl = DataLoader(T.(sine_cosine); suppress_info = true)\n\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"The third axis (i.e. the parameter axis) has length two, meaning we have two different kinds of curves, i.e. the data look like this:","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"using CairoMakie, LaTeXStrings # hide\nmorange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256) # hide\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256) # hide\nmgreen = RGBf(44 / 256, 160 / 256, 44 / 256) # hide\nfunction make_comparison_plot(; theme = :dark) # hide\ntextcolor = theme == :dark ? :white : :black # hide\nfig = Figure(; backgroundcolor = :transparent)\nax = Axis(fig[1, 1]; \n    backgroundcolor = :transparent,\n    bottomspinecolor = textcolor, \n    topspinecolor = textcolor,\n    leftspinecolor = textcolor,\n    rightspinecolor = textcolor,\n    xtickcolor = textcolor, \n    ytickcolor = textcolor,\n    xticklabelcolor = textcolor,\n    yticklabelcolor = textcolor,\n    xlabel=L\"t\", \n    ylabel=L\"z\",\n    xlabelcolor = textcolor,\n    ylabelcolor = textcolor,\n    )\n\nlines!(ax, dl.input[1, 1:200, 1], label=L\"\\sin(t)\", color = morange)\nlines!(ax, dl.input[1, 1:200, 2], label=L\"\\cos(t)\", color = mpurple)\naxislegend(; position = (.82, .75), backgroundcolor = theme == :dark ? :transparent : :white, labelcolor = textcolor) # hide\nfig_name = theme == :dark ? \"curve_comparison_dark.png\" : \"curve_comparison_light.png\" # hide\nsave(fig_name, fig; px_per_unit = 1.2) # hide\nend # hide\nmake_comparison_plot(; theme = :dark) # hide\nmake_comparison_plot(; theme = :light) # hide\n\nnothing","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"(Image: The data we treat here contains two different curves.) (Image: The data we treat here contains two different curves.)","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"We want to train a single neural network on both these curves. We already noted before that a simple feedforward neural network cannot do this. Here we compare three networks which are of the following form: ","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"mathttnetwork = mathcalNN_dcircPsicircmathcalNN_u","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"where mathcalNN_u refers to a neural network that scales up and mathcalNN_d refers to a neural network that scales down. The up and down scaling is done with simple dense layers: ","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"mathcalNN_u(x) = mathrmtanh(a_ux + b_u) text and  mathcalNN_d(x) = a_d^Tx + b_d","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"where a_u b_u a_dinmathbbR^mathrmud and b_d is a scalar. ud refers to upscaling dimension. For Psi we consider three different choices:","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"a volume-preserving attention with skew-symmetric weighting,\na volume-preserving attention with arbitrary weighting,\nan identity layer.","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"We further choose a sequence length 5 (i.e. the network always sees the last 5 time steps) and always predict one step into the future (i.e. the prediction window is set to 1):","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"const seq_length = 3\nconst prediction_window = 1\nconst upscale_dimension_1 = 2\n\nfunction set_up_networks(upscale_dimension::Int = upscale_dimension_1)\n    model_skew = Chain( Dense(1, upscale_dimension, tanh), \n                        VolumePreservingAttention(upscale_dimension, seq_length; skew_sym = true),\n                        Dense(upscale_dimension, 1, identity; use_bias = true)\n                        )\n\n    model_arb  = Chain( Dense(1, upscale_dimension, tanh), \n                        VolumePreservingAttention(upscale_dimension, seq_length; skew_sym = false), \n                        Dense(upscale_dimension, 1, identity; use_bias = true)\n                        )\n\n    model_comp = Chain( Dense(1, upscale_dimension, tanh), \n                        Dense(upscale_dimension, 1, identity; use_bias = true)\n                        )\n\n    nn_skew = NeuralNetwork(model_skew, CPU(), T)\n    nn_arb  = NeuralNetwork(model_arb,  CPU(), T)\n    nn_comp = NeuralNetwork(model_comp, CPU(), T)\n\n    nn_skew, nn_arb, nn_comp\nend\n\nnn_skew, nn_arb, nn_comp = set_up_networks()\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"We expect the third network to not be able to learn anything useful since it cannot resolve time series data: a regular feedforward network only ever sees one datum at a time. ","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"Next we train the networks (here we pick a batch size of 30 and train for 1000 epochs):","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"function set_up_optimizers(nn_skew, nn_arb, nn_comp)\n    o_skew = Optimizer(AdamOptimizer(T), nn_skew)\n    o_arb  = Optimizer(AdamOptimizer(T), nn_arb)\n    o_comp = Optimizer(AdamOptimizer(T), nn_comp)\n\n    o_skew, o_arb, o_comp\nend\n\no_skew, o_arb, o_comp = set_up_optimizers(nn_skew, nn_arb, nn_comp)\n\nconst n_epochs = 1000\n\nconst batch_size = 30\n\nconst batch = Batch(batch_size, seq_length, prediction_window)\nconst batch2 = Batch(batch_size)\n\nfunction train_networks!(nn_skew, nn_arb, nn_comp)\n    loss_array_skew = o_skew(nn_skew, dl, batch, n_epochs, TransformerLoss(batch); show_progress = false)\n    loss_array_arb  = o_arb( nn_arb,  dl, batch, n_epochs, TransformerLoss(batch); show_progress = false)\n    loss_array_comp = o_comp(nn_comp, dl, batch2, n_epochs, FeedForwardLoss(); show_progress = false)\n\n    loss_array_skew, loss_array_arb, loss_array_comp\nend\n\nloss_array_skew, loss_array_arb, loss_array_comp = train_networks!(nn_skew, nn_arb, nn_comp)\n\nfunction plot_training_losses(loss_array_skew, loss_array_arb, loss_array_comp; theme = :dark)\n    textcolor = theme == :dark ? :white : :black\n    fig = Figure(; backgroundcolor = :transparent)\n    ax = Axis(fig[1, 1]; \n        backgroundcolor = :transparent,\n        bottomspinecolor = textcolor, \n        topspinecolor = textcolor,\n        leftspinecolor = textcolor,\n        rightspinecolor = textcolor,\n        xtickcolor = textcolor, \n        ytickcolor = textcolor,\n        xticklabelcolor = textcolor,\n        yticklabelcolor = textcolor,\n        xlabel=\"Epoch\", \n        ylabel=\"Training loss\",\n        xlabelcolor = textcolor,\n        ylabelcolor = textcolor,\n        yscale = log10\n    )\n    lines!(ax, loss_array_skew, color = mblue, label = \"skew\")\n    lines!(ax, loss_array_arb,  color = mred, label = \"arb\")\n    lines!(ax, loss_array_comp, color = mgreen, label = \"comp\")\n    axislegend(; position = (.82, .75), backgroundcolor = :transparent, labelcolor = textcolor)\n\n    fig, ax\nend\n\nfig_dark, ax_dark = plot_training_losses(loss_array_skew, loss_array_arb, loss_array_comp; theme = :dark)\nfig_light, ax_light = plot_training_losses(loss_array_skew, loss_array_arb, loss_array_comp; theme = :light)\nsave(\"training_loss_vpa_light.png\", fig_light; px_per_unit = 1.2)\nsave(\"training_loss_vpa_dark.png\", fig_dark; px_per_unit = 1.2)\n\nnothing","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"(Image: The training losses for the three networks.) (Image: The training losses for the three networks.)","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"Looking at the training errors, we can see that the network with the skew-symmetric weighting is stuck at a relatively high error rate, whereas the loss for  the network with the arbitrary weighting is decreasing to a significantly lower level. The feedforward network without the attention mechanism is not able to learn anything useful (as was expected). ","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"Before we can use the trained neural networks for prediction we have to make them TransformerIntegrators or NeuralNetworkIntegrators[1]:","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"[1]: Here we have to use the architectures GeometricMachineLearning.DummyTransformer and GeometricMachineLearning.DummyNNIntegrator to reformulate the three neural networks defined here as NeuralNetworkIntegrators or TransformerIntegrators. These dummy architectures can be used if the user wants to specify new neural network integrators that are not yet defined in GeometricMachineLearning. ","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"initial_condition = dl.input[:, 1:seq_length, 2]\n\nfunction make_networks_neural_network_integrators(nn_skew, nn_arb, nn_comp)\n    nn_skew = NeuralNetwork(GeometricMachineLearning.DummyTransformer(seq_length), \n                            nn_skew.model, \n                            params(nn_skew), \n                            CPU())\n    nn_arb  = NeuralNetwork(GeometricMachineLearning.DummyTransformer(seq_length), \n                            nn_arb.model,  \n                            params(nn_arb), \n                            CPU())\n    nn_comp = NeuralNetwork(GeometricMachineLearning.DummyNNIntegrator(), \n                            nn_comp.model, \n                            params(nn_comp), \n                            CPU())\n\n    nn_skew, nn_arb, nn_comp\nend\n\nnn_skew, nn_arb, nn_comp = make_networks_neural_network_integrators(nn_skew, nn_arb, nn_comp)\n\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"function produce_validation_plot_single(n_points::Int, nn_skew = nn_skew, nn_arb = nn_arb, nn_comp = nn_comp; initial_condition::Matrix=initial_condition, type = :cos, theme = :dark)\ntextcolor = theme == :dark ? :white : :black # hide\n    fig = Figure(; backgroundcolor = :transparent)\n    ax = Axis(fig[1, 1]; \n        backgroundcolor = :transparent,\n        bottomspinecolor = textcolor, \n        topspinecolor = textcolor,\n        leftspinecolor = textcolor,\n        rightspinecolor = textcolor,\n        xtickcolor = textcolor, \n        ytickcolor = textcolor,\n        xticklabelcolor = textcolor,\n        yticklabelcolor = textcolor,\n        xlabel=L\"t\", \n        ylabel=L\"z\",\n        xlabelcolor = textcolor,\n        ylabelcolor = textcolor,\n        )\n    validation_skew = iterate(nn_skew, initial_condition; n_points = n_points, prediction_window = 1)\n    validation_arb  = iterate(nn_arb,  initial_condition; n_points = n_points, prediction_window = 1)\n    validation_comp = iterate(nn_comp, initial_condition[:, 1]; n_points = n_points)\n\n    p2 = type == :cos ? lines!(dl.input[1, 1:n_points, 2], color = mpurple, label = \"reference\") : plot(dl.input[1, 1:n_points, 1], color = mpurple, label = \"reference\")\n\n    lines!(ax, validation_skew[1, :], color = mblue, label = \"skew\")\n    lines!(ax, validation_arb[1, :], color = mred, label = \"arb\")\n    lines!(ax, validation_comp[1, :], color = mgreen, label = \"comp\")\n    vlines!(ax, [seq_length], color = mred, label = \"start of prediction\")\n\n    axislegend(; position = (.82, .75), backgroundcolor = theme == :dark ? :transparent : :white, labelcolor = textcolor)\n    fig, ax\nend\n\nfunction produce_validation_plot(n_points::Int, nn_skew = nn_skew, nn_arb = nn_arb, nn_comp = nn_comp; initial_condition::Matrix=initial_condition, type = :cos)\n    fig_dark, ax_dark = produce_validation_plot_single(n_points, nn_skew, nn_arb, nn_comp; initial_condition = initial_condition, type = type, theme = :dark)\n    fig_light, ax_light = produce_validation_plot_single(n_points, nn_skew, nn_arb, nn_comp; initial_condition = initial_condition, type = type, theme = :light)\n\n    fig_dark, fig_light, ax_dark, ax_light\nend\n\nnothing","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"fig_dark, fig_light, ax_dark, ax_light  = produce_validation_plot(40) # hide\nsave(\"plot40_dark.png\", fig_dark; px_per_unit = 1.2) # hide\nsave(\"plot40_light.png\", fig_light; px_per_unit = 1.2) # hide\nnothing","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"(Image: Comparing the two volume-preserving attention mechanisms for 40 points.) (Image: Comparing the two volume-preserving attention mechanisms for 40 points.)","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"In the plot above we can see that the network with the arbitrary weighting performs much better; even though the red line does not fit the purple line perfectly, it manages to least qualitatively reflect the training data.  We can also plot the predictions for longer time intervals: ","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"fig_dark, fig_light, ax_dark, ax_light  = produce_validation_plot(400) # hide\nsave(\"plot400_dark.png\", fig_dark; px_per_unit = 1.2) # hide\nsave(\"plot400_light.png\", fig_light; px_per_unit = 1.2) # hide\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"(Image: Comparing the two volume-preserving attention mechanisms for 400 points.) (Image: Comparing the two volume-preserving attention mechanisms for 400 points.)","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"This advantage of the volume-preserving attention with arbitrary weighting may however be due to the fact that the skew-symmetric attention only has 3 learnable parameters, as opposed to 9 for the arbitrary weighting. We can increase the upscaling dimension and see how it affects the result: ","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"const upscale_dimension_2 = 10\n\nnn_skew, nn_arb, nn_comp = set_up_networks(upscale_dimension_2)\n\no_skew, o_arb, o_comp = set_up_optimizers(nn_skew, nn_arb, nn_comp)\n\nloss_array_skew, loss_array_arb, loss_array_comp = train_networks!(nn_skew, nn_arb, nn_comp) # hide\nfig_dark, ax_dark = plot_training_losses(loss_array_skew, loss_array_arb, loss_array_comp; theme = :dark) # hide\nfig_light, ax_light = plot_training_losses(loss_array_skew, loss_array_arb, loss_array_comp; theme = :light) # hide\nsave(\"training_loss2_vpa_light.png\", fig_light; px_per_unit = 1.2) # hide\nsave(\"training_loss2_vpa_dark.png\", fig_dark; px_per_unit = 1.2) # hide\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"(Image: Comparison for 40 points, but with an upscaling of ten.) (Image: Comparison for 40 points, but with an upscaling of ten.)","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"initial_condition = dl.input[:, 1:seq_length, 2]\n\nnn_skew, nn_arb, nn_comp = make_networks_neural_network_integrators(nn_skew, nn_arb, nn_comp)\n\nfig_dark, fig_light, ax_dark, ax_light = produce_validation_plot(40, nn_skew, nn_arb, nn_comp)\n\nsave(\"plot40_sine2_dark.png\", fig_dark; px_per_unit = 1.2) # hide\nsave(\"plot40_sine2_light.png\", fig_light; px_per_unit = 1.2) # hide\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"(Image: ) (Image: )","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"And for a longer time interval: ","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"fig_dark, fig_light, ax_dark, ax_light = produce_validation_plot(200, nn_skew, nn_arb, nn_comp)\n\n\nsave(\"plot200_sine2_dark.png\", fig_dark; px_per_unit = 1.2) # hide\nsave(\"plot200_sine2_light.png\", fig_light; px_per_unit = 1.2) # hide\nnothing","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"(Image: ) (Image: )","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"Here we see that the arbitrary weighting quickly fails and the skew-symmetric weighting performs better on longer time scales.","category":"page"},{"location":"tutorials/volume_preserving_attention/#Library-Functions","page":"Volume-Preserving Attention","title":"Library Functions","text":"","category":"section"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"GeometricMachineLearning.DummyNNIntegrator\nGeometricMachineLearning.DummyTransformer","category":"page"},{"location":"tutorials/volume_preserving_attention/#GeometricMachineLearning.DummyNNIntegrator","page":"Volume-Preserving Attention","title":"GeometricMachineLearning.DummyNNIntegrator","text":"DummyNNIntegrator()\n\nMake an instance of DummyNNIntegrator.\n\nThis dummy architecture can be used if the user wants to define a new NeuralNetworkIntegrator.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/volume_preserving_attention/#GeometricMachineLearning.DummyTransformer","page":"Volume-Preserving Attention","title":"GeometricMachineLearning.DummyTransformer","text":"DummyTransformer(seq_length)\n\nMake an instance of DummyTransformer.\n\nThis dummy architecture can be used if the user wants to define a new TransformerIntegrator.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"\\section*{Chapter Summary}\n\nIn this chapter we showed concrete examples of how to improve transformer neural networks by imbuing them with structure. The two examples we gave were (i) enforcing orthogonality constraints for some of the weights in a vision transformer (i.e. putting some of the weights on the \\textit{Stiefel manifold}) and (ii) using a volume-preserving transformer to learn the dynamics of a rigid body. In both cases we observed big improvements over the standard transformer that does not consider structure. In the first case the network was not able to learn anything if orthogonality constraints were not imposed and in the second case we obtained greatly improved long-time performance. At the end we also compared two different approaches to designing the volume-preserving transformer: computing correlations based on a \\textit{skew-symmetric weighting} and \\textit{computing correlations based on an arbitrary weighting}. We saw that often the arbitrary weighting should be preferred over the skew-symmetric weighting, but the arbitrary weighting may also fail in other cases.","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"<!--","category":"page"},{"location":"tutorials/volume_preserving_attention/#References","page":"Volume-Preserving Attention","title":"References","text":"","category":"section"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"B. Brantner. Generalizing Adam To Manifolds For Efficiently Training Transformers, arXiv preprint arXiv:2305.16901 (2023).\n\n\n\nB. Brantner, G. de Romemont, M. Kraus and Z. Li. Volume-Preserving Transformers for Learning Time Series Data with Structure, arXiv preprint arXiv:2312:11166v2 (2024).\n\n\n\n","category":"page"},{"location":"tutorials/volume_preserving_attention/","page":"Volume-Preserving Attention","title":"Volume-Preserving Attention","text":"-->","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"In this chapter we discuss \\textit{data-driven reduced order modeling}. This serves as a motivation for designing structure-preserving neural networks. We here first discuss the general workflow of reduced order modeling and explain the split into an \\textit{offline phase} and an \\textit{online phase}. Next we discuss \\textit{proper orthogonal decomposition} and \\textit{autoencoders} as examples of machine learning techniques that can be used for the offline phase of reduced order modeling. The chapter concludes with an explanation of how to adjust a reduced order model for the Hamiltonian setting in order to make it structure-preserving.","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/#Basic-Concepts-of-Reduced-Order-Modeling","page":"General Framework","title":"Basic Concepts of Reduced Order Modeling","text":"","category":"section"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"Reduced order modeling is a data-driven technique that exploits the structure of parametric partial differential equations (PPDEs) to make repeated simulations of this PPDE much cheaper.","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"For this consider a PPDE written in the form: F(z(mu)mu)=0 where z(mu) evolves on an infinite-dimensional Hilbert space V. ","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"In modeling any PDE we have to choose a discretization (particle discretization, finite element method, ...) of V which will be denoted by V_h simeq mathbbR^N. The space V_h is not infinite-dimensional but its dimension N is still very large. Solving a discretized PDE in this space is typically very expensive. In reduced order modeling we utilize the fact that slightly different choices of parameters mu will give qualitatively similar solutions. We can therefore perform a few simulations in the full space V_h and then make successive simulations cheaper by learning from the past simulations:","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"(Image: Schematic representation of a reduced order modeling framework. The width of the individual blocks represent how long it takes to perform a simulation.) (Image: Schematic representation of a reduced order modeling framework. The width of the individual blocks represent how long it takes to perform a simulation.)","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"In the figure above we refer to the discretized PDE as the full order model (FOM) and to the cheaper representation (that we construct in a data-driven manner) as the reduced order model (ROM). We now introduce the solution manifold, which is a crucial concept in reduced order modeling.","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/#The-Solution-Manifold","page":"General Framework","title":"The Solution Manifold","text":"","category":"section"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"To any PPDE and a certain parameter set mathbbP we associate a solution manifold: ","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"mathcalM = z(mu)F(z(mu)mu)=0 muinmathbbP","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"A motivation for reduced order modeling is that even though the space V_h is of very high-dimension, the solution manifold will typically be a very small space. The image here shows a two-dimensional solution manifold[1] embedded in V_hequivmathbbR^3.","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"[1]: The systems we deal with usually have much greater dimension of course. The dimension of V_h will be in the thousands and the dimension of the solution manifold will be a few orders of magnitudes smaller. Because this cannot be easily visualized, we resort to showing a two-dimensional manifold in a three-dimensional space here. ","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"(Image: A representation of a two-dimensional solution manifold embedded in three-dimensional Euclidean space.) (Image: A representation of a two-dimensional solution manifold embedded in three-dimensional Euclidean space.)","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"As an actual example of a solution manifold consider the one-dimensional wave equation [60]: ","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"partial_tt^2q(tomegamu) = mu^2partial_omegaomega^2q(tomegamu)text on ItimesOmega","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"where I = (01) and Omega=(-1212). As initial condition for the first derivative we have partial_tq(0omegamu) = -mupartial_omegaq_0(ximu) and furthermore q(tomegamu)=0 on the boundary (i.e. omegain-1212).","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"The solution manifold is a two-dimensional submanifold of an infinite-dimensional function space: ","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"mathcalM = (t omega)mapstoq(tomegamu)=q_0(omega-mutmu)muinmathbbPsubsetmathbbR","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"We can plot some of the points on mathcalM (each curve at a specific time corresponds to one point on the solution manifold): ","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"using CairoMakie\nimport GeometricProblems.LinearWave as lw\n\n# specify different μ values\nμs = (0.416, 0.508, 0.6)\ntime_steps = (0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)\n\nmorange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256)\n\ncolors = (morange, mpurple, mred)\n\nfunction make_plot(; theme = :dark, plot_name = \"wave_equation_different_parameters\")\nfig = Figure(; backgroundcolor = :transparent)\ntext_color = theme == :dark ? :white : :black\n\nfunction make_axis(i)\n    μ = μs[i]\n    ax = Axis(fig[i, 1]; \n        xlabel = L\"\\omega\",\n        ylabel = L\"q(t, \\omega, \\mu)\",\n        xgridcolor = text_color,\n        ygridcolor = text_color,\n        xtickcolor = text_color,\n        ytickcolor = text_color,\n        xlabelcolor = text_color,\n        ylabelcolor = text_color,\n        backgroundcolor = :transparent)\n    # plot 6 time steps\n    domain = lw.compute_domain(lw.Ñ + 2)\n    for time_step in time_steps\n        lines!(ax, domain, lw.u₀(domain .- μ * time_step, μ), color = colors[i])\n    end\n    ax\nend\n\nax1 = make_axis(1)\nax2 = make_axis(2)\nax3 = make_axis(3)\n\npx_per_unit = Main.output_type == :html ? 1 : 2\nadd_on = theme == :dark ? \"_dark\" : \"_light\"\nsave(plot_name * add_on * \".png\", fig; px_per_unit = px_per_unit)\nend\n\nmake_plot(; theme = :dark)\nmake_plot(; theme = :light)\n\nnothing","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"(Image: Solution of the one-dimensional wave equation for three different parameters. These solutions evolve on a two-dimensional solution manifold.) (Image: Solution of the one-dimensional wave equation for three different parameters. These solutions evolve on a two-dimensional solution manifold.)","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"Here we plotted the curves for the time steps (0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0) and the parameter values (0.416, 0.508, 0.6). We see that, depending on the parameter value mu, the wave travels at different speeds. In reduced order modeling we try to find an approximation to the solution manifolds, i.e. model the evolution of the curve in a cheap way for different parameter values mu. Neural networks offer a way of doing so efficiently!","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/#General-Workflow","page":"General Framework","title":"General Workflow","text":"","category":"section"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"In reduced order modeling we aim to construct an approximation to the solution manifold and that is ideally of a dimension not much greater than that of the solution manifold and then (approximately) solve the so-called reduced equations in the small space. Constructing this approximation to the solution manifold can be divided into three steps[2]: ","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"Discretize the PDE, i.e. find VtoV_h.\nSolve the discretized PDE on V_h for a set of parameter instances muinmathbbP.\nBuild a reduced basis with the data obtained from having solved the discretized PDE. This step consists of finding two mappings: the reduction mathcalP and the reconstruction mathcalR.","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"[2]: Approximating the solution manifold is referred to as the offline phase of reduced order modeling.","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"The third step can be done with various machine learning (ML) techniques. Traditionally the most popular of these has been proper orthogonal decomposition (POD), but in recent years autoencoders have become a widely-used alternative [61, 62].","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"After having obtained mathcalP and mathcalR we still need to solve the reduced system. Solving the reduced system is typically referred to as the online phase in reduced order modeling. This is sketched below: ","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"(Image: The offline phase in reduced order modeling consists of finding the reduction and the reconstruction. In the online phase we solve the reduced model.) (Image: The offline phase in reduced order modeling consists of finding the reduction and the reconstruction. In the online phase we solve the reduced model.)","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"In this figure the online phase consists of applying the mapping mathcalNN in the low-dimensional space in order to predict the next time step; this can either be done with a standard integrator [2] or, as is indicated here, with a neural network. Crucially this step can be made very cheap when compared to the full-order model[3]. In the following we discuss how an equation for the reduced model can be found classically, without relying on a neural network for the online phase.","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"[3]: Solving the reduced system is typically faster by a factor of 10^2 or more.","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/#Obtaining-the-Reduced-System-via-Galerkin-Projection","page":"General Framework","title":"Obtaining the Reduced System via Galerkin Projection","text":"","category":"section"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"Galerkin projection [63] offers a way of constructing an ODE on the reduced space once the reconstruction mathcalR has been found. ","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"Main.definition(raw\"Given a full-order model described by a differential equation ``\\hat{F}(\\cdot; \\mu):V\\to{}V``, where ``V`` may be an infinite-dimensional Hilbert space (PDE case) or a finite-dimensional vector space ``\\mathbb{R}^N`` (ODE case), and a reconstruction ``\\mathcal{R}:\\mathbb{R}^n\\to{}V``, we can find an equation on the reduced space ``\\mathbb{R}^n.`` For this we first take as possible solutions for the equation\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\hat{F}(\\hat{u}(t); \\mu) - \\hat{u}'(t) =: F(\\hat{u}(t); \\mu) = 0\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"the ones that are the *result of a reconstruction*:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\hat{F}(\\mathcal{R}(u(t)); \\mu) - d\\mathcal{R}u'(t) = 0,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``u:[0, T]\\to\\mathbb{R}^n`` is an orbit on the reduced space and ``d\\mathcal{R}`` is the differential of the reconstruction; this is ``\\nabla{}\\mathcal{R}`` if ``V`` is finite-dimensional. Typically we test this expression with a set of basis functions or vectors ``\\{\\tilde{\\psi}_1, \\ldots, \\tilde{\\psi}_n \\}`` and hence obtain ``n`` scalar equations:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\langle \\hat{F}(\\mathcal{R}(u(t)); \\mu) - d\\mathcal{R}u'(t), \\psi_i \\rangle_V \\text{ for $1\\leq{}i\\leq{}n$}.\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"Such a procedure to obtain a reduced equation is known as **Galerkin projection**.\")","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"We give specific examples of reduced systems obtained with a Galerkin projection when introducing proper orthogonal decomposition, autoencoders, proper symplectic decomposition and symplectic auteoncoders.","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/#Kolmogorov-n-width","page":"General Framework","title":"Kolmogorov n-width","text":"","category":"section"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"The Kolmogorov n-width [64] measures how well some set mathcalM (typically the solution manifold) can be approximated with a linear subspace:","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"d_n(mathcalM) = mathrminf_V_nsubsetVmathrmdimV_n=nmathrmsup(uinmathcalM)mathrminf_v_ninV_n u - v_n _V","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"with mathcalMsubsetV and V is a (typically infinite-dimensional) Banach space. For advection-dominated problems (among others) the decay of the Kolmogorov n-width is very slow, i.e. one has to pick n very high in order to obtain useful approximations (see [65] and [60]). As proper orthogonal decomposition is a linear approximation to the solution manifold, this does not work very well if the decay of the Kolmogorov n-width is slow.","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"In order to overcome this, techniques based on neural networks [62] and optimal transport [60] have been used. ","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"\\begin{comment}","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/#References","page":"General Framework","title":"References","text":"","category":"section"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"S. Fresca, L. Dede’ and A. Manzoni. A comprehensive deep learning-based approach to reduced order modeling of nonlinear time-dependent parametrized PDEs. Journal of Scientific Computing 87, 1–36 (2021).\n\n\n\nK. Lee and K. T. Carlberg. Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders. Journal of Computational Physics 404, 108973 (2020).\n\n\n\nT. Blickhan. A registration method for reduced basis problems using linear optimal transport, arXiv preprint arXiv:2304.14884 (2023).\n\n\n\n","category":"page"},{"location":"reduced_order_modeling/reduced_order_modeling/","page":"General Framework","title":"General Framework","text":"\\end{comment}","category":"page"},{"location":"docstring_index/#Index-of-Docstrings","page":"Index of Docstrings","title":"Index of Docstrings","text":"","category":"section"},{"location":"docstring_index/","page":"Index of Docstrings","title":"Index of Docstrings","text":"\\thispagestyle{empty}","category":"page"},{"location":"docstring_index/#Manifolds","page":"Index of Docstrings","title":"Manifolds","text":"","category":"section"},{"location":"docstring_index/","page":"Index of Docstrings","title":"Index of Docstrings","text":"Pages = Dict(Main.index_latex_pages)[\"Manifolds\"]","category":"page"},{"location":"docstring_index/#Geometric-Structure","page":"Index of Docstrings","title":"Geometric Structure","text":"","category":"section"},{"location":"docstring_index/","page":"Index of Docstrings","title":"Index of Docstrings","text":"Pages = Dict(Main.index_latex_pages)[\"Geometric Structure\"]","category":"page"},{"location":"docstring_index/#Reduced-Order-Modeling","page":"Index of Docstrings","title":"Reduced Order Modeling","text":"","category":"section"},{"location":"docstring_index/","page":"Index of Docstrings","title":"Index of Docstrings","text":"Pages = Dict(Main.index_latex_pages)[\"Reduced Order Modeling\"]","category":"page"},{"location":"docstring_index/#General-Framework-for-Manifold-Optimization","page":"Index of Docstrings","title":"General Framework for Manifold Optimization","text":"","category":"section"},{"location":"docstring_index/","page":"Index of Docstrings","title":"Index of Docstrings","text":"Pages = Dict(Main.index_latex_pages)[\"General Framework for Manifold Optimization\"]","category":"page"},{"location":"docstring_index/#Optimizer-Methods","page":"Index of Docstrings","title":"Optimizer Methods","text":"","category":"section"},{"location":"docstring_index/","page":"Index of Docstrings","title":"Index of Docstrings","text":"Pages = Dict(Main.index_latex_pages)[\"Optimizer Methods\"]","category":"page"},{"location":"docstring_index/#Layers","page":"Index of Docstrings","title":"Layers","text":"","category":"section"},{"location":"docstring_index/","page":"Index of Docstrings","title":"Index of Docstrings","text":"Pages = Dict(Main.index_latex_pages)[\"Layers\"]","category":"page"},{"location":"docstring_index/#Architectures","page":"Index of Docstrings","title":"Architectures","text":"","category":"section"},{"location":"docstring_index/","page":"Index of Docstrings","title":"Index of Docstrings","text":"Pages = Dict(Main.index_latex_pages)[\"Architectures\"]","category":"page"},{"location":"docstring_index/#Transformers-with-Structure","page":"Index of Docstrings","title":"Transformers with Structure","text":"","category":"section"},{"location":"docstring_index/","page":"Index of Docstrings","title":"Index of Docstrings","text":"Pages = Dict(Main.index_latex_pages)[\"Transformers with Structure\"]","category":"page"},{"location":"docstring_index/#Learning-Nonlinear-Spaces","page":"Index of Docstrings","title":"Learning Nonlinear Spaces","text":"","category":"section"},{"location":"docstring_index/","page":"Index of Docstrings","title":"Index of Docstrings","text":"Pages = [Dict(Main.index_latex_pages)[\"Learning Nonlinear Spaces\"]]","category":"page"},{"location":"docstring_index/#Data-Loader","page":"Index of Docstrings","title":"Data Loader","text":"","category":"section"},{"location":"docstring_index/","page":"Index of Docstrings","title":"Index of Docstrings","text":"Pages = Dict(Main.index_latex_pages)[\"Data Loader\"]","category":"page"},{"location":"docstring_index/#Special-Arrays,-Tensors-and-Pullbacks","page":"Index of Docstrings","title":"Special Arrays, Tensors and Pullbacks","text":"","category":"section"},{"location":"docstring_index/","page":"Index of Docstrings","title":"Index of Docstrings","text":"Pages = Dict(Main.index_latex_pages)[\"Special Arrays, Tensors and Pullbacks\"]","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/#Proper-Orthogonal-Decomposition","page":"POD and Autoencoders","title":"Proper Orthogonal Decomposition","text":"","category":"section"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"Proper orthogonal decomposition (POD, [66]) is perhaps the most widely-used technique for data-driven reduced order modeling. POD approximates the reduction and the reconstruction through linear maps. Assume that the big discretized space has dimension N and we try to model the solution manifold with an n-dimensional subspace. POD then models the reduction mathcalPmathbbR^NtomathbbR^n through a matrix inmathbbR^ntimesN and the reconstruction mathcalRmathbbR^ntomathbbR^N through a matrix inmathbbR^Ntimesn If we are given a snapshot matrix finding mathcalP and mathcalR amounts to a simple application of singular value decomposition (SVD).","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"Main.theorem(raw\"Given a snapshot matrix ``M = [u_1, \\ldots, u_\\mathtt{nts}] \\in\\mathbb{R}^{N\\times\\mathtt{nts}},`` where ``\\mathtt{nts}`` is the *number of time steps*, the ideal linear subspace that can best approximate the data stored in ``M`` are the first ``n`` columns of the ``V`` matrix in an SVD: ``M = VDU^T.`` The problem of finding this subspace can either be phrased as a maximization problem:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\max_{\\psi_1, \\ldots, \\psi_n\\in\\mathbb{R}^N} \\sum_{i = 1}^n \\sum_{j = 1}^{\\mathtt{nts}}| \\langle u_j, \\psi_i \\rangle_{\\mathbb{R}^N} |^2 \\text{ s.t. $\\langle \\psi_i, \\psi_j \\rangle = \\delta_{ij}$ for $1 \\leq i$, $j \\leq n$,}\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"or as a minimization problem:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\min_{\\psi_1, \\ldots, \\psi_n\\in\\mathbb{R}^N} \\sum_{j = 1}^{\\mathtt{nts}} | u_j - \\sum_{i = 1}^n \\psi_i\\langle u_j, u_i \\rangle |^2\\text{ s.t. $\\langle \\psi_i, \\psi_j \\rangle = \\delta_{ij}$ for $1 \\leq i$, $j \\leq n$.}\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"In both these cases we have \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    [\\psi_1, \\psi_2, \\ldots, \\psi_n] = V\\mathtt{[1:N, 1:n]},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``V`` is obtained via an SVD of ``M``.\")","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"A proof of the statement above can be found in e.g. [67]. We can obtain the reduced equations via Galerkin projection:","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"Main.theorem(raw\"Consider a full-order model on ``\\mathbb{R}^N`` described by the vector field ``{\\hat{u}}'(t) = X(\\hat{u}(t))``. For a POD basis the reduced vector field, obtained via Galerkin projection, is:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    u'(t) = V^TX(Vu(t)),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where we used ``\\{\\tilde{\\psi}_i = Ve_i\\}_{i = 1,\\ldots, n}`` as test functions. ``e_i\\in\\mathbb{R}^n`` is the vector that is zero everywhere except for the ``i``-th entry, where it is one.\")","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"Main.proof(raw\"If we take as test function ``\\tilde{\\psi}_i = Ve_i``, then we get:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    e_i^TV^TX(Vu(t)) \\overset{!}{=} e_i^TV^TVu'(t) = e_i^Tu'(t),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"and since this must be true for every ``i = 1, \\ldots, n`` we obtain the desired expression for the reduced vector field.\")","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"In recent years another approach to model mathcalP and mathcalR has become popular, namely to use neural networks to do so.","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/#Autoencoders","page":"POD and Autoencoders","title":"Autoencoders","text":"","category":"section"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"Autoencoders are a popular tool in machine learning to perform data compression [43]. The idea is always to find a low-dimensional representation of high-dimensional data. This is also referred to as learning a feature space. This idea straightforwardly lends itself towards an application in reduced order modeling. In this setting we learn two mappings that are modeled with neural networks:","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"Main.definition(raw\"An **autoencoder** is a tuple of two mappings ``(\\mathcal{P}, \\mathcal{R})`` called the **reduction** and the **reconstruction**:\n\" * Main.indentation * raw\"1. The reduction ``\\mathcal{P}:\\mathbb{R}^N\\to\\mathbb{R}^n`` is modeled with a neural network that maps high-dimensional data to a low-dimensional feature space. This network is also referred to as the **encoder** and we routinely denote it by ``\\Psi^\\mathrm{enc}_{\\theta_1}`` to stress the parameter-dependence on ``\\theta_1``.\n\" * Main.indentation * raw\"2. The reconstruction ``\\mathcal{R}:\\mathbb{R}^n\\to\\mathbb{R}^N`` is modeled with a neural network that maps inputs from the low-dimensional feature space to the high-dimensional space in which the original data were collected. This network is also referred to as the **decoder** and we routinely denote it by ``\\Psi^\\mathrm{dec}_{\\theta_2}`` to stress the parameter-dependence on ``\\theta_2``.\n\" * Main.indentation * raw\"During training we optimize the autoencoder for minimizing the *projection error*.\")","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"Unlike in the POD case we have to resort to using neural network optimizers in order to adapt the neural network to the data at hand as opposed to simply using SVD. The use of autoencoders instead of POD is extremely advantageous in the case when we deal with problems that exhibit a slowly-decaying Kolmogorov n-width. During training we minimize the projection error.","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"Main.remark(raw\"Note that POD can be seen as a special case of an autoencoder where the encoder and the decoder both consist of only one matrix. If we restrict this matrix to be orthonormal, i.e. optimize on the Stiefel manifold, then the best solution we can obtain is equivalent to applying SVD and finding the POD basis.\")","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/#The-Reduced-Equations-for-the-Autoencoder","page":"POD and Autoencoders","title":"The Reduced Equations for the Autoencoder","text":"","category":"section"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"Equivalently to the POD case, we get a reduced vector field when we reduce with the autoencoder:","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"Main.theorem(raw\"Consider a full-order model on ``\\mathbb{R}^N`` described by the vector field ``{\\hat{u}}'(t) = X(\\hat{u}(t))``. If we reduce with an autoencoder ``(\\Psi^\\mathrm{enc}, \\Psi^\\mathrm{dec})`` we obtain a reduced vector field via Galerkin projection:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    u'(t) = (\\nabla\\Psi^\\mathrm{dec})^TX(\\Psi^\\mathrm{dec}(u(t))),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where we used ``\\{\\tilde{\\psi}_i = \\Psi^\\mathrm{dec}(e_i)\\}_{i = 1,\\ldots, n}`` as test functions. ``e_i\\in\\mathbb{R}^n`` is the vector that is zero everywhere except for the ``i``-th entry, where it is one.\")","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"Main.proof(raw\"If we take as test function ``\\tilde{\\psi}_i = (\\nabla\\Psi^\\mathrm{dec})e_i`` we get:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    e_i^T(\\Psi^\\mathrm{dec})^+X(\\Psi^\\mathrm{dec}(u(t))) \\overset{!}{=} e_i^T(\\Psi^\\mathrm{dec})^+(\\nabla\\Psi^\\mathrm{dec})u'(t) = e_i^Tu'(t),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"and since this must be true for every ``i = 1, \\ldots, n`` we obtain the desired expression for the reduced vector field.\")","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"Both POD and standard autoencoders suffer from the problem that they completely neglect the structure of the differential equation and the data they are applied to. This can have grave consequences [68–70]. Hamiltonian model order reduction can improve the approximation significantly in these situations.","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/#Library-Functions","page":"POD and Autoencoders","title":"Library Functions","text":"","category":"section"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"GeometricMachineLearning.AutoEncoder\nGeometricMachineLearning.Encoder\nGeometricMachineLearning.Decoder\nGeometricMachineLearning.UnknownEncoder\nGeometricMachineLearning.UnknownDecoder\nencoder\ndecoder","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/#GeometricMachineLearning.AutoEncoder","page":"POD and Autoencoders","title":"GeometricMachineLearning.AutoEncoder","text":"AutoEncoder <: Architecture\n\nThe abstract AutoEncoder type.\n\nAn autoencoder [43] is a neural network consisting of an encoder Psi^e and a decoder Psi^d. In the simplest case they are trained on some data set mathcalD to reduce the following error: \n\nPsi^dcircPsi^e(mathcalD) - mathcalD\n\nwhich we call the reconstruction error, projection error or autoencoder error (see the docs for AutoEncoderLoss) and cdot is some norm.\n\nImplementation\n\nAutoEncoder is an abstract type. If a custom <:AutoEncoder architecture is implemented it should have the fields full_dim, reduced_dim, n_encoder_blocks and n_decoder_blocks. \n\nn_encoder_blocks and n_decoder_blocks indicate how often the dimension is changed in the encoder (respectively the decoder).\n\nFurther the routines encoder and decoder should be extended.\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/pod_autoencoders/#GeometricMachineLearning.Encoder","page":"POD and Autoencoders","title":"GeometricMachineLearning.Encoder","text":"Encoder <: Architecture\n\nThis is the abstract Encoder type. \n\nMost often this should not be called directly, but rather through the encoder function.\n\nImplementation\n\nIf a custom <:Encoder architecture is implemented it should have the fields full_dim, reduced_dim and n_encoder_blocks.\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/pod_autoencoders/#GeometricMachineLearning.Decoder","page":"POD and Autoencoders","title":"GeometricMachineLearning.Decoder","text":"Decoder <: Architecture\n\nThis is the abstract Decoder type. \n\nMost often this should not be called directly, but rather through the decoder function.\n\nImplementation\n\nIf a custom <:Decoder architecture is implemented it should have the fields full_dim, reduced_dim and n_decoder_blocks.\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/pod_autoencoders/#GeometricMachineLearning.UnknownEncoder","page":"POD and Autoencoders","title":"GeometricMachineLearning.UnknownEncoder","text":"UnknownEncoder(full_dim, reduced_dim, n_encoder_blocks)\n\nMake an instance of UnknownEncoder.\n\nThis should be used if one wants to use an Encoder that does not have any specific structure.\n\nExamples\n\nWe show how to make an encoder from a custom architecture:\n\nusing GeometricMachineLearning\nusing GeometricMachineLearning: UnknownEncoder, params\n\nmodel = Chain(Dense(5, 3, tanh; use_bias = false), Dense(3, 2, identity; use_bias = false))\nnn = NeuralNetwork(UnknownEncoder(5, 2, 2), model, params(NeuralNetwork(model)), CPU())\n\ntypeof(nn) <: NeuralNetwork{<:GeometricMachineLearning.Encoder}\n\n# output\n\ntrue\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/pod_autoencoders/#GeometricMachineLearning.UnknownDecoder","page":"POD and Autoencoders","title":"GeometricMachineLearning.UnknownDecoder","text":"UnknownDecoder(full_dim, reduced_dim, n_encoder_blocks)\n\nMake an instance of UnknownDecoder.\n\nThis should be used if one wants to use an Decoder that does not have any specific structure.\n\nAn example of using this can be constructed analogously to UnknownDecoder.\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/pod_autoencoders/#GeometricMachineLearning.encoder","page":"POD and Autoencoders","title":"GeometricMachineLearning.encoder","text":"encoder(nn::NeuralNetwork{<:AutoEncoder})\n\nObtain the encoder from an AutoEncoder neural network. \n\n\n\n\n\nencoder(nn)\n\nMake a neural network of type Encoder out of an arbitrary neural network.\n\nImplementation\n\nInternally this allocates a new nerual network of type UnknownEncoder and takes the parameters and the backend from nn.\n\n\n\n\n\n","category":"function"},{"location":"reduced_order_modeling/pod_autoencoders/#GeometricMachineLearning.decoder","page":"POD and Autoencoders","title":"GeometricMachineLearning.decoder","text":"decoder(nn::NeuralNetwork{<:AutoEncoder})\n\nObtain the decoder from an AutoEncoder neural network.\n\n\n\n\n\ndecoder(nn)\n\nMake a neural network of type Decoder out of an arbitrary neural network.\n\nImplementation\n\nInternally this allocates a new nerual network of type UnknownDecoder and takes the parameters and the backend from nn.\n\n\n\n\n\n","category":"function"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"\\begin{comment}","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/#References","page":"POD and Autoencoders","title":"References","text":"","category":"section"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"A. Chatterjee. An introduction to the proper orthogonal decomposition. Current science, 808–817 (2000).\n\n\n\n","category":"page"},{"location":"reduced_order_modeling/pod_autoencoders/","page":"POD and Autoencoders","title":"POD and Autoencoders","text":"\\end{comment}","category":"page"},{"location":"introduction/#Introduction-and-Outline","page":"Introduction and Outline","title":"Introduction and Outline","text":"","category":"section"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"One may argue that structure-preserving machine learning is a contradiction. One of the most popular books on neural networks [43] introduces the term machine learning the following way: \"The difficulties faced by systems relying on hard-coded knowledge suggest that [artificial intelligence] systems need the ability to acquire their own knowledge, by extracting patterns from raw data. This capability is known as machine learning.\" The many success stories of deep neural networks such as ChatGPT [95] have shown that abandoning hard-coded knowledge in favour of extracting patterns can yield enormous improvement for many applications. In scientific computing the story is a different one however. Hard coding of certain properties into an algorithm has proved indispensible for many numerical applications. The introductory chapter to one of the canonical references on geometric numerical integration [1] contains the sentence: \"It turned out that the preservation of geometric properties of the flow not only produces an improved qualitative behaviour, but also allows for a more accurate long-time integration than with general-purpose methods.\" Here \"preservation of geometric properties\" means hard-coding physical information into an algorithm.","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"Despite the allure of neglecting hard-coded knowledge in an \"era of big data\" [96] many researchers have very early realized that systems that work for image recognition, natural language processing and other purely data-driven tasks may not be suitable to treat problems from physics [97]. Scientific machine learning [98], which in this work refers to the application of machine learning techniques for the solution of differential equations from science and engineering, has however much too often neglected the preservation of geometric properties that has proved to be so important in traditional numerics. An ostensible solution is offered by so-called physics-informed neural networks (PINNS), whose eponymous paper [71] is one of the most-cited in scientific machine learning. The authors write: \"Coming to our rescue, for many cases pertaining to the modeling of physical and biological systems, there exists a vast amount of prior knowledge that is currently not being utilized in modern machine learning practice.\" It is stated that PINNS \"[enrich] deep learning with the longstanding developments in mathematical physics;\" one should however add that they also ignore longstanding developments in numerics, like preserving the geometric properties which are observed to be crucial in [1].","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"What this work aims at doing is not \"to set the foundations for a new paradigm\" [71], but rather to show that in many cases it is advantageous to imbue neural networks with specific structure and one should to do this whenever possible. In this regard this work is much more closely related to traditional numerics than to neural network research as we try to design problem-specific algorithms rather than \"universal approximators\" [30]. The structure-preserving neural networks in this work are never fundamentally new architectures but build on existing neural network designs [5, 54] or more classical methods [68]. We design neural networks that have a specific structure encoded in them (modeling part) and then make their behavior reflect information found in data (machine learning part). We refer to this as geometric machine learning.","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"(Image: Geometric machine learning (GML) like traditional geometric numerical integration (GNI) and other structure-preserving numerical methods aims at building models that share properties with the analytic solution of a differential equation.) (Image: Geometric machine learning (GML) like traditional geometric numerical integration (GNI) and other structure-preserving numerical methods aims at building models that share properties with the analytic solution of a differential equation.)","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"In the picture above we visualize that geometric machine learning aims at constructing so-called structure-preserving mappings that are ideally close to the analytic solution and perform better than classical methods (e.g. GNI). Structure-preserving here means that the model shares properties with the analytic solution of the underlying differential equation. In this work the most important of these properties are symplecticity and volume preservation, but this may extend to others such as the null space of certain operators [99] and symmetries encoded into a differential equation [100, 101].","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"For us the biggest motivation for geometric machine learning comes from data-driven reduced order modeling. There we want to find reduced representations of so-called full order models of which we have data available; such reduced representation ideally have much lower computational complexity then the full order model. Data-driven reduced order modeling is especially useful when solving parametric partial differential equations (PPDEs). In this case we can solve the full order model for a few parameter instances and then build a cheaper representation of the full model (a so-called reduced model) with neural networks. This can bring dramatic speed-ups in performance. ","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"Closely linked to the research presented here is the development of a software package written in Julia called GeometricMachineLearning [102]. Throughout this work we will demonstrate concepts such as neural network architecture and (Riemannian) optimization by using GeometricMachineLearning[0]. Most sections contain a subsection Library Functions that explains types and functions in GeometricMachineLearning that pertain to the text in that section (they are generated as so-called docstrings [103]). We show an example here:","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"[0]: This document was produced with GeometricMachineLearning v0.3. It may be that the interface will slightly change in future versions, but efforts will be made to keep these changes as small as possible.","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"GradientCache","category":"page"},{"location":"introduction/#GeometricMachineLearning.GradientCache-introduction","page":"Introduction and Outline","title":"GeometricMachineLearning.GradientCache","text":"GradientCache(Y)\n\nDo not store anything.\n\nThe cache for the GradientOptimizer does not consider past information.\n\n\n\n\n\n","category":"type"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"So the docstring shows the name of the type or method, in most cases how to call it and then gives some information explaining what it does and potentially hyperlinks to other similar docstrings (GradientOptimizer in this case); all of this information is indented by a tab. Docstrings may include other information under subheaders Arguments (showing the arguments the method can be supplied with), Examples (giving more detailed examples (including results) of how to use the method) and Implementation (giving details on how the method is implemented). When we reference a docstring it is always printed in blue (e.g. GradientOptimizer), indicating a hyperlink. In addition there is an index of docstrings showing all docstrings in chronological order with the associated page number.","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"Similar to Library Functions, which is included in most sections, almost every chapter concludes with a section Chapter Summary and an additional section References that shows further related reading material. The Chapter Summary recaps the important aspects of the corresponding chapter, states again what is new (this may be mathematical or software aspects) and gives information to what other parts of the dissertation the contents of the present chapter are relevant.","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"All the code necessary to reproduce the results is included in the text and does not have any specific hardware requirements. Except for training some of the neural networks (which was done on an NVIDA Geforce RTX 4090 [104]) all the code snippets were run on CPU (via GitHub runners [105]). All plots have been generated with Makie [106].","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"This dissertation is structures into four main parts: (i) background information, (ii) an explanation of the optimizer framework used for training neural networks, (iii) a detailed explanation of the various neural network layers and architectures that we use and (iv) a number of examples of how to use GeometricMachineLearning. We explain the content of these parts in some detail[2].","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"[2]: In addition there is also an appendix that provides more implementation details.","category":"page"},{"location":"introduction/#Background-Information","page":"Introduction and Outline","title":"Background Information","text":"","category":"section"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"The background material, which does not include any original work, covers all the prerequisites for introducing our new optimizers in Part II. In addition it introduces some basic functionality of GeometricMachineLearning. It contains (among others) the following sections:","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"Concepts from general topology: here we introduce topological spaces, closedness, compactness, countability and Hausdorffness amongst others. These concepts are prerequisites for defining manifolds.\nGeneral theory on manifolds: we introduce manifolds, the preimage theorem and submersion theorem. These theorems will be used to construct manifolds; the preimage theorem is used to give structure to the Stiefel and the Grassmann manifold, and the immersion theorem gives structure to the solution manifold which is used in reduced order modeling.\nRiemannian manifolds: for optimizing on manifolds we need to define a metric on them, which leads to Riemannian manifolds. We introduce geodesics and the Riemannian gradient here.\nHomogeneous spaces: homogeneous spaces are a special class of manifolds to which our generalized optimizer framework can be applied. They trivially include all Lie groups and spaces like the Stiefel manifold, the Grassmann manifold and the \"homogeneous space of positions and orientations\" [107].\nGlobal tangent spaces: homogeneous spaces allow for identifying for an invariant representation of all tangent spaces which we call global tangent spaces[3]. We explain this concept in this section.\nGeometric structure: structure preservation takes a prominent role in this dissertation. In general structure refers to some property that the analytic solution of a differential equation also has and that we want to preserve when modeling the system. Here we discuss symplecticity and volume preservation in detail. We also introduce neural networks in this chapter and give a definition of geometric neural networks.\nReduced order modeling: reduced order modeling serves as a motivation for most of the architectures introduced here. In this section we introduce the basic idea behind reduced order modeling, show a typical workflow and explain what structure preservation looks like in this context.","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"[3]: These spaces are also discussed in [23, 37].","category":"page"},{"location":"introduction/#The-Optimizer-Framework","page":"Introduction and Outline","title":"The Optimizer Framework","text":"","category":"section"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"One of the central parts of this dissertation is an optimizer framework that allows the generalization of existing optimizers such as Adam [108] and BFGS [44, Chapter 6.1] to homogeneous spaces in a consistent way[4]. This part contains the following sections:","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"Neural Network Optimizers: here we introduce the concept of a neural network optimizer and discuss the modifications we have to make in order to generalize them to homogeneous spaces.\nRetractions: an important concept in manifold optimization are retractions [22]. We introduce them in this section, discuss how they can be constructed for homogeneous spaces and show the two examples of the geodesic retraction and the Cayley retraction.\nParallel Transport: whenever we have an optimizer that contains momentum terms (such as Adam for example) we need to transport these momenta. In this section we explain how this can be done straightforwardly when dealing with homogeneous spaces. \nOptimizer methods: in this section we introduce simple optimizers such as the gradient optimizer, the momentum optimizer and Adam and show how to generalize them to our setting. Due to its increased complexity the BFGS optimizer gets its own section.","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"[4]: The optimizer framework was introduced in [7].","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"(Image: Weights can be put on manifolds to achieve structure preservation or improved stability.) (Image: Weights can be put on manifolds to achieve structure preservation or improved stability.)","category":"page"},{"location":"introduction/#Special-Neural-Network-Layers-and-Architectures","page":"Introduction and Outline","title":"Special Neural Network Layers and Architectures","text":"","category":"section"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"In here we first discuss specific neural network layers and then architectures. A neural network architecture is always a composition of many neural network layers that is designed for a specific task.","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"Special neural network layers include:","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"SympNet layers: symplectic neural networks (SympNets) [5] are special neural networks that are universal approximators in the class of canonical symplectic maps. SympNet layers comprise three different types: linear layers, activation layers and gradient layers. All these are introduced here.\nVolume-preserving layers: the volume-preserving layers presented here are inspired by linear and activation SympNet layers. They slightly differ from other approaches with the same aim [50].\nAttention layers: many fields in neural network research have seen big improvements due to attention mechanisms [54, 87, 88]. Here we introduce this mechanism (which is a neural network layer) and also discuss how to make it volume-preserving [4]. It serves as a basis for multihead attention and linear symplectic attention.","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"Special neural network architectures include:","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"Symplectic autoencoders: the symplectic autoencoder constitutes one of the central elements of this dissertation. It offers a way of flexibly performing nonlinear model order reduction for Hamiltonian systems. In this section we explain its architecture and how it is implemented in GeometricMachineLearning in detail.\nSympNet architectures: based on SympNet layers (which are simple building blocks) one can construct two main types of architectures which are called LA-SympNets and G-SympNets. We explain both here.\nVolume-preserving feedforward neural networks: based on LA-SympNets we build volume-preserving feedforward neural networks that can learn arbitrary volume-preserving maps. \nTransformers: transformer neural networks have revolutionized many fields in machine learning like natural language processing [54] and image recognition [88]. We discuss them here and further imbue them with structure-preserving properties to arrive at volume-preserving transformers and linear symplectic transformers.","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"We note that SympNets, volume-preserving feedforward neural networks and the three transformer types presented here belong to a category of neural network integrators, which are used in the online stage of reduced order modeling. That makes them different from symplectic autoencoders which are used in the offline stage.","category":"page"},{"location":"introduction/#Examples","page":"Introduction and Outline","title":"Examples","text":"","category":"section"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"In this part we demonstrate the neural network architectures implemented in GeometricMachineLearning with a few examples:","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"Symplectic autoencoders: here we show how to reduce the Toda lattice [89], which is a 400-dimensional Hamiltonian system in our case, to a two-dimensional Hamiltonian system with symplectic autoencoders.\nSympNets: this serves as an introductory example into using GeometricMachineLearning and does not contain any new results. It simply shows how to use SympNets to learn the flow of a harmonic oscillator.\nImage classification: Here we perform image classification for the MNIST dataset [90] with vision transformers and show that manifold optimization can enable convergence that would otherwise not be possible.\nThe Grassmann manifold in neural networks: in this example we model a surface embedded in mathbbR^3 with the help of the Grassmann manifold.\nDifferent volume-preserving attention mechanisms: the volume-preserving attention mechanism in GeometricMachineLearning is based on computing correlations in the input sequence. These correlations can be constructed in two different ways. Here we compare these two.\nLinear Symplectic Transformer: the linear symplectic transformer is used to integrate the four-dimensional Hamiltonian system of the coupled harmonic oscillator. Here we compare the linear symplectic transformer to the standard transformer and SympNets.","category":"page"},{"location":"introduction/#Associated-Papers-and-Contributions","page":"Introduction and Outline","title":"Associated Papers and Contributions","text":"","category":"section"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"The following papers have emerged in connection with the development of GeometricMachineLearning:","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"In [7] a new class of optimizers for homogeneous spaces, a category that includes the Stiefel manifold and the Grassmann manifold, is introduced. The results presented in this paper are reproduced in the examples.\nIn [3] we introduced a new neural network architectures that we call symplectic autoencoders. This is capable of performing non-linear Hamiltonian model reduction. During training of these symplectic autoencoders we use the optimizers introduced in [7]. Similar results to what is presented in the paper are reproduced as an example.\nIn [4] we introduce a new neural network architecture that we call volume-preserving transformers. This is a structure-preserving version of the standard transformer [54] for which all components have been made volume preserving. As application we foresee the online phase in reduced order modeling.","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"In addition there are new results presented in this work that have not been written up as a separate paper:","category":"page"},{"location":"introduction/","page":"Introduction and Outline","title":"Introduction and Outline","text":"Similar to the volume-preserving transformer [4] we introduce a linear symplectic transformer that preserves a symplectic product structure and is also foreseen to be used in reduced order modeling.\nWe show how the Grassmann manifold can be included into a neural network and construct a loss based on the Wasserstein distance to approximate a nonlinear space from which we can then sample.","category":"page"},{"location":"data_loader/snapshot_matrix/","page":"Snapshot matrix & tensor","title":"Snapshot matrix & tensor","text":"\\pagestyle{headings}\n\nHere we discuss the \\textit{data loader}, which forms an important part of \\texttt{GeometricMachineLearning}. It is used to store data and sample from them.","category":"page"},{"location":"data_loader/snapshot_matrix/#Snapshot-Matrix","page":"Snapshot matrix & tensor","title":"Snapshot Matrix","text":"","category":"section"},{"location":"data_loader/snapshot_matrix/","page":"Snapshot matrix & tensor","title":"Snapshot matrix & tensor","text":"The snapshot matrix stores solutions of the high-dimensional ODE (obtained from discretizing a PDE). In the offline phase of reduced order modeling this is then used to construct reduced bases in a data-driven way. So (for a single parameter[1]) the snapshot matrix takes the following form: ","category":"page"},{"location":"data_loader/snapshot_matrix/","page":"Snapshot matrix & tensor","title":"Snapshot matrix & tensor","text":"[1]: If we deal with a parametrized PDE then the data can be interpreted as a snapshot tensor. For training an autoencoder in the offline phase however, we interpret these data as a matrix because they come from the same solution manifold; each column of the snapshot matrix M represents a point on the solution manifold.","category":"page"},{"location":"data_loader/snapshot_matrix/","page":"Snapshot matrix & tensor","title":"Snapshot matrix & tensor","text":"M = leftbeginarraycccc\nhatu_1(t_0)   hatu_1(t_1)  quadldotsquad  hatu_1(t_f) \nhatu_2(t_0)   hatu_2(t_1)  ldots  hatu_2(t_f) \nhatu_3(t_0)   hatu_3(t_1)  ldots  hatu_3(t_f) \nldots   ldots  ldots  ldots \nhatu_2N(t_0)   hatu_2N(t_1)  ldots  hatu_2N(t_f) \nendarrayright","category":"page"},{"location":"data_loader/snapshot_matrix/","page":"Snapshot matrix & tensor","title":"Snapshot matrix & tensor","text":"In the example above we store a matrix whose first axis is the system dimension (i.e. a column is an element of mathcalMsubsetmathbbR^2N) and the second dimension gives the time step. ","category":"page"},{"location":"data_loader/snapshot_matrix/","page":"Snapshot matrix & tensor","title":"Snapshot matrix & tensor","text":"The starting point for using the snapshot matrix as data for a machine learning model is that all the columns of M live on a lower-dimensional solution manifold mathcalM and we can use techniques such as proper orthogonal decomposition and autoencoders to find or approximate this solution manifold. We also note that the second axis of M does not necessarily indicate time but can also represent various parameters (including initial conditions).","category":"page"},{"location":"data_loader/snapshot_matrix/#Snapshot-Tensor","page":"Snapshot matrix & tensor","title":"Snapshot Tensor","text":"","category":"section"},{"location":"data_loader/snapshot_matrix/","page":"Snapshot matrix & tensor","title":"Snapshot matrix & tensor","text":"The snapshot tensor fulfills the same role as the snapshot matrix but has a third axis that describes different initial parameters (such as different initial conditions). ","category":"page"},{"location":"data_loader/snapshot_matrix/","page":"Snapshot matrix & tensor","title":"Snapshot matrix & tensor","text":"(Image: ) (Image: )","category":"page"},{"location":"data_loader/snapshot_matrix/","page":"Snapshot matrix & tensor","title":"Snapshot matrix & tensor","text":"When drawing samples from the snapshot tensor to train a neural networks we also need to specify a sequence length (as an argument to the Batch struct). When sampling a batch from the snapshot tensor we sample over the second axis (the time dimension) and the third axis of the tensor (the parameter dimension). The total number of batches[2] is ","category":"page"},{"location":"data_loader/snapshot_matrix/","page":"Snapshot matrix & tensor","title":"Snapshot matrix & tensor","text":"[2]: The number of batches shown here is for the case mathttprediction_window = 0 If mathttprediction_window neq 0 this number may be smaller. The corresponding function is GeometricMachineLearning.number_of_batches.","category":"page"},{"location":"data_loader/snapshot_matrix/","page":"Snapshot matrix & tensor","title":"Snapshot matrix & tensor","text":"lceilmathtt(dlinput_time_steps - batchseq_length) * dln_params  batchbatch_sizerceil","category":"page"},{"location":"data_loader/snapshot_matrix/","page":"Snapshot matrix & tensor","title":"Snapshot matrix & tensor","text":"where lceilcdotrceil is the ceiling operation.","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"In the previous chapter we introduced a general optimizer framework without giving explicit examples of neural network optimizers; this is done here. This chapter is divided into two sections: we first discuss \\textit{standard neural network optimizers} (including Adam) and then the more complicated BFGS optimizer. In the implementation of all these optimizers the \\textit{optimizer cache} will play an important role.","category":"page"},{"location":"optimizers/optimizer_methods/#Standard-Neural-Network-Optimizers","page":"Optimizer Methods","title":"Standard Neural Network Optimizers","text":"","category":"section"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"In this section we discuss optimization methods that are often used in training neural networks. The BFGS optimizer may also be viewed as a standard neural network optimizer but is treated in a separate section because of its complexity. From a perspective of manifolds the optimizer methods outlined here operate on mathfrakg^mathrmhor only. Each of them has a cache associated with it[1] and this cache is updated with the function update!. The precise role of this function is described below.","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"[1]: In the case of the gradient optimizer this cache is trivial.","category":"page"},{"location":"optimizers/optimizer_methods/#The-Gradient-Optimizer","page":"Optimizer Methods","title":"The Gradient Optimizer","text":"","category":"section"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"The gradient optimizer is the simplest optimization algorithm used to train neural networks. It was already briefly discussed when we introduced Riemannian manifolds.","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"It simply does: ","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"mathrmweight leftarrow mathrmweight + (-etacdotmathrmgradient)","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"where addition has to be replaced with appropriate operations in the manifold case[2].","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"[2]: In the manifold case the expression -etacdotmathrmgradient is an element of the global tangent space mathfrakg^mathrmhor and a retraction maps from mathfrakg^mathrmhor. We then still have to compose it with the updated global section Lambda^(t).","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"When calling GradientOptimizer we can specify a learning rate eta (or use the default).","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"using GeometricMachineLearning  # hide\nconst η = 0.01\nmethod = GradientOptimizer(η)","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"In order to use the optimizer we need an instance of Optimizer that is called with the method and the weights of the neural network:","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"weight = (A = zeros(4, 4), )\no = Optimizer(method, weight)","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"If we operate on a derivative with update! this will compute a final velocity that is then used to compute a retraction (or simply perform addition if we do not deal with a manifold):","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"dx = (A = one(weight.A), )\nupdate!(o, o.cache, dx)\n\ndx.A","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"So what has happened here is that the gradient dx was simply multiplied with -eta as the cache of the gradient optimizer is trivial.","category":"page"},{"location":"optimizers/optimizer_methods/#The-Momentum-Optimizer","page":"Optimizer Methods","title":"The Momentum Optimizer","text":"","category":"section"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"The momentum optimizer is similar to the gradient optimizer but further stores past information as first moments. We let these first moments decay with a decay parameter alpha:","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"mathrmweights leftarrow mathrmweights + (alphacdotmathrmmoment - etacdotmathrmgradient)","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"where addition has to be replaced with appropriate operations in the manifold case.","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"In the case of the momentum optimizer the cache is non-trivial:","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"const α = 0.5\nmethod = MomentumOptimizer(η, α)\no = Optimizer(method, weight)\n\no.cache.A # the cache is stored for each array in `weight` (which is a `NamedTuple`)","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"But as the cache is initialized with zeros it will lead to the same result as the gradient optimizer in the first iteration:","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"dx = (A = one(weight.A), )\n\nupdate!(o, o.cache, dx)\n\ndx.A","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"The cache has changed however:","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"o.cache.A","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"If we have weights on manifolds calling Optimizer will automatically allocate the correct cache on mathfrakg^mathrmhor:","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"weight = (Y = rand(StiefelManifold, 5, 3), )\n\nOptimizer(method, weight).cache.Y","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"So if the weight is YinSt(nN) the corresponding cache is initialized as the zero element on mathfrakg^mathrmhorsubsetmathbbR^NtimesN as this is the global tangent space representation corresponding to the StiefelManifold.","category":"page"},{"location":"optimizers/optimizer_methods/#The-Adam-Optimizer","page":"Optimizer Methods","title":"The Adam Optimizer","text":"","category":"section"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"The Adam Optimizer is one of the most widely neural network optimizers. The cache of the Adam optimizer consists of first and second moments. The first moments B_1, similar to the momentum optimizer, store linear information about the current and previous gradients, and the second moments B_2 store quadratic information about current and previous gradients. These second moments can be interpreted as approximating the curvature of the optimization landscape.  ","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"If all the weights are on a vector space, then we directly compute updates for B_1 and B_2:","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"B_1 gets ((rho_1 - rho_1^t)(1 - rho_1^t))cdotB_1 + (1 - rho_1)(1 - rho_1^t)cdotnablaL\nB_2 gets ((rho_2 - rho_1^t)(1 - rho_2^t))cdotB_2 + (1 - rho_2)(1 - rho_2^t)cdotnablaLodotnablaL","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"where odotmathbbR^ntimesmathbbR^ntomathbbR^n is the Hadamard product: aodotb_i = a_ib_i rho_1 and rho_2 are hyperparameters. Their defaults, rho_1=09 and rho_2=099, are taken from [43, page 301]. After having updated the cache (i.e. B_1 and B_2) we compute a velocity with which the parameters of the network are then updated:","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"W_tgets -etaB_1sqrtB_2 + delta\nY^(t+1) gets Y^(t) + W^(t)","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"where the last addition has to be replaced with appropriate operations when dealing with manifolds. Further eta is the learning rate and delta is a small constant that is added for stability. The division, square root and addition in the computation of W_t are performed element-wise.","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"In the following we show a schematic update that Adam performs for the case when no elements are on manifolds (also compare this figure with the general optimization framework):","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"(Image: Schematic representation the Adam optimizer. The first Adam step updates the first and second moments, and the second Adam step outputs the final velocity.) (Image: Schematic representation the Adam optimizer. The first Adam step updates the first and second moments, and the second Adam step outputs the final velocity.)","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"We demonstrate the Adam cache on the same example from before:","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"const ρ₁ = 0.9\nconst ρ₂ = 0.99\nconst δ = 1e-8\n\nmethod = AdamOptimizer(η, ρ₁, ρ₂, δ)\no = Optimizer(method, weight)\n\no.cache.Y","category":"page"},{"location":"optimizers/optimizer_methods/#Weights-on-Manifolds","page":"Optimizer Methods","title":"Weights on Manifolds","text":"","category":"section"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"The problem with generalizing Adam to manifolds is that the Hadamard product odot as well as the other element-wise operations (, sqrt and +) lack a clear geometric interpretation. In GeometricMachineLearning we get around this issue by utilizing the global tangent space representation. A similar approach is shown in [8].","category":"page"},{"location":"optimizers/optimizer_methods/#The-Adam-Optimizer-with-Decay","page":"Optimizer Methods","title":"The Adam Optimizer with Decay","text":"","category":"section"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"The Adam optimizer with decay is similar to the standard Adam optimizer with the difference that the learning rate eta decays exponentially. We start with a relatively high learning rate eta_1 (e.g. 10^-2) and end with a low learning rate eta_2 (e.g. 10^-8). If we want to use this optimizer we have to tell it beforehand how many epochs we train for such that it can adjust the learning rate decay accordingly:","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"const η₁ = 1e-2 \nconst η₂ = 1e-6\nconst n_epochs = 1000 \n\nmethod = AdamOptimizerWithDecay(n_epochs, η₁, η₂, ρ₁, ρ₂, δ)\no = Optimizer(method, weight)\n\nnothing # hide","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"The cache is however exactly the same as for the Adam optimizer:","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"o.cache.Y","category":"page"},{"location":"optimizers/optimizer_methods/#Library-Functions","page":"Optimizer Methods","title":"Library Functions","text":"","category":"section"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"OptimizerMethod\nGradientOptimizer\nMomentumOptimizer\nAdamOptimizer\nAdamOptimizerWithDecay\nAbstractCache\nGradientCache\nMomentumCache\nAdamCache\nupdate!(::Optimizer, ::AbstractCache, ::AbstractArray)","category":"page"},{"location":"optimizers/optimizer_methods/#GeometricMachineLearning.OptimizerMethod","page":"Optimizer Methods","title":"GeometricMachineLearning.OptimizerMethod","text":"OptimizerMethod\n\nEach Optimizer has to be called with an OptimizerMethod. This specifies how the neural network weights are updated in each optimization step.\n\n\n\n\n\n","category":"type"},{"location":"optimizers/optimizer_methods/#GeometricMachineLearning.GradientOptimizer","page":"Optimizer Methods","title":"GeometricMachineLearning.GradientOptimizer","text":"GradientOptimizer(η)\n\nMake an instance of a gradient optimizer. \n\nThis is the simplest neural network optimizer. It has no cache and computes the final velocity as:\n\n    mathrmvelocity gets - etanabla_mathrmweightL\n\nImplementation\n\nThe operations are done as memory efficiently as possible. This means the provided nabla_WL is mutated via:\n\nrmul!(∇L, -method.η)\n\n\n\n\n\n","category":"type"},{"location":"optimizers/optimizer_methods/#GeometricMachineLearning.MomentumOptimizer","page":"Optimizer Methods","title":"GeometricMachineLearning.MomentumOptimizer","text":"MomentumOptimizer(η, α)\n\nMake an instance of the momentum optimizer.\n\nThe momentum optimizer is similar to the GradientOptimizer. It however has a nontrivial cache that stores past history (see MomentumCache). The cache is updated via:\n\n    B^mathrmcache gets alphaB^mathrmcache + nabla_mathrmweightsL\n\nand then the final velocity is computed as\n\n    mathrmvelocity gets  - etaB^mathrmcache\n\nImplementation\n\nTo save memory the velocity is stored in the input nabla_WL. This is similar to the case of the GradientOptimizer.\n\n\n\n\n\n","category":"type"},{"location":"optimizers/optimizer_methods/#GeometricMachineLearning.AdamOptimizer","page":"Optimizer Methods","title":"GeometricMachineLearning.AdamOptimizer","text":"AdamOptimizer(η, ρ₁, ρ₂, δ)\n\nMake an instance of the Adam Optimizer.\n\nHere the cache consists of first and second moments that are updated as \n\nB_1 gets ((rho_1 - rho_1^t)(1 - rho_1^t))cdotB_1 + (1 - rho_1)(1 - rho_1^t)cdotnablaL\n\nand\n\nB_2 gets ((rho_2 - rho_1^t)(1 - rho_2^t))cdotB_2 + (1 - rho_2)(1 - rho_2^t)cdotnablaLodotnablaL\n\nThe final velocity is computed as:\n\nmathrmvelocity gets -etaB_1sqrtB_2 + delta\n\nImplementation\n\nThe velocity is stored in the input to save memory:\n\nmul!(B, -o.method.η, /ᵉˡᵉ(C.B₁, scalar_add(racᵉˡᵉ(C.B₂), o.method.δ)))\n\nwhere B is the input to the [update!] function.\n\nThe algorithm and suggested defaults are taken from [43, page 301].\n\n\n\n\n\n","category":"type"},{"location":"optimizers/optimizer_methods/#GeometricMachineLearning.AdamOptimizerWithDecay","page":"Optimizer Methods","title":"GeometricMachineLearning.AdamOptimizerWithDecay","text":"AdamOptimizerWithDecay(n_epochs, η₁=1f-2, η₂=1f-6, ρ₁=9f-1, ρ₂=9.9f-1, δ=1f-8)\n\nMake an instance of the Adam Optimizer with weight decay.\n\nAll except the first argument (the number of epochs) have defaults.\n\nThe difference to the standard AdamOptimizer is that we change the learning reate eta in each step. Apart from the time dependency of eta the two algorithms are however equivalent. eta(0) starts with a high value eta_1 and then exponentially decrease until it reaches eta_2 with\n\n eta(t) = gamma^teta_1\n\nwhere gamma = exp(log(eta_1  eta_2)  mathttn_epochs)\n\n\n\n\n\n","category":"type"},{"location":"optimizers/optimizer_methods/#GeometricMachineLearning.AbstractCache","page":"Optimizer Methods","title":"GeometricMachineLearning.AbstractCache","text":"AbstractCache\n\nAbstractCache has subtypes: AdamCache, MomentumCache, GradientCache and BFGSCache.\n\nAll of them can be initialized with providing an array (also supporting manifold types).\n\n\n\n\n\n","category":"type"},{"location":"optimizers/optimizer_methods/#GeometricMachineLearning.GradientCache","page":"Optimizer Methods","title":"GeometricMachineLearning.GradientCache","text":"GradientCache(Y)\n\nDo not store anything.\n\nThe cache for the GradientOptimizer does not consider past information.\n\n\n\n\n\n","category":"type"},{"location":"optimizers/optimizer_methods/#GeometricMachineLearning.MomentumCache","page":"Optimizer Methods","title":"GeometricMachineLearning.MomentumCache","text":"MomentumCache(Y)\n\nStore the moment for Y (initialized as zeros).\n\nThe moment is called B.\n\nIf the cache is called with an instance of a Manifold it initializes the moments as elements of mathfrakg^mathrmhor (AbstractLieAlgHorMatrix).\n\nSee AdamCache.\n\n\n\n\n\n","category":"type"},{"location":"optimizers/optimizer_methods/#GeometricMachineLearning.AdamCache","page":"Optimizer Methods","title":"GeometricMachineLearning.AdamCache","text":"AdamCache(Y)\n\nStore the first and second moment for Y (initialized as zeros).\n\nFirst and second moments are called B₁ and B₂.\n\nIf the cache is called with an instance of a homogeneous space, e.g. the StiefelManifold St(nN) it initializes the moments as elements of mathfrakg^mathrmhor (StiefelLieAlgHorMatrix).\n\nExamples\n\nusing GeometricMachineLearning\n\nY = rand(StiefelManifold, 5, 3)\nAdamCache(Y).B₁\n\n# output\n\n5×5 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:\n 0.0  -0.0  -0.0  -0.0  -0.0\n 0.0   0.0  -0.0  -0.0  -0.0\n 0.0   0.0   0.0  -0.0  -0.0\n 0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0\n\n\n\n\n\n","category":"type"},{"location":"optimizers/optimizer_methods/#AbstractNeuralNetworks.update!-Tuple{Optimizer, AbstractCache, AbstractArray}","page":"Optimizer Methods","title":"AbstractNeuralNetworks.update!","text":"update!(o, cache, B)\n\nUpdate the cache and output a final velocity that is stored in B.\n\nNote that Binmathfrakg^mathrmhor in general.\n\nIn the manifold case the final velocity is the input to a retraction.\n\n\n\n\n\n","category":"method"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"\\begin{comment}","category":"page"},{"location":"optimizers/optimizer_methods/#References","page":"Optimizer Methods","title":"References","text":"","category":"section"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"I. Goodfellow, Y. Bengio and A. Courville. Deep learning (MIT press, Cambridge, MA, 2016).\n\n\n\n","category":"page"},{"location":"optimizers/optimizer_methods/","page":"Optimizer Methods","title":"Optimizer Methods","text":"\\end{comment}","category":"page"},{"location":"structure_preservation/volume_preservation/#Divergence-Free-Vector-Fields","page":"Volume-Preservation","title":"Divergence-Free Vector Fields","text":"","category":"section"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"The quality of being divergence-free greatly restricts the number of possible vector fields and also the dynamically accessible states of the flow map. It is however a weaker property then being Hamiltonian. We first define what it means to be divergence-free:","category":"page"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"Main.definition(raw\"A vector field ``X:\\mathcal{M}\\to{}T\\mathcal{M}`` defined on a ``d``-dimensional Riemannian manifold ``(\\mathcal{M}, g)`` is called **divergence-free** if ``\\forall{}z\\in\\mathcal{M}`` we have:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\mathrm{div} (X)= \\sum_{i = 1}^d {\\frac {1}{\\rho }}{\\frac {\\partial \\left({\\frac {\\rho }{\\sqrt {g_{ii}}}}{\\hat {X}}^{i}\\right)}{\\partial z^{i}}}= \\sum_{i = 1}^d {\\frac {1}{\\sqrt {\\det g}}}{\\frac {\\partial \\left({\\sqrt {\\frac {\\det g}{g_{ii}}}}\\,{\\hat {X}}^{i}\\right)}{\\partial z^{i}}} = 0,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"for some parametrization of a neighborhood around ``z``.\")","category":"page"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"Main.remark(raw\"If we do not deal with a general Riemannian manifold but simply with a vector space, the divergence for ``X:\\mathbb{R}^d\\to\\mathbb{R}^d`` is usually written as\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\mathrm{div}(X) = \\nabla\\cdot{}X = \\sum_{i=1}^d\\frac{\\partial{}X_i}{\\partial{}z_i},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``z`` are global coordinates.\")","category":"page"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"We further define what it means to be volume-preserving:","category":"page"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"Main.definition(raw\"We call a map ``\\phi:\\mathcal{M}\\to\\mathcal{M}`` **volume-preserving** if for all volume elements ``V\\subset\\mathcal{M}`` we have that ``\\mathrm{vol}(V) = \\mathrm{vol}(\\phi(V)),`` where vol is a measure of a volume in a Riemannian manifold.\")","category":"page"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"Main.remark(raw\"If we deal with vector spaces instead of more general manifolds the property of being **volume-preserving** in some domain ``\\mathcal{D}\\subset\\mathbb{R}^d`` can be expressed as\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\det\\nabla_z\\varphi^t = 1 \\quad \\forall{}z\\in\\mathcal{D}, t\\in[0, T].\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"So the columns of the Jacobian of ``\\varphi^t`` span a volume element of size 1 for each ``z`` and each ``t``.\")","category":"page"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"We can proof the theorem: ","category":"page"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"Main.theorem(raw\"The flow of a divergence-free vector field is volume-preserving.\")","category":"page"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"Here we only proof this statement for the case of a vector space. A proof of the more general statement can be found in standard textbooks on differential geometry[1], e.g. [15, 16].","category":"page"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"[1]: Together with a precise definition of Riemannian integration and the volume form introduced above.","category":"page"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"Main.proof(raw\"We refer to the flow of ``X`` by ``\\varphi^t:\\mathbb{R}^d\\to\\mathbb{R}^d`` and have the following property:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\frac{d}{dt}\\nabla\\varphi^t(z) = \\nabla{}X(\\varphi^t(z))\\nabla\\varphi^t(z).\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"Note that we used the convention ``[\\nabla{}X(z)]_{ij} = \\partial/\\partial{}z_jX_i`` here. This expression for ``d/dt\\nabla{}\\varphi^t(x)`` further implies:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\mathrm{Tr}\\left( (\\nabla\\varphi^t(z))^{-1}\\frac{d}{dt}\\nabla\\varphi^t(z) \\right) = \\mathrm{Tr}\\left( (\\nabla\\varphi^t(z))^{-1}\\nabla{}X(\\varphi^t(z))\\nabla\\varphi^t(z) \\right) = \\mathrm{Tr}(\\nabla{}X(\\varphi^t(z))) = 0,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where we have used ``\\mathrm{Tr}(ABC) = \\mathrm{Tr}(BCA)`` in the second equality, and ``\\mathrm{Tr}(\\nabla{}X) = \\sum_{i=1}^d\\partial{}X_i/\\partial{}z_i = \\mathrm{div}(X)`` and the divergence-freeness of ``X`` in the third equality. We further have\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\mathrm{Tr}(A^{-1}\\dot{A}) = \\frac{\\frac{d}{dt}\\mathrm{det}(A)}{\\mathrm{det}(A)},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"which can be derived from the classical result ``\\mathrm{det}(\\mathrm{exp}(A)) = \\mathrm{exp}(\\mathrm{Tr}(A)).`` Hence we have\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\frac{d}{dt}\\mathrm{det}(\\nabla\\varphi^t(z)) = 0,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"and the result is proved.\")","category":"page"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"It is a classical result that all Hamiltonian vector fields are divergence-free, so volume-preservation is weaker than preservation of symplecticity [27].","category":"page"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"\\begin{comment}","category":"page"},{"location":"structure_preservation/volume_preservation/#References","page":"Volume-Preservation","title":"References","text":"","category":"section"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"S. I. Richard L. Bishop. Tensor Analysis on Manifolds (Dover Publications, Mineola, New York, 1980).\n\n\n\nS. Lang. Fundamentals of differential geometry. Vol. 191 (Springer Science & Business Media, 2012).\n\n\n\nV. I. Arnold. Mathematical methods of classical mechanics. Vol. 60 of Graduate Texts in Mathematics (Springer Verlag, Berlin, 1978).\n\n\n\n","category":"page"},{"location":"structure_preservation/volume_preservation/","page":"Volume-Preservation","title":"Volume-Preservation","text":"\\end{comment}","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/#Structure-Preserving-Neural-Networks","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"","category":"section"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"What we means by a structure-preserving neural network or a geometric neural network is a modification of a standard neural networks such that it satisfies certain properties like symplecticity or volume preservation. We first define standard neural networks:","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"Main.definition(raw\"A **neural network architecture** is a parameter-dependent realization of a function:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\mathrm{architecture}: \\mathbb{P} \\to \\mathcal{C}(\\mathcal{D}, \\mathcal{M}), \\Theta \\mapsto \\mathcal{NN}_\\Theta,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``\\Theta`` are the *parameters of the neural network* (we call ``\\mathbb{P}`` the parameter space). ``\\mathbb{P}``, the domain space ``\\mathcal{D}`` and the target space ``\\mathcal{M}`` of the neural network may be spaces with arbitrary structure in general (i.e. need not be vector spaces).\")","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"In this text the spaces mathcalD and mathcalM are vector spaces in most cases[1]. The parameter space mathbbP is however build from manifolds in many cases. Weights have to be put on manifolds to realize certain architectures that would otherwise not be possible and can make training more efficient in other cases.","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"[1]: One exception is Grassmann learning where we learn a vector space.","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"It is a classical result [30] that one-layer feedforward neural networks[2] are universal approximators:","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"[2]: We obtain one-layer feedforward neural networks by identifying mathcalP = mathbbR^NtimesntimesmathbbR^NtimesmathbbR^mtimesNni(A b C) = Theta and mathcalNN_Theta(x) = Csigma(Ax + b) for some scalar function sigmamathbbRtomathbbR that is non-polynomial.","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"Main.theorem(raw\"Neural networks are dense in the space of continuous functions ``\\mathcal{C}^(U, \\mathbb{R}^m)`` in the *compact-open topology*, i.e. for every compact subset ``K\\subset{}U,`` real number ``\\varepsilon>0`` and function ``f\\in\\mathcal{C}(U, \\mathbb{R}^m)`` we can find an integer ``N`` as well as weights\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\Theta = (A, b, C) \\in \\mathbb{R}^{N\\times{}n}\\times\\mathbb{R}^{N}\\times\\mathbb{R}^{m\\times{}N} =: \\mathbb{P}\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"such that \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\sup_{x \\in K}|| f(x) - C\\sigma(Ax + b) || < \\varepsilon,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"i.e. neural networks can approximate ``f`` arbitrarily well on any compact set ``K``.\")","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"The universal approximation theorem has also been generalized to other neural network architectures [31–33].","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"A structure-preserving or geometric neural networks is a neural network that has additional properties:","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"Main.definition(raw\"A **structure-preserving neural network architecture** is a parameter-dependent realization of a function:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\mathrm{sp}\\cdot\\mathrm{architecture}: \\mathbb{P} \\to \\mathcal{C}(\\mathcal{D}, \\mathcal{M}), \\Theta \\mapsto \\mathcal{NN}_\\Theta,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"such that ``\\mathcal{NN}_\\Theta`` preserves some structure.\")","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"Main.example(raw\"We say that a neural network is **symplectic** if ``\\mathcal{NN}_\\Theta:\\mathbb{R}^n\\to\\mathbb{R}^m`` (with ``m\\geq{}n``) preserves ``\\mathbb{J}``, i.e. \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    (\\nabla_z\\mathcal{NN}_\\Theta)^T\\mathbb{J}_{2m}(\\nabla_z\\mathcal{NN}_\\Theta) = \\mathbb{J}_{2n},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``z`` are coordinates on ``\\mathbb{R}^n``.\")","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"If we have m = n then we can use SympNets to realize such architectures; SympNets furthermore are universal approximators for the set of canonical symplectic maps[3] [5]. If m neq n we can use symplectic autoencoders to realize such an architecture. A different class of neural networks are volume-preserving neural networks:","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"[3]: Other neural network architectures that were developed with the same aim are Hamiltonian neural networks [6], Hénon nets [34] and generalized Hamiltonian neural networks [35]. [36] gives an overview over structure preserving neural networks.","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"Main.example(raw\"We say that a neural network is **volume-preserving** if ``\\mathcal{NN}_\\Theta:\\mathbb{R}^n\\to\\mathbb{R}^n`` is such that: \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\det(\\nabla_z\\mathcal{NN}_\\Theta) = 1,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``z`` are coordinates on ``\\mathbb{R}^n``.\")","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"Note that here we keep the dimension constant. Volume-preserving neural networks can be built on the basis of feedforward neural networks or transformers.","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"\\section*{Chapter Summary}\n\nIn this chapter we introduced \\textit{symplecticity} and \\textit{volume presentation} (or divergence-freeness for the corresponding vector field) as examples of geometric structure. \\textit{Symplecticity} is a property of the flow of Hamiltonian vector fields that dramatically restricts the accessible states of freedom. In many applications, neural networks, similar to classical numerical methods, aim at modeling the flow of a differential equation. Because symplecticity is a very restrictive property that we know the flow of the differential equation has, it is advantageous to also imbue the neural network with this property. We hence defined \\textit{structure-preserving neural networks} as the ones that preserve symplecticity (or other structure) in this chapter.\n\\begin{comment}","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/#References","page":"Structure-Preserving Neural Networks","title":"References","text":"","category":"section"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"S. Greydanus, M. Dzamba and J. Yosinski. Hamiltonian neural networks. Advances in neural information processing systems 32 (2019).\n\n\n\nJ. W. Burby, Q. Tang and R. Maulik. Fast neural Poincaré maps for toroidal magnetic fields. Plasma Physics and Controlled Fusion 63, 024001 (2020).\n\n\n\nP. Horn, V. Saz Ulibarrena, B. Koren and S. Portegies Zwart. A Generalized Framework of Neural Networks for Hamiltonian Systems. SSRN preprint SSRN:4555181 (2023).\n\n\n\nE. Celledoni, M. J. Ehrhardt, C. Etmann, R. I. McLachlan, B. Owren, C.-B. Schonlieb and F. Sherry. Structure-preserving deep learning. European journal of applied mathematics 32, 888–936 (2021).\n\n\n\n","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"\\end{comment}","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"<!--","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/#References-2","page":"Structure-Preserving Neural Networks","title":"References","text":"","category":"section"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"V. I. Arnold. Mathematical methods of classical mechanics. Vol. 60 of Graduate Texts in Mathematics (Springer Verlag, Berlin, 1978).\n\n\n\nE. Hairer, C. Lubich and G. Wanner. Geometric Numerical integration: structure-preserving algorithms for ordinary differential equations (Springer, Heidelberg, 2006).\n\n\n\nB. Leimkuhler and S. Reich. Simulating hamiltonian dynamics. No. 14 (Cambridge university press, 2004).\n\n\n\nS. Greydanus, M. Dzamba and J. Yosinski. Hamiltonian neural networks. Advances in neural information processing systems 32 (2019).\n\n\n\nJ. W. Burby, Q. Tang and R. Maulik. Fast neural Poincaré maps for toroidal magnetic fields. Plasma Physics and Controlled Fusion 63, 024001 (2020).\n\n\n\nP. Horn, V. Saz Ulibarrena, B. Koren and S. Portegies Zwart. A Generalized Framework of Neural Networks for Hamiltonian Systems. SSRN preprint SSRN:4555181 (2023).\n\n\n\nE. Celledoni, M. J. Ehrhardt, C. Etmann, R. I. McLachlan, B. Owren, C.-B. Schonlieb and F. Sherry. Structure-preserving deep learning. European journal of applied mathematics 32, 888–936 (2021).\n\n\n\n","category":"page"},{"location":"structure_preservation/structure_preserving_neural_networks/","page":"Structure-Preserving Neural Networks","title":"Structure-Preserving Neural Networks","text":"-->","category":"page"},{"location":"toc/","page":"-","title":"-","text":"\\tableofcontents\n\\clearpage\n\\mainmatter","category":"page"},{"location":"architectures/symplectic_autoencoder/#The-Symplectic-Autoencoder","page":"Symplectic Autoencoders","title":"The Symplectic Autoencoder","text":"","category":"section"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"Symplectic autoencoders offer a structure-preserving way of mapping a high-dimensional system to a low-dimensional system. Concretely this means that if we obtain a reduced system by means of a symplectic autoencoder, this system will again be symplectic; we can thus model a symplectic FOM with a symplectic ROM. ","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"The architecture is represented by the figure below[1]:","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"[1]: For the symplectic autoencoder we only use SympNet gradient layers because they seem to outperform LA-SympNets in many cases and are easier to interpret: their nonlinear part is the gradient of a function that only depends on half the coordinates.","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"(Image: A visualization of the symplectic autoencoder architecture. It is a composition of SympNet layers and PSD-like layers.) (Image: A visualization of the symplectic autoencoder architecture. It is a composition of SympNet layers and PSD-like layers.)","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"It is a composition of SympNet gradient layers and PSD-like matrices, so a matrix A_i (respectively A_i^+) is of the form","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"    A_i^(+) = beginbmatrix Phi_i  mathbbO  mathbbO  Phi_i endbmatrix text where begincases Phi_iinSt(d_id_i+1)subsetmathbbR^d_i+1timesd_i  textif d_i+1  d_i\n    \n    Phi_iinSt(d_i+1d_i)subsetmathbbR^ditimesd_i+1  textif d_i  d_i+1\n    endcases","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"where A_i^(+) = A_i if d_i+1  d_i and A_i^(+) = A_i^+ if d_i+1  d_i Also note that for cotangent lift-like matrices we have","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"beginaligned\n    A_i^+ = mathbbJ_2N A_i^T mathbbJ_2n^T  = beginbmatrix mathbbO_ntimesn  mathbbI_n  -mathbbI_n  mathbbO_ntimesn endbmatrix beginbmatrix Phi_i^T  mathbbO_ntimesN  mathbbO_ntimesN  Phi_i^T endbmatrix beginbmatrix mathbbO_NtimesN  - mathbbI_N  mathbbI_N  mathbbO_NtimesN endbmatrix   = beginbmatrix Phi_i^T  mathbbO_ntimesN  mathbbO_ntimesN  Phi_i^T endbmatrix = A_i^T\nendaligned","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"so the symplectic inverse is equivalent to a matrix transpose in this case. In the symplectic autoencoder we use SympNets as a form of symplectic preprocessing before the linear symplectic reduction (i.e. the PSD layer) is employed. The resulting neural network has some of its weights on manifolds, which is why we cannot use standard neural network optimizers, but have to resort to manifold optimizers. Note that manifold optimization is not necessary for the weights corresponding to the SympNet layers, these are still updated with standard neural network optimizers during training. Also note that SympNets are nonlinear and preserve symplecticity, but they cannot change the dimension of a system while PSD layers can change the dimension of a system and preserve symplecticity, but are strictly linear. Symplectic autoencoders have all three properties: they preserve symplecticity, can change dimension and are nonlinear mappings. We can visualize this in a Venn diagram:","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"(Image: Venn diagram visualizing that a symplectic autoencoder (SAE) is symplectic, can change dimension and is nonlinear.) (Image: Venn diagram visualizing that a symplectic autoencoder (SAE) is symplectic, can change dimension and is nonlinear.)","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We now show the proof that shows nabla_mathcalR(z)psi = (nabla_zmathcalR)^+ which was used when showing the equivalence between Hamiltonian systems on the full and the reduced space:","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"Main.proof(raw\"\"\"The symplectic autoencoder is a composition of ``G``-SympNet layers and PSD-like matrices:\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"\\Psi^d = A_n\\circ\\psi_n\\circ\\cdots\\circ{}A_1\\circ\\psi_1.\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"It's local inverse is\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"(\\Psi^d)^{-1} = \\psi_1^{-1}\\circ{}A_1^+\\circ\\ldots\\circ\\psi_n^{-1}\\circ{}A_n^+.\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"The jacobian of ``\\Psi^d`` is:\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"\\nabla_z\\Psi^d = A_n\\nabla_{A_{n-1}\\cdots{}A_1\\psi_1(z)}\\psi_n\\cdots{}A_1\\nabla_z\\psi_1,\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"and thus\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"(\\nabla_z\\Psi^d)^+ = (\\nabla\\psi)^+A_1^+\\cdots(\\nabla\\psi_n)^+A_n^+,\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"where we dropped the argument in the derivative of the nonlinear parts. We further have\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"A^+ = A^T\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"for PSD-like matrices and\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"(\\nabla_z\\psi)^+ = \\begin{pmatrix} \\mathbb{O} & \\mathbb{I} \\\\ -\\mathbb{I} & \\mathbb{O}  \\end{pmatrix} \\begin{pmatrix} \\mathbb{I} & \\nabla_pf \\\\ \\mathbb{O} & \\mathbb{I} \\end{pmatrix}^T \\begin{pmatrix} \\mathbb{O} & -\\mathbb{I} \\\\ \\mathbb{I} & \\mathbb{O}  \\end{pmatrix} = \\begin{pmatrix} \\mathbb{I} & -\\nabla_pf \\\\ \\mathbb{O} & \\mathbb{I} \\end{pmatrix},\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"for the ``G``-SympNet layers, where we assumed that ``\\psi`` only changes the ``q`` component. Because these matrices are square, the inverse ``(\\nabla\\psi)^+ = (\\nabla\\psi)^{-1}`` is unique.\"\"\")","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"The SympNet layers in the symplectic autoencoder operate in intermediate dimensions (as well as the input and output dimensions). In the following we explain how GeometricMachineLearning computes those intermediate dimensions. ","category":"page"},{"location":"architectures/symplectic_autoencoder/#Intermediate-Dimensions","page":"Symplectic Autoencoders","title":"Intermediate Dimensions","text":"","category":"section"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"For a high-fidelity system of dimension 2N and a reduced system of dimension 2n, the intermediate dimensions in the symplectic encoder and the decoder are computed according to: ","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"iterations = Vector{Int}(n : (N - n) ÷ (n_blocks - 1) : N)\niterations[end] = full_dim2\niterations * 2","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"So for e.g. 2N = 100 2n = 10 and mathttn_blocks = 3 we get ","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"mathrmiterations = 5mathtt(45 div 2)mathtt50 = 5mathtt22mathtt50 = (5 27 49)","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We still have to perform the two other modifications in the algorithm above:","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"iterations[end] = full_dim2 ldots assign full_dim2 to the last entry,\niterations * 2 ldots multiply all the intermediate dimensions by two.","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"The resulting dimensions are:","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"(10 54 100)","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"The second step (the multiplication by two) is needed to arrive at intermediate dimensions that are even. This is necessary to preserve the canonical symplectic structure of the system.","category":"page"},{"location":"architectures/symplectic_autoencoder/#Example","page":"Symplectic Autoencoders","title":"Example","text":"","category":"section"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"A visualization of an instance of SymplecticAutoencoder is shown below: ","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"(Image: Example of a symplectic autoencoder. The SympNet layers are in green, the PSD-like layers are in blue.) (Image: Example of a symplectic autoencoder. The SympNet layers are in green, the PSD-like layers are in blue.)","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"In this figure we have the following configuration: n_encoder_blocks is two, n_encoder_layers is four, n_decoder_blocks is three and n_decoder_layers is two. For a full dimension of 100 and a reduced dimension of ten we can build such an instance of a symplectic autoencoder by calling:","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"using GeometricMachineLearning\n\nconst full_dim = 100\nconst reduced_dim = 10\n\nmodel = SymplecticAutoencoder(full_dim, reduced_dim; \n                                                    n_encoder_blocks = 2, \n                                                    n_encoder_layers = 4, \n                                                    n_decoder_blocks = 3, \n                                                    n_decoder_layers = 2)\n@assert Chain(model).layers[1] == GradientLayerQ{100, 100, typeof(tanh)}(500, tanh) # hide\n@assert Chain(model).layers[2] == GradientLayerP{100, 100, typeof(tanh)}(500, tanh) # hide\n@assert Chain(model).layers[3] == GradientLayerQ{100, 100, typeof(tanh)}(500, tanh) # hide\n@assert Chain(model).layers[4] == GradientLayerP{100, 100, typeof(tanh)}(500, tanh) # hide\n@assert Chain(model).layers[5] == PSDLayer{100, 10}() # hide\n@assert Chain(model).layers[6] == GradientLayerQ{10, 10, typeof(tanh)}(50, tanh) # hide\n@assert Chain(model).layers[7] == GradientLayerP{10, 10, typeof(tanh)}(50, tanh) # hide\n@assert Chain(model).layers[8] == PSDLayer{10, 54}() # hide\n@assert Chain(model).layers[9] == GradientLayerQ{54, 54, typeof(tanh)}(270, tanh) # hide\n@assert Chain(model).layers[10] == GradientLayerP{54, 54, typeof(tanh)}(270, tanh) # hide\n@assert Chain(model).layers[11] == PSDLayer{54, 100}() # hide\n\nfor layer in Chain(model)\n    println(stdout, layer)\nend","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We also see that the intermediate dimension in the decoder is 54  for the specified dimensions and n_decoder_blocks = 3 as was outlined before.","category":"page"},{"location":"architectures/symplectic_autoencoder/#Library-Functions","page":"Symplectic Autoencoders","title":"Library Functions","text":"","category":"section"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"SymplecticAutoencoder","category":"page"},{"location":"architectures/symplectic_autoencoder/#GeometricMachineLearning.SymplecticAutoencoder","page":"Symplectic Autoencoders","title":"GeometricMachineLearning.SymplecticAutoencoder","text":"SymplecticAutoencoder(full_dim, reduced_dim)\n\nMake an instance of SymplecticAutoencoder for dimensions full_dim and reduced_dim.\n\nThe architecture\n\nThe symplectic autoencoder architecture was introduced in [3]. Like any other autoencoder it consists of an encoder Psi^emathbbR^2NtomathbbR^2n and a decoder Psi^dmathbbR^2ntomathbbR^2N with nllN. These satisfy the following properties: \n\nbeginaligned\n    nabla_zPsi^emathbbJ_2N(nabla_zPsi^emathbbJ_2N)^T = mathbbJ_2n  quadtextand \n    (nabla_xiPsi^d)^TmathbbJ_2Nnabla_xiPsi^d = mathbbJ_2n  \nendaligned\n\nBecause the decoder has this particular property, the reduced system can be described by the Hamiltonian HcircPsi^d: \n\nmathbbJ_2nnabla_xi(HcircPsi^d) = mathbbJ_2n(nabla_xiPsi^d)^Tnabla_Psi^d(xi)H = mathbbJ_2n(nabla_xiPsi^d)^TmathbbJ_2N^TmathbbJ_2Nnabla_Psi^d(xi)H = (nabla_xiPsi^d)^+X_H(Psi^d(xi))\n\nwhere (nabla_xiPsi^d)^+ is the symplectic inverse of nabla_xiPsi^d (for more details see the docs on the AutoEncoder type).\n\nArguments\n\nBesides the required arguments full_dim and reduced_dim you can provide the following keyword arguments:\n\nn_encoder_layers::Integer = 4: The number of layers in one encoder block.\nn_encoder_blocks::Integer = 2: The number of encoder blocks.\nn_decoder_layers::Integer = 1: The number of layers in one decoder block.\nn_decoder_blocks::Integer = 3: The number of decoder blocks.\nsympnet_upscale::Integer = 5: The upscaling dimension of the GSympNet. See GradientLayerQ and GradientLayerP.\nactivation = tanh: The activation in the gradient layers.\nencoder_init_q::Bool = true: Specifies if the first layer in each encoder block should be of q type.\ndecoder_init_q::Bool = true: Specifies if the first layer in each decoder block should be of p type.\n\n\n\n\n\n","category":"type"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"\\begin{comment}","category":"page"},{"location":"architectures/symplectic_autoencoder/#References","page":"Symplectic Autoencoders","title":"References","text":"","category":"section"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"B. Brantner and M. Kraus. Symplectic autoencoders for Model Reduction of Hamiltonian Systems, arXiv preprint arXiv:2312.10004 (2023).\n\n\n\n","category":"page"},{"location":"architectures/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"\\end{comment}","category":"page"},{"location":"tutorials/linear_symplectic_transformer/#linear_symplectic_transformer_tutorial","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"","category":"section"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"In this section we compare the linear symplectic transformer to the standard transformer. The example we treat here is the coupled harmonic oscillator:","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"(Image: Visualization of the coupled harmonic oscillator.) (Image: Visualization of the coupled harmonic oscillator.)","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"It is a Hamiltonian system with ","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"H(q_1 q_2 p_1 p_2) = fracq_1^22m_1 + fracq_2^22m_2 + k_1fracq_1^22 + k_2fracq_2^22 +  ksigma(q_1)frac(q_2 - q_1)^22","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"where sigma(x) = 1  (1 + e^-x) is the sigmoid activation function. The system parameters are:","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"k_1: spring constant belonging to m_1,\nk_2: spring constant belonging to m_2,\nm_1: mass 1,\nm_2: mass 2,\nk: coupling strength between the two masses. ","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"To demonstrate the efficacy of the linear symplectic transformer here we will leave the parameters fixed but alter the initial conditions[1]:","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"[1]: We here use the implementation of the coupled harmonic oscillator from GeometricProblems.","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"using GeometricMachineLearning # hide\nusing GeometricProblems.CoupledHarmonicOscillator: hodeensemble, default_parameters\nusing GeometricIntegrators: ImplicitMidpoint, integrate # hide\nusing LaTeXStrings # hide\nusing CairoMakie  # hide\nCairoMakie.activate!() # hide\nimport Random # hide\nRandom.seed!(123) # hide\n\nconst tstep = .3\nconst n_init_con = 5\n\n# ensemble problem\nep = hodeensemble([rand(2) for _ in 1:n_init_con], [rand(2) for _ in 1:n_init_con]; tstep = tstep)\ndl = DataLoader(integrate(ep, ImplicitMidpoint()); suppress_info = true)\n# dl = DataLoader(vcat(dl_nt.input.q, dl_nt.input.p))  # hide\n\nnothing # hide","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"We now define the architectures and train them: ","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"const seq_length = 4\nconst batch_size = 1024\nconst n_epochs = 2000\n\narch_standard = StandardTransformerIntegrator(dl.input_dim; n_heads = 2, \n                                                            L = 1, \n                                                            n_blocks = 2)\narch_symplectic = LinearSymplecticTransformer(  dl.input_dim, \n                                                seq_length; n_sympnet = 2,\n                                                L = 1, \n                                                upscaling_dimension = 2 * dl.input_dim)\narch_sympnet = GSympNet(dl.input_dim;   n_layers = 4, \n                                        upscaling_dimension = 2 * dl.input_dim)\n\nnn_standard = NeuralNetwork(arch_standard)\nnn_symplectic = NeuralNetwork(arch_symplectic)\nnn_sympnet = NeuralNetwork(arch_sympnet)\n\no_method = AdamOptimizerWithDecay(n_epochs, Float64)\n\no_standard = Optimizer(o_method, nn_standard)\no_symplectic = Optimizer(o_method, nn_symplectic)\no_sympnet = Optimizer(o_method, nn_sympnet)\n\nbatch = Batch(batch_size, seq_length)\nbatch2 = Batch(batch_size)\n\nloss_array_standard = o_standard(nn_standard, dl, batch, n_epochs; show_progress = false)\nloss_array_symplectic = o_symplectic(nn_symplectic, dl, batch, n_epochs; show_progress = false)\nloss_array_sympnet = o_sympnet(nn_sympnet, dl, batch2, n_epochs; show_progress = false)\n\nnothing # hide","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"And the corresponding training losses look as follows:","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"morange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256) # hide\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256) # hide\nmgreen = RGBf(44 / 256, 160 / 256, 44 / 256) # hide\n\nfunction plot_training_losses(loss_array_standard, loss_array_symplectic, loss_array_sympnet; theme = :dark)\n    textcolor = theme == :dark ? :white : :black\n    fig = Figure(; backgroundcolor = :transparent)\n    ax = Axis(fig[1, 1]; \n        backgroundcolor = :transparent,\n        bottomspinecolor = textcolor, \n        topspinecolor = textcolor,\n        leftspinecolor = textcolor,\n        rightspinecolor = textcolor,\n        xtickcolor = textcolor, \n        ytickcolor = textcolor,\n        xticklabelcolor = textcolor,\n        yticklabelcolor = textcolor,\n        xlabel=\"Epoch\", \n        ylabel=\"Training loss\",\n        xlabelcolor = textcolor,\n        ylabelcolor = textcolor,\n        yscale = log10\n    )\n    lines!(ax, loss_array_standard, color = mpurple, label = \"ST\")\n    lines!(ax, loss_array_symplectic,  color = mred, label = \"LST\")\n    lines!(ax, loss_array_sympnet, color = morange, label = \"SympNet\")\n    axislegend(; position = (.82, .75), backgroundcolor = :transparent, labelcolor = textcolor)\n\n    fig, ax\nend\n\nfig_dark, ax_dark = plot_training_losses(loss_array_standard, loss_array_symplectic, loss_array_sympnet; theme = :dark)\nfig_light, ax_light = plot_training_losses(loss_array_standard, loss_array_symplectic, loss_array_sympnet; theme = :light)\n\nsave(\"lst_dark.png\", fig_dark; px_per_unit = 1.2)\nsave(\"lst_light.png\", fig_light; px_per_unit = 1.2)\n\nnothing","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"(Image: Training loss for the different networks.) (Image: Training loss for the different networks.)","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"We further evaluate a trajectory with the trained networks for thirty time steps: ","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"const index = 1\ninit_con = (q = dl.input.q[:, 1:seq_length, index], p = dl.input.p[:, 1:seq_length, index])\n# when we iterate with a feedforward neural network we only need a vector as input\ninit_con_ff = (q = dl.input.q[:, 1, index], p = dl.input.p[:, 1, index])\n\nconst n_steps = 30\n\nfunction make_validation_plot(n_steps = n_steps; theme = :dark)\n    textcolor = theme == :dark ? :white : :black\n    fig = Figure(; backgroundcolor = :transparent)\n    ax = Axis(fig[1, 1]; \n        backgroundcolor = :transparent,\n        bottomspinecolor = textcolor, \n        topspinecolor = textcolor,\n        leftspinecolor = textcolor,\n        rightspinecolor = textcolor,\n        xtickcolor = textcolor, \n        ytickcolor = textcolor,\n        xticklabelcolor = textcolor,\n        yticklabelcolor = textcolor,\n        xlabel=L\"t\", \n        ylabel=L\"q_1\",\n        xlabelcolor = textcolor,\n        ylabelcolor = textcolor,\n    )\n    prediction_standard = iterate(nn_standard, init_con; n_points = n_steps, prediction_window = seq_length)\n    prediction_symplectic = iterate(nn_symplectic, init_con; n_points = n_steps, prediction_window = seq_length)\n    prediction_sympnet = iterate(nn_sympnet, init_con_ff; n_points = n_steps)\n\n    # we use linewidth  = 2\n    lines!(ax, dl.input.q[1, 1:n_steps, index]; color = mblue, label = \"Implicit midpoint\", linewidth = 2)\n    lines!(ax, prediction_standard.q[1, :]; color = mpurple, label = \"ST\", linewidth = 2)\n    lines!(ax, prediction_symplectic.q[1, :]; color = mred, label = \"LST\", linewidth = 2)\n    lines!(ax, prediction_sympnet.q[1, :]; color = morange, label = \"SympNet\", linewidth = 2)\n    axislegend(; position = (.55, .75), backgroundcolor = :transparent, labelcolor = textcolor)\n\n    fig, ax\nend\n\nfig_light, ax_light = make_validation_plot(n_steps; theme = :light)\nfig_dark, ax_dark = make_validation_plot(n_steps; theme = :dark)\nsave(\"lst_validation_light.png\", fig_light; px_per_unit = 1.2)\nsave(\"lst_validation_dark.png\", fig_dark; px_per_unit = 1.2)\n\nnothing","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"(Image: Validation of the different networks.) (Image: Validation of the different networks.)","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"We can see that the standard transformer is not able to stay close to the trajectory coming from implicit midpoint very well. The linear symplectic transformer outperforms the standard transformer as well as the SympNet while needing fewer parameters than the standard transformer: ","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"parameterlength(nn_standard), parameterlength(nn_symplectic), parameterlength(nn_sympnet)","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"It is also interesting to note that the training error for the SympNet gets lower than the one for the linear symplectic transformer, but it does not manage to outperform it when looking at the validation. ","category":"page"},{"location":"tutorials/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"\\section*{Chapter Summary}\n\nIn this chapter we demonstrated the efficacy of neural network-based symplectic integrators. We showed two examples: SympNets as an example of a symplectic one-step method and linear symplectic transformers as an example of a symplectic multi-step method. We compared the two different SympNet architectures ($LA$-SympNets and $G$-SympNets) with a standard ResNet and gave an example where symplecticity is important to achieve long-term stability: the ResNet was shown to fail on longer time scales even though it outperforms the $LA$-SympNet on smaller ones; this is because the ResNet does not have any structure encoded into it. The linear symplectic transformer was compared to the standard transformer and a SympNet (on the example of a complex coupled harmonic oscillator) and shown to outperform these two networks.\n\nOf the two symplectic neural networks shown here the linear symplectic transformer constitutes a novel neural network architecture which was introduced in this dissertation.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/#Hamiltonian-Model-Order-Reduction","page":"Symplectic Model Order Reduction","title":"Hamiltonian Model Order Reduction","text":"","category":"section"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"Hamiltonian PDEs are partial differential equations that, like its ODE counterpart, have a Hamiltonian associated with it. The linear wave equation can be written as such a Hamiltonian PDE with ","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"mathcalH(q p mu) = frac12int_Omegamu^2(partial_xiq(tximu))^2 + p(tximu)^2dxi","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"Note that in contrast to the ODE case where the Hamiltonian is a function H(cdot mu)mathbbR^2dtomathbbR we now have a functional mathcalH(cdot cdot mu)mathcalC^infty(mathcalD)timesmathcalC^infty(mathcalD)tomathbbR The PDE for this Hamiltonian can be obtained similarly as in the ODE case:","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"partial_tq(tximu) = fracdeltamathcalHdeltap = p(tximu) quad partial_tp(tximu) = -fracdeltamathcalHdeltaq = mu^2partial_xixiq(tximu)","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"Neglecting the Hamiltonian structure of a system can have grave consequences on the performance of the reduced order model [68–70] which is why all algorithms in GeometricMachineLearning designed for producing reduced order models respect the structure of the system.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/#The-Symplectic-Solution-Manifold","page":"Symplectic Model Order Reduction","title":"The Symplectic Solution Manifold","text":"","category":"section"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"As with regular parametric PDEs, we also associate a solution manifold with Hamiltonian PDEs. This is a finite-dimensional manifold, on which the dynamics can be described through a Hamiltonian ODE. The reduced system, with which we approximate this symplectic solution manifold, is a low dimensional symplectic vector space mathbbR^2n together with a reduction mathcalP and a reconstruction mathcalR If we now take an initial condition on the solution manifold hatu_0inmathcalM approx mathcalR(mathbbR^2n) and project it to the reduced space with mathcalP, we get u = mathcalP(hatu_0) We can now integrate it on the reduced space via the induced differential equation, which is of canonical Hamiltonian form, and obtain an orbit u(t) which can then be mapped back to an orbit on the solution manifold[1] via mathcalR The resulting orbit mathcalR(u(t)) is ideally the unique orbit on the full order model hatu(t)inmathcalM.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"[1]: To be precise, an approximation of the solution manifold mathcalR(mathbbR^2n), as we are not able to find the solution manifold exactly in practice. ","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"For Hamiltonian model order reduction we additionally require that the reduction mathcalP satisfies","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"    nabla_zmathcalPmathbbJ_2N(nabla_zmathcalP)^T = mathbbJ_2n text for zinmathbbR^2N","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"and the reconstruction mathcalR satisfies[2]","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"[2]: We should note that satisfying this symplecticity condition is much more important for the reconstruction than for the reduction. There is a lack of research on whether the symplecticity condition for the projection is really needed; in [70] it is entirely ignored for example.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"    (nabla_zmathcalR)^TmathbbJ_2Nnabla_zmathcalR = mathbbJ_2n","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"With this we have","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"Main.theorem(raw\"A Hamiltonian system on the reduced space ``(\\mathbb{R}^{2n}, \\mathbb{J}_{2n}^T)`` is equivalent to a *non-canonical symplectic system* ``(\\mathcal{M}, \\mathbb{J}_{2N}^T|_\\mathcal{M})`` where \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\mathcal{M} = \\mathcal{R}(\\mathbb{R}^{2n})\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"is an approximation to the solution manifold. We further have\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\mathbb{J}_{2N}|_\\mathcal{M}(z) = ((\\nabla_z\\mathcal{R})^+)^T\\mathbb{J}_{2n}(\\nabla_z\\mathcal{R})^+,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"so the dynamics on ``\\mathcal{M}`` can be described through a Hamiltonian ODE on ``\\mathbb{R}^{2n}.``\")","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"For the proof we use the fact that mathcalM = mathcalR(mathbbR^2n) is a manifold whose coordinate chart is the local inverse of mathcalR which we will call psi, i.e. around a point yinmathcalM we have psicircmathcalR(y) = y[3] We further define the symplectic inverse of a matrix AinmathbbR^2Ntimes2n as ","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"[3]: A similar proof can be found in [72]. Further note that, if we enforced the condition mathcalPcircmathcalR = mathrmid exactly, the projection mathcalP would be equal to the local inverse psi For the proof here we however only require the existence of psi, not its explicit construction as mathcalP","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"    A^+ = mathbbJ_2n^TA^TmathbbJ_2N","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"which gives:","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"    A^+A = mathbbJ_2n^TA^TmathbbJ_2NA = mathbbI_2n","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"iff A is symplectic, i.e. A^TmathbbJ_2NA = mathbbJ_2n.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"Main.proof(raw\"Note that the tangent space at ``y = \\mathcal{R}(z)`` to ``\\mathcal{M}`` is:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    T_y\\mathcal{M} = \\{(\\nabla_z\\mathcal{R})v: v\\in\\mathbb{R}^{2n}\\}.\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"The mapping \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\mathcal{M} \\to T\\mathcal{M}, y \\mapsto (\\nabla_z\\mathcal{R})\\mathbb{J}_{2n}\\nabla_zH\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"is clearly a vector field. We now prove that it is symplectic and equal to ``\\mathbb{J}_{2N}\\nabla_y(H\\circ\\psi).`` For this first note that we have ``\\mathbb{I} = (\\nabla_z\\mathcal{R})^+\\nabla_z\\mathcal{R} = (\\nabla_{\\mathcal{R}(z)}\\psi)\\nabla_z\\mathcal{R}`` and that the pseudoinverse is unique. We then have:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\mathbb{J}_{2N}\\nabla_yH\\circ\\psi = \\mathbb{J}_{2N}(\\nabla_y\\psi)^T\\nabla_{\\psi(y)}H = \\mathbb{J}_{2N}\\left((\\nabla_{\\psi(y)}\\mathcal{R})^+\\right)^T\\nabla_{\\psi(y)}H = (\\nabla_{\\psi(y)}\\mathcal{R})\\mathbb{J}_{2n}\\nabla_{\\psi(y)}H,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"which proves that every Hamiltonian system on ``\\mathbb{R}^{2n}`` induces a Hamiltonian system on ``\\mathcal{M}``. Conversely assume we are given a Hamiltonian vector field whose flow map evolves on ``\\mathcal{M}``, which we denote by\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\hat{X}(z) = \\mathbb{J}_{2N}\\nabla_{z}\\hat{H} = (\\nabla_{\\psi(z)}\\mathcal{R})\\bar{X}(\\psi(z)),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``\\bar{X}`` is a vector field on the reduced space. In the last equality we used that the flow map evolves on ``\\mathcal{M}``, so the corresponding vector field needs to map to ``T\\mathcal{M}.`` We further have:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"(\\nabla_{\\psi(z)}\\mathcal{R})^+\\hat{X}(z) = \\mathbb{J}_{2n}(\\nabla_{\\psi(z)}\\mathcal{R})^T\\nabla_z\\hat{H} = \\mathbb{J}_{2n}\\nabla_{\\psi(z)}(\\hat{H}\\circ\\mathcal{R}) = \\bar{X}(\\psi(z)),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"and we see that the vector field on the reduced space also has to be Hamiltonian. We can thus express a high-dimensional Hamiltonian system on ``\\mathcal{M}`` with a low-dimensional Hamiltonian system on ``\\mathbb{R}^{2n}``.\")","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"In the proof we used that the pseudoinverse is unique. This is not true in general [68], but holds for the architectures discussed here (proper symplectic decomposition and symplectic autoencoders). We will postpone the proof of this until after we introduced symplectic autoencoders in detail.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"This theorem serves as the basis for Hamiltonian model order reduction via proper symplectic decomposition and symplectic autoencoders. We will now briefly introduce these two approaches[4].","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"[4]: We will discuss symplectic autoencoders later in a dedicated section.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/#Proper-Symplectic-Decomposition","page":"Symplectic Model Order Reduction","title":"Proper Symplectic Decomposition","text":"","category":"section"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"For proper symplectic decomposition (PSD) the reduction mathcalP and the reconstruction mathcalR are constrained to be linear, orthonormal and symplectic. Note that these first two properties are shared with POD. The easiest way[5] to enforce this is through the so-called \"cotangent lift\" [68]: ","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"[5]: The original PSD paper [68] proposes another approach to build linear reductions and reconstructions with the so-called \"complex SVD.\" In practice this only brings minor advantages over the cotangent lift however [69].","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"mathcalR equiv Psi_mathrmCL = beginbmatrix Phi  mathbbO  mathbbO  Phi endbmatrix text where PhiinSt(nN)subsetmathbbR^Ntimesn","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"i.e. both Phi and Psi_mathrmCL are elements of the Stiefel manifold and we furthermore have Psi_mathrmCL^TmathbbJ_2NPsi_mathrmCL = mathbbJ_2n, i.e. Psi_mathrmCL is symplectic. If the snapshot matrix is of the form: ","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"M = leftbeginarraycccc\nhatq_1(t_0)   hatq_1(t_1)  quadldotsquad  hatq_1(t_f) \nhatq_2(t_0)   hatq_2(t_1)  ldots  hatq_2(t_f) \nldots  ldots  ldots  ldots \nhatq_N(t_0)   hatq_N(t_1)  ldots  hatq_N(t_f) \nhatp_1(t_0)  hatp_1(t_1)  ldots  hatp_1(t_f) \nhatp_2(t_0)   hatp_2(t_1)  ldots  hatp_2(t_f) \nldots   ldots  ldots  ldots \nhatp_N(t_0)   hatp_N(t_1)  ldots  hatp_N(t_f) \nendarrayright","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"with mathttnts = f - 1 is the number of time steps. Then Phi_mathrmCL can be computed in a very straight-forward manner:","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"Main.theorem(raw\"The ideal cotangent lift ``\\Psi_\\mathrm{CL}`` for the snapshot matrix of form\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    M = \\begin{bmatrix} M_q \\\\ M_p \\end{bmatrix},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"i.e. the cotangent lift that minimizes the projection error, can be obtained the following way:\n\" * Main.indentation * raw\"1. Rearrange the rows of the matrix ``M`` such that we end up with a ``N\\times(2\\mathtt{nts})`` matrix: ``\\hat{M} := [M_q, M_p]``.\n\" * Main.indentation * raw\"2. Perform SVD: ``\\hat{M} = V\\Sigma{}U^T.`` \n\" * Main.indentation * raw\"3. Set ``\\Phi\\gets{}U\\mathtt{[:,1:n]}.``\n\" * Main.indentation * raw\"``\\Psi_\\mathrm{CL}`` is then built based on this ``\\Phi``.\")","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"For details on the cotangent lift (and other methods for linear symplectic model reduction) consult [68]. In GeometricMachineLearning we use the function solve! for this task.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/#Symplectic-Autoencoders","page":"Symplectic Model Order Reduction","title":"Symplectic Autoencoders","text":"","category":"section"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"Symplectic Autoencoders are a type of neural network suitable for treating Hamiltonian parametrized PDEs with slowly decaying Kolmogorov n-width. It is based on PSD and symplectic neural networks[6].","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"[6]: We call these SympNets most of the time. This term was coined in [5].","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"Symplectic autoencoders are motivated similarly to standard autoencoders for model order reduction: PSD suffers from similar shortcomings as regular POD. PSD is a linear map and the approximation space tildemathcalM= Psi^mathrmdec(z_r)inmathbbR^2Nz_rinmathbbR^2n is therefore also linear. For problems with slowly-decaying Kolmogorov n-width this leads to very poor approximations.  ","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"In order to overcome this difficulty we use neural networks, more specifically SympNets, together with cotangent lift-like matrices. The resulting architecture, symplectic autoencoders, are discussed in the dedicated section on neural network architectures.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/#Workflow-for-Symplectic-ROM","page":"Symplectic Model Order Reduction","title":"Workflow for Symplectic ROM","text":"","category":"section"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"As with any other data-driven reduced order modeling technique we first discretize the PDE. This should be done with a structure-preserving scheme, thus yielding a (high-dimensional) Hamiltonian ODE as a result. Going back to the example of the linear wave equation, we can discretize this equation with finite differences to obtain a Hamiltonian ODE: ","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"mathcalH_mathrmdiscr(z(tmu)mu) = frac12x(tmu)^Tbeginbmatrix  -mu^2D_xixi  mathbbO  mathbbO  mathbbI  endbmatrix x(tmu)","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"In Hamiltonian reduced order modeling we try to find a symplectic submanifold in the solution space[7] that captures the dynamics of the full system as well as possible.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"[7]: The submanifold, that approximates the solution manifold, is tildemathcalM = Psi^mathrmdec(z_r)inmathbbR^2Nu_rinmathrmR^2n where z_r is the reduced state of the system. By a slight abuse of notation we also denote tildemathcalM by mathcalM as we have done previously when showing equivalence between Hamiltonian vector fields on mathbbR^2n and mathcalM. ","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"Similar to the regular PDE case we again build an encoder mathcalP equiv Psi^mathrmenc and a decoder mathcalR equiv Psi^mathrmdec; but now both these mappings are required to be symplectic.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"Concretely this means: ","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"The encoder is a mapping from a high-dimensional symplectic space to a low-dimensional symplectic space, i.e. Psi^mathrmencmathbbR^2NtomathbbR^2n such that nablaPsi^mathrmencmathbbJ_2N(nablaPsi^mathrmenc)^T = mathbbJ_2n.\nThe decoder is a mapping from a low-dimensional symplectic space to a high-dimensional symplectic space, i.e. Psi^mathrmdecmathbbR^2ntomathbbR^2N such that (nablaPsi^mathrmdec)^TmathbbJ_2NnablaPsi^mathrmdec = mathbbJ_2n.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"If these two maps are constrained to linear maps this amounts to PSD.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"After we obtained Psi^mathrmenc and Psi^mathrmdec we can construct the reduced model. GeometricMachineLearning has a symplectic Galerkin projection implemented. This symplectic Galerkin projection does:","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"    beginpmatrix v_r(q p)  f_r(q p) endpmatrix = fracddt beginpmatrix q  p endpmatrix = mathbbJ_2n(nabla_zmathcalR)^TmathbbJ_2N^T beginpmatrix v(mathcalR(q p))  f(mathcalR(q p)) endpmatrix","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"where v are the first n components of the vector field and f are the second n components of the vector field. The superscript r indicated a reduced vector field. These reduced vector fields are built with GeometricMachineLearning.build_v_reduced and GeometricMachineLearning.build_f_reduced.","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/#Library-Functions","page":"Symplectic Model Order Reduction","title":"Library Functions","text":"","category":"section"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"GeometricMachineLearning.SymplecticEncoder\nGeometricMachineLearning.SymplecticDecoder\nGeometricMachineLearning.NonLinearSymplecticEncoder\nGeometricMachineLearning.NonLinearSymplecticDecoder\nHRedSys\nGeometricMachineLearning.build_v_reduced\nGeometricMachineLearning.build_f_reduced\nPSDLayer\nPSDArch\nsolve!","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.SymplecticEncoder","page":"Symplectic Model Order Reduction","title":"GeometricMachineLearning.SymplecticEncoder","text":"SymplecticEncoder <: Encoder\n\nThis is the abstract SymplecticEncoder type. \n\nSee Encoder for the super type and NonLinearSymplecticEncoder for a derived struct.\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.SymplecticDecoder","page":"Symplectic Model Order Reduction","title":"GeometricMachineLearning.SymplecticDecoder","text":"SymplecticDecoder <: Decoder\n\nThis is the abstract SymplecticDecoder type.\n\nSee Decoder for the super type and NonLinearSymplecticDecoder for a derived struct.\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.NonLinearSymplecticEncoder","page":"Symplectic Model Order Reduction","title":"GeometricMachineLearning.NonLinearSymplecticEncoder","text":"NonLinearSymplecticEncoder\n\nThis should not be called direclty. Instad use SymplecticAutoencoder.\n\nAn instance of NonLinearSymplecticEncoder can be generated by calling encoder on an instance of SymplecticAutoencoder. \n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.NonLinearSymplecticDecoder","page":"Symplectic Model Order Reduction","title":"GeometricMachineLearning.NonLinearSymplecticDecoder","text":"NonLinearSymplecticDecoder\n\nThis should not be called direclty. Instad use SymplecticAutoencoder.\n\nAn instance of NonLinearSymplecticDecoder can be generated by calling decoder on an instance of SymplecticAutoencoder. \n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.HRedSys","page":"Symplectic Model Order Reduction","title":"GeometricMachineLearning.HRedSys","text":"HRedSys\n\nHRedSys computes the reconstructed dynamics in the full system based on the reduced one. Optionally it can be compared to the FOM solution.\n\nIt can be called using two different constructors.\n\nConstructors\n\nThe standard constructor is:\n\nHRedSys(N, n; encoder, decoder, v_full, f_full, v_reduced, f_reduced, parameters, tspan, tstep, ics, projection_error)\n\nwhere \n\nencoder: a function mathbbR^2NmapstomathbbR^2n\ndecoder: a (differentiable) function mathbbR^2nmapstomathbbR^2N\nv_full: a (differentiable) mapping defined the same way as in GeometricIntegrators.\nf_full: a (differentiable) mapping defined the same way as in GeometricIntegrators.\nv_reduced: a (differentiable) mapping defined the same way as in GeometricIntegrators.\nf_reduced: a (differentiable) mapping defined the same way as in GeometricIntegrators.\nintegrator: is used for integrating the reduced system.\nparameters: a NamedTuple that parametrizes the vector fields (the same for fullvectorfield and reducedvectorfield)\ntspan: a tuple (t₀, tₗ) that specifies start and end point of the time interval over which integration is performed. \ntstep: the time step \nics: the initial condition for the big system.\n\nThe other, and more convenient, constructor is:\n\nHRedSys(odeensemble, encoder, decoder) \n\nwhere odeensemble can be a HODEEnsemble or a HODEProblem. encoder and decoder have to be neural networks. With this constructor one can also pass an integrator via the keyword argument integrator. The default is ImplicitMidpoint().  Internally this calls the functions build_v_reduced and build_f_reduced in order to build the reduced vector fields.\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.build_v_reduced","page":"Symplectic Model Order Reduction","title":"GeometricMachineLearning.build_v_reduced","text":"build_v_reduced(v_full, f_full, decoder)\n\nBuilds the reduced vector field (q part) based on the full vector field for a Hamiltonian system. \n\nWe derive the reduced vector field via the reduced Hamiltonian: tildeH = HcircPsi^mathrmdec. \n\nWe then get \n\nbeginaligned\nmathbbJ_2nnabla_xitildeH = mathbbJ_2n(nablaPsi^mathrmdec)^TmathbbJ_2N^TmathbbJ_2Nnabla_zH  = mathbbJ_2n(nablaPsi^mathrmdec)^TmathbbJ_2N^T beginpmatrix v(z)  f(z) endpmatrix   = beginpmatrix - (nabla_pPsi_q)^Tf(z) + (nabla_pPsi_p)^Tv(z)  (nabla_qPsi_q)^Tf(z) - (nabla_qPsi_p)^Tv(z) endpmatrix\nendaligned\n\nbuild_v_reduced outputs the first half of the entries of this vector field.\n\n\n\n\n\n","category":"function"},{"location":"reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.build_f_reduced","page":"Symplectic Model Order Reduction","title":"GeometricMachineLearning.build_f_reduced","text":"build_f_reduced(v_full, f_full, decoder)\n\nBuilds the reduced vector field (p part) based on the full vector field for a Hamiltonian system. \n\nbuild_f_reduced outputs the second half of the entries of this vector field.\n\nSee build_v_reduced for more information.\n\n\n\n\n\n","category":"function"},{"location":"reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.PSDLayer","page":"Symplectic Model Order Reduction","title":"GeometricMachineLearning.PSDLayer","text":"PSDLayer(input_dim, output_dim)\n\nMake an instance of PSDLayer.\n\nThis is a PSD-like layer used for symplectic autoencoders.  One layer has the following shape:\n\nA = beginbmatrix Phi  mathbbO  mathbbO  Phi endbmatrix\n\nwhere Phi is an element of the Stiefel manifold St(n N).\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.PSDArch","page":"Symplectic Model Order Reduction","title":"GeometricMachineLearning.PSDArch","text":"PSDArch <: SymplecticCompression\n\nPSDArch is the architecture that corresponds to PSDLayer. \n\nThe architecture\n\nProper symplectic decomposition (PSD) can be seen as a SymplecticAutoencoder for which the decoder and the encoder are both PSD-like matrices (see the docs for PSDLayer. \n\nTraining\n\nFor optimizing the parameters in this architecture no neural network training is necessary (see the docs for solve!).\n\nThe constructor\n\nThe constructor only takes two arguments as input:\n\nfull_dim::Integer\nreduced_dim::Integer\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.solve!","page":"Symplectic Model Order Reduction","title":"GeometricMachineLearning.solve!","text":"solve!(nn::NeuralNetwork{<:PSDArch}, input)\n\nSolve the cotangent lift problem for an input.\n\nPSDArch does not require neural network training since it is a strictly linear operation that can be solved with singular value decomposition (SVD).\n\n\n\n\n\n","category":"function"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"\\section*{Chapter Summary}\n\nIn this chapter we introduced the concept of \\textit{data-driven reduced order modeling}. It was shown how data-driven reduced order modeling can be motivated by the \\textit{solution manifold}, the (nonlinear) space containing all the solutions to the (parameter-dependent) differential equation. \\textit{Proper orthogonal decomposition} (POD) and \\textit{autoencoders} were discussed as specific examples of performing the offline phase in reduced order modeling. POD is an application of \\textit{singular value decomposition} and as such belongs to the realm of linear algebra. Autoencoders are more general approximators that are build with neural networks and have to be optimized during a \\textit{training stage}. Autoencoders are especially well-suited for problems with a \\textit{slowly-decaying Kolmogorov $n$-width}. The Kolmogorov $n$-width provides a quantitative measure for how well something can be approximated with a linear subspace.\n\nWe furthermore discussed how a reduced order model can be made structure-preserving and showed \\textit{proper symplectic decomposition} (PSD) to be the structure-preserving alternative to POD. PSD is a more restrictive form of POD where additional conditions on the reduction $\\mathcal{P}$ and the reconstruction $\\mathcal{R}$ are imposed. With this construction a Hamiltonian full order model is reduced to a Hamiltonian reduced order model. At the very end of the chapter we teased \\textit{symplectic autoencoders} which will be discussed in detail in Part III. These offer, similar to standard autoencoders, a way of constructing more general symplectic reductions and are thus well-suited to problems with a slowly-decaying Kolmogorov $n$-width.\n\n\\begin{comment}","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/#References","page":"Symplectic Model Order Reduction","title":"References","text":"","category":"section"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"P. Buchfink, S. Glas and B. Haasdonk. Symplectic model reduction of Hamiltonian systems on nonlinear manifolds and approximation with weakly symplectic autoencoder. SIAM Journal on Scientific Computing 45, A289–A311 (2023).\n\n\n\nL. Peng and K. Mohseni. Symplectic model reduction of Hamiltonian systems. SIAM Journal on Scientific Computing 38, A1–A27 (2016).\n\n\n\n","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"\\end{comment}","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"<!--","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/#References-2","page":"Symplectic Model Order Reduction","title":"References","text":"","category":"section"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"K. Lee and K. T. Carlberg. Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders. Journal of Computational Physics 404, 108973 (2020).\n\n\n\nS. Fresca, L. Dede’ and A. Manzoni. A comprehensive deep learning-based approach to reduced order modeling of nonlinear time-dependent parametrized PDEs. Journal of Scientific Computing 87, 1–36 (2021).\n\n\n\nT. Blickhan. A registration method for reduced basis problems using linear optimal transport, arXiv preprint arXiv:2304.14884 (2023).\n\n\n\nA. Chatterjee. An introduction to the proper orthogonal decomposition. Current science, 808–817 (2000).\n\n\n\nL. Peng and K. Mohseni. Symplectic model reduction of Hamiltonian systems. SIAM Journal on Scientific Computing 38, A1–A27 (2016).\n\n\n\nP. Buchfink, S. Glas and B. Haasdonk. Symplectic model reduction of Hamiltonian systems on nonlinear manifolds and approximation with weakly symplectic autoencoder. SIAM Journal on Scientific Computing 45, A289–A311 (2023).\n\n\n\n","category":"page"},{"location":"reduced_order_modeling/symplectic_mor/","page":"Symplectic Model Order Reduction","title":"Symplectic Model Order Reduction","text":"-->","category":"page"},{"location":"layers/multihead_attention_layer/#Multihead-Attention","page":"Multihead Attention","title":"Multihead Attention","text":"","category":"section"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"In order to arrive from the attention layer at the multihead attention layer we have to do a few modifications. Here note that these neural networks were originally developed for natural language processing (NLP) tasks and the terminology used here bears some resemblance to that field.  The input to a multihead attention layer typicaly comprises three components:","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"Values VinmathbbR^NtimesT: a matrix whose columns are value vectors, \nQueries QinmathbbR^NtimesT: a matrix whose columns are query vectors, \nKeys KinmathbbR^NtimesT: a matrix whose columns are key vectors.","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"Regular attention performs the following operation[1]: ","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"[1]: The division by sqrtN is optional here and sometimes left out.","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"mathrmAttention(QKV) = Vmathrmsoftmaxleft(fracK^TQsqrtNright)","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"where N is the dimension of the vectors in V, Q and K. The softmax activation function here acts column-wise:","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"mathrmsoftmaxmathbbR^TtomathbbR^T text with mathrmsoftmax(v)_i = e^v_ileft(sum_j=1e^v_jright)","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"The K^TQ term is a similarity matrix between the queries and the vectors. ","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"The transformer contains a self-attention mechanism, i.e. takes an input X and then transforms it linearly to V, Q and K via V = P^VX, Q = P^QX and K = P^KX. What distinguishes the multihead attention layer from the singlehead attention layer is that there is not just one P^V, P^Q and P^K, but there are several: one for each head of the multihead attention layer. After computing the individual values, queries and vectors, and after applying the softmax, the outputs are then concatenated together in order to obtain again an array that is of the same size as the input array:","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"(Image: A representation of a multihead attention layer with three heads.) (Image: A representation of a multihead attention layer with three heads.)","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"Written as an equation we get:","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"mathrmMultiHeadAttention(Z) = beginpmatrix mathrmAttention(P^Q_1Z P^K_1Z P^V_1Z)  mathrmAttention(P^Q_2Z P^K_2Z P^V_2Z)  cdots  mathrmAttention(P^Q_mathttn_headsZ P^K_mathttn_headsZ P^V_mathttn_headsZ) endpmatrix","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"where P^(cdot)_iinmathbbR^(Ndivmathttn_heads)timesN for ZinmathbbR^NtimesT Note that we implicitly require that N is divisible by mathttn_heads here.","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"Here the various P matrices can be interpreted as being projections onto lower-dimensional subspaces, hence the designation by the letter P. The columns of the projection matrices span smaller spaces that should capture features in the input data. We will show in an example how training of a neural network can benefit from putting the P^(cdot)_i matrices on the Stiefel manifold.   ","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"Main.remark(raw\"The `MultiHeadAttention` implemented in `GeometricMachineLearning` has an optional keyword `add_connection`. If this is set to `true` then the output of the `MultiHeadAttention` layer is:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\mathrm{MultiHeadAttention}(Z) = Z + \\begin{pmatrix} \\mathrm{Attention}(P^Q_1Z, P^K_1Z, P^V_1Z) \\\\ \\mathrm{Attention}(P^Q_2Z, P^K_2Z, P^V_2Z) \\\\ \\cdots \\\\ \\mathrm{Attention}(P^Q_{\\mathtt{n\\_heads}}Z, P^K_{\\mathtt{n\\_heads}}Z, P^V_{\\mathtt{n\\_heads}}Z) \\end{pmatrix},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"so we add the input again to the output.\")","category":"page"},{"location":"layers/multihead_attention_layer/#Computing-Correlations-in-the-Multihead-Attention-Layer","page":"Multihead Attention","title":"Computing Correlations in the Multihead-Attention Layer","text":"","category":"section"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"The attention mechanism describes a reweighting of the \"values\" V_i based on correlations between the \"keys\" K_i and the \"queries\" Q_i. First note the structure of these matrices: they are all a collection of T (Ndivmathttn_heads)-dimensional vectors, i.e. V_i=v_i^(1) ldots v_i^(T) K_i=k_i^(1) ldots k_i^(T) Q_i=q_i^(1) ldots q_i^(T) with i = 1 ldots mathttn_heads. Those vectors have been obtained by applying the respective projection matrices onto the original input.","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"When performing the reweighting of the columns of V_i we first compute the correlations between the vectors in K_i and in Q_i and store the results in a correlation matrix C_i: ","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"    C_i_mn = left(k_i^(m)right)^Tq_i^(n)","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"The columns of this correlation matrix are than rescaled with a softmax function, obtaining a matrix of probability vectors[2] mathcalP_i:","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"[2]: Also called a stochastic matrix.","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"    mathcalP_i_bulletn = mathrmsoftmaxleft(fracC_i_bulletnsqrtNdivmathttn_headsright)","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"Finally the matrix mathcalP_i is multiplied onto V_i from the right, resulting in T convex combinations of the T vectors v_i^(m) with m=1ldotsT:","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"    V_imathcalP_i = leftsum_m=1^TmathcalP_i_m1v_i^(m) ldots sum_m=1^TmathcalP_i_mTv_i^(m)right","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"With this we can now give a better interpretation of what the projection matrices W_i^V, W_i^K and W_i^Q should do: they map the original data to lower-dimensional subspaces. We then compute correlations between the representation in the K and in the Q basis and use this correlation to perform a convex reweighting of the vectors in the V basis. These reweighted values are then fed into a standard feedforward neural network as is further explained in the section on the standard transformer.","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"Because the main task of the W_i^V, W_i^K and W_i^Q matrices here is for them to find bases, it makes sense to constrain them onto the Stiefel manifold; they do not and should not have the maximum possible generality.","category":"page"},{"location":"layers/multihead_attention_layer/#Using-a-Matrix-Softmax","page":"Multihead Attention","title":"Using a Matrix Softmax","text":"","category":"section"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"Usually the attention layer is using a VectorSoftmax, i.e. one that produces a series of probability vectors. In GeometricMachineLearning we can also use a MatrixSoftmax instead. An example application of this is shown in the tutorials section.","category":"page"},{"location":"layers/multihead_attention_layer/#Library-Functions","page":"Multihead Attention","title":"Library Functions","text":"","category":"section"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"MultiHeadAttention","category":"page"},{"location":"layers/multihead_attention_layer/#GeometricMachineLearning.MultiHeadAttention","page":"Multihead Attention","title":"GeometricMachineLearning.MultiHeadAttention","text":"MultiHeadAttention(dim, n_heads)\n\nMake a MultiHeadAttention layer with n_heads for a system of dimension dim. \n\nNote that the dim has to be divisible by n_heads.\n\nMultiHeadAttention (MHA) serves as a preprocessing step in the transformer. \n\nIt reweights the input vectors bases on correlations within those data.\n\nThis is used for the neural networks StandardTransformerIntegrator and ClassificationTransformer.\n\nArguments\n\nThe optional keyword arguments to MultiHeadAttention are:\n\nStiefel::Bool=false\nadd_connection::Bool=true\nactivation::AbstractSoftmax=VectorSoftmax.\n\nStiefel indicates whether weights are put on the StiefelManifold St(mathrmdim mathrmdimdivmathrmn_heads).\n\nadd_connection indicates whether the input is again added to the output.\n\n\n\n\n\n","category":"type"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"\\begin{comment}","category":"page"},{"location":"layers/multihead_attention_layer/#References","page":"Multihead Attention","title":"References","text":"","category":"section"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and I. Polosukhin. Attention is all you need. Advances in neural information processing systems 30 (2017).\n\n\n\n","category":"page"},{"location":"layers/multihead_attention_layer/","page":"Multihead Attention","title":"Multihead Attention","text":"\\end{comment}","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"When talking about \\textit{structure} we largely limited ourselves to \\textit{symplecticity} and \\textit{volume preservation} in this dissertation\\footnote{The orthogonality constraints imposed on the vision transformer can however also be seen as structure.}. A more general type of structure is that of a \\textit{port-Hamiltonian system}. In addition to a conservative Hamiltonian part it also includes external (driving) forces and a dissipative part.","category":"page"},{"location":"port_hamiltonian_systems/#Using-Symplectic-Autoencoders-for-Port-Hamiltonian-Systems","page":"port-Hamiltonian Systems","title":"Using Symplectic Autoencoders for Port-Hamiltonian Systems","text":"","category":"section"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"Symplectic autoencoders can also be used to reduce port-Hamiltonian systems [73]. Here we focus on linear port-Hamiltonian systems[1]. These are of the form:","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"[1]: For a broader class of such systems see [74]. A generalization to manifolds of such systems is also possible [75, 76].","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"Sigma_mathrmlpH(mathbbR^2N) = Sigma_mathrmlpH  begincases dothatz(t)  =  (mathbbJ_2N - hatR)nablaH(hatz(t)) + hatBu(t)  y(t)  = hatB^TnablaH(hatz(t))  endcases","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"where mathbbJ_2N is the Poisson tensor and hatRinmathbbR^2Ntimes2N is symmetric semi-positive definite (i.e. all its eigenvalues are non-negative). hatzinmathbbR^2N is called the state of the system, uinmathbbR^m are the system inputs, yinmathbbR^m are the system outputs, and hatBinmathbbR^2Ntimesm connects the inputs to the state. We also refer to linear port-Hamiltonian systems as lpH systems.","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"Similar to energy conservation of standard Hamiltonian systems, lpH systems have an associated energy balance equation:","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"Main.definition(raw\"The **energy balance equation** of a lpH system is:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\frac{d}{dt}H(z(t)) \\leq y(t)^Tu(t).\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"If we further have ``R = 0,`` then the inequality becomes an equality.\")","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"Main.proof(raw\"\"\"We evaluate the derivative of ``H(z(t))`` with respect to ``t``:\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"\\begin{aligned}\n\"\"\" * Main.indentation * raw\"\"\"\\frac{d}{dt}H(\\varphi^t(z_0))& = \\nabla{}H(\\varphi^t(z_0))\\frac{d}{dt}\\varphi^t(z_0) \\\\\n\"\"\" * Main.indentation * raw\"\"\"                             & = \\nabla{}H(z(t))(\\mathbb{J} - R)\\nabla{}H(z(t)) + (\\nabla{}H(z(t)))^TBu(t) \\\\\n\"\"\" * Main.indentation * raw\"\"\"                             & = (\\nabla{}H(z(t)))^TR\\nabla{}H(z(t)) + \\underbrace{(\\nabla{H}(z(t)))^TB}_{y(t)^T}u(t) \\\\\n\"\"\" * Main.indentation * raw\"\"\"                             & \\leq y(t)^Tu(t),\n\"\"\" * Main.indentation * raw\"\"\"\\end{aligned}\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"where we used that ``R`` is symmetric and positive semi-definite in the last step.\"\"\")","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"The analogue to the Poisson tensor for lpH systems are so-called Dirac structures:","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"llangle = Main.output_type == :latex ? raw\"\\llangle\" : raw\"\\langle\\langle\"\nrrangle = Main.output_type == :latex ? raw\"\\rrangle\" : raw\"\\rangle\\rangle\"\nMain.definition(raw\"\"\"A Dirac structure for a vector space ``\\mathbb{R}^{n}`` is a subspace ``D\\subset\\mathbb{R}^n\\times(\\mathbb{R}^n)^* \\simeq \\mathbb{R}^{2n}`` such that\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"D^\\perp = D,\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"i.e. the orthogonal complement of ``D`` is equal to itself. Here the orthogonal complement is taken with respect to the pairing:\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * llangle * raw\"\"\"\\cdot,\\cdot\"\"\" * rrangle * raw\"\"\":\\mathbb{R}^{2n}\\times\\mathbb{R}^{2n}\\to\\mathbb{R}, (e, f)\\times(\\tilde{e}, \\tilde{f}) \\mapsto e^T\\tilde{f} + \\tilde{e}^Tf.\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"Note that ``\"\"\" * llangle * raw\"\"\"\\cdot, \\cdot\"\"\" * rrangle * raw\"\"\"`` is a symmetric bilinear form.\"\"\")","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"Main.example(raw\"\"\"The space:\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"    D = \\{ (e, f): e\\in\\mathbb{R}^{2n}, f = \\mathbb{J}_{2n}e \\}\\subset\\mathbb{R}^{4n}\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"forms a Dirac structure. Note that we also have:\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"    (\\nabla_z{H}, X_H(z)) = (\\nabla_zH, \\mathbb{J}\\nabla_zH) \\in D.\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"So every Hamiltonian System has an associated Dirac structure.\"\"\")","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"Main.example(raw\"\"\"For the lpH shown above we have the relation:\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\" \\begin{pmatrix} f \\\\ y \\\\e \\end{pmatrix} = \\begin{pmatrix} \\mathbb{J}_{2N}^T & -B & -\\mathbb{I}_{2N} \\\\ B^T & \\mathbb{O} & \\mathbb{O} \\\\ \\mathbb{I}_{2N} & \\mathbb{O} & \\mathbb{O} \\end{pmatrix} \\begin{pmatrix} \\bar{e} \\\\ u \\\\ \\bar{\\bar{e}} \\end{pmatrix},\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"where we further have the constraints and identifications ``f = -\\dot{z},`` ``\\bar{e} = \\nabla_zH`` and ``\\bar{\\bar{e}} = -Re`` to fully describe the lpH. Note that points ``(f, y, e, \\bar{e}, u, \\bar{\\bar{e}})\\in\\mathbb{R}^{8N + 2m}`` that satisfy the relation shown above form a Dirac structure.\"\"\")","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"In numerically solving lpH systems the Dirac structure takes a similar role to the symplectic structure of canonical Hamiltonian systems [77] and the energy-balance equation takes a similar role to energy conservation for canonical Hamiltonian systems. Structure-preserving discretization of lpH systems discretize the Dirac structure with collocation methods [78]. A one-step update can be written as","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"beginaligned\nz_f  = z_0 + hsum_i=1^sbeta_if_i \nz_i  = z_0 + hsum_j=1^salpha_ijf_j \ne_i  = bare_i = nabla_z_iH \nbarbare_i  = -Re_i \nf_i  = mathbbJ_2N^Tbare_i - Bu_i - barbare_i \ny_i  = B^Tbare_i\nendaligned","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"Model order reduction of port-Hamiltonian systems can be divided into two approaches: projection-based methods and interpolations of the transfer function [79]. The first approach is equivalent to Galerkin projection and we limit the discussion here to this approach. Similar to the case of canonical Hamiltonian systems, we reduce the system with a symplectic autoencoder.","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"When discussing symplectic model order reduciton we showed that a Hamiltonian system on the reduced space mathbbR^2n is equivalent to a Hamiltonian system on mathcalM = mathcalR(mathbbR^2n) where mathcalR is the reconstruction in a reduced order modeling framework. Similar statements are also true for lpH systems.","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"We will now demonstrate how to obtain a reduced-order lpH system from a full-order lpH system and vice-versa:","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"(Image: We can derive full lpH systems from reduced lpH systems and vice-versa (in some cases). The solid arrows indicate that we have an explicit construction available, the dashed arrow indicates that in this specific case we do not yet know if a structure-preserving reduction is possible.) (Image: We can derive full lpH systems from reduced lpH systems and vice-versa (in some cases). The solid arrows indicate that we have an explicit construction available, the dashed arrow indicates that in this specific case we do not yet know if a structure-preserving reduction is possible.)","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"The figure above indicates that we can derive a full system tildeSigma_mathrmlpH(mathbbR^2N) = Sigma_mathrmlpH(mathcalM) from a reduced one Sigma_mathrmlpH(mathbbR^2n) If we have R = 0 i.e. if the dissipative part of the system is zero, then we can also derive a reduced system Sigma^R=0_mathrmlpH(mathbbR^2n) from a full one tildeSigma^R=0_mathrmlpH(mathbbR^2N) = Sigma^R=0_mathrmlpH(mathcalM) If and when this is true for Rneq0 is an open question[2]. We now proceed with showing this equivalence, first for the special case R = 0","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"[2]: We indicate this with a dashed arrow.","category":"page"},{"location":"port_hamiltonian_systems/#The-Special-Case-R-0","page":"port-Hamiltonian Systems","title":"The Special Case R = 0","text":"","category":"section"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"We first focus on the case where R = 0 This case was also discussed in [77].","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"Main.theorem(raw\"For ``R = 0,`` model reduction of a lpH system with a symplectic autoencoder ``(\\Psi^e, \\Psi^d)`` yields a lpH system in reduced dimension of the form:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\bar{\\Sigma}_\\mathrm{lpH} : \\begin{cases} \\dot{z}(t) & = \\mathbb{J}_{2n}\\bar{H}(z(t)) + Bu(t) \\\\ y(t) & = B^T\\nabla\\bar{H}(z(t)) \\end{cases},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``B := (\\nabla\\Psi^d)^+\\hat{B}`` and ``\\bar{H}(z(t)) := H(\\Psi^d(z(t))).``\") ","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"(nablaPsi^d)^+ = mathbbJ_2n^T(nablaPsi^d)^TmathbbJ_2N is the symplectic inverse.","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"Main.proof(raw\"We have to proof that the dynamics of ``\\Psi^d(z)``, that approximate ``\\hat{z}``, are described by a lpH system. We first insert ``\\bar{z}(t) \\approx \\Psi^d(z(t))`` into the first equation of ``\\Sigma_{lpH}``:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"(\\nabla\\Psi^d)\\dot{z}(t) = \\mathbb{J}_{2N}\\nabla{}H(\\Psi^d(z(t))) + Bu(t).\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"We then multiply the equation above with the *symplectic inverse* ``(\\nabla\\Psi^d)^+``:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\begin{aligned}\n\" * Main.indentation * raw\"    \\dot{z}(t)  & = (\\nabla\\Psi^d)^+\\mathbb{J}_{2N}\\nabla{}H(\\Psi^d(z(t))) + (\\nabla\\Psi^d)^+\\hat{B}u(t) \\\\\n\" * Main.indentation * raw\"                & = \\mathbb{J}_{2n}(\\nabla\\Psi^d)^T\\nabla{}H(\\Psi^d(z(t))) + (\\nabla\\Psi^d)^+\\hat{B}u(t) \\\\\n\" * Main.indentation * raw\"                & = \\mathbb{J}_{2n}\\nabla(H\\circ\\Psi^d(z(t))) + Bu(t),\n\" * Main.indentation * raw\"\\end{aligned}\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"thus proving our assertion.\")","category":"page"},{"location":"port_hamiltonian_systems/#From-the-Reduced-Space-to-the-Full-Space-for-R\\neq0","page":"port-Hamiltonian Systems","title":"From the Reduced Space to the Full Space for Rneq0","text":"","category":"section"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"Here we show that we can construct a lpH system on the full space from a lpH system on the reduced space; this holds for both R=0 and Rneq0 The corresponding proof was already introduced in similar form by [80].","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"Main.theorem(raw\"A lpH system on the reduced space induces a lpH system on the full space.\")","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"Main.proof(raw\"\"\"Consider a reduced lpH system:\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"\\Sigma_\\mathrm{lpH} : \\begin{cases} \\dot{z}(t) & =  (\\mathbb{J}_{2n} - R)\\nabla{}H(z(t)) + Bu(t) \\\\ y(t) & = B^T\\nabla{}H(z(t)),  \\end{cases}\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"where ``R\\in\\mathbb{R}^{2n\\times2n}`` and ``B\\in\\mathbb{R}^{2n\\times{}m}.`` After multiplying the first equation with ``\\nabla_z\\mathcal{R}`` from the left we get:\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"\\frac{d}{dt} \\mathcal{R}(z(t)) = \\nabla_z\\mathcal{R}(\\mathbb{J}_{2n} - R)\\nabla_zH + (\\nabla_z\\mathcal{R})Bu(t).\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"From now on we call ``\\tilde{B} := (\\nabla_z\\mathcal{R})B.`` We then look at the terms (i) ``(\\nabla_z\\mathcal{R})\\mathbb{J}_{2n}\\nabla_zH`` and (ii) ``(\\nabla_z\\mathcal{R})R\\nabla_zH.`` The first one (i) becomes:\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"\\begin{aligned}\n\"\"\" * Main.indentation * raw\"\"\"(\\nabla_z\\mathcal{R})\\mathbb{J}_{2n}\\nabla_zH & = \\mathbb{J}_{2N}\\mathbb{J}_{2N}^T(\\nabla_z\\mathcal{R})\\mathbb{J}_{2n}\\nabla_zH \\\\\n\"\"\" * Main.indentation * raw\"\"\"                                            & = \\mathbb{J}_{2N}((\\nabla_z\\mathcal{R})^+)^T\\nabla_zH \\\\\n\"\"\" * Main.indentation * raw\"\"\"                                            & = \\mathbb{J}_{2N}\\nabla_{\\mathcal{R}(z)}(H\\circ\\psi)\n\"\"\" * Main.indentation * raw\"\"\"\\end{aligned}\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"And the second one (ii) becomes:\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"\\begin{aligned}\n\"\"\" * Main.indentation * raw\"\"\"(\\nabla_z\\mathcal{R})R\\nabla_zH   & = (\\nabla_z\\mathcal{R})R\\nabla_z(H\\circ\\psi\\circ\\mathcal{R}) \\\\\n\"\"\" * Main.indentation * raw\"\"\"                                & = (\\nabla_z\\mathcal{R})R(\\nabla_z\\mathcal{R})^T\\nabla_{\\mathcal{R}(z)}(H\\circ\\psi) \\\\\n\"\"\" * Main.indentation * raw\"\"\"                                & =: \\tilde{R}\\nabla_{\\mathcal{R}(z)}\\bar{H}.\n\"\"\" * Main.indentation * raw\"\"\"\\end{aligned}\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"We then have in total:\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"\\tilde{\\Sigma}_\\mathrm{lpH}:\\begin{cases} \\frac{d}{dt}\\mathcal{R}(z(t)) & = (\\mathbb{J}_{2N} - \\tilde{B})\\nabla_{\\mathcal{R}(z)}\\bar{H} + \\tilde{B}u(t) \\\\ \\tilde{y}(t) & = \\tilde{B}^Tu(t) \\end{cases}\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"where  ``\\tilde{R}|_{\\mathcal{R}(z)} = (\\nabla_z\\mathcal{R})^TR(\\nabla_z\\mathcal{R}),`` ``\\tilde{B} := (\\nabla_z\\mathcal{R})B`` and ``\\bar{H} = H\\circ\\psi``.\"\"\")","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"As was already discussed in the section on Hamiltonian model order reduction the encoder Psi^e can be constructed such that it is exactly the local inverse psi This was done in e.g. [81]. Enforcing this for symplectic autoencoders is also straightforward.","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"\\begin{comment}","category":"page"},{"location":"port_hamiltonian_systems/#References","page":"port-Hamiltonian Systems","title":"References","text":"","category":"section"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"\\end{comment}","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"<!--","category":"page"},{"location":"port_hamiltonian_systems/#References-2","page":"port-Hamiltonian Systems","title":"References","text":"","category":"section"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"-->","category":"page"},{"location":"port_hamiltonian_systems/","page":"port-Hamiltonian Systems","title":"port-Hamiltonian Systems","text":"A. Van Der Schaft, D. Jeltsema and others. Port-Hamiltonian systems theory: An introductory overview. Foundations and Trends in Systems and Control 1, 173–378 (2014).\n\n\n\nP. Kotyczka and L. Lefevre. Discrete-time port-Hamiltonian systems: A definition based on symplectic integration. Systems & Control Letters 133, 104530 (2019).\n\n\n\nV. Mehrmann and R. Morandin. Structure-preserving discretization for port-Hamiltonian descriptor systems. In: 2019 IEEE 58th Conference on Decision and Control (CDC) (IEEE, 2019); pp. 6863–6868.\n\n\n\nR. Morandin. Modeling and numerical treatment of port-Hamiltonian descriptor systems. Ph.D. Thesis, Technische Universität Berlin (2023).\n\n\n\nJ. Rettberg, J. Kneifl, J. Herb, P. Buchfink, J. Fehr and B. Haasdonk. Data-driven identification of latent port-Hamiltonian systems, arXiv preprint arXiv:2408.08185 (2024).\n\n\n\n","category":"page"},{"location":"acknowledgements/#Preliminary-Remarks-and-Acknowledgements","page":"Preliminary Remarks and Acknowledgements","title":"Preliminary Remarks and Acknowledgements","text":"","category":"section"},{"location":"acknowledgements/","page":"Preliminary Remarks and Acknowledgements","title":"Preliminary Remarks and Acknowledgements","text":"During the time I conducted my PhD the TUM School of Computation, Information, and Technology (CIT) replaced the former departments of Mathematics, Informatics, and Electrical and Computer Engineering. Its mission statement reads: \"[CIT unites] a wide spectrum of different competencies:","category":"page"},{"location":"acknowledgements/","page":"Preliminary Remarks and Acknowledgements","title":"Preliminary Remarks and Acknowledgements","text":"from theoretical foundations to implementation in different contexts of application,\nfrom elementary components via hardware and software architectures to holistic technical systems,\nfrom mathematical abstraction to technological engineering processes,\nfrom formal specification to partially automated implementation.\"","category":"page"},{"location":"acknowledgements/","page":"Preliminary Remarks and Acknowledgements","title":"Preliminary Remarks and Acknowledgements","text":"Although not touching on all these aspects, I tried to make my dissertation fit into this framework: it is closely linked to the development of a software package that can be used \"in different contexts of application\" and contains algorithms that emerged as a result of \"mathematical abstraction\". In the spirit of the CIT this text contains both code snippets demonstrating the software package and mathematical theorems (including proofs). ","category":"page"},{"location":"acknowledgements/","page":"Preliminary Remarks and Acknowledgements","title":"Preliminary Remarks and Acknowledgements","text":"At this point I want to acknowledge several people who helped me professionally and personally. My foremost thanks go to my scientific mentor Michael Kraus who throughout my PhD provided me with guidance, nudged me in the right direction when I needed it, and was extremely understanding whenever progress was slow. He almost always gave me the maximum amount of freedom to pursue things I found interesting. The only instance I can think of, where this freedom was not granted, was his insistence that all his students use Julia, a demand that sped up getting results immensely.","category":"page"},{"location":"acknowledgements/","page":"Preliminary Remarks and Acknowledgements","title":"Preliminary Remarks and Acknowledgements","text":"I am also very grateful to Eric Sonnendrücker, the head of our department, who gave me many opportunities to present my work at international conferences which in turn enabled me to meet new people (very often the faces behind the papers I read) and exchange ideas. My thanks also go to all my colleagues, especially those I regularly had lunch with, for stimulating discussions on work and everything else in the world.","category":"page"},{"location":"acknowledgements/","page":"Preliminary Remarks and Acknowledgements","title":"Preliminary Remarks and Acknowledgements","text":"On a more personal note I want to thank my parents for their patience, understanding and relentless encouragement; I often think back to them telling me: \"Irgendwie geht's immer.\"","category":"page"},{"location":"acknowledgements/","page":"Preliminary Remarks and Acknowledgements","title":"Preliminary Remarks and Acknowledgements","text":"Lastly I want to thank Lejie for her love and support, remarks that very often helped me organize thoughts in my head, and her extreme understanding when I opened my laptop for the umpteenth time for a late-night programming session. 路漫漫其修远兮，我们将上下而求索。我永远爱你。","category":"page"},{"location":"acknowledgements/","page":"Preliminary Remarks and Acknowledgements","title":"Preliminary Remarks and Acknowledgements","text":"\\clearpage","category":"page"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"In this chapter we introduce various special neural network layers. A neural network layer is a parametrized function that is \\textit{relatively simple} and serves as a basic building block of a neural network \\text{architecture} (neural network architectures will be introduced in the next chapter). Some of the neural network layers in this chapter (like SympNet layers and multihead attention) are well-established while others (like volume-preserving attention and linear symplectic attention) constitute novel work.","category":"page"},{"location":"layers/sympnet_gradient/#SympNet-Layers","page":"Sympnet Layers","title":"SympNet Layers","text":"","category":"section"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"The SympNet paper [5] discusses three different kinds of sympnet layers: activation layers, linear layers and gradient layers. We discuss them below. Because activation layers are just a simplified form of gradient layers those are introduced together. A neural network that consists of many of these layers we call a SympNet.","category":"page"},{"location":"layers/sympnet_gradient/#SympNet-Gradient-Layer","page":"Sympnet Layers","title":"SympNet Gradient Layer","text":"","category":"section"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"The Sympnet gradient layer is based on the following theorem: ","category":"page"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"Main.theorem(raw\"\"\"Given a symplectic vector space ``\\mathbb{R}^{2n}`` with coordinates ``q_1, \\ldots, q_n, p_1, \\ldots, p_n`` and a function ``f:\\mathbb{R}^n\\to\\mathbb{R}`` that only acts on the ``q`` part, the map ``(q, p) \\mapsto (q, p + \\nabla_qf)`` is symplectic. A similar statement holds if ``f`` only acts on the ``p`` part.\"\"\")","category":"page"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"Main.proof(raw\"\"\"Proofing this is straightforward by looking at the gradient of the mapping:\n\"\"\" * Main.indentation * raw\"\"\"\n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\"    \\begin{pmatrix}\n\"\"\" * Main.indentation * raw\"\"\"        \\mathbb{I} & \\mathbb{O} \\\\ \n\"\"\" * Main.indentation * raw\"\"\"        \\nabla_q^2f & \\mathbb{I}\n\"\"\" * Main.indentation * raw\"\"\"    \\end{pmatrix},\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"where ``\\nabla_q^2f`` is the Hessian of ``f``. This matrix is symmetric and for any symmetric matrix ``A`` we have that: \n\"\"\" * Main.indentation * raw\"\"\"```math\n\"\"\" * Main.indentation * raw\"\"\" \\begin{pmatrix}\n\"\"\" * Main.indentation * raw\"\"\"     \\mathbb{I} & \\mathbb{O} \\\\ \n\"\"\" * Main.indentation * raw\"\"\"     A & \\mathbb{I}\n\"\"\" * Main.indentation * raw\"\"\" \\end{pmatrix}^T \\mathbb{J}_{2n} \n\"\"\" * Main.indentation * raw\"\"\" \\begin{pmatrix} \n\"\"\" * Main.indentation * raw\"\"\"     \\mathbb{I} & \\mathbb{O} \\\\ \n\"\"\" * Main.indentation * raw\"\"\"     A & \\mathbb{I} \n\"\"\" * Main.indentation * raw\"\"\" \\end{pmatrix} = \n\"\"\" * Main.indentation * raw\"\"\" \\begin{pmatrix}\n\"\"\" * Main.indentation * raw\"\"\"     \\mathbb{I} & A \\\\ \n\"\"\" * Main.indentation * raw\"\"\"     \\mathbb{O} & \\mathbb{I}\n\"\"\" * Main.indentation * raw\"\"\" \\end{pmatrix} \n\"\"\" * Main.indentation * raw\"\"\" \\begin{pmatrix} \n\"\"\" * Main.indentation * raw\"\"\"     \\mathbb{O} & \\mathbb{I} \\\\ \n\"\"\" * Main.indentation * raw\"\"\"     -\\mathbb{I} & \\mathbb{O} \n\"\"\" * Main.indentation * raw\"\"\" \\end{pmatrix} \n\"\"\" * Main.indentation * raw\"\"\" \\begin{pmatrix}\n\"\"\" * Main.indentation * raw\"\"\"     \\mathbb{I} & \\mathbb{O} \\\\ \n\"\"\" * Main.indentation * raw\"\"\"     A & \\mathbb{I}\n\"\"\" * Main.indentation * raw\"\"\" \\end{pmatrix} = \n\"\"\" * Main.indentation * raw\"\"\" \\begin{pmatrix}\n\"\"\" * Main.indentation * raw\"\"\"     \\mathbb{O} & \\mathbb{I} \\\\ \n\"\"\" * Main.indentation * raw\"\"\"     -\\mathbb{I} & \\mathbb{O} \n\"\"\" * Main.indentation * raw\"\"\" \\end{pmatrix} = \\mathbb{J}_{2n},\n\"\"\" * Main.indentation * raw\"\"\"```\n\"\"\" * Main.indentation * raw\"\"\"thus showing symplecticity.\"\"\")","category":"page"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"If we deal with GSympNets this function f is ","category":"page"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"    f(q) = a^T Sigma(Kq + b)","category":"page"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"where a binmathbbR^m, KinmathbbR^mtimesn and Sigma is the antiderivative of some common activation function sigma. We routinely refer to m as the upscaling dimension in GeometricMachineLearning. Computing the gradient of f gives: ","category":"page"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"    nabla_qf_k = sum_i=1^m a_i sigma(sum_j=1^nk_ijq_j + b_i)k_ik = K^T (a odot sigma(Kq + b))","category":"page"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"where odot is the element-wise product, i.e. aodotv_k = a_kv_k. This is the form that gradient layers take. In addition to gradient layers GeometricMachineLearning also has linear and activation layers implemented. Activation layers are simplified versions of gradient layers. These are equivalent to taking m = n and K = mathbbI","category":"page"},{"location":"layers/sympnet_gradient/#SympNet-Linear-Layer","page":"Sympnet Layers","title":"SympNet Linear Layer","text":"","category":"section"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"Linear layers of type p are of the form:","category":"page"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"beginpmatrix q  p endpmatrix mapsto beginpmatrix mathbbI  mathbbO  A  mathbbI endpmatrix beginpmatrix q  p endpmatrix","category":"page"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"where A is a symmetric matrix. This is implemented very efficiently in GeometricMachineLearning with the special matrix SymmetricMatrix.","category":"page"},{"location":"layers/sympnet_gradient/#Library-Functions","page":"Sympnet Layers","title":"Library Functions","text":"","category":"section"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"GeometricMachineLearning.SympNetLayer\nGeometricMachineLearning.GradientLayer\nGradientLayerQ\nGradientLayerP\nGeometricMachineLearning.LinearLayer\nLinearLayerQ\nLinearLayerP\nGeometricMachineLearning.ActivationLayer\nActivationLayerQ\nActivationLayerP","category":"page"},{"location":"layers/sympnet_gradient/#GeometricMachineLearning.SympNetLayer","page":"Sympnet Layers","title":"GeometricMachineLearning.SympNetLayer","text":"SympNetLayer <: AbstractExplicitLayer\n\nImplements the various layers from the SympNet paper [5].\n\nThis is a super type of GradientLayer, ActivationLayer and LinearLayer.\n\nSee the relevant docstrings of those layers for more information.\n\n\n\n\n\n","category":"type"},{"location":"layers/sympnet_gradient/#GeometricMachineLearning.GradientLayer","page":"Sympnet Layers","title":"GeometricMachineLearning.GradientLayer","text":"GradientLayer <: SympNetLayer\n\nSee the docstrings for the constructors GradientLayerQ and GradientLayerP.\n\n\n\n\n\n","category":"type"},{"location":"layers/sympnet_gradient/#GeometricMachineLearning.GradientLayerQ","page":"Sympnet Layers","title":"GeometricMachineLearning.GradientLayerQ","text":"GradientLayerQ(n, upscaling_dimension, activation)\n\nMake an instance of a gradient-q layer.\n\nThe gradient layer that changes the q component. It is of the form: \n\nbeginbmatrix\n        mathbbI  nablaV  mathbbO  mathbbI \nendbmatrix\n\nwith V(p) = sum_i=1^Ma_iSigma(sum_jk_ijp_j+b_i), where mathttactivation equiv Sigma is the antiderivative of the activation function sigma (one-layer neural network). We refer to M as the upscaling dimension. \n\nSuch layers are by construction symplectic.\n\n\n\n\n\n","category":"type"},{"location":"layers/sympnet_gradient/#GeometricMachineLearning.GradientLayerP","page":"Sympnet Layers","title":"GeometricMachineLearning.GradientLayerP","text":"GradientLayerP(n, upscaling_dimension, activation)\n\nMake an instance of a gradient-p layer.\n\nThe gradient layer that changes the p component. It is of the form: \n\nbeginbmatrix\n        mathbbI  mathbbO  nablaV  mathbbI \nendbmatrix\n\nwith V(p) = sum_i=1^Ma_iSigma(sum_jk_ijq_j+b_i), where mathttactivation equiv Sigma is the antiderivative of the activation function sigma (one-layer neural network). We refer to M as the upscaling dimension. \n\nSuch layers are by construction symplectic.\n\n\n\n\n\n","category":"type"},{"location":"layers/sympnet_gradient/#GeometricMachineLearning.LinearLayer","page":"Sympnet Layers","title":"GeometricMachineLearning.LinearLayer","text":"LinearLayer <: SympNetLayer\n\nSee the constructors LinearLayerQ and LinearLayerP.\n\nImplementation\n\nLinearLayer uses the custom matrix SymmetricMatrix for its weight. \n\n\n\n\n\n","category":"type"},{"location":"layers/sympnet_gradient/#GeometricMachineLearning.LinearLayerQ","page":"Sympnet Layers","title":"GeometricMachineLearning.LinearLayerQ","text":"LinearLayerQ(n)\n\nMake a linear layer of dimension ntimesn that only changes the q component.\n\nThis is equivalent to a left multiplication by the matrix:\n\nbeginpmatrix\nmathbbI  A  \nmathbbO  mathbbI\nendpmatrix \n\nwhere A is a SymmetricMatrix.\n\n\n\n\n\n","category":"type"},{"location":"layers/sympnet_gradient/#GeometricMachineLearning.LinearLayerP","page":"Sympnet Layers","title":"GeometricMachineLearning.LinearLayerP","text":"LinearLayerP(n)\n\nMake a linear layer of dimension ntimesn that only changes the p component.\n\nThis is equivalent to a left multiplication by the matrix:\n\nbeginpmatrix\nmathbbI  mathbbO  \nA  mathbbI\nendpmatrix \n\nwhere A is a SymmetricMatrix.\n\n\n\n\n\n","category":"type"},{"location":"layers/sympnet_gradient/#GeometricMachineLearning.ActivationLayer","page":"Sympnet Layers","title":"GeometricMachineLearning.ActivationLayer","text":"ActivationLayer <: SympNetLayer\n\nSee the constructors ActivationLayerQ and ActivationLayerP.\n\n\n\n\n\n","category":"type"},{"location":"layers/sympnet_gradient/#GeometricMachineLearning.ActivationLayerQ","page":"Sympnet Layers","title":"GeometricMachineLearning.ActivationLayerQ","text":"ActivationLayerQ(n, σ)\n\nMake an activation layer of size n and with activation σ that only changes the q component. \n\nPerforms:\n\nbeginpmatrix\n        q  p\nendpmatrix mapsto \nbeginpmatrix\n        q + mathrmdiag(a)sigma(p)  p\nendpmatrix\n\nThis can be recovered from GradientLayerQ by setting M equal to n, K equal to mathbbI and b equal to zero.\n\n\n\n\n\n","category":"type"},{"location":"layers/sympnet_gradient/#GeometricMachineLearning.ActivationLayerP","page":"Sympnet Layers","title":"GeometricMachineLearning.ActivationLayerP","text":"ActivationLayerP(n, σ)\n\nMake an activation layer of size n and with activation σ that only changes the p component. \n\nPerforms:\n\nbeginpmatrix\n        q  p\nendpmatrix mapsto \nbeginpmatrix\n        q  p + mathrmdiag(a)sigma(q)\nendpmatrix\n\nThis can be recovered from GradientLayerP by setting M equal to n, K equal to mathbbI and b equal to zero.\n\n\n\n\n\n","category":"type"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"\\begin{comment}","category":"page"},{"location":"layers/sympnet_gradient/#References","page":"Sympnet Layers","title":"References","text":"","category":"section"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"P. Jin, Z. Zhang, A. Zhu, Y. Tang and G. E. Karniadakis. SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems. Neural Networks 132, 166–179 (2020).\n\n\n\n","category":"page"},{"location":"layers/sympnet_gradient/","page":"Sympnet Layers","title":"Sympnet Layers","text":"\\end{comment}","category":"page"},{"location":"manifolds/metric_and_vector_spaces/#(Topological)-Metric-Spaces","page":"Metric and Vector Spaces","title":"(Topological) Metric Spaces","text":"","category":"section"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"A metric space is a certain class of a topological space where the topology is induced through a metric. We define this notion now:","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"Main.definition(raw\"A **metric** on a topological space ``\\mathcal{M}`` is a mapping ``d:\\mathcal{M}\\times\\mathcal{M}\\to\\mathbb{R}`` such that the following three conditions hold: \n\" * \nMain.indentation * raw\"1. ``d(x, y) = 0 \\iff x = y`` for every ``x,y\\in\\mathcal{M}``, i.e. the distance between two points is zero if and only if they are the same,\n\" * \nMain.indentation * raw\"2. ``d(x, y) = d(y, x)``,\n\" *\nMain.indentation * raw\"3. ``d(x, z) \\leq d(x, y) + d(y, z)``.\n\" *\nMain.indentation * raw\"The second condition is referred to as *symmetry* and the third condition is referred to as the *triangle inequality*.\")","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"We give some examples of metric spaces that are relevant for us: ","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"Main.example(raw\"The real line ``\\mathbb{R}`` with the metric defined by the absolute distance between two points: ``d(x, y) = |y - x|``.\")","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"Main.example(raw\"The vector space ``\\mathbb{R}^n`` with the *Euclidean distance* ``d_2(x, y) = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}``.\")","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"Main.example(raw\"The space of continuous functions ``\\mathcal{C} = \\{f:(-\\epsilon, \\epsilon)\\to\\mathbb{R}^n\\}`` with the metric ``d_\\infty(f_1, f_2) = \\mathrm{sup}_{t\\in(-\\epsilon, \\epsilon)}|f_1(t) - f_2(t)|.``\")","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"Main.proof(raw\"We have to show the triangle inequality: \n\" * \nMain.indentation * raw\"```math\n\" * \nMain.indentation * raw\"\\begin{aligned}\n\" *\nMain.indentation * raw\"d_\\infty(d_1, d_3) = \\mathrm{sup}_{t\\in(-\\epsilon, \\epsilon)}|f_1(t) - f_3(t)| & \\leq \\mathrm{sup}_{t\\in(-\\epsilon, \\epsilon)}(|f_1(t) - f_2(t)| + |f_2(t) - f_3(t)|) \\\\\n\" *\nMain.indentation * raw\"& \\leq \\mathrm{sup}_{t\\in(-\\epsilon, \\epsilon)}|f_1(t) - f_2(t)| + \\mathrm{sup}_{t\\in(-\\epsilon, \\epsilon)}|f_1(t) - f_2(t)|.\n\" * \nMain.indentation * raw\"\\end{aligned}\n\" * \nMain.indentation * raw\"```\n\" *\nMain.indentation * raw\"This shows that ``d_\\infty`` is indeed a metric.\")","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"Main.example(raw\"Any Riemannian manifold is a metric space.\")","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"This last example shows that metric spaces need not be vector spaces, i.e. spaces for which we can define a metric but not addition of two elements. This will be discussed in more detail in the section on Riemannian manifolds.","category":"page"},{"location":"manifolds/metric_and_vector_spaces/#Complete-Metric-Spaces","page":"Metric and Vector Spaces","title":"Complete Metric Spaces","text":"","category":"section"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"To define complete metric spaces we first need the definition of a Cauchy sequence.","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"Main.definition(raw\"A **Cauchy sequence** is a sequence ``(a_n)_{n\\in\\mathbb{N}}`` for which, given any ``\\epsilon>0``, we can find an integer ``N`` such that ``d(a_n, a_m) < \\epsilon`` for all ``n, m \\geq N``.\")","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"Now we can give the definition of a complete metric space:","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"Main.definition(raw\"A **complete metric space** is one for which every Cauchy sequence converges.\")","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"Completeness of the real numbers is most often seen as an axiom and therefore stated without proof. This also implies completeness of mathbbR^n [17].","category":"page"},{"location":"manifolds/metric_and_vector_spaces/#(Topological)-Vector-Spaces","page":"Metric and Vector Spaces","title":"(Topological) Vector Spaces","text":"","category":"section"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"Vector Spaces are, like metric spaces, topological spaces which we endow with additional structure. ","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"Main.definition(raw\"A **vector space** ``\\mathcal{V}`` is a topological space for which we define an operation called *addition* and denoted by ``+`` and an operation called *scalar multiplication* (by elements of ``\\mathbb{R}``) denoted by ``x \\mapsto ax`` for ``x\\in\\mathcal{V}`` and ``x\\in\\mathbb{R}`` for which the following hold for all ``x, y, z\\in\\mathcal{V}`` and ``a, b\\in\\mathbb{R}``:\n\" * \nMain.indentation * raw\"1. ``x + (y + z) = (x + y) + z,``\n\" * \nMain.indentation * raw\"2. ``x + y = y + x,``\n\" * \nMain.indentation * raw\"3. ``\\exists 0 \\in \\mathcal{V}\\text{such that }x + 0 = x,``\n\" * \nMain.indentation * raw\"4. ``\\exists -x \\in \\mathcal{V}\\text{ such that }x + (-x) = 0,``\n\" * \nMain.indentation * raw\"5. ``a(ax) = (ab)x,``\n\" * \nMain.indentation * raw\"6. ``1x = x`` for ``1\\in\\mathbb{R},``\n\" * \nMain.indentation * raw\"7. ``a(x + y) = ax + ay,``\n\" * \nMain.indentation * raw\"8. ``(a + b)x = ax + bx.``\n\" * \nMain.indentation * raw\"The first law is known as *associativity*, the second one as *commutativity* and the last two ones are known as *distributivity*.\")","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"The topological spaces mathbbR and mathbbR^n are (almost) trivially vector spaces. The same is true for many function spaces. One of the special aspects of GeometricMachineLearning is that it can deal with spaces that are not vector spaces, but manifolds. All vector spaces are however manifolds.  ","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"\\begin{comment}","category":"page"},{"location":"manifolds/metric_and_vector_spaces/#References","page":"Metric and Vector Spaces","title":"References","text":"","category":"section"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"S. Lang. Real and functional analysis. Vol. 142 (Springer Science & Business Media, 2012).\n\n\n\n","category":"page"},{"location":"manifolds/metric_and_vector_spaces/","page":"Metric and Vector Spaces","title":"Metric and Vector Spaces","text":"\\end{comment}","category":"page"},{"location":"architectures/volume_preserving_transformer/#Volume-Preserving-Transformer","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"","category":"section"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"The volume-preserving transformer [4] is, similar to the standard transformer, a combination of two different neural networks: a volume-preserving attention layer and a volume-preserving feedforward layer. It is visualized below:","category":"page"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"(Image: Visualization of the Volume-Preserving Transformer architecture.) (Image: Visualization of the Volume-Preserving Transformer architecture.)","category":"page"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"In the figure we indicate that we leave out the add connection. When talking about the standard transformer we said that the add connection is optional and can be included via the keyword argument add_connection. For the volume-preserving transformer this is not true: it is always excluded.","category":"page"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"Note that the volume-preserving transformer preserves the volume in the sense of the product spaces. That the VolumePreservingAttention layer preserves this structure was discussed when we introduced it. That the VolumePreservingFeedForwardLayers preserve this structure on the product space is also easy to see. We take a VolumePreservingFeedForwardLayer, e.g. ","category":"page"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"    psi z mapsto sigma(Lz + b)","category":"page"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"and look at its action on the element of the product space z^(1) ldots z^(T) = ZinmathbbR^dtimesT:","category":"page"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"    psi_T z^(1) ldots z^(T) mapsto psi(z^(1)) ldots psi(z^(T))","category":"page"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"The jacobian of hatpsi_TmathbbR^dTtomathbbR^dT, the representation of psi_T in the coordinate system of the big vector, is of the form[1]","category":"page"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"[1]: In order to arrive at this representation we possibly have to exchange the order of the rows in the matrix. This is however not critical since it may only cause a sign change in the determinant.","category":"page"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"    nablahatpsi_T = beginbmatrix J  mathbbO  cdots  mathbbO  \n                                         mathbbO  J  cdots  mathbbO  \n                                         vdots  ddots  vdots  vdots \n                                         mathbbO  mathbbO  cdots  Jendbmatrix = J otimes mathbbI_T","category":"page"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"where J is the jacobian of psi. We now see that mathrmdet(nablahatpsi_T) = 1 and volume in the product space is preserved. ","category":"page"},{"location":"architectures/volume_preserving_transformer/#Library-Functions","page":"Volume-Preserving Transformer","title":"Library Functions","text":"","category":"section"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"VolumePreservingTransformer","category":"page"},{"location":"architectures/volume_preserving_transformer/#GeometricMachineLearning.VolumePreservingTransformer","page":"Volume-Preserving Transformer","title":"GeometricMachineLearning.VolumePreservingTransformer","text":"VolumePreservingTransformer(sys_dim, seq_length)\n\nMake an instance of the volume-preserving transformer for a given system dimension and sequence length.\n\nArguments\n\nThe following are keyword argumetns:\n\nn_blocks::Int = 1: The number of blocks in one transformer unit (containing linear layers and nonlinear layers).\nn_linear::Int = 1: The number of linear VolumePreservingLowerLayers and VolumePreservingUpperLayers in one block.\nL::Int = 1: The number of transformer units. \nactivation = tanh: The activation function.\ninit_upper::Bool = false: Specifies if the network first acts on the q component. \nskew_sym::Bool = false: specifies if we the weight matrix is skew symmetric or arbitrary.\n\n\n\n\n\n","category":"type"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"\\begin{comment}","category":"page"},{"location":"architectures/volume_preserving_transformer/#References","page":"Volume-Preserving Transformer","title":"References","text":"","category":"section"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"B. Brantner, G. de Romemont, M. Kraus and Z. Li. Volume-Preserving Transformers for Learning Time Series Data with Structure, arXiv preprint arXiv:2312:11166v2 (2024).\n\n\n\n","category":"page"},{"location":"architectures/volume_preserving_transformer/","page":"Volume-Preserving Transformer","title":"Volume-Preserving Transformer","text":"\\end{comment}","category":"page"},{"location":"layers/attention_layer/#The-Attention-Layer","page":"(Volume-Preserving) Attention","title":"The Attention Layer","text":"","category":"section"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"The attention mechanism was originally developed for image recognition and natural language processing (NLP) tasks. It is motivated by the need to handle time series data in an efficient way[1]. Its essential idea is to compute correlations between vectors in input sequences. So given two sequences","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"[1]: Recurrent neural networks [52] have the same motivation. The have however been replaced by transformers, of which attention is the most important component, in many applications.","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"(z_q^(1) z_q^(2) ldots z_q^(T)) text and  (z_k^(1) z_k^(2) ldots z_k^(T))","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"an attention mechanism computes pair-wise correlations between all combinations of two input vectors from these sequences. In [53] \"additive\" attention is used to compute such correlations: ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"(z_q z_k) mapsto v^Tsigma(Wz_q + Uz_k) ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"where z_q z_k in mathbbR^d are elements of the input sequences. The learnable parameters are W U in mathbbR^ntimesd and v in mathbbR^n.","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"However multiplicative attention [54] is more straightforward to interpret and cheaper to handle computationally: ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"(z_q z_k) mapsto z_q^TWz_k","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"where W in mathbbR^dtimesd is a learnable weight matrix with respect to which correlations are computed as scalar products. Regardless of the type of attention used, they all try to compute correlations among input sequences on whose basis further computation is performed. Given two input sequences Z_q = (z_q^(1) ldots z_q^(T)) and Z_k = (z_k^(1) ldots z_k^(T)), we can arrange the various correlations into a correlation matrix CinmathbbR^TtimesT with entries C_ij = mathttattention(z_q^(i) z_k^(j)). In the case of multiplicative attention this matrix is just C = Z^TWZ.","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"Main.remark(raw\"The notation with designating different vectors with a ``q`` and ``k`` label comes from natural language processing. These labels stand for *queries* and *keys*\")","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"In the section on multihead attention this will be explained further.","category":"page"},{"location":"layers/attention_layer/#Reweighting-of-the-Input-Sequence","page":"(Volume-Preserving) Attention","title":"Reweighting of the Input Sequence","text":"","category":"section"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"In GeometricMachineLearning we always compute self-attention, meaning that the two input sequences Z_q and Z_k are the same, i.e. Z = Z_q = Z_k.[2]","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"[2]: Multihead attention also falls into this category. Here the input Z is multiplied from the left with several projection matrices P^Q_i and P^K_i, where i indicates the head. For each head we then compute a correlation matrix (P^Q_i Z)^T(P^K Z). ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"This is then used to reweight the columns in the input sequence Z. For this we first apply a nonlinearity sigma onto C and then multiply sigma(C) onto Z from the right, i.e. the output of the attention layer is Zsigma(C). So we perform the following mappings:","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"Z xrightarrowmathrmcorrelations C(Z) = C xrightarrowsigma sigma(C) xrightarrowtextright multiplication Z sigma(C)","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"After the right multiplication the outputs is of the following form: ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"    sum_i=1^Tp^(1)_iz^(i) ldots sum_i=1^Tp^(T)_iz^(i)","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"for p^(i) = sigma(C)_bulleti. What is learned during training are T different linear combinations of the input vectors, where the coefficients p^(i)_j in these linear combinations depend on the input Z nonlinearly. ","category":"page"},{"location":"layers/attention_layer/#Volume-Preserving-Attention","page":"(Volume-Preserving) Attention","title":"Volume-Preserving Attention","text":"","category":"section"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"The VolumePreservingAttention layer (and the activation function sigma defined for it) in GeometricMachineLearning was specifically designed to apply it to data coming from physical systems that can be described through a divergence-free or a symplectic vector field.  Traditionally the nonlinearity in the attention mechanism is a softmax[3] [54] and the self-attention layer performs the following mapping: ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"[3]: The softmax acts on the matrix C in a vector-wise manner, i.e. it operates on each column of the input matrix C = mathrmsoftmax(c_bullet1) ldots mathrmsoftmax(c_bulletT) equiv c^(1) ldots c^(T). The result is a sequence of probability vectors y^(1) ldots y^(T) = mathrmsoftmax(y^(1)) ldots mathrmsoftmax(y^(T)) for which sum_i=1^Ty^(j)_i=1quadforalljin1dotsT","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"Z = z^(1) ldots z^(T) mapsto Zmathrmsoftmax(Z^TWZ)","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"The softmax activation acts vector-wise, i.e. if we supply it with a matrix C as input it returns: ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"mathrmsoftmax(C) = mathrmsoftmax(c_bullet1) ldots mathrmsoftmax(c_bulletT)","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"The output of a softmax is a probability vector (also called stochastic vector) and the matrix Y = y^(1) ldots y^(T), where each column is a probability vector, is sometimes referred to as a \"stochastic matrix\" [55]. This attention mechanism finds application in transformer neural networks [54]. The problem with this matrix from a geometric point of view is that all the columns are independent of each other and the nonlinear transformation could in theory produce a stochastic matrix for which all columns are identical and thus lead to a loss of information. So the softmax activation function is inherently non-geometric. We visualize this with the figure below:","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"(Image: Visualization of the reweighting of the input sequence. Here the different coefficients are mostly independent of each other and could in theory produce the same reweighting for each output vector.) (Image: Visualization of the reweighting of the input sequence. Here the different coefficients are mostly independent of each other and could in theory produce the same reweighting for each output vector.)","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"So the y coefficients responsible for producing the first output vector are independent from those producing the second output vector etc., they have the condition sum_i=1^Ty^(j)_iz_mu^(i) for each column j imposed on them, but the coefficients for two different columns are independent of each other.","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"Main.remark(raw\"We said that the coefficients are *independent of each other*, which means that in theory, they could be the same or ill-conditioned, i.e. we could have\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\mathrm{det}(\\mathrm{softmax}(C)) \\approx 0.\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"With *volume-preserving attention* we make the coefficients dependent of each other, such that the columns of ``\\sigma(C)`` are *independent of each other:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\sigma(C)^T\\sigma(C) = \\mathbb{I},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"which further implies ``\\mathrm{det}(\\sigma(C)) = 1``.\")","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"Besides the traditional attention mechanism GeometricMachineLearning therefore also has a volume-preserving transformation that fulfills a similar role, but imposes additional structure on the y coefficients. There are two approaches implemented to realize this volume-preserving transformation. Both of them however utilize the Cayley transform to produce orthogonal matrices instead of stochastic matrices. For an orthogonal matrix Sigma we have Sigma^TSigma = mathbbI, so all the columns are linearly independent, which is not necessarily true for a stochastic matrix P. In the following we explain how this new activation function is implemented. First we need to briefly discuss the Cayley transform. ","category":"page"},{"location":"layers/attention_layer/#The-Cayley-Transform","page":"(Volume-Preserving) Attention","title":"The Cayley Transform","text":"","category":"section"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"The Cayley transform maps from skew-symmetric matrices to orthonormal matrices[4]. It takes the form[4]:","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"[4]: The Cayley transform here does not have the factor 12 hat we used when talking about the Cayley retraction. This is because now we do not need the retraction property ddtmathrmCayley(tV)_t=0 = V, but only a map mathfrakgtoG=SO(N)","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"mathrmCayley A mapsto (mathbbI - A)(mathbbI + A)^-1","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"Analogously to when we used the Cayley transform as a retraction, we can easily check that mathrmCayley(A) is orthogonal if A is skew-symmetric. For this consider varepsilon mapsto A(varepsilon)inmathcalS_mathrmskew with A(0) = mathbbI and A(0) = B. Then we have: ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"fracdelta(mathrmCayley(A)^TmathrmCayley(A))deltaA = fracddvarepsilon_varepsilon=0 mathrmCayley(A(varepsilon))^T mathrmCayley(A(varepsilon)) = A(0)^T + A(0) = mathbbO","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"So mathrmCayley(A)^TmathrmCayley(A) remains unchanged among varepsilon. In order to use the Cayley transform as an activation function we further need a mapping from the input Z to a skew-symmetric matrix. This is realized in two ways in GeometricMachineLearning: via a scalar-product with a skew-symmetric weighting and via a scalar-product with an arbitrary weighting.","category":"page"},{"location":"layers/attention_layer/#First-approach:-scalar-products-with-a-skew-symmetric-weighting","page":"(Volume-Preserving) Attention","title":"First approach: scalar products with a skew-symmetric weighting","text":"","category":"section"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"For this the attention layer is modified in the following way: ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"Z = z^(1) ldots z^(T) mapsto Zsigma(Z^TAZ)","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"where sigma(C)=mathrmCayley(C) and A is a matrix of type SkewSymMatrix that is learnable, i.e. the parameters of the attention layer are stored in A.","category":"page"},{"location":"layers/attention_layer/#Second-approach:-scalar-products-with-an-arbitrary-weighting","page":"(Volume-Preserving) Attention","title":"Second approach: scalar products with an arbitrary weighting","text":"","category":"section"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"For this approach we compute correlations between the input vectors based on scalar product with an arbitrary weighting. This arbitrary TtimesT matrix A constitutes the learnable parameters of the attention layer. The correlations we consider here are based on: ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"(z^(2))^TAz^(1) (z^(3))^TAz^(1) ldots (z^(d))^TAz^(1) (z^(3))^TAz^(2) ldots (z^(d))^TAz^(2) ldots (z^(d))^TAz^(d-1)","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"So we consider correlations (z^(i))^Tz^(j) for which i  j. We now arrange these correlations into a skew-symmetric matrix: ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"C = beginbmatrix\n        0                -(z^(2))^TAz^(1)  -(z^(3))^TAz^(1)      ldots  -(z^(d))^TAz^(1) \n    (z^(2))^TAz^(1)        0               -(z^(3))^TAz^(2)      ldots  -(z^(d))^TAz^(2) \n    ldots                     ldots                 ldots             ldots     ldots             \n    (z^(d))^TAz^(1)  (z^(d))^TAz^(2)   (z^(d))^TAz^(3)       ldots         0               \nendbmatrix","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"This correlation matrix can now again be used as an input for the Cayley transform to produce an orthogonal matrix. Mathematically this is also equivalent to first computing all correlations Z^TAZ and then mapping the lower triangular to the upper triangular and negating these elements. This is visualized below: ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"(Image: The lower-triangular part is copied to the upper-triangular part and the respective entries are negated.) (Image: The lower-triangular part is copied to the upper-triangular part and the respective entries are negated.)","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"Internally GeometricMachineLearning computes this more efficiently with the function GeometricMachineLearning.tensor_mat_skew_sym_assign. We show a comparison of the two approaches in the examples section.","category":"page"},{"location":"layers/attention_layer/#How-is-Structure-Preserved?","page":"(Volume-Preserving) Attention","title":"How is Structure Preserved?","text":"","category":"section"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"In order to discuss how structure is preserved we first have to define what structure we mean precisely. This structure is strongly inspired by traditional multi-step methods [56]. ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"Main.remark(raw\"We define what volume preservation means for the product space ``\\mathbb{R}^{d}\\times\\cdots\\times\\mathbb{R}^{d}\\equiv\\times_\\text{$T$ times}\\mathbb{R}^{d}``, i.e. *neural network multi-step methods* in our case are mappings whose domain and image are of the same dimension:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\varphi: \\times_\\text{$T$ times}\\mathbb{R}^{d} \\to \\times_\\text{$T$ times}\\mathbb{R}^{d}.\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"This is not the case for *traditional multi-step methods*; there one usually has a number ``T>1`` of input vectors, but only one output vector. There are however empirical and theoretical advantages of having equally-sized inputs and outputs.\")","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"Consider an isomorphism hat times_text(T times)mathbbR^dstackrelapproxlongrightarrowmathbbR^dT. Specifically, we use:","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"Z =  leftbeginarraycccc\n            z_1^(1)   z_1^(2)  quadcdotsquad  z_1^(T) \n            z_2^(1)   z_2^(2)  cdots  z_2^(T) \n            cdots   cdots  cdots  cdots \n            z_d^(1)  z_d^(2)  cdots  z_d^(T)\n            endarrayright mapsto \n            leftbeginarrayc  z_1^(1)  z_1^(2)  cdots  z_1^(T)  z_2^(1)  cdots  z_d^(T) endarrayright = Z_mathrmvec","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"so we arrange the rows consecutively into a vector. The inverse of Z mapsto hatZ we refer to as Y mapsto tildeY. In the following we also write hatvarphi for the mapping hatcircvarphicirctilde.","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"Main.definition(raw\"We say that a mapping ``\\varphi: \\times_\\text{$T$ times}\\mathbb{R}^{d} \\to \\times_\\text{$T$ times}\\mathbb{R}^{d}`` is **volume-preserving** if the associated ``\\hat{\\varphi}`` is volume-preserving.\")","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"In the transformed coordinate system (in terms of the vector Z_mathrmvec defined above) this is equivalent to multiplication by a sparse matrix tildeLambda(Z) from the left:","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"    tildeLambda(Z) Z_mathrmvec = (Lambda(Z) otimes mathbbI_T) Z_mathrmvec = \n    beginpmatrix\n    Lambda(Z)  mathbbO  cdots   mathbbO \n    mathbbO  Lambda(Z)  cdots  mathbbO \n    cdots  cdots  ddots  cdots  \n    mathbbO  mathbbO  cdots  Lambda(Z) \n    endpmatrix\n    leftbeginarrayc  z_1^(1)  z_1^(2)  ldots  z_1^(T)  z_2^(1)  ldots  z_d^(T) endarrayright","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"where otimesmathbbR^ntimesntimesmathbbR^ptimesq to mathbbR^pmtimesqn is the Kronecker product. tildeLambda(Z) is easily shown to be an orthogonal matrix and a symplectic matrix, i.e. it satisfies","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"tildeLambda(Z)^TtildeLambda(Z) = mathbbI","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"and","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"tildeLambda(Z)^TmathbbJtildeLambda(Z) = mathbbJ","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"So tildeLambda(Z) is both orthogonal and symplectic.","category":"page"},{"location":"layers/attention_layer/#Historical-Note","page":"(Volume-Preserving) Attention","title":"Historical Note","text":"","category":"section"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"Attention was used before the transformer was introduced, but mostly in connection with recurrent neural networks see [53, 57]. ","category":"page"},{"location":"layers/attention_layer/#Library-Functions","page":"(Volume-Preserving) Attention","title":"Library Functions","text":"","category":"section"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"GeometricMachineLearning.tensor_mat_skew_sym_assign\nVolumePreservingAttention","category":"page"},{"location":"layers/attention_layer/#GeometricMachineLearning.tensor_mat_skew_sym_assign","page":"(Volume-Preserving) Attention","title":"GeometricMachineLearning.tensor_mat_skew_sym_assign","text":"tensor_mat_skew_sym_assign(Z::AbstractArray{<:Number, 3}, A::AbstractMatrix)\n\nCompute scalar products of columns of Z along the second axis.\n\nThe scalar products are weighted by A.\n\nScalar products are computed for any two vectors of the form Z[:, i, k] and Z[:, j, k], i.e.\n\n    (z^(i) z^(j)) mapsto (z^(i))^TAz^(j) text for  i  j\n\nThe result of this are n(n-2)div2 scalar products for each index k from the third axis. \n\nThese scalar products are written into a lower-triangular matrix and the final output of the function is a tensor of these lower-triangular matrices. \n\nThis is used in VolumePreservingAttention when skew_sym is set to false.\n\nExamples\n\nHere we consider a weighting\n\nA = beginpmatrix 1  0  0  0  2  0  0  0  3endpmatrix\n\nand three sequences:\n\nZ_1 = beginpmatrix 1  1  0  1  0  1 endpmatrixquad Z_2 = beginpmatrix 0  1  1  1  0  1 endpmatrix quad Z_3 = beginpmatrix 0  1  0  1  1  1 endpmatrix\n\nThe result of applying tensor_mat_skew_sym_assign is a tensor inmathbbR^2times2times3\n\nusing GeometricMachineLearning: tensor_mat_skew_sym_assign\n\nA = [1 0 0; 0 2 0; 0 0 3]\nZ = [1; 0; 0;; 1; 1; 1;;; 0; 1; 0;; 1; 1; 1;;; 0; 0; 1;; 1; 1; 1]\n\ntensor_mat_skew_sym_assign(Z, A)\n\n# output\n\n2×2×3 Array{Int64, 3}:\n[:, :, 1] =\n 0  0\n 1  0\n\n[:, :, 2] =\n 0  0\n 2  0\n\n[:, :, 3] =\n 0  0\n 3  0\n\n\n\n\n\n","category":"function"},{"location":"layers/attention_layer/#GeometricMachineLearning.VolumePreservingAttention","page":"(Volume-Preserving) Attention","title":"GeometricMachineLearning.VolumePreservingAttention","text":"VolumePreservingAttention(dim, seq_length)\n\nMake an instance of VolumePreservingAttention for a specific dimension and sequence length.\n\nThe sequence length is 0 by default. \n\nSetting seq_length to 0 for all sequence lengths but does not apply the fast Cayley activation.\n\nArguments\n\nThe constructor can be called with an optional keyword argument: \n\nskew_sym::Bool = false: specifies if the weight matrix is skew symmetric (true) or arbitrary (false).\n\nFunctor\n\nApplying a layer of type VolumePreservingAttention does the following: \n\nFirst we perform the operation Z mapsto Z^T A Z = C, where ZinmathbbR^Ntimesmathttseq_length is a vector containing time series data and A is the skew symmetric matrix associated with the layer (if skew_sym = true). \nIn a second step we compute the Cayley transform of C; Lambda = mathrmCayley(C).\nThe output of the layer is then ZLambda.\n\nImplementation\n\nThe fast activation is only implemented for sequence lengths of 2, 3, 4 and 5.  Other sequence lengths only work on CPU (for now).\n\nThe fast Cayley activation is using inverses that have been computed symbolically.\n\n\n\n\n\n","category":"type"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"\\begin{comment}","category":"page"},{"location":"layers/attention_layer/#References","page":"(Volume-Preserving) Attention","title":"References","text":"","category":"section"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"D. Bahdanau, K. Cho and Y. Bengio. Neural machine translation by jointly learning to align and translate, arXiv preprint arXiv:1409.0473 (2014).\n\n\n\nM.-T. Luong, H. Pham and C. D. Manning. Effective approaches to attention-based neural machine translation, arXiv preprint arXiv:1508.04025 (2015).\n\n\n\n","category":"page"},{"location":"layers/attention_layer/","page":"(Volume-Preserving) Attention","title":"(Volume-Preserving) Attention","text":"\\end{comment}","category":"page"},{"location":"manifolds/homogeneous_spaces/#Homogeneous-Spaces","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"","category":"section"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"Homogeneous spaces are very important in GeometricMachineLearning as we can generalize existing neural network optimizers from vector spaces to such homogenous spaces. They are intricately linked to the notion of a Lie Group and its Lie Algebra[1].","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"[1]: Recall that a Lie group is a manifold that also has group structure. We say that a Lie group G acts on a manifold mathcalM if there is a map GtimesmathcalM to mathcalM such that (ab)x = a(bx) for abinG and xinmathcalM. For us the Lie algebra belonging to a Lie group, denoted by mathfrakg, is the tangent space to the identity element T_mathbbIG. ","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"Main.definition(raw\"A **homogeneous space** is a manifold ``\\mathcal{M}`` on which a Lie group ``G`` acts transitively, i.e.\n\" * Main.indentation * raw\" ```math\n\" * Main.indentation * raw\"\\forall X,Y\\in\\mathcal{M} \\quad \\exists{}A\\in{}G\\text{ s.t. }AX = Y.\n\" * Main.indentation * raw\"```\n\")","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"Now fix a distinct element EinmathcalM; we will refer to this as the canonical element or StiefelProjection. We can also establish an isomorphism between mathcalM and the quotient space Gsim with the equivalence relation: ","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"A_1 sim A_2 iff A_1E = A_2E","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"Note that this is independent of the chosen E.","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"The tangent spaces of mathcalM are of the form T_YmathcalM = mathfrakgcdotY, i.e. can be fully described through its Lie algebra.  Based on this we can perform a splitting of mathfrakg into two parts:","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"Main.definition(raw\"A **splitting of the Lie algebra** ``\\mathfrak{g}`` at an element of a homogeneous space ``Y`` is a decomposition into a **vertical** and a **horizontal** component, denoted by ``\\mathfrak{g} = \\mathfrak{g}^{\\mathrm{ver},Y} \\oplus \\mathfrak{g}^{\\mathrm{hor},Y}`` such that\n\" * Main.indentation * raw\"1. The **vertical component** ``\\mathfrak{g}^{\\mathrm{ver},Y}`` is the kernel of the map ``\\mathfrak{g}\\to{}T_Y\\mathcal{M}, V \\mapsto VY``, i.e. ``\\mathfrak{g}^{\\mathrm{ver},Y} = \\{V\\in\\mathfrak{g}:VY = 0\\}.``\n\" * Main.indentation * raw\"2. The **horizontal component** ``\\mathfrak{g}^{\\mathrm{hor},Y}`` is the orthogonal complement of ``\\mathfrak{g}^{\\mathrm{ver},Y}`` in ``\\mathfrak{g}``. It is isomorphic to ``T_Y\\mathcal{M}``.\n\" * Main.indentation * raw\"*Orthogonal complement* means that ``\\forall{}V\\in\\mathfrak{g}^{\\mathrm{ver}, Y}`` and ``\\forall{}B\\in\\mathfrak{g}^{\\mathrm{hor}, Y}`` we have ``\\langle V, B \\rangle = 0`` for some metric ``\\langle\\cdot,\\cdot\\rangle`` defined on ``\\mathfrak{g}``.\")","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"We will refer to the isomorphism from T_YmathcalM to mathfrakg^mathrmhor Y by Omega. We will give explicit examples of Omega below. The metric langlecdotcdotrangle on mathfrakg further induces a Riemannian metric on mathcalM:","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"g_Y(Delta_1 Delta_2) = langleOmega(YDelta_1)Omega(YDelta_2)rangletext for Delta_1Delta_2inT_YmathcalM","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"Two examples of homogeneous spaces implemented in GeometricMachineLearning are the Stiefel manifold and the Grassmann manifold. The Lie group SO(N) acts transitively on both of these manifolds, i.e. turns them into homogeneous spaces. We give its Lie algebra as an example here: ","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"Main.example(raw\"The Lie algebra of ``SO(N)`` are the skew-symmetric matrices ``\\mathfrak{so}(N):=\\{V\\in\\mathbb{R}^{N\\times{}N}:V^T + V = 0\\}`` and the canonical metric associated with it is simply ``(V_1,V_2)\\mapsto\\frac{1}{2}\\mathrm{Tr}(V_1^TV_2)``.\")","category":"page"},{"location":"manifolds/homogeneous_spaces/#The-Stiefel-Manifold","page":"Homogeneous Spaces","title":"The Stiefel Manifold","text":"","category":"section"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"The Stiefel manifold St(n N) is the space of all orthonormal frames in mathbbR^Ntimesn, i.e. matrices YinmathbbR^Ntimesn s.t. Y^TY = mathbbI_n. It can also be seen as SO(N) modulo an equivalence relation: AsimBiffAE = BE for ","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"E = beginbmatrix\nmathbbI_n  \nmathbbO\nendbmatrixinSt(n N)","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"which is the canonical element of the Stiefel manifold that we call StiefelProjection. In words: the first n columns of A and B are the same. We also use this principle to draw random elements from the Stiefel manifold.","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"Main.remark(raw\"Drawing random elements from the Stiefel (and the Grassmann) manifold is done by first calling `rand(N, n)` (i.e. drawing from a normal distribution) and then performing a ``QR`` decomposition. We then take the first ``n`` columns of the ``Q`` matrix to be an element of the Stiefel manifold.\")","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"The tangent space to the element YinSt(nN) can be determined by considering C^infty curves on SO(N) through mathbbI We write those curves as tmapstoA(t). Because SO(N) acts transitively on St(n N) each C^infty curve on St(n N) through Y can be written as A(t)Y and we get: ","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"T_YSt(nN)=BY  Binmathfrakg = DeltainmathbbR^Ntimesn Delta^TY + Y^TDelta = mathbbO","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"where the last equality[2] can be established through the isomorphism:","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"[2]: Note that we can easily check BY  Binmathfrakg subset DeltainmathbbR^Ntimesn Delta^TY + Y^TDelta = mathbbO The isomorphism is used to proof DeltainmathbbR^Ntimesn Delta^TY + Y^TDelta = mathbbO subset BY  Binmathfrakg as Omega(Delta)inmathfrakg^mathrmhorYsubsetmathfrakg and Delta = Omega(Delta)Y","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"Omega T_YSt(n N) to mathfrakg^mathrmhor Y Delta mapsto (mathbbI - frac12YY^T)DeltaY^T - YDelta^T(mathbbI - frac12YY^T)","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"That this is an isomorphism can be easily checked: ","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"    Omega(Delta)Y = (mathbbI - frac12YY^T)Delta - frac12YDelta^TY = Delta","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"This isomorphism is implemented in GeometricMachineLearning:","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"using GeometricMachineLearning # hide\nusing GeometricMachineLearning: Ω\nY = rand(StiefelManifold, 5, 3)\nΔ = rgrad(Y, rand(5, 3))\n@assert Ω(Y, Δ) * Y.A ≈ Δ # hide\nΩ(Y, Δ) * Y.A ≈ Δ","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"The function rgrad, which maps mathbbR^Ntimesn to T_YSt(n N) is introduced below. We can now also introduce the Riemannian metric on St(nN):","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"g_Y(Delta_1 Delta_2)  = mathrmTrleft( frac12 Omega(Delta_1)^T Omega(Delta_2) right) = mathrmTr(Delta_1^T(mathbbI - frac12YY^T)Delta_2)","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"We can check that this is true:","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"using LinearAlgebra: tr\nΔ₂ = rgrad(Y, rand(5, 3))\n@assert .5 * tr(Ω(Y, Δ)' * Ω(Y, Δ₂)) ≈ metric(Y, Δ, Δ₂) # hide\n.5 * tr(Ω(Y, Δ)' * Ω(Y, Δ₂)) ≈ metric(Y, Δ, Δ₂)","category":"page"},{"location":"manifolds/homogeneous_spaces/#The-Riemannian-Gradient-for-the-Stiefel-Manifold","page":"Homogeneous Spaces","title":"The Riemannian Gradient for the Stiefel Manifold","text":"","category":"section"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"We defined the Riemannian gradient to be a vector field mathrmgrad^gL such that it is compatible with the Riemannian metric in some sense; the definition we gave relied on an explicit coordinate chart. We can also express the Riemannian gradient for matrix manifolds by not relying on an explicit coordinate representation (which would be computationally expensive) [20].","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"Main.definition(raw\"Given a Riemannian matrix manifold ``\\mathcal{M}`` we define the **Riemannian gradient** of ``L:\\mathcal{M}\\to\\mathbb{R}`` at ``Y``, called ``\\mathrm{grad}_YL\\in{}T_Y\\mathcal{M}``, as the unique element of ``T_Y\\mathcal{M}`` such that for any other ``\\Delta\\in{}T_Y\\mathcal{M}`` we have\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\mathrm{Tr}((\\nabla{}L)^T\\Delta) = g_Y(\\mathrm{grad}_YL, \\Delta),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where Tr indicates the usual matrix trace.\")","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"For the Stiefel manifold the Riemannian gradient is given by: ","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"    mathrmgrad_YL = nabla_YL - Y(nabla_YL)^TY = mathttrgrad(Y nabla_YL)","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"where nabla_YL refers to the Euclidean gradient, i.e. ","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"    nabla_YL_ij = fracpartialLpartialy_ij","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"The Euclidean gradient nablaL can in practice be obtained with an AD routine. We then use the function rgrad to map nabla_YL from mathbbR^Ntimesn to T_YSt(nN). We can check that this mapping indeed produces the Riemannian gradient[3]:","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"[3]: Here we are testing with a randomly drawn element DeltainT_YmathcalM","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"using GeometricMachineLearning # hide\nusing LinearAlgebra: tr\n\nY = rand(StiefelManifold, 5, 3)\n∇L = rand(5, 3)\ngradL = rgrad(Y, ∇L)\nΔ = rgrad(Y, rand(5, 3))\n\n@assert metric(Y, gradL, Δ) ≈ tr(∇L' * Δ) # hide\nmetric(Y, gradL, Δ) ≈ tr(∇L' * Δ)","category":"page"},{"location":"manifolds/homogeneous_spaces/#The-Grassmann-Manifold","page":"Homogeneous Spaces","title":"The Grassmann Manifold","text":"","category":"section"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"The Grassmann manifold is closely related to the Stiefel manifold, and an element of the Grassmann manifold can be represented through an element of the Stiefel manifold (but not vice-versa). An element of the Grassmann manifold Gr(nN) is a vector subspace subsetmathbbR^N of dimension n. Each such subspace (i.e. element of the Grassmann manifold) can be represented by a full-rank matrix AinmathbbR^Ntimesn and we identify two elements with the following equivalence relation: ","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"    A_1 sim A_2 iff existsCinmathbbR^ntimesntext st A_1C = A_2","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"The resulting manifold is of dimension n(N-n). One can find a parametrization of the manifold the following way: Because the matrix Y has full rank, there have to be n independent columns in it: i_1 ldots i_n. For simplicity assume that i_1 = 1 i_2=2 ldots i_n=n and call the matrix made up of these columns C. Then the mapping to the coordinate chart is: YC^-1 and the last N-n columns are the coordinates.","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"We can also define the Grassmann manifold based on the Stiefel manifold since elements of the Stiefel manifold are already full-rank matrices. In this case we have the following equivalence relation (for Y_1 Y_2inSt(nN)): ","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"    Y_1 sim Y_2 iff existsCinSO(n)text st Y_1C = Y_2","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"In GeometricMachineLearning elements of the Grassmann manifold are drawn the same way as elements of the Stiefel manifold:","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"using GeometricMachineLearning # hide\nrand(GrassmannManifold{Float32}, 5, 3)","category":"page"},{"location":"manifolds/homogeneous_spaces/#The-Riemannian-Gradient-of-the-Grassmann-Manifold","page":"Homogeneous Spaces","title":"The Riemannian Gradient of the Grassmann Manifold","text":"","category":"section"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"Obtaining the Riemannian Gradient for the Grassmann manifold is slightly more difficult than it is in the case of the Stiefel manifold [20]. Since the Grassmann manifold can be obtained from the Stiefel manifold through an equivalence relation, we can however use this as a starting point. ","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"Main.theorem(raw\"The Riemannian gradient of a function ``L`` defined on the Grassmann manifold can be written as\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\mathrm{grad}_\\mathcal{Y}^{Gr}L \\simeq \\nabla_Y{}L - YY^T\\nabla_YL,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``\\nabla_Y{}L`` is again the Euclidean gradient.\")","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"Main.proof(raw\"In a first step we identify charts on the Grassmann manifold to make dealing with it easier. For this consider the following open cover of the Grassmann manifold. \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\{\\mathcal{U}_W\\}_{W\\in{}St(n, N)} \\quad\\text{where}\\quad \\mathcal{U}_W = \\{\\mathrm{span}(Y):\\mathrm{det}(W^TY)\\neq0\\}.\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"We can find a canonical bijective mapping from the set ``\\mathcal{U}_W`` to the set ``\\mathcal{S}_W := \\{Y\\in\\mathbb{R}^{N\\times{}n}:W^TY=\\mathbb{I}_n\\}``:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\sigma_W: \\mathcal{U}_W \\to \\mathcal{S}_W,\\, \\mathcal{Y}=\\mathrm{span}(Y)\\mapsto{}Y(W^TY)^{-1} =: \\hat{Y}.\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"That ``\\sigma_W`` is well-defined is easy to see: Consider ``YC`` with ``C\\in\\mathbb{R}^{n\\times{}n}`` non-singular. Then ``YC(W^TYC)^{-1}=Y(W^TY)^{-1} = \\hat{Y}``. With this isomorphism we can also find a representation of elements of the tangent space:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"T_\\mathcal{Y}\\sigma_W: T_\\mathcal{Y}Gr(n,N)\\to{}T_{\\hat{Y}}\\mathcal{S}_W.\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"We give an explicit representation of this isomorphism; because the map ``\\sigma_W`` does not care about the representation of ``\\mathrm{span}(Y)`` we can perform the variations in ``St(n,N)``. We write the variations as ``Y(t)\\in{}St(n,N)`` for ``t\\in(-\\varepsilon,\\varepsilon)``. We also set ``Y(0) = Y`` and hence\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\frac{d}{dt}Y(t)(W^TY(t))^{-1} = (\\dot{Y}(0) - Y(W^TY)^{-1}W^T\\dot{Y}(0))(W^TY)^{-1},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``\\dot{Y}(0)\\in{}T_YSt(n,N)``. Also note  note that we have ``T_\\mathcal{Y}\\mathcal{U}_W = T_\\mathcal{Y}Gr(n,N)`` because ``\\mathcal{U}_W`` is an open subset of ``Gr(n,N)``. We thus can identify the tangent space ``T_\\mathcal{Y}Gr(n,N)`` with the following set:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"T_{\\hat{Y}}\\mathcal{S}_W = \\{(\\Delta - YW^T\\Delta)(W^TY)^{-1}: Y\\in{}St(n,N)\\text{ s.t. }\\mathrm{span}(Y)=\\mathcal{Y}\\text{ and }\\Delta\\in{}T_YSt(n,N)\\}.\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"Further note that we can pick any element ``W`` to construct the charts for a neighborhood around the point ``\\mathcal{Y}\\in{}Gr(n,N)`` as long as we have ``\\mathrm{det}(W^TY)\\neq0`` for ``\\mathrm{span}(Y)=\\mathcal{Y}``. We  hence take ``W=Y`` and get the identification: \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"T_\\mathcal{Y}Gr(n,N) \\equiv \\{\\Delta - YY^T\\Delta: Y\\in{}St(n,N)\\text{ s.t. }\\mathrm{span}(Y)=\\mathcal{Y}\\text{ and }\\Delta\\in{}T_YSt(n,N)\\},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"which is very easy to handle computationally (we simply store and change the matrix ``Y`` that represents an element of the Grassmann manifold). In this representation the Riemannian gradient is then \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\mathrm{grad}_\\mathcal{Y}^{Gr}L = \\mathrm{grad}_Y^{St}L - YY^T\\mathrm{grad}_Y^{St}L = \\nabla_Y{}L - YY^T\\nabla_YL,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``\\mathrm{grad}^{St}_YL`` is the Riemannian gradient of the Stiefel manifold at ``Y``. We proved our assertion.\")","category":"page"},{"location":"manifolds/homogeneous_spaces/#Library-Functions","page":"Homogeneous Spaces","title":"Library Functions","text":"","category":"section"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"StiefelManifold\nStiefelProjection\nGrassmannManifold\nGeometricMachineLearning.metric(::StiefelManifold, ::AbstractMatrix, ::AbstractMatrix)\nGeometricMachineLearning.rgrad(::StiefelManifold, ::AbstractMatrix)\nGeometricMachineLearning.metric(::GrassmannManifold, ::AbstractMatrix, ::AbstractMatrix)\nGeometricMachineLearning.rgrad(::GrassmannManifold, ::AbstractMatrix)\nGeometricMachineLearning.Ω(::StiefelManifold{T}, ::AbstractMatrix{T}) where T\nGeometricMachineLearning.Ω(::GrassmannManifold{T}, ::AbstractMatrix{T}) where T","category":"page"},{"location":"manifolds/homogeneous_spaces/#GeometricMachineLearning.StiefelManifold","page":"Homogeneous Spaces","title":"GeometricMachineLearning.StiefelManifold","text":"StiefelManifold <: Manifold\n\nAn implementation of the Stiefel manifold [1]. The Stiefel manifold is the collection of all matrices YinmathbbR^Ntimesn whose columns are orthonormal, i.e. \n\n    St(n N) = Y Y^TY = mathbbI_n \n\nThe Stiefel manifold can be shown to have manifold structure (as the name suggests) and this is heavily used in GeometricMachineLearning. It is further a compact space.  More information can be found in the docstrings for rgrad(::StiefelManifold, ::AbstractMatrix) and metric(::StiefelManifold, ::AbstractMatrix, ::AbstractMatrix).\n\n\n\n\n\n","category":"type"},{"location":"manifolds/homogeneous_spaces/#GeometricMachineLearning.StiefelProjection","page":"Homogeneous Spaces","title":"GeometricMachineLearning.StiefelProjection","text":"StiefelProjection(backend, T, N, n)\n\nMake a matrix of the form beginbmatrix mathbbI  mathbbO endbmatrix^T for a specific backend and data type.\n\nAn array that essentially does vcat(I(n), zeros(N-n, n)) with GPU support. \n\nExtended help\n\nAn instance of StiefelProjection should technically also belong to StiefelManifold. \n\n\n\n\n\n","category":"type"},{"location":"manifolds/homogeneous_spaces/#GeometricMachineLearning.GrassmannManifold","page":"Homogeneous Spaces","title":"GeometricMachineLearning.GrassmannManifold","text":"GrassmannManifold <: Manifold\n\nThe GrassmannManifold is based on the StiefelManifold.\n\n\n\n\n\n","category":"type"},{"location":"manifolds/homogeneous_spaces/#GeometricMachineLearning.metric-Tuple{StiefelManifold, AbstractMatrix, AbstractMatrix}","page":"Homogeneous Spaces","title":"GeometricMachineLearning.metric","text":"metric(Y::StiefelManifold, Δ₁::AbstractMatrix, Δ₂::AbstractMatrix)\n\nCompute the dot product for Δ₁ and Δ₂ at Y.\n\nThis uses the canonical Riemannian metric for the Stiefel manifold:\n\ng_Y (Delta_1 Delta_2) mapsto mathrmTr(Delta_1^T(mathbbI - frac12YY^T)Delta_2)\n\n\n\n\n\n","category":"method"},{"location":"manifolds/homogeneous_spaces/#GeometricMachineLearning.rgrad-Tuple{StiefelManifold, AbstractMatrix}","page":"Homogeneous Spaces","title":"GeometricMachineLearning.rgrad","text":"rgrad(Y::StiefelManifold, ∇L::AbstractMatrix)\n\nCompute the Riemannian gradient for the Stiefel manifold at Y based on ∇L.\n\nHere YinSt(Nn) and nablaLinmathbbR^Ntimesn is the Euclidean gradient. \n\nThe function computes the Riemannian gradient with respect to the canonical metric: metric(::StiefelManifold, ::AbstractMatrix, ::AbstractMatrix).\n\nThe precise form of the mapping is: \n\nmathttrgrad(Y nablaL) mapsto nablaL - Y(nablaL)^TY\n\nNote the property Y^Tmathttrgrad(Y nablaL)inmathcalS_mathrmskew(n)\n\nExamples\n\nusing GeometricMachineLearning\n\nY = StiefelManifold([1 0 ; 0 1 ; 0 0; 0 0])\nΔ = [1 2; 3 4; 5 6; 7 8]\nrgrad(Y, Δ)\n\n# output\n\n4×2 Matrix{Int64}:\n 0  -1\n 1   0\n 5   6\n 7   8\n\n\n\n\n\n","category":"method"},{"location":"manifolds/homogeneous_spaces/#GeometricMachineLearning.metric-Tuple{GrassmannManifold, AbstractMatrix, AbstractMatrix}","page":"Homogeneous Spaces","title":"GeometricMachineLearning.metric","text":"metric(Y::GrassmannManifold, Δ₁::AbstractMatrix, Δ₂::AbstractMatrix)\n\nCompute the metric for vectors Δ₁ and Δ₂ at Y. \n\nThe representation of the Grassmann manifold is realized as a quotient space of the Stiefel manifold. \n\nThe metric for the Grassmann manifold is:\n\ng^Gr_Y(Delta_1 Delta_2) = g^St_Y(Delta_1 Delta_2) = mathrmTr(Delta_1^T (mathbbI - Y Y^T) Delta_2) = mathrmTr(Delta_1^T Delta_2)\n\nwhere we used that Y^TDelta_i for i = 1 2\n\n\n\n\n\n","category":"method"},{"location":"manifolds/homogeneous_spaces/#GeometricMachineLearning.rgrad-Tuple{GrassmannManifold, AbstractMatrix}","page":"Homogeneous Spaces","title":"GeometricMachineLearning.rgrad","text":"rgrad(Y::GrassmannManifold, ∇L::AbstractMatrix)\n\nCompute the Riemannian gradient for the Grassmann manifold at Y based on ∇L.\n\nHere Y is a representation of mathrmspan(Y)inGr(n N) and nablaLinmathbbR^Ntimesn is the Euclidean gradient. \n\nThis gradient has the property that it is orthogonal to the space spanned by Y.\n\nThe precise form of the mapping is: \n\nmathttrgrad(Y nablaL) mapsto nablaL - YY^TnablaL\n\nNote the property Y^Tmathrmrgrad(Y nablaL) = mathbbO\n\nAlso see rgrad(::StiefelManifold, ::AbstractMatrix).\n\nExamples\n\nusing GeometricMachineLearning\n\nY = GrassmannManifold([1 0 ; 0 1 ; 0 0; 0 0])\nΔ = [1 2; 3 4; 5 6; 7 8]\nrgrad(Y, Δ)\n\n# output\n\n4×2 Matrix{Int64}:\n 0  0\n 0  0\n 5  6\n 7  8\n\n\n\n\n\n","category":"method"},{"location":"manifolds/homogeneous_spaces/#GeometricMachineLearning.Ω-Union{Tuple{T}, Tuple{StiefelManifold{T, AT} where AT<:AbstractMatrix{T}, AbstractMatrix{T}}} where T","page":"Homogeneous Spaces","title":"GeometricMachineLearning.Ω","text":"Ω(Y::StiefelManifold{T}, Δ::AbstractMatrix{T}) where T\n\nPerform canonical horizontal lift for the Stiefel manifold:\n\n    Delta mapsto (mathbbI - frac12YY^T)DeltaY^T - YDelta^T(mathbbI - frac12YY^T)\n\nInternally this performs \n\nSkewSymMatrix(2 * (I(n) - .5 * Y * Y') * Δ * Y')\n\nIt uses SkewSymMatrix to save memory. \n\nExamples\n\nusing GeometricMachineLearning\nE = StiefelManifold(StiefelProjection(5, 2))\nΔ = [0. -1.; 1. 0.; 2. 3.; 4. 5.; 6. 7.]\nGeometricMachineLearning.Ω(E, Δ)\n\n# output\n\n5×5 SkewSymMatrix{Float64, Vector{Float64}}:\n 0.0  -1.0  -2.0  -4.0  -6.0\n 1.0   0.0  -3.0  -5.0  -7.0\n 2.0   3.0   0.0  -0.0  -0.0\n 4.0   5.0   0.0   0.0  -0.0\n 6.0   7.0   0.0   0.0   0.0\n\nNote that the output of Ω is a skew-symmetric matrix, i.e. an element of mathfrakg.\n\n\n\n\n\n","category":"method"},{"location":"manifolds/homogeneous_spaces/#GeometricMachineLearning.Ω-Union{Tuple{T}, Tuple{GrassmannManifold{T, AT} where AT<:AbstractMatrix{T}, AbstractMatrix{T}}} where T","page":"Homogeneous Spaces","title":"GeometricMachineLearning.Ω","text":"Ω(Y::GrassmannManifold{T}, Δ::AbstractMatrix{T}) where T\n\nPerform the canonical horizontal lift for the Grassmann manifold:\n\n    Delta mapsto Omega^St(Delta)\n\nwhere Omega^St is the canonical horizontal lift for the Stiefel manifold.\n\nusing GeometricMachineLearning\nE = GrassmannManifold(StiefelProjection(5, 2))\nΔ = [0. 0.; 0. 0.; 2. 3.; 4. 5.; 6. 7.]\nGeometricMachineLearning.Ω(E, Δ)\n\n# output\n\n5×5 SkewSymMatrix{Float64, Vector{Float64}}:\n 0.0  -0.0  -2.0  -4.0  -6.0\n 0.0   0.0  -3.0  -5.0  -7.0\n 2.0   3.0   0.0  -0.0  -0.0\n 4.0   5.0   0.0   0.0  -0.0\n 6.0   7.0   0.0   0.0   0.0\n\n\n\n\n\n","category":"method"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"\\begin{comment}","category":"page"},{"location":"manifolds/homogeneous_spaces/#References","page":"Homogeneous Spaces","title":"References","text":"","category":"section"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"P.-A. Absil, R. Mahony and R. Sepulchre. Riemannian geometry of Grassmann manifolds with a view on algorithmic computation. Acta Applicandae Mathematica 80, 199–220 (2004).\n\n\n\nT. Frankel. The geometry of physics: an introduction (Cambridge university press, Cambridge, UK, 2011).\n\n\n\nT. Bendokat and R. Zimmermann. The real symplectic Stiefel and Grassmann manifolds: metrics, geodesics and applications, arXiv preprint arXiv:2108.12447 (2021).\n\n\n\n","category":"page"},{"location":"manifolds/homogeneous_spaces/","page":"Homogeneous Spaces","title":"Homogeneous Spaces","text":"\\end{comment}","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/#Parallel-Transport","page":"Parallel Transport","title":"Parallel Transport","text":"","category":"section"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"The concept of parallel transport along a geodesic gamma0 TtomathcalM describes moving a tangent vector from T_xmathcalM to T_gamma(t)mathcalM such that its orientation with respect to the geodesic is preserved.","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"A precise definition of parallel transport needs a notion of a connection [15, 16, 23] and we forego it here. We simply state how to parallel transport vectors on the Lie group SO(N) and the homogeneous spaces St(n N) and Gr(n N).","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"Main.theorem(raw\"Given two elements ``B^A_1, B^A_2\\in{}T_AG`` the parallel transport of ``B^A_2`` along the geodesic of ``B^A_1`` is given by\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\Pi_{A\\to\\gamma_{B^A_1}(t)}B^A_2 = A\\exp(t\\cdot{}A^{-1}B^A_1)A^{-1}B^A_2 = A\\exp(t\\cdot{}B_1)B_2,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``B_i := A^{-1}B^A_i.``\")","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"For the Stiefel manifold this is not much more complicated[1]:","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"[1]: Here we do not provide a detailed proof that this constitutes a sound expression from the perspective of Riemannian geometry. A proof can be found in [42].","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"Main.theorem(raw\"Given two elements ``\\Delta_1, \\Delta_2\\in{}T_Y\\mathcal{M}``, the parallel transport of ``\\Delta_2`` along the geodesic of ``\\Delta_1`` is given by\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\Pi_{Y\\to\\gamma_{\\Delta_1}(t)}\\Delta_2 = \\exp(t\\cdot\\Omega(Y, \\Delta_1))\\Delta_2 =  \\lambda(Y)\\exp(\\bar{B}_1)\\lambda(Y)^{-1}\\Delta_2,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``\\bar{B}_1 = \\lambda(Y)^{-1}\\Omega(Y, \\Delta_1)\\lambda(Y).``\")","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"We can further modify the expression of parallel transport for the Stiefel manifold: ","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"Pi_Ytogamma_Delta_1(t)Delta_2 = lambda(Y)exp(B_1)lambda(Y)Omega(Y Delta_2)Y = lambda(Y)exp(B_1)B_2E","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"where B_2 = lambda(Y)^-1Omega(Y Delta_2)lambda(Y) We can now define explicit updating rules for the global section Lambda^(cdot), the element of the homogeneous space Y^(cdot), the tangent vector Delta^(cdot) and D^(cdot) = (Lambda^(cdot))^-1Omega(Delta^(cdot))Lambda^(cdot), its representation in mathfrakg^mathrmhor.","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"We thus have:","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"Lambda^(t) leftarrow Lambda^(t-1)exp(B^(t-1))\nY^(t) leftarrow Lambda^(t)E\nDelta^(t) leftarrow  Lambda^(t-1)exp(B^(t-1))(Lambda^(t-1))^-1Delta^(t-1) = Lambda^(t)D^(t-1)E\nD^(t) leftarrow D^(t-1)","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"So we conveniently take parallel transport of vectors into account by representing them in mathfrakg^mathrmhor: D does not change.","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"To demonstrate parallel transport we again use the example from when we introduced the concept of geodesics. We first set up the problem:","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"using GLMakie\n\ninclude(\"../../../gl_makie_transparent_background_hack.jl\")","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"using GeometricMachineLearning\nimport Random # hide\nRandom.seed!(123) # hide\n\nY = rand(StiefelManifold, 3, 1)\n# needed because we will change `Y` later on\nY_copy = StiefelManifold(copy(Y.A))\n\nv = 2 * rand(3, 1)\nv₂ = 1 * rand(3, 1)\nΔ = rgrad(Y, v)\nΔ₂ = rgrad(Y, v₂)\n\nmorange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256) # hide\n\nfunction set_up_plot(; theme = :dark) # hide\nfig = Figure(; backgroundcolor = :transparent, size = (900, 675)) # hide\ntext_color = theme == :dark ? :white : :black # hide\nax = Axis3(fig[1, 1]; # hide\n    backgroundcolor = (:tomato, .5), # hide\n    aspect = (1., 1., 1.), # hide\n    xlabel = L\"x_1\", # hide\n    ylabel = L\"x_2\", # hide\n    zlabel = L\"x_3\", # hide\n    xgridcolor = text_color, # hide\n    ygridcolor = text_color, # hide\n    zgridcolor = text_color, # hide\n    xtickcolor = text_color, # hide\n    ytickcolor = text_color, # hide\n    ztickcolor = text_color, # hide\n    xlabelcolor = text_color, # hide\n    ylabelcolor = text_color, # hide\n    zlabelcolor = text_color, # hide\n    xypanelcolor = :transparent, # hide\n    xzpanelcolor = :transparent, # hide\n    yzpanelcolor = :transparent, # hide\n    limits = ([-1, 1], [-1, 1], [-1, 1]),\n    azimuth = π / 7, # hide\n    elevation = π / 7, # hide\n    # height = 75.,\n    ) # hide\n\n# plot a sphere with radius one and origin 0\nsurface!(ax, Main.sphere(1., [0., 0., 0.])...; alpha = .5, transparency = true)\n\npoint_vec = ([Y_copy[1]], [Y_copy[2]], [Y_copy[3]])\nscatter!(ax, point_vec...; color = morange, marker = :star5, markersize = 30)\n\narrow_vec = ([Δ[1]], [Δ[2]], [Δ[3]])\narrows!(ax, point_vec..., arrow_vec...; color = mred, linewidth = .02)\n\narrow_vec2 = ([Δ₂[1]], [Δ₂[2]], [Δ₂[3]])\narrows!(ax, point_vec..., arrow_vec2...; color = mpurple, linewidth = .02)\n\nfig, ax # hide\nend # hide\n\nfig_light = set_up_plot(; theme = :light)[1]\nfig_dark = set_up_plot(; theme = :dark)[1]\nsave(\"two_vectors_light.png\", alpha_colorbuffer(fig_light)) # hide\nsave(\"two_vectors_dark.png\", alpha_colorbuffer(fig_dark)) # hide\n\nnothing # hide","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"(Image: ) (Image: )","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"Note that we have chosen the arrow here to have the same direction as before but only about half the magnitude. We further drew another arrow that we want to parallel transport (the purple arrow). ","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"using GeometricMachineLearning: update_section! # hide\n\nλY = GlobalSection(Y)\nB = global_rep(λY, Δ)\nB₂ = global_rep(λY, Δ₂)\n\nE = StiefelProjection(3, 1)\nY_increments = []\nΔ_transported = []\nΔ₂_transported = []\n\nconst n_steps = 6\nconst tstep = 2\n\nfor _ in 1:n_steps\n    update_section!(λY, tstep * B, geodesic)\n    push!(Y_increments, copy(λY.Y))\n    push!(Δ_transported, Matrix(λY) * B * E)\n    push!(Δ₂_transported, Matrix(λY) * B₂ * E)\nend\nnothing # hide","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"function plot_parallel_transport(; theme = :dark) # hide\nfig, ax = set_up_plot(; theme = theme) # hide\nfor Y_increment in Y_increments \n    scatter!(ax, [Y_increment[1]], [Y_increment[2]], [Y_increment[3]]; \n        color = mred)\nend\n\nfor (color, vec_transported) in zip((mred, mpurple), (Δ_transported, Δ₂_transported))\n    for (Y_increment, vec_increment) in zip(Y_increments, vec_transported)\n        point_vec = ([Y_increment[1]], [Y_increment[2]], [Y_increment[3]])\n        arrow_vec = ([vec_increment[1]], [vec_increment[2]], [vec_increment[3]])\n        arrows!(ax, point_vec..., arrow_vec...; color = color, linewidth = .02) \n    end\nend\n\nfig, ax\nend # hide\n\nfig_light, ax_light = plot_parallel_transport(; theme = :light) # hide\nfig_dark, ax_dark = plot_parallel_transport(; theme = :dark) # hide\nsave(\"parallel_transport_light.png\", fig_light |> alpha_colorbuffer) # hide\nsave(\"parallel_transport_dark.png\", fig_dark |> alpha_colorbuffer) # hide\nhidedecorations!(ax_light)  # hide\nhidespines!(ax_light) # hide\nsave(\"parallel_transport_naked.png\", fig_light |> alpha_colorbuffer) # hide\n\nnothing # hide","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"(Image: ) (Image: )","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"Note that the angle between the two vector is preserved as we go along the geodesic.","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"\\section*{Chapter Summary}\n\nIn this chapter we introduced our \\textit{optimizer framework} which will be used to efficiently train symplectic autoencoders and transformers with orthogonality constraints in Part IV. We proposed extending standard neural network optimizers to homogeneous spaces by introducing the extra operations \\texttt{rgrad}, \\texttt{global\\_rep} and ``Retraction.'' The definition of a retraction we used here was slightly different from the usual one. We defined retractions as maps from the \\textit{global tangent space representation} $\\mathfrak{g}^\\mathrm{hor}$ to the associated Lie group (and in addition satisfy two more conditions), i.e.\n\\begin{equation*}\n    \\mathrm{Retraction}: \\mathfrak{g}^\\mathrm{hor} \\to G.\n\\end{equation*}\n\nWe further discussed what the operations \\texttt{rgrad}, \\texttt{global\\_rep} and ``Retraction'' look like in practice and concluded by introducing the concept of \\textit{parallel transport}. The presentation was accompanied by code snippets that demonstrate the application interface of \\texttt{GeometricMachineLearning} throughout the chapter.\n\n\\begin{comment}","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/#References","page":"Parallel Transport","title":"References","text":"","category":"section"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"S. Lang. Fundamentals of differential geometry. Vol. 191 (Springer Science & Business Media, 2012).\n\n\n\nS. I. Richard L. Bishop. Tensor Analysis on Manifolds (Dover Publications, Mineola, New York, 1980).\n\n\n\nT. Bendokat, R. Zimmermann and P.-A. Absil. A Grassmann manifold handbook: Basic geometry and computational aspects, arXiv preprint arXiv:2011.13699 (2020).\n\n\n\nM. Schlarb. Covariant Derivatives on Homogeneous Spaces: Horizontal Lifts and Parallel Transport. The Journal of Geometric Analysis 34, 1–43 (2024).\n\n\n\n","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"\\end{comment}","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"<!--","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/#References-2","page":"Parallel Transport","title":"References","text":"","category":"section"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"B. Brantner. Generalizing Adam To Manifolds For Efficiently Training Transformers, arXiv preprint arXiv:2305.16901 (2023).\n\n\n\nM. Schlarb. Covariant Derivatives on Homogeneous Spaces: Horizontal Lifts and Parallel Transport. The Journal of Geometric Analysis 34, 1–43 (2024).\n\n\n\nP.-A. Absil, R. Mahony and R. Sepulchre. Optimization algorithms on matrix manifolds (Princeton University Press, Princeton, New Jersey, 2008).\n\n\n\nB. Gao, N. T. Son, P.-A. Absil and T. Stykel. Riemannian optimization on the symplectic Stiefel manifold. SIAM Journal on Optimization 31, 1546–1575 (2021).\n\n\n\nB. Gao, N. T. Son and T. Stykel. Optimization on the symplectic Stiefel manifold: SR decomposition-based retraction and applications. Linear Algebra and its Applications 682, 50–85 (2024).\n\n\n\nT. Bendokat, R. Zimmermann and P.-A. Absil. A Grassmann manifold handbook: Basic geometry and computational aspects, arXiv preprint arXiv:2011.13699 (2020).\n\n\n\nT. Bendokat and R. Zimmermann. The real symplectic Stiefel and Grassmann manifolds: metrics, geodesics and applications, arXiv preprint arXiv:2108.12447 (2021).\n\n\n\n","category":"page"},{"location":"optimizers/manifold_related/parallel_transport/","page":"Parallel Transport","title":"Parallel Transport","text":"-->","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"In this chapter we discuss examples of improving transformers by imbuing them with structure\\footnotemark[1]. We show two examples: an example of using the vision transformer where we put orthogonality constraints on some of the weights (which effectively leads to manifold optimization) and using the volume-preserving transformer to learn the dynamics of a rigid body. At the end we further compare two different approaches to realizing the volume-preserving transformer.\n\n\\footnotetext[1]{Technically the linear symplectic transformer from the previous chapter could also be included here, but because of the very severe modification/limitation of the attention layer we performed there, we decided against it.}","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/#MNIST-Tutorial","page":"MNIST","title":"MNIST Tutorial","text":"","category":"section"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"In this tutorial we show how we can use GeometricMachineLearning to build a vision transformer and apply it for MNIST [90], while also putting some of the weights on a manifold. This is also the result presented in [7].","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"We get the dataset from MLDatasets. Before we use it we allocate it on gpu with cu from CUDA.jl [11]:","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"using GeometricMachineLearning\nimport MLDatasets\ntrain_x, train_y = MLDatasets.MNIST(split=:train)[:]\ntest_x, test_y = MLDatasets.MNIST(split=:test)[:]\n\nnothing # hide","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"using MLDatasets\nusing CUDA\ntrain_x, train_y = MLDatasets.MNIST(split=:train)[:]\ntest_x, test_y = MLDatasets.MNIST(split=:test)[:]\n\ntrain_x = train_x |> cu\ntrain_y = train_y |> cu\ntest_x = test_x |> cu\ntest_y = test_y |> cu","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"Next we call DataLoader on these data. For this we first need to specify a patch length[1].","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"[1]: When DataLoader is called this way it uses split_and_flatten internally.","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"Main.remark(raw\"In order to apply the transformer to a data set we should typically cast these data into a *time series format*. MNIST images are pictures with ``28\\times28`` pixels. Here we cast these images into *time series* of length 16, so one image is represented by a matrix ``\\in\\mathbb{R}^{49\\times{}16}``.\")","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"const patch_length = 7\ndl = DataLoader(train_x, train_y, patch_length = patch_length; suppress_info = true)\n\nnothing # hide","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"Here we called DataLoader on a tensor and a vector of integers (targets) as input. DataLoader automatically converts the data to the correct input format for easy handling. This is visualized below:","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"(Image: Visualization of how the data are preprocessed. An image is first split and then flattened.) (Image: Visualization of how the data are preprocessed. An image is first split and then flattened.)","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"Internally DataLoader calls split_and_flatten which splits each image into a number of patches according to the keyword arguments patch_length and number_of_patches. We also load the test data with DataLoader:","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"dl_test = DataLoader(test_x, test_y, patch_length=patch_length)\n\nnothing # hide","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"We next define the model with which we want to train:","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"const n_heads = 7\nconst L = 16\nconst add_connection = false\n\nmodel1 = ClassificationTransformer(dl; \n                                    n_heads = n_heads, \n                                    L = L, \n                                    add_connection = add_connection, \n                                    Stiefel = false)\nmodel2 = ClassificationTransformer(dl; \n                                    n_heads = n_heads, \n                                    L = L, \n                                    add_connection = add_connection, \n                                    Stiefel = true)\n\nnothing # hide","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"Here we have chosen a ClassificationTransformer, i.e. a composition of a specific number of transformer layers composed with a classification layer. We also set the Stiefel option to true, i.e. we are optimizing on the Stiefel manifold.","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"We now have to initialize the neural network weights. This is done with the constructor for NeuralNetwork:","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"backend = GeometricMachineLearning.networkbackend(dl)\nT = eltype(dl)\nnn1 = NeuralNetwork(model1, backend, T)\nnn2 = NeuralNetwork(model2, backend, T)\n\nnothing # hide","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"We still have to initialize the optimizers:","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"const batch_size = 2048\nconst n_epochs = 500\n# an instance of batch is needed for the optimizer\nbatch = Batch(batch_size, dl)\n\nopt1 = Optimizer(AdamOptimizer(T), nn1)\nopt2 = Optimizer(AdamOptimizer(T), nn2)\n\nnothing # hide","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"And with this we can finally perform the training:","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"loss_array1 = opt1(nn1, dl, batch, n_epochs, FeedForwardLoss())\nloss_array2 = opt2(nn2, dl, batch, n_epochs, FeedForwardLoss())","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"We furthermore optimize the second neural network (with weights on the manifold) with the GradientOptimizer and the MomentumOptimizer:","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"nn3 = NeuralNetwork(model2, backend, T)\nnn4 = NeuralNetwork(model2, backend, T)\n\nopt3 = Optimizer(GradientOptimizer(T(0.001)), nn3)\nopt4 = Optimizer(MomentumOptimizer(T(0.001), T(0.5)), nn4)\n\nnothing # hide","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"For training we use the same data, the same batch and the same number of epochs:","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"loss_array3 = opt3(nn3, dl, batch, n_epochs, FeedForwardLoss())\nloss_array4 = opt4(nn4, dl, batch, n_epochs, FeedForwardLoss())","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"And we get the following result:","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"using JLD2\nusing CairoMakie\n\ndata = load(\"mnist_parameters.jld2\")\nloss_array1 = data[\"loss_array1\"]\nloss_array2 = data[\"loss_array2\"]\nloss_array3 = data[\"loss_array3\"]\nloss_array4 = data[\"loss_array4\"]\n\naccuracy_score1 = data[\"accuracy_score1\"]\naccuracy_score2 = data[\"accuracy_score2\"]\naccuracy_score3 = data[\"accuracy_score3\"]\naccuracy_score4 = data[\"accuracy_score4\"]\n\n_nnp(ps::Tuple) = NeuralNetworkParameters{Tuple(Symbol(\"L$(i)\") for i in 1:length(ps))}(ps)\nnn1 = NeuralNetwork(nn1.architecture, nn1.model, _nnp(data[\"nn1weights\"]), CPU())\nnn2 = NeuralNetwork(nn2.architecture, nn2.model, _nnp(data[\"nn2weights\"]), CPU())\nnn3 = NeuralNetwork(nn3.architecture, nn3.model, _nnp(data[\"nn3weights\"]), CPU())\nnn4 = NeuralNetwork(nn4.architecture, nn4.model, _nnp(data[\"nn4weights\"]), CPU())\n\nmorange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256) # hide\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256) # hide\n\nfunction make_error_plot(; theme = :dark) # hide\ntextcolor = theme == :dark ? :white : :black # hide\nfig = Figure(; backgroundcolor = :transparent)\nax = Axis(fig[1, 1]; \n    backgroundcolor = :transparent,\n    bottomspinecolor = textcolor, \n    topspinecolor = textcolor,\n    leftspinecolor = textcolor,\n    rightspinecolor = textcolor,\n    xtickcolor = textcolor, \n    ytickcolor = textcolor,\n    xticklabelcolor = textcolor,\n    yticklabelcolor = textcolor,\n    xlabel=\"Epoch\", \n    ylabel=\"Training loss\",\n    xlabelcolor = textcolor,\n    ylabelcolor = textcolor,\n    )\n\nlines!(ax, loss_array1, label=\"Adam\", color=mblue)\nlines!(ax, loss_array2, label=\"Adam + Stiefel\", color=mred)\nlines!(ax, loss_array3, label=\"Gradient + Stiefel\", color=mpurple)\nlines!(ax, loss_array4, label=\"Momentum + Stiefel\", color=morange)\naxislegend(; position = (.82, .75), backgroundcolor = :transparent, labelcolor = textcolor) # hide\nfig_name = theme == :dark ? \"mnist_training_loss_dark.png\" : \"mnist_training_loss_light.png\" # hide\nsave(fig_name, fig; px_per_unit = 1.2) # hide\nend # hide\nmake_error_plot(; theme = :dark) # hide\nmake_error_plot(; theme = :light) # hide","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"(Image: Comparison between the standard Adam optimizer (blue), the Adam optimizer with weights on the Stiefel manifold (purple), the gradient optimizer with weights on the Stiefel manifold (purple) and the momentum optimizer with weights on the Stiefel manifold (orange).) (Image: Comparison between the standard Adam optimizer (blue), the Adam optimizer with weights on the Stiefel manifold (purple), the gradient optimizer with weights on the Stiefel manifold (purple) and the momentum optimizer with weights on the Stiefel manifold (orange).)","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"Main.remark(raw\"We see that the loss value for the Adam optimizer without parameters on the Stiefel manifold is stuck at around 1.34 which means that it *always predicts the same value*. So in 1 out of ten cases we have error 0 and in 9 out of ten cases we have error ``\\sqrt{2}``, giving\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\sqrt{2\\frac{9}{10}} = 1.342,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"which is what we see in the error plot.\")","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"We can also call GeometricMachineLearning.accuracy to obtain the test accuracy instead of the training error:","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"using GeometricMachineLearning: accuracy # hide\n(accuracy(nn1, dl_test), accuracy(nn2, dl_test), accuracy(nn3, dl_test), accuracy(nn4, dl_test))","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"Main.remark(raw\"We note here that conventional convolutional neural networks and other vision transformers achieve much better accuracy on MNIST in a training time that is often shorter than what we presented here. Our aim here is not to outperform existing neural networks in terms of accuracy on image classification problems, but to demonstrate two things: (i) in many cases putting weights on the Stiefel manifold (which is a compact space) can enable training that would otherwise not be possible and (ii) as is the case with standard Adam, the manifold version also seems to achieve similar performance gain over the gradient and momentum optimizer. Both of these observations are demonstrated figure above.\")","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/#Library-Functions","page":"MNIST","title":"Library Functions","text":"","category":"section"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"split_and_flatten\nGeometricMachineLearning.accuracy\nGeometricMachineLearning.onehotbatch\nGeometricMachineLearning.ClassificationLayer","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/#GeometricMachineLearning.split_and_flatten","page":"MNIST","title":"GeometricMachineLearning.split_and_flatten","text":"split_and_flatten(input::AbstractArray)::AbstractArray\n\nPerform a preprocessing of an image into flattened patches.\n\nThis rearranges the input data so that it can easily be processed with a transformer.\n\nExamples\n\nConsider a matrix of size 6times6 which we want to divide into patches of size 3times3.\n\nusing GeometricMachineLearning\n\ninput = [ 1  2  3  4  5  6; \n          7  8  9 10 11 12; \n         13 14 15 16 17 18;\n         19 20 21 22 23 24; \n         25 26 27 28 29 30; \n         31 32 33 34 35 36]\n\nsplit_and_flatten(input; patch_length = 3, number_of_patches = 4)\n\n# output\n\n9×4 Matrix{Int64}:\n  1  19   4  22\n  7  25  10  28\n 13  31  16  34\n  2  20   5  23\n  8  26  11  29\n 14  32  17  35\n  3  21   6  24\n  9  27  12  30\n 15  33  18  36\n\nHere we see that split_and_flatten:\n\nsplits the original matrix into four 3times3 matrices and then \nflattens each matrix into a column vector of size 9\n\nAfter this all the vectors are put together again to yield a 9times4 matrix.\n\nArguments\n\nThe optional keyword arguments are: \n\npatch_length: by default this is 7. \nnumber_of_patches: by default this is 16.\n\nThe sizes of the first and second axis of the output of split_and_flatten are \n\nmathttpath_length^2 and \nnumber_of_patches.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/mnist/mnist_tutorial/#GeometricMachineLearning.accuracy","page":"MNIST","title":"GeometricMachineLearning.accuracy","text":"accuracy(model, ps, dl)\n\nCompute the accuracy of a neural network classifier.\n\nThis needs an instance of DataLoader that stores the test data.\n\n\n\n\n\naccuracy(nn, dl)\n\nCompute the accuracy of a neural network classifier.\n\nThis is like accuracy(::Chain, ::Tuple, ::DataLoader), but for a NeuralNetwork.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/mnist/mnist_tutorial/#GeometricMachineLearning.onehotbatch","page":"MNIST","title":"GeometricMachineLearning.onehotbatch","text":"onehotbatch(target)\n\nPerforms a one-hot-batch encoding of a vector of integers: inputin01ldots9^ell. \n\nThe output is a tensor of shape 10times1timesell.\n\nIf the input is 0, this function produces:\n\n0 mapsto beginbmatrix 1  0  ldots  0 endbmatrix^T\n\nIn more abstract terms: i mapsto e_i.\n\nExamples\n\nusing GeometricMachineLearning: onehotbatch\n\ntarget = [0]\nonehotbatch(target)\n\n# output\n\n10×1×1 Array{Int64, 3}:\n[:, :, 1] =\n 1\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n 0\n\n\n\n\n\n","category":"function"},{"location":"tutorials/mnist/mnist_tutorial/#GeometricMachineLearning.ClassificationLayer","page":"MNIST","title":"GeometricMachineLearning.ClassificationLayer","text":"ClassificationLayer(input_dim, output_dim, activation)\n\nMake an instance of ClassificationLayer.\n\nClassificationLayer takes a matrix as an input and returns a vector that is used for classification. \n\nIt does:\n\n    X mapsto sigma(mathttcompute_vector(AX))\n\nwhere X is a matrix and mathttcompute_vector specifices how this matrix is turned into a vector. \n\nmathttcompute_vector can be specified with the keyword average.\n\nArguments\n\nClassificationLayer has the following optional keyword argument: \n\naverage:Bool=false.\n\nIf this keyword argument is set to true, then the output is computed as \n\n    input mapsto frac1Nsum_i=1^NmathcalNN(input)_bulleti\n\nIf set to false (the default) it picks the last column of the input. \n\nExamples\n\nusing GeometricMachineLearning\n\nl = ClassificationLayer(2, 2, identity; average = true)\nps = (weight = [1 0; 0 1], )\n\ninput = [1 2 3; 1 1 1]\n\nl(input, ps)\n\n# output\n\n2×1 Matrix{Float64}:\n 2.0\n 1.0\n\nusing GeometricMachineLearning\n\nl = ClassificationLayer(2, 2, identity; average = false)\nps = (weight = [1 0; 0 1], )\n\ninput = [1 2 3; 1 1 1]\n\nl(input, ps)\n\n# output\n\n2×1 Matrix{Int64}:\n 3\n 1\n\n\n\n\n\n","category":"type"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"\\begin{comment}","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/#References","page":"MNIST","title":"References","text":"","category":"section"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"B. Brantner. Generalizing Adam To Manifolds For Efficiently Training Transformers, arXiv preprint arXiv:2305.16901 (2023).\n\n\n\n","category":"page"},{"location":"tutorials/mnist/mnist_tutorial/","page":"MNIST","title":"MNIST","text":"\\end{comment}","category":"page"},{"location":"zusammenfassung/#Zusammenfassung","page":"Zusammenfassung","title":"Zusammenfassung","text":"","category":"section"},{"location":"zusammenfassung/","page":"Zusammenfassung","title":"Zusammenfassung","text":"Wissenschaftliches Rechnen ist ein unverzichtbares Werkzeug für viele Disziplinen wie Biologie, Ingenieurwesen und Physik. Es ist wichtig z.B. für (i) die Verknüpfung zwischen Theorien und empirischen Beobachtungen, (ii) das Treffen von Vorhersagen ohne physische Experimente und (iii) das Design von Apparaturen wie Fusionsreaktoren. In der Praxis bedeutet wissenschaftliches Rechnen fast immer, dass partielle Differentialgleichungen gelöst werden müssen, meist auf Supercomputern; und das ist gewöhnlich sehr teuer. ","category":"page"},{"location":"zusammenfassung/","page":"Zusammenfassung","title":"Zusammenfassung","text":"Wissenschaftler versuchen seit langem die Kosten für das Lösen dieser Gleichungen zu senken, indem sie z. B. Modelle vereinfachen oder reduzierte Darstellungen konstruieren. Einer dieser Ansätze ist Data-Driven Reduced Order Modeling (DDROM), das in den letzten Jahren aufgrund seiner Eignung für moderne Hardware und Algorithmen an Bedeutung gewonnen hat. In der Praxis bedeutet das oft die Anwendung von Techniken des maschinellen Lernens wie neuronale Netze.","category":"page"},{"location":"zusammenfassung/","page":"Zusammenfassung","title":"Zusammenfassung","text":"Neuronale Netze haben in verschiedenen Anwendungen ein enormes Potenzial gezeigt. Für DDROM sind die entsprechenden Ergebnisse oft allerdings noch nicht sehr zufriedenstellend. Wissenschaftler haben bei ihrer Anwendung von neuronalen Netzen oft Eigenschaften vernachlässigt, die sich im traditionellen wissenschaftlichen Rechnen als sehr wichtig erwiesen haben. In dieser Arbeit beziehen sich diese Eigenschaften auf die Struktur der Differentialgleichungen; diese ist bei der Durchführung von Simulationen oft unverzichtbar um Stabilität zu gewährleisten.","category":"page"},{"location":"zusammenfassung/","page":"Zusammenfassung","title":"Zusammenfassung","text":"In dieser Arbeit bezeichnen wir Methoden des maschinellen Lernens, die auf die spezifische Struktur einer Differentialgleichung zugeschnitten sind, als geometrisches maschinelles Lernen. Der Begriff geometrisch wird in diesem Zusammenhang traditionell auch verwendet und ist als Synonym des Wortes strukturerhaltend zu verstehen. Die Idee neuronale Netze geometrisch zu machen ist nicht neu; viele der Netzwerke die wir hier präsentieren sind es aber und bilden einen wichtigen Teil dieser Dissertation. Diese Arbeit teilt sich in vier Teile.","category":"page"},{"location":"zusammenfassung/","page":"Zusammenfassung","title":"Zusammenfassung","text":"In Teil I geben wir Hintergrundinformationen wieder, die keine neue Arbeit darstellen, aber die Grundlage für die folgenden Kapitel bilden. Dieser erste Teil enthält eine grundlegende Einführung in die Theorie der Riemannschen Mannigfaltigkeiten, eine Diskussion über Strukturerhaltung und eine kurze Erläuterung von DDROM.","category":"page"},{"location":"zusammenfassung/","page":"Zusammenfassung","title":"Zusammenfassung","text":"In Teil II wird ein neues Optimierungsframework eingeführt, das bestehende Optimierer für neuronale Netze auf Mannigfaltigkeiten verallgemeinert. Beispiele hierfür sind der Adam-Optimierer und der BFGS-Optimierer. Diese neuen Optimierer waren notwendig, um das Training einer neuen neuronalen Netzwerkarchitektur zu ermöglichen, die wir symplektische Autoenkoder (SAE) nennen.","category":"page"},{"location":"zusammenfassung/","page":"Zusammenfassung","title":"Zusammenfassung","text":"In Teil III werden schließlich verschiedene spezielle neuronale Netzwerkarchitekturen vorgestellt. Einige von ihnen, wie SympNets und Multi-Head Attention, stellen keine Neuheiten dar, aber andere, wie SAEs, Volume-Preserving Attention und der lineare symplektische Transformer, sind originell.","category":"page"},{"location":"zusammenfassung/","page":"Zusammenfassung","title":"Zusammenfassung","text":"In Teil IV geben wir einige Ergebnisse an, die auf den neuen Architekturen basieren. Die meisten dieser Anwendungen beziehen sich auf Anwendungen aus der Physik; um jedoch die neuen Optimierer zu demonstrieren, greifen wir auf ein klassisches Problem aus der Bildklassifikation zurück. Wir wollen damit zeigen dass geometrisches maschinelles Lernen auch in Bereichen außerhalb des wissenschaftlichen Rechnens Anwendung finden kann. In allen behandelten Problemen ist zu sehen dass unsere Modelle genauer oder schneller als vergleichbare Architekturen sind. In einem Beispiel zeigen wir wie ein SAE-reduziertes Model die Auswertung eines Problems um einen Faktor 1000 beschleunigt.","category":"page"},{"location":"zusammenfassung/","page":"Zusammenfassung","title":"Zusammenfassung","text":"\\clearpage","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"CurrentModule = GeometricMachineLearning","category":"page"},{"location":"#Geometric-Machine-Learning","page":"HOME","title":"Geometric Machine Learning","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"GeometricMachineLearning is a package for structure-preserving scientific machine learning. It contains models that can learn dynamical systems with geometric structure, such as Hamiltonian (symplectic) or Lagrangian (variational) systems.","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"In that regard its aim is similar to traditional geometric numerical integration [1, 2] in that it models maps that share properties with the analytic solution of a differential equation:","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"(Image: ) (Image: )","category":"page"},{"location":"#Installation","page":"HOME","title":"Installation","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"GeometricMachineLearning and all of its dependencies can be installed via the Julia REPL by typing ","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"]add GeometricMachineLearning","category":"page"},{"location":"#Architectures","page":"HOME","title":"Architectures","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"Some of the neural network architectures in GeometricMachineLearning [3, 4] have emerged in connection with developing this package[1], other have existed before [5, 6].","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"[1]: The work on this software package was done in connection with a PhD thesis. You can read its introduction and conclusion here.","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"New architectures include:","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"symplectic autoencoder,\nvolume-preserving transformers, \nlinear-symplectic transformer. ","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"Existing architectures include:","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"SympNets,\nstandard transormer.","category":"page"},{"location":"#Manifolds","page":"HOME","title":"Manifolds","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"GeometricMachineLearning supports putting neural network weights on manifolds such as the Stiefel manifold and the Grassmann manifold and Riemannian optimization.","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"(Image: Weights can be put on manifolds to achieve structure preservation or improved stability.) (Image: Weights can be put on manifolds to achieve structure preservation or improved stability.)","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"When GeometricMachineLearning optimizes on manifolds it uses the framework introduced in [7]. Optimization is necessary for some neural network architectures such as symplectic autoencoders and can be critical for others such as the standard transformer [8, 9].","category":"page"},{"location":"#Special-Neural-Network-Layer","page":"HOME","title":"Special Neural Network Layer","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"Many layers have been adapted in order to be used for problems in scientific machine learning, such as the attention layer.","category":"page"},{"location":"#GPU-Support","page":"HOME","title":"GPU Support","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"All neural network layers and architectures that are implemented in GeometricMachineLearning have GPU support via the package KernelAbstractions.jl [10], so GeometricMachineLearning naturally integrates CUDA.jl [11], AMDGPU.jl, Metal.jl [12] and oneAPI.jl [13].","category":"page"},{"location":"#Tutorials","page":"HOME","title":"Tutorials","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"There are several tutorials demonstrating how GeometricMachineLearning can be used.","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"These tutorials include:","category":"page"},{"location":"","page":"HOME","title":"HOME","text":"a tutorial on SympNets that shows how we can model a flow map corresponding to data coming from an unknown canonical Hamiltonian system,\na tutorial on symplectic Autoencoders that shows how this architecture can be used in structure-preserving reduced order modeling,\na tutorial on the volume-preserving attention mechanism which serves as a basis for the volume-preserving transformer,\na tutorial on training a transformer with manifold weights for image classification to show that manifold optimization is also useful outside of scientific machine learning.","category":"page"},{"location":"#Data-Driven-Reduced-Order-Modeling","page":"HOME","title":"Data-Driven Reduced Order Modeling","text":"","category":"section"},{"location":"","page":"HOME","title":"HOME","text":"The main motivation behind developing GeometricMachineLearning is reduced order modeling, especially structure-preserving reduced order modeling. For this purpose we give a short introduction into this topic.","category":"page"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"\\thispagestyle{empty}","category":"page"},{"location":"references/","page":"References","title":"References","text":"E. Hairer, C. Lubich and G. Wanner. Geometric Numerical integration: structure-preserving algorithms for ordinary differential equations (Springer, Heidelberg, 2006).\n\n\n\nM. Kraus. GeometricIntegrators.jl: Geometric Numerical Integration in Julia, https://github.com/JuliaGNI/GeometricIntegrators.jl (2020).\n\n\n\nB. Brantner and M. Kraus. Symplectic autoencoders for Model Reduction of Hamiltonian Systems, arXiv preprint arXiv:2312.10004 (2023).\n\n\n\nB. Brantner, G. de Romemont, M. Kraus and Z. Li. Volume-Preserving Transformers for Learning Time Series Data with Structure, arXiv preprint arXiv:2312:11166v2 (2024).\n\n\n\nP. Jin, Z. Zhang, A. Zhu, Y. Tang and G. E. Karniadakis. SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems. Neural Networks 132, 166–179 (2020).\n\n\n\nS. Greydanus, M. Dzamba and J. Yosinski. Hamiltonian neural networks. Advances in neural information processing systems 32 (2019).\n\n\n\nB. Brantner. Generalizing Adam To Manifolds For Efficiently Training Transformers, arXiv preprint arXiv:2305.16901 (2023).\n\n\n\nL. Kong, Y. Wang and M. Tao. Momentum stiefel optimizer, with applications to suitably-orthogonal attention, and optimal transport, arXiv preprint arXiv:2205.14173v3 (2023).\n\n\n\nA. Zhang, A. Chan, Y. Tay, J. Fu, S. Wang, S. Zhang, H. Shao, S. Yao and R. K.-W. Lee. On orthogonality constraints for transformers. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Vol. 2 (Association for Computational Linguistics, 2021); pp. 375–382.\n\n\n\nV. Churavy. KernelAbstractions.jl, https://github.com/JuliaGPU/KernelAbstractions.jl (2024). Used on August 14, 2024.\n\n\n\nT. Besard, C. Foket and B. De Sutter. Effective Extensible Programming: Unleashing Julia on GPUs. IEEE Transactions on Parallel and Distributed Systems (2018), arXiv:1712.03112 [cs.PL].\n\n\n\nT. Besard and M. Hawkins. Metal.jl, https://github.com/JuliaGPU/Metal.jl (2022). Used on August 14, 2024.\n\n\n\nT. Besard, oneAPI.jl, https://github.com/JuliaGPU/oneAPI.jl (2022). Used on August 14, 2024.\n\n\n\nS. Lipschutz. General Topology (McGraw-Hill Book Company, New York City, New York, 1965).\n\n\n\nS. Lang. Fundamentals of differential geometry. Vol. 191 (Springer Science & Business Media, 2012).\n\n\n\nS. I. Richard L. Bishop. Tensor Analysis on Manifolds (Dover Publications, Mineola, New York, 1980).\n\n\n\nS. Lang. Real and functional analysis. Vol. 142 (Springer Science & Business Media, 2012).\n\n\n\nF. Mezzadri. How to generate random matrices from the classical compact groups, arXiv preprint math-ph/0609050 (2006).\n\n\n\nM. P. Do Carmo and J. Flaherty Francis. Riemannian geometry. Vol. 2 (Springer, 1992).\n\n\n\nP.-A. Absil, R. Mahony and R. Sepulchre. Riemannian geometry of Grassmann manifolds with a view on algorithmic computation. Acta Applicandae Mathematica 80, 199–220 (2004).\n\n\n\nD. D. Holm, T. Schmah and C. Stoica. Geometric mechanics and symmetry: from finite to infinite dimensions. Vol. 12 (Oxford University Press, Oxford, UK, 2009).\n\n\n\nP.-A. Absil, R. Mahony and R. Sepulchre. Optimization algorithms on matrix manifolds (Princeton University Press, Princeton, New Jersey, 2008).\n\n\n\nT. Bendokat, R. Zimmermann and P.-A. Absil. A Grassmann manifold handbook: Basic geometry and computational aspects, arXiv preprint arXiv:2011.13699 (2020).\n\n\n\nW. S. Moses, V. Churavy, L. Paehler, J. Hückelheim, S. H. Narayanan, M. Schanen and J. Doerfert. Reverse-Mode Automatic Differentiation and Optimization of GPU Kernels via Enzyme. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '21 (Association for Computing Machinery, New York, NY, USA, 2021).\n\n\n\nM. Betancourt. A geometric theory of higher-order automatic differentiation, arXiv preprint arXiv:1812.11592 (2018).\n\n\n\nJ. Bolte and E. Pauwels. A mathematical model for automatic differentiation in machine learning. Advances in Neural Information Processing Systems 33, 10809–10819 (2020).\n\n\n\nV. I. Arnold. Mathematical methods of classical mechanics. Vol. 60 of Graduate Texts in Mathematics (Springer Verlag, Berlin, 1978).\n\n\n\nM. Kraus, K. Kormann, P. J. Morrison and E. Sonnendrücker. GEMPIC: geometric electromagnetic particle-in-cell methods. Journal of Plasma Physics 83, 905830401 (2017).\n\n\n\nZ. Ge and J. E. Marsden. Lie-poisson hamilton-jacobi theory and lie-poisson integrators. Physics Letters A 133, 134–139 (1988).\n\n\n\nK. Hornik, M. Stinchcombe and H. White. Multilayer feedforward networks are universal approximators. Neural networks 2, 359–366 (1989).\n\n\n\nC. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi and S. Kumar. Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077 (2019).\n\n\n\nD.-X. Zhou. Universality of deep convolutional neural networks. Applied and computational harmonic analysis 48, 787–794 (2020).\n\n\n\nZ. Liu, Y. Wang, S. Vaidya, F. Ruehle, J. Halverson, M. Soljačić, T. Y. Hou and M. Tegmark. Kan: Kolmogorov-arnold networks, arXiv preprint arXiv:2404.19756 (2024).\n\n\n\nJ. W. Burby, Q. Tang and R. Maulik. Fast neural Poincaré maps for toroidal magnetic fields. Plasma Physics and Controlled Fusion 63, 024001 (2020).\n\n\n\nP. Horn, V. Saz Ulibarrena, B. Koren and S. Portegies Zwart. A Generalized Framework of Neural Networks for Hamiltonian Systems. SSRN preprint SSRN:4555181 (2023).\n\n\n\nE. Celledoni, M. J. Ehrhardt, C. Etmann, R. I. McLachlan, B. Owren, C.-B. Schonlieb and F. Sherry. Structure-preserving deep learning. European journal of applied mathematics 32, 888–936 (2021).\n\n\n\nT. Bendokat and R. Zimmermann. The real symplectic Stiefel and Grassmann manifolds: metrics, geodesics and applications, arXiv preprint arXiv:2108.12447 (2021).\n\n\n\nB. Gao, N. T. Son, P.-A. Absil and T. Stykel. Riemannian optimization on the symplectic Stiefel manifold. SIAM Journal on Optimization 31, 1546–1575 (2021).\n\n\n\nB. O'neill. Semi-Riemannian geometry with applications to relativity (Academic press, New York City, New York, 1983).\n\n\n\nE. Celledoni and A. Iserles. Approximating the exponential from a Lie algebra to a Lie group. Mathematics of Computation 69, 1457–1480 (2000).\n\n\n\nC. Fraikin, K. Hüper and P. V. Dooren. Optimization over the Stiefel manifold. In: PAMM: Proceedings in Applied Mathematics and Mechanics, Vol. 7 no. 1 (Wiley Online Library, 2007); pp. 1062205–1062206.\n\n\n\nM. Schlarb. Covariant Derivatives on Homogeneous Spaces: Horizontal Lifts and Parallel Transport. The Journal of Geometric Analysis 34, 1–43 (2024).\n\n\n\nI. Goodfellow, Y. Bengio and A. Courville. Deep learning (MIT press, Cambridge, MA, 2016).\n\n\n\nJ. N. Stephen J. Wright. Numerical optimization (Springer Science+Business Media, New York, NY, 2006).\n\n\n\nA.Γ. (math.stackexchange user 253273). Quasi-newton methods: Understanding DFP updating formula, https://math.stackexchange.com/q/2279304 (2017). Accessed on September 19, 2024.\n\n\n\nP. Kenneweg, T. Kenneweg and B. Hammer. Improving Line Search Methods for Large Scale Neural Network Training. In: 2024 International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA) (IEEE, 2024); pp. 1–6.\n\n\n\nS. Vaswani, A. Mishkin, I. Laradji, M. Schmidt, G. Gidel and S. Lacoste-Julien. Painless stochastic gradient: Interpolation, line-search, and convergence rates. Advances in neural information processing systems 32 (2019).\n\n\n\nW. Huang, P.-A. Absil and K. A. Gallivan. A Riemannian BFGS method for nonconvex optimization problems. In: Numerical Mathematics and Advanced Applications ENUMATH 2015 (Springer, 2016); pp. 627–634.\n\n\n\nB. Gao, N. T. Son and T. Stykel. Symplectic Stiefel manifold: tractable metrics, second-order geometry and Newton's methods, arXiv preprint arXiv:2406.14299 (2024).\n\n\n\nJ. Bajārs. Locally-symplectic neural networks for learning volume-preserving dynamics. Journal of Computational Physics 476, 111911 (2023).\n\n\n\nF. Kang and S. Zai-Jiu. Volume-preserving algorithms for source-free dynamical systems. Numerische Mathematik 71, 451–463 (1995).\n\n\n\nH. Cardot. Recurrent neural networks for temporal data processing (BoD–Books on Demand, 2011).\n\n\n\nD. Bahdanau, K. Cho and Y. Bengio. Neural machine translation by jointly learning to align and translate, arXiv preprint arXiv:1409.0473 (2014).\n\n\n\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and I. Polosukhin. Attention is all you need. Advances in neural information processing systems 30 (2017).\n\n\n\nK. Jacobs. Discrete Stochastics (Birkhäuser Verlag, Basel, Switzerland, 1992).\n\n\n\nK. Feng. The step-transition operators for multi-step methods of ODE's. Journal of Computational Mathematics, 193–202 (1998).\n\n\n\nM.-T. Luong, H. Pham and C. D. Manning. Effective approaches to attention-based neural machine translation, arXiv preprint arXiv:1508.04025 (2015).\n\n\n\nK. Feng and M.-z. Qin. The symplectic methods for the computation of Hamiltonian equations. In: Numerical Methods for Partial Differential Equations: Proceedings of a Conference held in Shanghai, PR China, March 25–29, 1987 (Springer, 1987); pp. 1–37.\n\n\n\nZ. Ge and K. Feng. On the approximation of linear Hamiltonian systems. Journal of Computational Mathematics, 88–97 (1988).\n\n\n\nT. Blickhan. A registration method for reduced basis problems using linear optimal transport, arXiv preprint arXiv:2304.14884 (2023).\n\n\n\nS. Fresca, L. Dede’ and A. Manzoni. A comprehensive deep learning-based approach to reduced order modeling of nonlinear time-dependent parametrized PDEs. Journal of Scientific Computing 87, 1–36 (2021).\n\n\n\nK. Lee and K. T. Carlberg. Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders. Journal of Computational Physics 404, 108973 (2020).\n\n\n\nM. J. Gander and G. Wanner. From Euler, Ritz, and Galerkin to modern computing. Siam Review 54, 627–666 (2012).\n\n\n\nF. Arbes, C. Greif and K. Urban. The Kolmogorov N-width for linear transport: Exact representation and the influence of the data, arXiv preprint arXiv:2305.00066 (2023).\n\n\n\nC. Greif and K. Urban. Decay of the Kolmogorov N-width for wave problems. Applied Mathematics Letters 96, 216–222 (2019).\n\n\n\nA. Chatterjee. An introduction to the proper orthogonal decomposition. Current science, 808–817 (2000).\n\n\n\nS. Volkwein. Proper orthogonal decomposition: Theory and reduced-order modelling. Lecture Notes, University of Konstanz 4, 1–29 (2013).\n\n\n\nL. Peng and K. Mohseni. Symplectic model reduction of Hamiltonian systems. SIAM Journal on Scientific Computing 38, A1–A27 (2016).\n\n\n\nT. M. Tyranowski and M. Kraus. Symplectic model reduction methods for the Vlasov equation. Contributions to Plasma Physics 63, e202200046 (2023).\n\n\n\nP. Buchfink, S. Glas and B. Haasdonk. Symplectic model reduction of Hamiltonian systems on nonlinear manifolds and approximation with weakly symplectic autoencoder. SIAM Journal on Scientific Computing 45, A289–A311 (2023).\n\n\n\nM. Raissi, P. Perdikaris and G. E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics 378, 686–707 (2019).\n\n\n\nS. Yıldız, P. Goyal, T. Bendokat and P. Benner. Data-Driven Identification of Quadratic Representations for Nonlinear Hamiltonian Systems Using Weakly Symplectic Liftings. Journal of Machine Learning for Modeling and Computing 5 (2024).\n\n\n\nA. Van Der Schaft, D. Jeltsema and others. Port-Hamiltonian systems theory: An introductory overview. Foundations and Trends in Systems and Control 1, 173–378 (2014).\n\n\n\nR. Morandin. Modeling and numerical treatment of port-Hamiltonian descriptor systems. Ph.D. Thesis, Technische Universität Berlin (2023).\n\n\n\nH. Yoshimura and J. E. Marsden. Dirac structures in Lagrangian mechanics Part I: implicit Lagrangian systems. Journal of Geometry and Physics 57, 133–156 (2006).\n\n\n\nH. Yoshimura and J. E. Marsden. Dirac structures in Lagrangian mechanics Part II: Variational structures. Journal of Geometry and Physics 57, 209–250 (2006).\n\n\n\nP. Kotyczka and L. Lefevre. Discrete-time port-Hamiltonian systems: A definition based on symplectic integration. Systems & Control Letters 133, 104530 (2019).\n\n\n\nV. Mehrmann and R. Morandin. Structure-preserving discretization for port-Hamiltonian descriptor systems. In: 2019 IEEE 58th Conference on Decision and Control (CDC) (IEEE, 2019); pp. 6863–6868.\n\n\n\nT. F. Moser. Structure-Preserving Model Reduction of Port-Hamiltonian Descriptor Systems. Ph.D. Thesis, Technische Universität München (2023).\n\n\n\nJ. Rettberg, J. Kneifl, J. Herb, P. Buchfink, J. Fehr and B. Haasdonk. Data-driven identification of latent port-Hamiltonian systems, arXiv preprint arXiv:2408.08185 (2024).\n\n\n\nS. E. Otto, G. R. Macchio and C. W. Rowley. Learning nonlinear projections for reduced-order modeling of dynamical systems using constrained autoencoders. Chaos: An Interdisciplinary Journal of Nonlinear Science 33 (2023).\n\n\n\nB. Leimkuhler and S. Reich. Simulating hamiltonian dynamics. No. 14 (Cambridge university press, 2004).\n\n\n\nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation 9, 1735–1780 (1997).\n\n\n\nA. Hemmasian and A. Barati Farimani. Reduced-order modeling of fluid flows with transformers. Physics of Fluids 35 (2023).\n\n\n\nA. Solera-Rico, C. S. Vila, M. Gómez, Y. Wang, A. Almashjary, S. Dawson and R. Vinuesa, beta-Variational autoencoders and transformers for reduced-order modelling of fluid flows, arXiv preprint arXiv:2304.03571 (2023).\n\n\n\nP. Jin, Z. Lin and B. Xiao. Optimal unit triangular factorization of symplectic matrices. Linear Algebra and its Applications (2022).\n\n\n\nN. Patwardhan, S. Marrone and C. Sansone. Transformers in the real world: A survey on nlp applications. Information 14, 242 (2023).\n\n\n\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly and others. An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929 (2020).\n\n\n\nM. Toda. Vibration of a chain with nonlinear interaction. Journal of the Physical Society of Japan 22, 431–436 (1967).\n\n\n\nL. Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine 29, 141–142 (2012).\n\n\n\nC. Villani and others. Optimal transport: old and new. Vol. 338 (Springer, 2009).\n\n\n\nC. Villani. Topics in optimal transportation. Vol. 58 (American Mathematical Soc., 2021).\n\n\n\nT. Blickhan. BrenierTwoFluids.jl, https://github.com/ToBlick/BrenierTwoFluids (2023).\n\n\n\nM. Innes. Don't Unroll Adjoint: Differentiating SSA-Form Programs. CoRR abs/1810.07951 (2018), arXiv:1810.07951.\n\n\n\nJ. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat and others. Gpt-4 technical report, arXiv preprint arXiv:2303.08774 (2023).\n\n\n\nY. Duan, J. S. Edwards and Y. K. Dwivedi. Artificial intelligence for decision making in the era of Big Data–evolution, challenges and research agenda. International journal of information management 48, 63–71 (2019).\n\n\n\nD. C. Psichogios and L. H. Ungar. A hybrid neural network-first principles approach to process modeling. AIChE Journal 38, 1499–1511 (1992).\n\n\n\nN. Baker, F. Alexander, T. Bremer, A. Hagberg, Y. Kevrekidis, H. Najm, M. Parashar, A. Patra, J. Sethian, S. Wild and others. Workshop report on basic research needs for scientific machine learning: Core technologies for artificial intelligence (USDOE Office of Science (SC), Washington, DC (United States), 2019).\n\n\n\nD. N. Arnold, R. S. Falk and R. Winther. Finite element exterior calculus, homological techniques, and applications. Acta numerica 15, 1–155 (2006).\n\n\n\nY. Lishkova, P. Scherer, S. Ridderbusch, M. Jamnik, P. Liò, S. Ober-Blöbaum and C. Offen. Discrete Lagrangian neural networks with automatic symmetry discovery. IFAC-PapersOnLine 56, 3203–3210 (2023).\n\n\n\nE. Dierkes, C. Offen, S. Ober-Blöbaum and K. Flaßkamp. Hamiltonian neural networks with automatic symmetry detection. Chaos: An Interdisciplinary Journal of Nonlinear Science 33 (2023).\n\n\n\nB. Brantner and M. Kraus. GeometricMachineLearning.jl: Geometric Machine Learning in Julia, https://github.com/JuliaGNI/GeometricMachineLearning.jl (2020).\n\n\n\nThe Julia Company. Documentation, https://docs.julialang.org/en/v1/manual/documentation/ (2024). Accessed on August 19, 2024.\n\n\n\nNvidia Corporation. GeForce RTX 4090, https://www.nvidia.com/de-de/geforce/graphics-cards/40-series/rtx-4090/ (2022). Accessed on August 13, 2024.\n\n\n\nGitHub, Inc. About GitHub-hosted runners, https://docs.github.com/en/actions/using-github-hosted-runners/using-github-hosted-runners/about-github-hosted-runners (2024). Accessed on August 13, 2024.\n\n\n\nS. Danisch and J. Krumbiegel. Makie.jl: Flexible high-performance data visualization for Julia. Journal of Open Source Software 6, 3349 (2021).\n\n\n\nD. Bon, G. Pai, G. Bellaard, O. Mula and R. Duits. Optimal Transport on the Lie Group of Roto-translations, arXiv preprint arXiv:2402.15322 (2024).\n\n\n\nD. Kingma. Adam: a method for stochastic optimization, arXiv preprint arXiv:1412.6980 (2014).\n\n\n\nT. Frankel. The geometry of physics: an introduction (Cambridge university press, Cambridge, UK, 2011).\n\n\n\nJ. Li, L. Fuxin and S. Todorovic. Efficient riemannian optimization on the stiefel manifold via the cayley transform, arXiv preprint arXiv:2002.01113 (2020).\n\n\n\nL. Huang, X. Liu, B. Lang, A. Yu, Y. Wang and B. Li. Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. In: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32 no. 1 (2018).\n\n\n\nR. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang and T. Liu. On layer normalization in the transformer architecture. In: International Conference on Machine Learning (PMLR, 2020); pp. 10524–10533.\n\n\n\nS. Fresca and A. Manzoni. POD-DL-ROM: Enhancing deep learning-based reduced order models for nonlinear parametrized PDEs by proper orthogonal decomposition. Computer Methods in Applied Mechanics and Engineering 388, 114181 (2022).\n\n\n\nP. J. Morrison. A paradigm for joined Hamiltonian and dissipative systems. Physica D: Nonlinear Phenomena 18, 410–419 (1986).\n\n\n\nA. Gruber, M. Gunzburger, L. Ju and Z. Wang. Energetically consistent model reduction for metriplectic systems. Computer Methods in Applied Mechanics and Engineering 404, 115709 (2023).\n\n\n\nP. Schulze. Structure-preserving model reduction for port-hamiltonian systems based on a special class of nonlinear approximation ansatzes, arXiv preprint arXiv:2302.06479 (2023).\n\n\n\nM. Mamunuzzaman and H. Zwart. Structure preserving model order reduction of port-Hamiltonian systems, arXiv preprint arXiv:2203.07751 (2022).\n\n\n\nS. Duane, A. D. Kennedy, B. J. Pendleton and D. Roweth. Hybrid monte carlo. Physics letters B 195, 216–222 (1987).\n\n\n\nA. D. Cobb and B. Jalaian. Scaling Hamiltonian Monte Carlo inference for Bayesian neural networks with symmetric splitting. In: Uncertainty in Artificial Intelligence (PMLR, 2021); pp. 675–685.\n\n\n\nA. Fichtner and S. Simutė. Hamiltonian Monte Carlo inversion of seismic sources in complex media. Journal of Geophysical Research: Solid Earth 123, 2984–2999 (2018).\n\n\n\nA. Wibisono, A. C. Wilson and M. I. Jordan. A variational perspective on accelerated methods in optimization, proceedings of the National Academy of Sciences 113, E7351–E7358 (2016).\n\n\n\nV. Duruisseaux and M. Leok. Accelerated optimization on Riemannian manifolds via discrete constrained variational integrators. Journal of Nonlinear Science 32, 42 (2022).\n\n\n\nH. Sato and K. Aihara. Cholesky QR-based retraction on the generalized Stiefel manifold. Computational Optimization and Applications 72, 293–308 (2019).\n\n\n\nB. Gao, N. T. Son and T. Stykel. Optimization on the symplectic Stiefel manifold: SR decomposition-based retraction and applications. Linear Algebra and its Applications 682, 50–85 (2024).\n\n\n\nB. Brantner, G. de Romemont, M. Kraus and Z. Li. Structure-Preserving Transformers for Learning Parametrized Hamiltonian Systems, arXiv preprint arXiv:2312:11166 (2023).\n\n\n\nT. Lin and H. Zha. Riemannian manifold learning. IEEE transactions on pattern analysis and machine intelligence 30, 796–809 (2008).\n\n\n\n","category":"page"},{"location":"tutorials/hamiltonian_neural_network/#Hamiltonian-Neural-Network","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"","category":"section"},{"location":"tutorials/hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"In this tutorial we build a Hamiltonian neural network. We first need vector field data:","category":"page"},{"location":"tutorials/hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"using GeometricMachineLearning # hide\nusing GeometricMachineLearning: QPT\nusing LinearAlgebra: norm\nusing Zygote: gradient\n\n𝕁 = PoissonTensor(2)\nvf(z) = 𝕁 * z\ndomain = [[q, p] for q in -1:.1:1 for p in -1:.1:1]\nvf_data = vf.(domain)\ndomain_matrix = hcat(domain...)\nvf_matrix = hcat(vf_data...)\ndl = DataLoader(domain_matrix, vf_matrix)\nnothing # hide","category":"page"},{"location":"tutorials/hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"We then build the neural network:","category":"page"},{"location":"tutorials/hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"const intermediate_dim = 5\nhnn_arch = Chain(Dense(2, intermediate_dim, tanh), Dense(intermediate_dim, intermediate_dim, tanh), Linear(intermediate_dim, 1))\nhnn = NeuralNetwork(hnn_arch)\nnothing # hide","category":"page"},{"location":"tutorials/hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"Next we define the loss function","category":"page"},{"location":"tutorials/hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"struct HNNLoss <: NetworkLoss end\nfunction (loss::HNNLoss)(model::Chain, ps::Tuple, input::AT, output::AT) where {T, AT <: AbstractArray{T}}\n    vf = 𝕁(gradient(input -> sum(model(input, ps)), input)[1])\n    norm(input - output)\nend\nloss = HNNLoss()\nnothing # hide","category":"page"},{"location":"tutorials/hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"We can now train the network","category":"page"},{"location":"tutorials/hamiltonian_neural_network/","page":"Hamiltonian Neural Network","title":"Hamiltonian Neural Network","text":"batch = Batch(10)\nn_epochs = 100\no = Optimizer(AdamOptimizer(Float64), hnn)\nloss_array = o(hnn, dl, batch, n_epochs, loss)","category":"page"},{"location":"manifolds/manifolds/#(Matrix)-Manifolds","page":"General Theory on Manifolds","title":"(Matrix) Manifolds","text":"","category":"section"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Manifolds are topological spaces that locally look like vector spaces. In the following we restrict ourselves to finite-dimensional smooth[1] manifolds. In this section we routinely denote points on a manifold by lower case letters like x y and z if we speak about general properties and by upper case letters like A and B if we talk about specific examples of matrix manifolds. All manifolds that can be used to build neural networks in GeometricMachineLearning, such as the Stiefel manifold and the Grassmann manifold are matrix manifolds.","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"[1]: Smooth here means C^infty.","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.definition(raw\"A **finite-dimensional smooth manifold** of dimension ``n`` is a second-countable Hausdorff space ``\\mathcal{M}`` for which ``\\forall{}x\\in\\mathcal{M}`` we can find a neighborhood ``U``, that contains ``x,`` and a corresponding homeomorphism ``\\varphi_U:U\\cong{}W\\subset\\mathbb{R}^n`` where ``W`` is an open subset. The homeomorphisms ``\\varphi_U`` are referred to as *coordinate charts*. If two such coordinate charts overlap, i.e. if ``U_1\\cap{}U_2\\neq\\{\\}``, then the map ``\\varphi_{U_2}\\circ\\varphi_{U_1}^{-1}`` has to be ``C^\\infty`` on ``varphi_{U_1}(U_1\\cap{}U_2).`` We call the collection of coordinate charts ``\\{\\varphi_U\\}_{U\\subset\\mathcal{M}}`` an **atlas** for ``\\mathcal{M}.``\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"One example of a manifold that is also important for GeometricMachineLearning is the Lie group[2] of orthonormal matrices SO(N). Before we can proof that SO(N) is a manifold we first need the preimage theorem.","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"[2]: Lie groups are manifolds that also have a group structure, i.e. there is an operation mathcalMtimesmathcalMtomathcalM(ab)mapstoab s.t. (ab)c = a(bc) and  there exists a neutral elementemathcalM s.t. ae = a forallainmathcalM as well as an (for every a) inverse element a^-1 s.t. a(a^-1) = e. The neutral element e we refer to as mathbbI when dealing with matrix manifolds.","category":"page"},{"location":"manifolds/manifolds/#The-Preimage-Theorem","page":"General Theory on Manifolds","title":"The Preimage Theorem","text":"","category":"section"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"The preimage theorem is crucial for treating the specific manifolds that are part of GeometricMachineLearning; the preimage theorem gives spaces like the Stiefel manifold the structure of a manifold. Before we can state the preimage theorem we need another definition:","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.definition(raw\"Consider a smooth mapping ``g: \\mathcal{M}\\to\\mathcal{N}`` from one manifold to another. A point ``y\\in\\mathcal{N}`` is called **regular point of ``g``** if ``\\forall{}x\\in{}g^{-1}\\{y\\}`` the map ``T_xg:T_x\\mathcal{M}\\to{}T_{y}\\mathcal{N}`` is surjective.\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.remark(raw\"Here we already used the notation ``T_y\\mathcal{N}`` to denote the *tangent space to ``\\mathcal{N}`` at ``y``*. We will explain what we mean by this precisely below. For now we simply view ``T_y\\mathcal{N}`` as *something that is homemorphic to ``\\mathbb{R}^m``* and the *tangent map ``T_xg``* we will simply view as ``(\\psi\\circ{}g\\circ{}\\varphi^{-1})'(varphi(x)),`` where ``\\varphi`` is a coordinate chart as ``x`` and ``\\psi`` is a coordinate chart at ``y.`` In the examples we give below ``\\mathcal{M}`` and ``\\mathcal{N}`` will simply be vector spaces, and ``g`` will be differential map between vector spaces whose derivative at ``x\\in{}f^{-1}\\{y\\}`` (for a regular point ``y``) is surjective. For a vector space ``\\mathcal{V}`` we furthermore have ``T_x\\mathcal{V} = \\mathcal{V}.``\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"We now state the preimage theorem:","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.theorem(raw\"Consider a smooth map ``g:\\mathcal{M}\\to\\mathcal{N}`` from one manifold to another (we assume the dimensions of the two manifolds to be ``m+n`` and ``m`` respectively). Then the preimage of a regular point ``y`` of ``\\mathcal{N}`` is a submanifold of ``\\mathcal{M}``. Furthermore the codimension of ``g^{-1}\\{y\\}`` is equal to the dimension of ``\\mathcal{N}`` and the tangent space ``T_x(g^{-1}\\{y\\})`` is equal to the kernel of ``T_xg``.\"; name = \"Preimage Theorem\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.proof(raw\"Because ``\\mathcal{N}`` has manifold structure we can find a chart ``\\psi_U:U\\to\\mathbb{R}^m`` for some neighborhood ``U`` that contains ``y``. We further consider a point ``x\\in{}g^{-1}\\{y\\}`` and a chart around it ``\\varphi_V:V\\to\\mathbb{R}^{m+n}``. By the implicit function theorem we can then find a mapping ``h`` that turns ``\\psi_U\\circ{}g\\circ\\varphi_V^{-1}`` into a projection ``(x_1, \\ldots, x_{n+m}) \\mapsto (x_{n+1}, \\ldots, x_{n+m})``. We now consider the neighborhood ``V_1\\times\\{0\\} = \\varphi(V \\cup f^{-1}\\{y\\})`` for ``\\varphi(V) = V_1\\times{}V_2`` with the coordinate chart ``(x_1, \\ldots, x_n) \\mapsto \\varphi(x_1, \\ldots, x_n, 0, \\ldots, 0).`` As this map is also smooth by the implicit function theorem this proofs our assertion.\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"We now give some examples of manifolds that can be constructed this way:","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.example(raw\"The group ``SO(N)`` is a Lie group (i.e. has manifold structure). ``SO(N)`` has dimension ``N(N-1)/2.``\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.proof(raw\"The vector space ``\\mathbb{R}^{N\\times{}N}`` clearly has manifold structure. The group ``SO(N)`` is equivalent to one of the level sets of the mapping: ``g:\\mathbb{R}^{N\\times{}N}\\to\\mathcal{S}(N), A\\mapsto{}A^TA - \\mathbb{I}``, i.e. it is the component of ``f^{-1}\\{\\mathbb{I}\\}`` that contains ``\\mathbb{I}``; the image of ``f`` is contained in ``\\mathcal{S}(N),`` the symmetric matrices of size ``N\\times{}N.`` We still need to proof that ``\\mathbb{I}`` is a regular point of ``g``, i.e. that for ``A\\in{}SO(N)`` the mapping ``T_Ag`` is surjective. This means that ``\\forall{}B\\in\\mathcal{S}(N)`` ``\\exists{}C\\in\\mathbb{R}^{N\\times{}N}`` s.t. ``C^TA + A^TC = B``. The element ``C=\\frac{1}{2}AB\\in\\mathcal{R}^{N\\times{}N}`` satisfies this property. The dimension of ``SO(N)`` is ``N(N-1)/2`` as ``\\mathrm{dim}(\\mathcal{S}(N))=N(N+1)/2.``\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Similarly we can also proof: ","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.example(raw\"The sphere ``S^n:=\\{x\\in\\mathbb{R}^{n+1}: x^Tx = 1\\}`` is a manifold of dimension ``n``.\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.proof(raw\"Take ``g(x) = x^Tx - 1`` and proceed as in the case of ``SO(N)``.\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Note that both these manifolds, SO(N) and S^n are matrix manifolds, i.e. an element of mathcalM can be written as an element of mathbbR^NtimesN in the first case and mathbbR^(n+1)times1 in the second case. The additional conditions we impose on these manifolds are A^TA = mathbbI in the first case and x^Tx = 1 in the second case. Both of these manifolds belong to the category of Stiefel manifolds and therefore also to the bigger category of matrix manifolds, i.e. every element of SO(N) and of S^n can be represented as a matrix on which further conditions are imposed (e.g orthogonality).","category":"page"},{"location":"manifolds/manifolds/#The-Immersion-Theorem","page":"General Theory on Manifolds","title":"The Immersion Theorem","text":"","category":"section"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"The immersion theorem, similarly to the preimage theorem, gives another way of constructing a manifold based on a non-linear function. ","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.theorem(raw\"Consider a differentiable function ``\\mathcal{R}:\\mathcal{N}\\to\\mathcal{M}`` with ``\\mathrm{dim}(\\mathcal{M}) = N > n = \\mathrm{dim}(\\mathcal{N})`` tangent mapping ``T_x\\mathcal{R}`` has full rank at every point ``x\\in\\mathcal{N}``. Then ``\\mathcal{R}(\\mathcal{N})`` is a manifold *immersed* in ``\\mathcal{M}``.\"; name = \"Immersion Theorem\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"The proof is again based on the inverse function theorem.","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.proof(raw\"Consider a point ``x\\in\\mathcal{N},`` a coordinate chart ``\\varphi`` around ``x`` and a coordinate chart ``\\psi`` around ``f(x).`` We now define the function\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    F:(x_1, \\ldots, x_N) \\mapsto (\\psi\\circ{}f\\circ\\varphi^{-1}(x_1, \\ldots, x_n), x_{n+1}, \\ldots, x_N).\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"By the inverse function theorem we can find an inverse of ``F`` for a neighborhood around the point ``(x_1, \\ldots, x_n, 0, \\ldots, 0)\\in\\mathbb{R}^N.`` We call this neighborhood ``V = V_1\\times{}V_2`` and the inverse ``H.`` We now constrain ``V`` to the set ``V_1\\times{}0``, which is isomorphic to a neighborhood around ``x`` in ``\\mathbb{R}^n``. We then have in this neighborhood:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    H(\\psi\\circ{}f\\circ\\varphi^{-1}(x_1, \\ldots, x_n), 0, \\ldots, 0) = (x_1, \\ldots, x_n, 0, \\ldots, 0), \n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"And we can take \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    y \\mapsto \\pi\\circ{}H(\\psi(y), 0 \\ldots, 0)\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"as our coordinate chart. ``\\pi:\\mathbb{R}^N\\to\\mathbb{R}^n`` is the projection onto the first ``n`` coordinates.\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"We will use the immersion theorem when discussing the symplectic solution manifold.","category":"page"},{"location":"manifolds/manifolds/#Tangent-Spaces","page":"General Theory on Manifolds","title":"Tangent Spaces","text":"","category":"section"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"We already alluded to tangent spaces when talking about the preimage and the immersion theorems. Here we will give a precise definition. A tangent space can be seen as the collection of all possible velocities a curve can take at a point on a manifold. For this consider a manifold mathcalM and a point x on it and the collection of C^infty curves through x: ","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.definition(raw\"A mapping ``\\gamma:(-\\epsilon, \\epsilon)\\to\\mathcal{M}`` that is ``C^\\infty`` and for which we have ``\\gamma(0) = x`` is called a **``C^\\infty`` curve through ``x``**.\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"The tangent space of mathcalM at x is the collection of the first derivatives of all gamma: ","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.definition(raw\"The **tangent space** of ``\\mathcal{M}`` at ``x`` is the collection of all ``C^\\infty`` curves at ``x`` modulo the equivalence class ``\\gamma_1 \\sim \\gamma_2 \\iff \\gamma_1'(0) = \\gamma_2'(0)``. It is denoted by ``T_x\\mathcal{M}``.\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"As is customary we write gamma for the equivalence class of gamma and this is by definition equivalent to gamma(0). The tangent space T_xmathcalM can be shown to be homeomorphic[3] to mathbbR^n where n is the dimension of the manifold mathcalM. If the homeomorphism is constructed through the coordinate chart (varphi U) we call it varphi(x) or simply[4] varphi. If we are given a map gmathcalMtomathcalN we further define T_xg = (varphi)^-1circ(varphicircgcircpsi^-1)circpsi, i.e. a smooth map between two manifolds mathcalM and mathcalN induces a smooth map between the tangent spaces T_xmathcalM and T_g(x)mathcalN.","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"[3]: Note that we have not formally defined addition for T_xmathcalM. This can be done through the definition gamma + beta = alpha where alpha is any C^infty curve through x that satisfies alpha(0) = beta(0) + gamma(0). Note that we can always find such an alpha by the existence and uniqueness theorem.","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"[4]: We will further discuss this when we introduce the tangent bundle.","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"We want to demonstrate this principle of constructing the tangent space from curves through the example of S^2. We consider the following curves: ","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"gamma_1(t) = beginpmatrix 0  sin(t)  cos(t) endpmatrix\ngamma_2(t) = beginpmatrix sin(t)  0  cos(t) endpmatrix\ngamma_3(t) = beginpmatrix exp(-t ^ 2  2)  t sin(t)   exp(-t ^ 2  2) t cos(t)   sqrt1 - (t ^ 2) exp(-t^2) endpmatrix","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"We now plot the manifold S^2, the three curves described above and the associated tangent vectors (visualized as arrows). Note that the tangent vectors induced by gamma_1 and gamma_3 are the same; for these curves we have gamma_1 sim gamma_3 and the tangent vectors of those two curves coincide: ","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"using CairoMakie\nusing ForwardDiff\nusing LaTeXStrings\n\nfunction plot_curve!(ax, gamma::Function; epsilon_range::T = 1.4, epsilon_spacing::T = .01, kwargs...) where T\n    curve_domain = -epsilon_range : epsilon_spacing : epsilon_range\n    curve = zeros(T, 3, length(curve_domain))\n    for (i, t) in zip(axes(curve_domain, 1), curve_domain)\n        curve[:, i] .= gamma(t)\n    end\n    lines!(ax, curve[1, :], curve[2, :], curve[3, :]; kwargs...)\nend\n\nfunction plot_arrow!(ax, gamma::Function; kwargs...)\n    arrow_val = ForwardDiff.derivative(gamma, 0.)\n\n    gamma_vec = ([gamma(0)[1]], [gamma(0)[2]], [gamma(0)[3]])\n    gamma_deriv_vec = ([arrow_val[1]], [arrow_val[2]], [arrow_val[3]])\n\n    arrows!(ax, gamma_vec..., gamma_deriv_vec...; kwargs...)\nend\n\nfunction tangent_space(; n = 100)\n    xs = LinRange(-1.2, 1.2, n)\n    ys = LinRange(-1.2, 1.2, n)\n    zs = [one(x) * one(y) for x in xs, y in ys]\n    xs, ys, zs\nend\n\ngamma_1(t) = [zero(t), sin(t), cos(t)]\ngamma_2(t) = [sin(t), zero(t), cos(t)]\ngamma_3(t) = [exp(-t ^ 2 / 2) * (t ^ 1) * sin(t), exp(-t ^ 2 / 2) * (t ^ 1) * cos(t), sqrt(1 - (t ^ 2) * exp(-t^2))]\n\ncurves = (gamma_1, gamma_2, gamma_3)\n\nmorange = RGBf(255 / 256, 127 / 256, 14 / 256)\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256)\nmred = RGBf(214 / 256, 39 / 256, 40 / 256)\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256)\nmgreen = RGBf(44 / 256, 160 / 256, 44 / 256)\n\ncolors = (morange, mblue, mred)\n\nfunction make_plot(; theme = :light)\n    text_color = theme == :light ? :black : :white\n\n    fig = Figure(; backgroundcolor = :transparent)\n\n    ax = Axis3(fig[1, 1]; \n        backgroundcolor = :transparent, \n        aspect = (1., 1., 0.8), \n        azimuth = π / 6, \n        elevation = π / 8,\n        xlabel = L\"x_1\",\n        ylabel = L\"x_2\",\n        zlabel = L\"x_3\",\n        xlabelcolor = text_color,\n        ylabelcolor = text_color,\n        zlabelcolor = text_color,\n        )\n\n    surface!(Main.sphere(1., [0., 0., 0.])...; alpha = .6)\n\n    for (i, curve, color) in zip(1:length(curves), curves, colors)\n        plot_curve!(ax, curve; label = rich(\"γ\", subscript(string(i)); color = text_color, font = :italic), linewidth = 2, color = color)\n    end\n\n    surface!(ax, tangent_space()...; alpha = .2)\n    text!(.9, -.9, 1.; text = L\"T_x\\mathcal{M}\", color = text_color)\n\n    for (i, curve, color) in zip(1:length(curves), curves, colors)\n        plot_arrow!(ax, curve; linewidth = .03, color = color)\n    end\n\n    axislegend(; position = (.82, .75), backgroundcolor = :transparent, color = text_color)\n\n    fig, ax\nend\n\npx_per_unit = Main.output_type == :html ? 1.5 : 2\nsave(\"tangent_space_light.png\",        make_plot(; theme = :light)[1]; px_per_unit = px_per_unit)\nsave(\"tangent_space_dark.png\",   make_plot(; theme = :dark )[1]; px_per_unit = px_per_unit)\n\nnothing","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"(Image: Visualization of how the tangent space is constructed.) (Image: Visualization of how the tangent space is constructed.)","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"The tangent space T_xmathcalM for","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"x = beginpmatrix0  0  1 endpmatrix","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"is also shown. ","category":"page"},{"location":"manifolds/manifolds/#Vector-Fields","page":"General Theory on Manifolds","title":"Vector Fields","text":"","category":"section"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"A time-independent vector field[5] is an object that specifies a velocity for every point on a domain. We first give the definition of a vector field on the vector space mathbbR^n and limit ourselves here to C^infty vector fields:","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"[5]: Also called ordinary differential equation (ODE).","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.definition(raw\"A **vector field** on ``\\mathbb{R}^n`` is a smooth map ``X:\\mathbb{R}^n\\to\\mathbb{R}^n``.\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"The definition of a vector field on a manifold is not much more complicated: ","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Main.definition(raw\"A **vector field** on ``\\mathcal{M}`` is a map ``X`` defined on ``\\mathcal{M}`` such that ``X(x)\\in{}T_x\\mathcal{M}`` and ``\\varphi'\\circ{}X\\circ(\\varphi)^{-1}`` is smooth for any coordinate chart ``(\\varphi, U)`` that contains ``x``.\")","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"In the section on the existence-and-uniqueness theorem we show that every vector field has a unique solution given an initial condition; i.e. given a point xinmathcalM and a vector field X we can find a curve gamma such that gamma(0) = x and gamma(t) = X(gamma(t)) for all t in some interval (-epsilon epsilon).","category":"page"},{"location":"manifolds/manifolds/#The-Tangent-Bundle","page":"General Theory on Manifolds","title":"The Tangent Bundle","text":"","category":"section"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"To each manifold mathcalM we can associate another manifold which we call the tangent bundle and denote by TmathcalM. The points on this manifold are: ","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"TmathcalM =  (x v_x) xinmathcalM v_xinT_xmathcalM ","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Coordinate charts on this manifold can be constructed in a straightforward manner; for every coordinate chart varphi_U the map varphi_U(x) gives a homeomorphism between T_xmathcalM and mathbbR^n for any xinU. We can then find a neighborhood of any point (x v_x) by taking pi^-1(U) = (x v_x) xinU v_xinT_xmathcalM and this neighborhood is isomorphic to mathbbR^2n via (x v_x) mapsto (varphi_U(x) varphi(x)v_x). The geodesic spray is an important vector field defined on TmathcalM.","category":"page"},{"location":"manifolds/manifolds/#Library-Functions","page":"General Theory on Manifolds","title":"Library Functions","text":"","category":"section"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"Manifold\nrand(::Type{MT}, ::Integer, ::Integer) where MT <: Manifold\nrand(::GeometricMachineLearning.Backend, ::Type{MT}, ::Integer, ::Integer) where MT <: Manifold","category":"page"},{"location":"manifolds/manifolds/#GeometricMachineLearning.Manifold","page":"General Theory on Manifolds","title":"GeometricMachineLearning.Manifold","text":"Manifold <: AbstractMatrix\n\nA manifold in GeometricMachineLearning is a sutype of AbstractMatrix. All manifolds are matrix manifolds and therefore stored as matrices. More details can be found in the docstrings for the StiefelManifold and the GrassmannManifold.\n\n\n\n\n\n","category":"type"},{"location":"manifolds/manifolds/#Base.rand-Union{Tuple{MT}, Tuple{Type{MT}, Integer, Integer}} where MT<:Manifold","page":"General Theory on Manifolds","title":"Base.rand","text":"rand(manifold_type, N, n)\n\nDraw random elements from the Stiefel and the Grassmann manifold. \n\nBecause both of these manifolds are compact spaces we can sample them uniformly [18].\n\nExamples\n\nWhen we call ...\n\nusing GeometricMachineLearning\nusing GeometricMachineLearning: _round # hide\nimport Random\nRandom.seed!(123)\n\nN, n = 5, 3\nY = rand(StiefelManifold{Float32}, N, n)\n_round(Y; digits = 5) # hide\n\n# output\n\n5×3 StiefelManifold{Float32, Matrix{Float32}}:\n -0.27575   0.32991   0.77275\n -0.62485  -0.33224  -0.0686\n -0.69333   0.36724  -0.18988\n -0.09295  -0.73145   0.46064\n  0.2102    0.33301   0.38717\n\n... the sampling is done by first allocating a random matrix of size Ntimesn via Y = randn(Float32, N, n).\n\nWe then perform a QR decomposition Q, R = qr(Y) with the qr function from the LinearAlgebra package (this is using Householder reflections internally). \n\nThe final output are then the first n columns of the Q matrix. \n\n\n\n\n\n","category":"method"},{"location":"manifolds/manifolds/#Base.rand-Union{Tuple{MT}, Tuple{KernelAbstractions.Backend, Type{MT}, Integer, Integer}} where MT<:Manifold","page":"General Theory on Manifolds","title":"Base.rand","text":"rand(backend, manifold_type, N, n)\n\nDraw random elements for a specific device.\n\nExamples\n\nRandom elements of the manifold can be allocated on GPU.  Call ...\n\nrand(CUDABackend(), StiefelManifold{Float32}, N, n)\n\n... for drawing elements on a CUDA device.\n\n\n\n\n\n","category":"method"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"\\begin{comment}","category":"page"},{"location":"manifolds/manifolds/#References","page":"General Theory on Manifolds","title":"References","text":"","category":"section"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"P.-A. Absil, R. Mahony and R. Sepulchre. Optimization algorithms on matrix manifolds (Princeton University Press, Princeton, New Jersey, 2008).\n\n\n\n","category":"page"},{"location":"manifolds/manifolds/","page":"General Theory on Manifolds","title":"General Theory on Manifolds","text":"\\end{comment}","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"In this chapter we introduce a \\textit{general framework for manifold optimization} that is needed to efficiently train symplectic autoencoders. We start this chapter by discussing optimization for neural network in general and explain how we can generalize this to homogeneous spaces. We will see that an important ingredient for doing so are \\textit{retractions} which we then elaborate on. After discussing how to make the computation of retractions efficient for homogeneous spaces we conclude the chapter by introducing the notion of \\textit{parallel transport} which we need to extend the notion of \\textit{momentum} in neural network optimization.","category":"page"},{"location":"optimizers/optimizer_framework/#Neural-Network-Optimizers","page":"Optimizers","title":"Neural Network Optimizers","text":"","category":"section"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"In this section we present the general Optimizer framework used in GeometricMachineLearning. For more information on the particular steps involved in this consult the documentation on the various optimizer methods such as the gradient optimizer, the momentum optimizer and the Adam optimizer, and the documentation on retractions.","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"During optimization we aim at changing the neural network parameters in such a way to minimize the loss function. A loss function assigns a scalar value to the weights that parametrize the neural network:","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"    L mathbbPtomathbbR_geq0quad Theta mapsto L(Theta)","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"where mathbbP is the parameter space. We can then phrase the optimization task as: ","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"Main.definition(raw\"Given a neural network ``\\mathcal{NN}`` parametrized by ``\\Theta`` and a loss function ``L:\\mathbb{P}\\to\\mathbb{R}`` we call an algorithm an **iterative optimizer** (or simply **optimizer**) if it performs the following task:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\Theta \\leftarrow \\mathtt{Optimizer}(\\Theta, \\text{past history}, t),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"with the aim of decreasing the value ``L(\\Theta)`` in each optimization step.\")","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"The past history of the optimization is stored in a cache (AdamCache, MomentumCache, GradientCache, ldots) in GeometricMachineLearning.","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"Optimization for neural networks is (almost always) some variation on gradient descent. The most basic form of gradient descent is a discretization of the gradient flow equation:","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"dotTheta = -nabla_ThetaL","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"by means of an Euler time-stepping scheme: ","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"Theta^t+1 = Theta^t - hnabla_Theta^tL","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"where eta (the time step of the Euler scheme) is referred to as the learning rate. ","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"This equation can easily be generalized to manifolds with the following two steps:","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"modify -nabla_Theta^tLimplies-hmathrmgrad_Theta^tL i.e. replace the Euclidean gradient by a Riemannian gradient and\nreplace addition with the geodesic map.","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"To sum up, we then have:","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"Theta^t+1 = mathrmgeodesic(Theta^t -hmathrmgrad_Theta^tL)","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"In practice we very often do not use the geodesic map but approximations thereof. These approximations are called retractions.","category":"page"},{"location":"optimizers/optimizer_framework/#Generalization-to-Homogeneous-Spaces","page":"Optimizers","title":"Generalization to Homogeneous Spaces","text":"","category":"section"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"In order to generalize neural network optimizers to homogeneous spaces we utilize their corresponding global tangent space representation mathfrakg^mathrmhor. ","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"When introducing the notion of a global tangent space we discussed how an element of the tangent space T_YmathcalM can be represented in mathfrakg^mathrmhor by performing two mappings: ","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"the first one is the horizontal lift Omega (see the docstring for GeometricMachineLearning.Ω) and \nthe second one is performing the adjoint operation[1] of lambda(Y) the section of Y, on Omega(Delta) ","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"[1]: By the adjoint operation mathrmad_Amathfrakgtomathfrakg for an element AinG we mean B mapsto A^-1BA.","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"The two steps together are performed as global_rep in GeometricMachineLearning. So we lift to mathfrakg^mathrmhor:","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"mathttglobal_rep T_YmathcalM to mathfrakg^mathrmhor","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"and then perform all the steps of the optimizer in mathfrakg^mathrmhor We can visualize all the steps required in the generalization of the optimizers:","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"(Image: ) (Image: )","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"This picture summarizes all steps involved in an optimization step:","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"map the Euclidean gradient nablaLinmathbbR^Ntimesn that was obtained via automatic differentiation to the Riemannian gradient mathrmgradLinT_YmathcalM with the function rgrad,\nobtain the global tangent space representation of mathrmgradL in mathfrakg^mathrmhor with the function global_rep,\nperform an update!; this consists of two steps: (i) update the cache and (ii) output a final velocity,\nuse this final velocity to update the global section LambdainG\nuse the updated global section to update the neural network weight inmathcalM This is done with apply_section.","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"The cache stores information about previous optimization steps and is dependent on the optimizer. Typically the cache is represented as one or more elements in mathfrakg^mathrmhor. Based on this the optimizer method (represented by update! in the figure) computes a final velocity. This final velocity is again an element of mathfrakg^mathrmhor. The particular form of the cache and the updating rule depends on which optimizer method we use.","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"The final velocity is then fed into a retraction[2]. For computational reasons we split the retraction into two steps, referred to as \"Retraction\" and apply_section above. These two mappings together are equivalent to: ","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"[2]: A retraction is an approximation of the geodesic map","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"mathrmretraction(Delta) = mathrmretraction(lambda(Y)B^DeltaE) = lambda(Y)mathrmRetraction(B^Delta) ","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"where DeltainT_mathcalM and B^Delta is its representation in mathfrakg^mathrmhor as B^Delta = lambda(Y)^-1Omega(Delta)lambda(Y)","category":"page"},{"location":"optimizers/optimizer_framework/#Library-Functions","page":"Optimizers","title":"Library Functions","text":"","category":"section"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"Optimizer\noptimize_for_one_epoch!\noptimization_step!","category":"page"},{"location":"optimizers/optimizer_framework/#GeometricMachineLearning.Optimizer","page":"Optimizers","title":"GeometricMachineLearning.Optimizer","text":"Optimizer(method, cache, step, retraction)\n\nStore the method (e.g. AdamOptimizer with corresponding hyperparameters), the cache (e.g. AdamCache), the optimization step and the retraction.\n\nIt takes as input an optimization method and the parameters of a network. \n\nBefore one can call Optimizer a OptimizerMethod that stores all the hyperparameters of the optimizer needs to be specified. \n\nFunctor\n\nFor an instance o of Optimizer, we can call the corresponding functor as:\n\no(nn, dl, batch, n_epochs, loss)\n\nThe arguments are:\n\nnn::NeuralNetwork\ndl::DataLoader\nbatch::Batch\nn_epochs::Integer\nloss::NetworkLoss\n\nThe last argument is optional for many neural network architectures. We have the following defaults:\n\nA TransformerIntegrator uses TransformerLoss.\nA NeuralNetworkIntegrator uses FeedForwardLoss.\nAn AutoEncoder uses AutoEncoderLoss.\n\nIn addition there is an optional keyword argument that can be supplied to the functor:\n\nshow_progress=true: This specifies whether a progress bar should be shown during training.\n\nImplementation\n\nInternally the functor for Optimizer calls GlobalSection once at the start and then optimize_for_one_epoch! for each epoch.\n\n\n\n\n\n","category":"type"},{"location":"optimizers/optimizer_framework/#GeometricMachineLearning.optimize_for_one_epoch!","page":"Optimizers","title":"GeometricMachineLearning.optimize_for_one_epoch!","text":"optimize_for_one_epoch!(opt, model, ps, dl, batch, loss, λY)\n\nSample the data contained in dl according to batch and optimize for these batches.\n\nThis step also performs automatic differentiation on loss.\n\nThe output of optimize_for_one_epoch! is the average loss over all batches of the epoch:\n\noutput = frac1mathttsteps_per_epochsum_t=1^mathttsteps_per_epochmathttloss(theta^(t-1))\n\nThis is done because any reverse differentiation routine always has two outputs; for Zygote:\n\nloss_value, pullback = Zygote.pullback(ps -> loss(model, ps, input, output), ps)\n\nSo we get the value for the loss for free whenever we compute the pullback with AD.\n\nArguments\n\nAll the arguments are mandatory (there are no defaults): \n\nan instance of Optimizer.\nthe neural network model.\nthe neural network parameters ps.\nthe data (i.e. an instance of DataLoader).\nbatch::Batch: stores batch_size (and optionally seq_length and prediction_window).\nloss::NetworkLoss.\nthe section λY of the parameters ps.\n\nImplementation\n\nInternally this calls optimization_step! for each minibatch.\n\nThe number of minibatches can be determined with number_of_batches:\n\nusing GeometricMachineLearning\nusing GeometricMachineLearning: number_of_batches\n\ndata = [1, 2, 3, 4, 5]\nbatch = Batch(2)\ndl = DataLoader(data; suppress_info = true)\n\nnumber_of_batches(dl, batch)\n\n# output\n\n3\n\n\n\n\n\n","category":"function"},{"location":"optimizers/optimizer_framework/#GeometricMachineLearning.optimization_step!","page":"Optimizers","title":"GeometricMachineLearning.optimization_step!","text":"optimization_step!(o, λY, ps, dx)\n\nUpdate the weights ps based on an Optimizer, a cache and first-order derivatives dx.\n\noptimization_step! is calling update! internally.  update! has to be implemented for every OptimizerMethod.\n\nArguments\n\nAll arguments into optimization_step! are mandatory:\n\no::Optimizer,\nλY::NamedTuple: this named tuple has the same keys as ps, but contains GlobalSections,\nps::NamedTuple: the neural network parameters,\ndx::NamedTuple: the gradients stores as a NamedTuple.\n\nAll the arguments are given as NamedTuples  as the neural network weights are stores in that format.\n\nusing GeometricMachineLearning\nusing GeometricMachineLearning: params\n\nl = StiefelLayer(3, 5)\nps = params(NeuralNetwork(Chain(l), Float32)).L1\ncache = apply_toNT(MomentumCache, ps)\no = Optimizer(MomentumOptimizer(), cache, 0, geodesic)\nλY = GlobalSection(ps)\ndx = (weight = rand(Float32, 5, 3), )\n\n# call the optimizer\noptimization_step!(o, λY, ps, dx)\n\n_test_nt(x) = typeof(x) <: NamedTuple\n\n_test_nt(λY) & _test_nt(ps) & _test_nt(cache) & _test_nt(dx)\n\n# output\n\ntrue\n\nExtended help\n\nThe derivatives dx here are usually obtained via an AD routine by differentiating a loss function, i.e. dx is nabla_xL.\n\n\n\n\n\n","category":"function"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"\\begin{comment}","category":"page"},{"location":"optimizers/optimizer_framework/#References","page":"Optimizers","title":"References","text":"","category":"section"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"B. Brantner. Generalizing Adam To Manifolds For Efficiently Training Transformers, arXiv preprint arXiv:2305.16901 (2023).\n\n\n\n","category":"page"},{"location":"optimizers/optimizer_framework/","page":"Optimizers","title":"Optimizers","text":"\\end{comment}","category":"page"},{"location":"tutorials/softmax_comparison/#Comparing-Matrix-and-Vector-Softmax-as-Activation-Functions-in-a-Transformer","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"","category":"section"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"Transformers are usually build with VectorSoftmax as activation function, meaning that the activation function looks at each columns of a matrix (or tensor) independently:","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"mathrmsoftmax(v^1 v^2 ldots v^n) equiv left frace^v^1_isum_i=1^de^v^1_i frace^v^2_isum_i=1^de^v^2_i ldots frace^v^n_isum_i=1^de^v^n_i right","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"One can however also use a MatrixSoftmax:","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"mathrmMsoftmax(V) equiv frace^V_ijsum_ije^V_ij","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"using GeometricMachineLearning\n\nact1 = GeometricMachineLearning.VectorSoftmax()\nact2 = GeometricMachineLearning.MatrixSoftmax()\n\nA = [1 2 3; 1 2 3; 1 2 3]","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"act1(A)","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"act2(A)","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"We can now train transformers with these different activation functions in the MultiHeadAttention layers:","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"using GeometricProblems.CoupledHarmonicOscillator: hodeensemble, default_parameters\nusing GeometricIntegrators: ImplicitMidpoint, integrate # hide\nusing LaTeXStrings # hide\nusing CairoMakie  # hide\nCairoMakie.activate!() # hide\nimport Random # hide\nRandom.seed!(123) # hide\n\nconst tstep = .3\nconst n_init_con = 5\n\n# ensemble problem\nep = hodeensemble([rand(2) for _ in 1:n_init_con], [rand(2) for _ in 1:n_init_con]; tstep = tstep)\ndl = DataLoader(integrate(ep, ImplicitMidpoint()); suppress_info = true)\n\nnothing # hide","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"We now define the architectures and train them: ","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"const seq_length = 4\nconst batch_size = 1024\nconst n_epochs = 1000\n\narch1 = StandardTransformerIntegrator(dl.input_dim; transformer_dim = 20,\n                                                    n_heads = 4, \n                                                    L = 1, \n                                                    n_blocks = 2,\n                                                    attention_activation = act1)\n\narch2 = StandardTransformerIntegrator(dl.input_dim; transformer_dim = 20,\n                                                    n_heads = 4,\n                                                    L = 1,\n                                                    n_blocks = 2,\n                                                    attention_activation = act2)\n\nnn1 = NeuralNetwork(arch1)\nnn2 = NeuralNetwork(arch2)\n\no_method = AdamOptimizer()\n\no1 = Optimizer(o_method, nn1)\no2 = Optimizer(o_method, nn2)\n\nbatch = Batch(batch_size, seq_length)\n\nloss_array1 = o1(nn1, dl, batch, n_epochs; show_progress = false)\nloss_array2 = o2(nn2, dl, batch, n_epochs; show_progress = false)\n\nnothing # hide","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"morange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256) # hide\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256) # hide\nmgreen = RGBf(44 / 256, 160 / 256, 44 / 256) # hide\n\nfunction plot_training_losses(loss_array_vector_softmax, loss_array_matrix_softmax; theme = :dark)\n    textcolor = theme == :dark ? :white : :black\n    fig = Figure(; backgroundcolor = :transparent)\n    ax = Axis(fig[1, 1]; \n        backgroundcolor = :transparent,\n        bottomspinecolor = textcolor, \n        topspinecolor = textcolor,\n        leftspinecolor = textcolor,\n        rightspinecolor = textcolor,\n        xtickcolor = textcolor, \n        ytickcolor = textcolor,\n        xticklabelcolor = textcolor,\n        yticklabelcolor = textcolor,\n        xlabel=\"Epoch\", \n        ylabel=\"Training Loss\",\n        xlabelcolor = textcolor,\n        ylabelcolor = textcolor,\n        yscale = log10\n    )\n    lines!(ax, loss_array_vector_softmax, color = mpurple, label = \"VecSoftM\")\n    lines!(ax, loss_array_matrix_softmax,  color = mred, label = \"MatSoftM\")\n    axislegend(; position = (.82, .75), backgroundcolor = :transparent, labelcolor = textcolor)\n\n    fig, ax\nend\n\nfig_dark, ax_dark = plot_training_losses(loss_array1, loss_array2; theme = :dark)\nfig_light, ax_light = plot_training_losses(loss_array1, loss_array2; theme = :light)\n\nsave(\"softmax_comparison_dark.png\", fig_dark; px_per_unit = 1.2)\nsave(\"softmax_comparison_light.png\", fig_light; px_per_unit = 1.2)\n\nnothing","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"(Image: Training loss for the different networks.) (Image: Training loss for the different networks.)","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"We further evaluate a trajectory with the trained networks for 300 time steps: ","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"const index = 1\ninit_con = (q = dl.input.q[:, 1:seq_length, index], p = dl.input.p[:, 1:seq_length, index])\n\nconst n_steps = 300\n\nfunction make_validation_plot(n_steps = n_steps; theme = :dark)\n    textcolor = theme == :dark ? :white : :black\n    fig = Figure(; backgroundcolor = :transparent)\n    ax = Axis(fig[1, 1]; \n        backgroundcolor = :transparent,\n        bottomspinecolor = textcolor, \n        topspinecolor = textcolor,\n        leftspinecolor = textcolor,\n        rightspinecolor = textcolor,\n        xtickcolor = textcolor, \n        ytickcolor = textcolor,\n        xticklabelcolor = textcolor,\n        yticklabelcolor = textcolor,\n        xlabel=L\"t\", \n        ylabel=L\"q_1\",\n        xlabelcolor = textcolor,\n        ylabelcolor = textcolor,\n    )\n    prediction_vector = iterate(nn1, init_con; n_points = n_steps, prediction_window = seq_length)\n    prediction_matrix = iterate(nn2, init_con; n_points = n_steps, prediction_window = seq_length)\n\n    # we use linewidth  = 2\n    lines!(ax, dl.input.q[1, 1:n_steps, index]; color = mblue, label = \"Implicit midpoint\", linewidth = 2)\n    lines!(ax, prediction_vector.q[1, :]; color = mpurple, label = \"VecSoftM\", linewidth = 2)\n    lines!(ax, prediction_matrix.q[1, :]; color = mred, label = \"MatSoftM\", linewidth = 2)\n    axislegend(; position = (.55, .75), backgroundcolor = :transparent, labelcolor = textcolor)\n\n    fig, ax\nend\n\nfig_light, ax_light = make_validation_plot(n_steps; theme = :light)\nfig_dark, ax_dark = make_validation_plot(n_steps; theme = :dark)\nsave(\"softmax_comparison_validation_light.png\", fig_light; px_per_unit = 1.2)\nsave(\"softmax_comparison_validation_dark.png\", fig_dark; px_per_unit = 1.2)\n\nnothing","category":"page"},{"location":"tutorials/softmax_comparison/","page":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","title":"Comparing Matrix and Vector Softmax as Activation Functions in a Transformer","text":"(Image: Validation of the different networks.) (Image: Validation of the different networks.)","category":"page"},{"location":"outlook/#Conclusion","page":"Conclusion","title":"Conclusion","text":"","category":"section"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"\\pagestyle{plain}","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"In this dissertation it was shown how neural networks can be imbued with structure to improve their approximation capabilities when applied to physical systems. In the following we summarize the novelties of this work and give an outlook for how it can be expanded in the future.","category":"page"},{"location":"outlook/#Reduced-Order-Modeling-as-Motivation","page":"Conclusion","title":"Reduced Order Modeling as Motivation","text":"","category":"section"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"Most of the work presented in this dissertation is motivated by data-driven reduced order modeling. This is the discipline of building low-dimensional surrogate models from data that come from high-dimensional full order models. Both the low-dimensional surrogate model and the high-dimensional full order model are described by differential equations. When we talk about structure-preserving reduced order modeling we mean that the equation on the low-dimensional space shares features with the equation on the high-dimensional space. In this work these properties were mainly for the vector field to be symplectic or divergence-free. A typical reduced order modeling framework is further divided into two phases:","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"in the offline phase we find the low-dimensional surrogate model (reduced representation) and\nin the online phase we solve the equations in the reduced space.","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"For the offline phase we proposed symplectic autoencoders, and for the online phase we proposed volume-preserving transformers and linear symplectic transformers. In the following we summarize the three main methods that were developed in the course of this dissertation and constitute its main results: symplectic autoencoders, structure-preserving transformers and structure-preserving optimizers.","category":"page"},{"location":"outlook/#Structure-Preserving-Reduced-Order-Modeling-of-Hamiltonian-Systems-The-Offline-Phase","page":"Conclusion","title":"Structure-Preserving Reduced Order Modeling of Hamiltonian Systems - The Offline Phase","text":"","category":"section"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"A central part of this dissertation was the development of symplectic autoencoders [3]. Symplectic autoencoders build on existing approaches of symplectic neural networks (SympNets) [5] and proper symplectic decomposition (PSD) [68], both of which preserve symplecticity. SympNets can approximate arbitrary canonical symplectic maps in mathbbR^2n i.e.","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"    mathrmSympNet mathbbR^2n to mathbbR^2n","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"but the input has necessarily the same dimension as the output. PSD can change dimension, i.e.[0]","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"[0]: Here we only show the PSD encoder mathrmPSD^mathrmenc A complete reduced order modeling framework also need a decoder mathrmPSD^mathrmdec in addition to the encoder. When we use PSD both of these maps are linear, i.e. can be represented as (mathrmPSD^mathrmenc)^T mathrmPSD^mathrmdecinmathbbR^2Ntimes2n","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"    mathrmPSD^mathrmenc mathbbR^2N to mathbbR^2n","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"but is strictly linear. Symplectic autoencoders offer a way of (i) constructing nonlinear symplectic maps that (ii) can change dimension. We used these to reduce a 400-dimensional Hamiltonian system to a two-dimensional one[1]:","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"[1]: barH = HcircPsi^mathrmdec_theta_2mathbbR^2tomathbbR here refers to the induced Hamiltonian on the reduced space. SAE is short for symplectic autoencoder. ","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"(mathbbR^400 H) xRightarrowmathrmSAE^mathrmenc (mathbbR^2 barH)","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"For this case we observed speed-ups of up to a factor 1000 when a symplectic autoencoder was combined with a transformer in the online phase. We also compared the symplectic autoencoder to a PSD, and showed that the PSD was unable to learn a useful two-dimensional representation.","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"Like PSD, symplectic autoencoders have the property that they induce a Hamiltonian system on the reduced space. This distinguishes them from \"weakly symplectic autoencoders\" [70, 72] that only approximately obtain a Hamiltonian system on a restricted domain by using a \"physics-informed neural networks\" [71] approach.","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"We also mention that the development of symplectic autoencoders required generalizing existing neural network optimizers to manifolds[2]. This is further discussed below.","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"[2]: We also refer to optimizers that preserve manifold structure as structure-preserving optimizers.","category":"page"},{"location":"outlook/#Structure-Preserving-Neural-Network-Based-Integrators-The-Online-Phase","page":"Conclusion","title":"Structure-Preserving Neural Network-Based Integrators - The Online Phase","text":"","category":"section"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"For the online phase of reduced order modeling we developed new neural network architectures based on the transformer [54] which is a neural network architecture that is extensively used in other fields of neural network research such as natural language processing[3]. We used transformers to build an equivalent of structure-preserving multi-step methods [1].","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"[3]: The T in ChatGPT [95] stands for transformer.","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"The transformer consists of a composition of standard neural network layers and attention layers:","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"    mathrmTransformer(Z) = mathcalNN_ncircmathrmAttentionLayer_ncirccdotscircmathcalNN_1circmathrmAttentionLayer_1(Z)","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"where mathcalNN indicates a standard neural network layer (e.g. a multilayer perceptron). The attention layer makes it possible for a transformer to process time series data by acting on a whole series of vectors at once:","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"     mathrmAttentionLayer(Z) = mathrmAttentionLayer(z^(1) ldots z^(T)) = f^1(z^(1) ldots z^(T)) ldots f^T(z^(1) ldots z^(T))","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"The attention layer thus performs a preprocessing step after which the standard neural network layer mathcalNN is applied.","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"In this dissertation we presented two modifications of the standard transformer: the volume-preserving transformer [4] and the linear symplectic transformer. In both cases we modified the attention mechanism so that it is either volume-preserving (in the first case) or symplectic (in the second case). The standard neural network layer mathcalNN was replaced by a volume-preserving feedforward neural network or a symplectic neural network [5] respectively.","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"In this dissertation we applied the volume-preserving transformer for learning the trajectory of a rigid body and the linear symplectic transformer for learning the trajectory of a coupled harmonic oscillator. In both cases our new transformer architecture significantly outperformed the standard transformer. The trajectory modeled with the volume-preserving transformer for instance stays very close to a submanifold which is a level set of the quadratic invariant I(z_1 z_2 z_3) = z^2_1 + z^2_2 + z^2_3 This is not the case for the standard transformer: it moves away from this submanifold after a few time steps.","category":"page"},{"location":"outlook/#Structure-Preserving-Optimizers","page":"Conclusion","title":"Structure-Preserving Optimizers","text":"","category":"section"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"Training a symplectic autoencoder requires optimization on manifolds[4]. The particular manifolds we need in this case are \"homogeneous spaces\" [109]. In this dissertation we proposed a new optimizer framework that manages to generalize existing neural network optimizers to manifolds. This is done by identifying a global tangent space representation and dispenses with the need for a projection step as is necessary in other approaches [8, 110].","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"[4]: This is necessary to preserve the symplectic structure of the neural network.","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"As was already observed by others [8, 9, 111] putting weights on manifolds can improve training significantly in contexts other than scientific computing. Motivated by this we show an example of training a vision transformer [88] on the MNIST data set [90] to demonstrate the efficacy of the new optimizers. Contrary to other applications of the transformer we do not have to rely on layer normalization [112] or add connections to achieve convergent training for relatively big neural networks. We also applied the new optimizers to a neural network that contains weights on the Grassmann manifold to be able to sample from a nonlinear space.","category":"page"},{"location":"outlook/#Outlook","page":"Conclusion","title":"Outlook","text":"","category":"section"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"We believe that the topics structure-preserving autoencoders, structure-preserving transformers, structure-preserving optimizers and structure-preserving machine learning in general offer great potential for future research. ","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"Symplectic autoencoders could be used for model reduction of higher-dimensional systems [113] as well as using them for treating systems that are more general than canonical Hamiltonian ones; these include port-Hamiltonian [73] and metriplectic [114] systems. Structure-preserving model order reductions for such systems have been proposed [79, 115–117] but without using neural networks. In the appendix we sketch how symplectic autoencoders could be used for structure-preserving model reduction of port-Hamiltonian systems.","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"Structure-preserving transformers have shown great potential for learning dynamical systems, but their application should not be limited to that area. Structure-preserving machine learning techniques such as Hamilton Monte Carlo [118] has been used in various fields such as image classification [119] and inverse problems [120] and we believe that the structure-preserving transformers introduced in this work can also find applications in these fields, by replacing the activation function in the attention layers of a vision transformer for example.","category":"page"},{"location":"outlook/","page":"Conclusion","title":"Conclusion","text":"Lastly structure-preserving optimization is an exciting field, especially with regards to neural networks. The manifold optimizers introduced in this work can speed up neural network training significantly and are suitable for modern hardware (i.e. GPUs). They are however based on existing neural network optimizers such as Adam [108] and thus still lack a clear geometric interpretation. By utilizing a more geometric representation, as presented in this work, we hope to be able to find a differential equation describing Adam and other neural network optimizer, perhaps through a variational principle [121, 122]. One could also build on the existing optimization framework and use retractions other than the geodesic retraction and the Cayley retraction presented here; an example would be a QR-based retraction [123, 124]. This will be left for future work.","category":"page"},{"location":"architectures/volume_preserving_feedforward/#Volume-Preserving-Feedforward-Neural-Network","page":"Volume-Preserving FeedForward","title":"Volume-Preserving Feedforward Neural Network","text":"","category":"section"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"The volume-preserving feedforward neural network presented here can be seen as an adaptation of an LA-SympNet to the setting when we deal with a divergence-free vector field. It also serves as the feedforward module in the volume-preserving transformer. ","category":"page"},{"location":"architectures/volume_preserving_feedforward/#Neural-network-architecture","page":"Volume-Preserving FeedForward","title":"Neural network architecture","text":"","category":"section"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"The constructor for VolumePreservingFeedForward produces the following architecture[1]:","category":"page"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"[1]: Based on the input arguments n_linear and n_blocks. In this example init_upper is set to false, which means that the first layer is of type lower followed by a layer of type upper. ","category":"page"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"(Image: Visualization of how the keywords in the constructor are interpreted.) (Image: Visualization of how the keywords in the constructor are interpreted.)","category":"page"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"Here \"LinearLowerLayer\" performs ","category":"page"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"mathrmLinearLowerLayer_L x mapsto x + Lx","category":"page"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"and \"NonLinearLowerLayer\" performs ","category":"page"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"mathrmNonLinearLowerLayer_L x mapsto x + sigma(Lx + b) ","category":"page"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"The activation function sigma is the forth input argument to the constructor and tanh by default. We can make an instance of a VolumePreservingFeedForward neural network:","category":"page"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"using GeometricMachineLearning\n\nconst d = 3\n\narch = VolumePreservingFeedForward(d)\n\n@assert typeof(Chain(arch).layers[1]) <: VolumePreservingLowerLayer{3, 3, :no_bias, typeof(identity)} # hide\n@assert typeof(Chain(arch).layers[2]) <: VolumePreservingUpperLayer{3, 3, :bias, typeof(identity)} # hide\n@assert typeof(Chain(arch).layers[3]) <: VolumePreservingLowerLayer{3, 3, :bias, typeof(tanh)} # hide\n@assert typeof(Chain(arch).layers[4]) <: VolumePreservingUpperLayer{3, 3, :bias, typeof(tanh)} # hide\n@assert typeof(Chain(arch).layers[5]) <: VolumePreservingLowerLayer{3, 3, :no_bias, typeof(identity)} # hide\n@assert typeof(Chain(arch).layers[6]) <: VolumePreservingUpperLayer{3, 3, :bias, typeof(identity)} # hide\n\nfor layer in Chain(arch)\n    println(stdout, layer)\nend","category":"page"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"And we see that we get the same architecture as shown in the figure above, with the difference that the bias has been subsumed in the previous layers. Note that the nonlinear layers also contain a bias vector.","category":"page"},{"location":"architectures/volume_preserving_feedforward/#Note-on-Sympnets","page":"Volume-Preserving FeedForward","title":"Note on Sympnets","text":"","category":"section"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"In the general framework of feedforward neural networks SympNets are more restrictive than volume-preserving neural networks as symplecticity is a stronger property than volume-preservation:","category":"page"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"(Image: Symplectic neural networks are a more restrictive class of architectures than volume-preserving ones. But by construction they only work in even dimensions.) (Image: Symplectic neural networks are a more restrictive class of architectures than volume-preserving ones. But by construction they only work in even dimensions.)","category":"page"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"Note however that SympNets rely on data in canonical form, i.e. data that is of (q p) type (called GeometricMachineLearning.QPT in GeometricMachineLearning), so those data need to come from a vector space mathbbR^2n of even dimension. Volume-preserving feedforward neural networks also work for odd-dimensional spaces. This is also true for transformers: the volume-preserving transformer works in spaces of arbitrary dimension mathbbR^ntimesT, whereas the linear symplectic transformer only works in even-dimensional spaces mathbbR^2ntimesT.","category":"page"},{"location":"architectures/volume_preserving_feedforward/#Library-Functions","page":"Volume-Preserving FeedForward","title":"Library Functions","text":"","category":"section"},{"location":"architectures/volume_preserving_feedforward/","page":"Volume-Preserving FeedForward","title":"Volume-Preserving FeedForward","text":"VolumePreservingFeedForward","category":"page"},{"location":"architectures/volume_preserving_feedforward/#GeometricMachineLearning.VolumePreservingFeedForward","page":"Volume-Preserving FeedForward","title":"GeometricMachineLearning.VolumePreservingFeedForward","text":"VolumePreservingFeedForward(dim)\n\nMake an instance of a volume-preserving feedforward neural network for a specific system dimension.    \n\nThis architecture is a composition of VolumePreservingLowerLayer and VolumePreservingUpperLayer. \n\nArguments\n\nYou can provide the constructor with the following additional arguments:\n\nn_blocks::Int = 1: The number of blocks in the neural network (containing linear layers and nonlinear layers).\nn_linear::Int = 1: The number of linear VolumePreservingLowerLayers and VolumePreservingUpperLayers in one block.\nactivation = tanh: The activation function for the nonlinear layers in a block.\n\nThe following is a keyword argument:\n\ninit_upper::Bool = false: Specifies if the first layer is lower or upper. \n\n\n\n\n\n","category":"type"},{"location":"data_loader/TODO/#DATA-Loader-TODO","page":"DATA Loader TODO","title":"DATA Loader TODO","text":"","category":"section"},{"location":"data_loader/TODO/","page":"DATA Loader TODO","title":"DATA Loader TODO","text":"[x] Implement @views instead of allocating a new array in every step. \n[x] Implement sampling without replacement.\n[x] Store information on the epoch and the current loss. \n[x] Usually the training loss is computed over the entire data set, we are probably going to do this for one epoch via ","category":"page"},{"location":"data_loader/TODO/","page":"DATA Loader TODO","title":"DATA Loader TODO","text":"loss_e = frac1batchessum_batchinbatchesloss(batch)","category":"page"},{"location":"data_loader/TODO/","page":"DATA Loader TODO","title":"DATA Loader TODO","text":"Point 4 makes sense because the output of an AD routine is the value of the loss function as well as the pullback. ","category":"page"},{"location":"data_loader/data_loader/#The-Data-Loader","page":"Routines","title":"The Data Loader","text":"","category":"section"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"The DataLoader in GeometricMachineLearning is designed to make training convenient. ","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"The data loader can be called with various types of arrays as input, for example a snapshot matrix:","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"using GeometricMachineLearning # hide\nSnapshotMatrix = [ 1; 2;; 3; 4;; 5; 6;; 7; 8;; 9; 10 ]\n\ndl = DataLoader(SnapshotMatrix; suppress_info = true)","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"or a snapshot tensor: ","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"using GeometricMachineLearning # hide\nSnapshotTensor = [ 1;  2;; 3; 4;; 5; 6;; 7; 8;; 9; 10 ;;;]\n\ndl = DataLoader(SnapshotTensor; suppress_info = true)","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"Here the DataLoader has different properties :RegularData and :TimeSeries. This indicates that in the first case we treat all columns in the input tensor independently; this is mostly used for autoencoder problems. In the second case we have time series-like data, which are mostly used for integration problems. As shown above the default when using a matrix is :RegularData and the default when using a tensor is :TimeSeries.","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"We can also treat a problem with a matrix as input as a time series-like problem by providing an additional keyword argument: autoencoder=false:","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"dl = DataLoader(SnapshotMatrix; autoencoder=false, suppress_info = true)\n@assert typeof(dl) == DataLoader{Int64, Array{Int64, 3}, Nothing, :TimeSeries} # hide\ndl |> typeof","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"If we deal with hamiltonian systems we typically split the coordinates into a q and a p part. Such data can also be used as input arguments for DataLoader:","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"using GeometricMachineLearning # hide\nSymplecticSnapshotTensor = (q = SnapshotTensor, p = SnapshotTensor)\ndl = DataLoader(SymplecticSnapshotTensor)\n@assert typeof(dl) == DataLoader{Int64, @NamedTuple{q::Array{Int64, 3}, p::Array{Int64, 3}}, Nothing, :TimeSeries} # hide\ndl |> typeof","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"The dimension of the system is then the sum of the dimensions of the q and the p component:","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"@assert dl.input_dim == 4 # hide\ndl.input_dim","category":"page"},{"location":"data_loader/data_loader/#Drawing-Batches-with-GeometricMachineLearning","page":"Routines","title":"Drawing Batches with GeometricMachineLearning","text":"","category":"section"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"If we want to draw mini batches from a data set, we need to allocate an instance of Batch. If we call the corresponding functor on an instance of DataLoader we get the following result[1]:","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"[1]: We first demonstrate how to sample data on the example of a matrix. The case of sampling from a tensor is slightly more complicated and is explained below.","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"using GeometricMachineLearning # hide\nimport Random # hide\nRandom.seed!(123) # hide\nmatrix_data = [ 1 2 3 4  5;\n                6 7 8 9 10]\ndl = DataLoader(matrix_data; autoencoder = true)\n\nbatch = Batch(3)\nbatches = batch(dl)","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"The output of applying the batch functor is always of the form: ","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"((b_11^t b_11^p) (b_12^t b_12^p) ldots (b_21^t b_2 1^p) (b_2 2^t b_2 2^p) ldots (b_3 1^t b_3 2^p) ldots ldots)","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"so it is a tuple of vectors of tuples. One vector represents one batch:","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"for (minibatch, i) in zip(batches[1], axes(batches[1], 1))\n    println(stdout, minibatch[1], \" = bᵗ₁\" * join('₀' + d for d in digits(i)))\n    println(stdout, minibatch[2], \" = bᵖ₁\" * join('₀' + d for d in digits(i)))\n    println()\nend\nnothing # hide","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"The tuples that make up this vector always have two entries: a time index b^t_1i and a parameter index b^p_1i indicated by the superscripts t and p respectively. Because dl in this example is of autoencoder type, the time index is always one. The parameter index differs. Because the input to DataLoader was a 2times5 matrix and we specified a batch size of three, there are two batches in total. The second batch is:","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"@assert length(batches) == 2 # hide\n@assert length(batches[1]) + length(batches[2]) == 5 # hide\nfor (minibatch, i) in zip(batches[2], axes(batches[2], 1))\n    println(stdout, minibatch[1], \" = bᵗ₁\" * join('₀' + d for d in digits(i)))\n    println(stdout, minibatch[2], \" = bᵖ₁\" * join('₀' + d for d in digits(i)))\n    println()\nend\nnothing # hide","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"Looking at the first and the second batch together, we see that we sample with replacement, i.e. all indices b^p_1i = 1 2 3 4 5 appear. This also works if the data are in (q p) form:","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"using GeometricMachineLearning # hide\nqp_data = (q = rand(Float32, 2, 5), p = rand(Float32, 2, 5))\ndl = DataLoader(qp_data; autoencoder = true)\n\nbatch = Batch(3)\nbatch(dl)","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"In those two examples the autoencoder keyword was set to true (the default). This is why the first index was always 1. This changes if we set autoencoder = false: ","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"using GeometricMachineLearning # hide\nqp_data = (q = rand(Float32, 2, 5), p = rand(Float32, 2, 5))\ndl = DataLoader(qp_data; autoencoder = false) # false is default \n\nbatch = Batch(3)\nbatch(dl)","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"Specifically the sampling routines do the following: ","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"mathttn_indicesleftarrow mathttn_paramslormathttinput_time_steps \nmathttindices leftarrow mathttshuffle(mathtt1mathttn_indices)\nmathcalI_i leftarrow mathttindices(i - 1) cdot mathttbatch_size + 1 mathtt i cdot mathttbatch_sizetext for i=1 ldots (mathrmlast -1)\nmathcalI_mathttlast leftarrow mathttindices(mathttn_batches - 1) cdot mathttbatch_size + 1mathttend","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"Note that the routines are implemented in such a way that no two indices appear double, i.e. we sample without replacement. ","category":"page"},{"location":"data_loader/data_loader/#Sampling-from-a-tensor","page":"Routines","title":"Sampling from a tensor","text":"","category":"section"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"We can also sample from a tensor:","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"using GeometricMachineLearning # hide\nqp_data = (q = rand(Float32, 2, 5, 3), p = rand(Float32, 2, 5, 3))\ndl = DataLoader(qp_data)\n\n# also specify sequence length and a prediction window here\nbatch = Batch(4, 2, 0)\nbatch(dl)","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"Sampling from a tensor is done the following way (mathcalI_i again denotes the batch indices for the i-th batch): ","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"mathtttime_indices leftarrow mathttshuffle(mathtt1(mathttinput_time_steps - mathttseq_length - mathttprediction_window)\nmathttparameter_indices leftarrow mathttshuffle(mathtt1n_params)\nmathttcomplete_indices leftarrow mathttproduct(mathtttime_indices mathttparameter_indices)\nmathcalI_i leftarrow mathttcomplete_indices(i - 1) cdot mathttbatch_size + 1  i cdot mathttbatch_sizetext for i=1 ldots (mathrmlast -1)\nmathcalI_mathrmlast leftarrow mathttcomplete_indices(mathrmlast - 1) cdot mathttbatch_size + 1mathttend","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"Note that we supplied two additional arguments to the Batch constructor here: seq_length and prediction_window. These two arguments specify how many time are considered in one mini batch and how long the prediction runs into the future respectively. These two numbers are explained when we talk about structure on product spaces.","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"This algorithm can be visualized the following way (here batch_size = 4):","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"(Image: Visualization of sampling from a tensor. Here the batch size was specified as four, i.e. we sample four blocks.) (Image: Visualization of sampling from a tensor. Here the batch size was specified as four, i.e. we sample four blocks.)","category":"page"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"Here the sampling is performed over the second axis (the time step dimension) and the third axis (the parameter dimension). Whereas each block has thickness 1 in the x direction (i.e. pertains to a single parameter), its length in the y direction is seq_length. In total we sample as many such blocks as the batch size is big. By construction those blocks are never the same throughout a training epoch but may intersect each other!","category":"page"},{"location":"data_loader/data_loader/#Library-Functions","page":"Routines","title":"Library Functions","text":"","category":"section"},{"location":"data_loader/data_loader/","page":"Routines","title":"Routines","text":"DataLoader\nBatch\nBatch(::Int)\nBatch(::Int, ::Int, ::Int)\nGeometricMachineLearning.number_of_batches\nGeometricMachineLearning.convert_input_and_batch_indices_to_array","category":"page"},{"location":"data_loader/data_loader/#GeometricMachineLearning.DataLoader","page":"Routines","title":"GeometricMachineLearning.DataLoader","text":"DataLoader(data)\n\nMake an instance based on a data set.\n\nThis is designed to make training convenient.\n\nFields of DataLoader\n\nThe fields of the DataLoader struct are the following: \n\ninput: The input data with axes (i) system dimension, (ii) number of time steps and (iii) number of parameters.\noutput: The tensor that contains the output (supervised learning) - this may be of type Nothing if the constructor is only called with one tensor (unsupervised learning).\ninput_dim: The dimension of the system, i.e. what is taken as input by a regular neural network.\ninput_time_steps: The length of the entire time series (length of the second axis).\nn_params: The number of parameters that are present in the data set (length of third axis)\noutput_dim: The dimension of the output tensor (first axis). If output is of type Nothing, then this is also of type Nothing.\noutput_time_steps: The size of the second axis of the output tensor. If output is of type Nothing, then this is also of type Nothing.\n\nImplementation\n\nEven though DataLoader can be called with inputs of various forms, internally it always stores tensors with three axes.\n\nusing GeometricMachineLearning\n\ndata = [1 2 3; 4 5 6]\ndl = DataLoader(data)\ndl.input\n\n# output\n\n[ Info: You have provided a matrix as input. The axes will be interpreted as (i) system dimension and (ii) number of parameters.\n2×1×3 Array{Int64, 3}:\n[:, :, 1] =\n 1\n 4\n\n[:, :, 2] =\n 2\n 5\n\n[:, :, 3] =\n 3\n 6\n\n\n\n\n\n","category":"type"},{"location":"data_loader/data_loader/#GeometricMachineLearning.Batch","page":"Routines","title":"GeometricMachineLearning.Batch","text":"Batch\n\nBatch is a struct whose functor acts on an instance of DataLoader to produce a sequence of training samples for training for one epoch. \n\nSee Batch(::Int) and Batch(::Int, ::Int, ::Int) for the different constructors.\n\nThe functor\n\nAn instance of Batch can be called on an instance of DataLoader to produce a sequence of samples that contain all the input data, i.e. for training for one epoch. \n\nThe output of applying batch:Batch to dl::DataLoader is a tuple of vectors of integers. Each of these vectors contains two integers: the first is the time index and the second one is the parameter index.\n\nExamples\n\nConsider the following example for drawing batches of size 2 for an instance of DataLoader constructed with a vector:\n\nusing GeometricMachineLearning\nimport Random\n\nrng = Random.TaskLocalRNG()\nRandom.seed!(rng, 123)\n\ndl = DataLoader(rand(rng, 5))\nbatch = Batch(2)\n\nbatch(dl)\n\n# output\n\n[ Info: You have provided a matrix as input. The axes will be interpreted as (i) system dimension and (ii) number of parameters.\n([(1, 5), (1, 3)], [(1, 4), (1, 1)], [(1, 2)])\n\nHere the first index is always 1 (the time dimension). We get a total number of 3 batches.  The last batch is only of size 1 because we sample without replacement. Also see the docstring for DataLoader(::AbstractVector).\n\n\n\n\n\n","category":"type"},{"location":"data_loader/data_loader/#GeometricMachineLearning.Batch-Tuple{Int64}","page":"Routines","title":"GeometricMachineLearning.Batch","text":"Batch(batch_size)\n\nMake an instance of Batch for a specific batch size.\n\nThis is, among others, used to train neural networks of NeuralNetworkIntegrator type (as opposed to TransformerIntegrator).\n\n\n\n\n\n","category":"method"},{"location":"data_loader/data_loader/#GeometricMachineLearning.Batch-Tuple{Int64, Int64, Int64}","page":"Routines","title":"GeometricMachineLearning.Batch","text":"Batch(batch_size, seq_length)\n\nMake an instance of Batch for a specific batch size and a sequence length.\n\nThis is used to train neural networks of TransformerIntegrator type.\n\nOptionally the prediction window can also be specified by calling:\n\nusing GeometricMachineLearning\n\nbatch_size = 2\nseq_length = 3\nprediction_window = 2\n\nBatch(batch_size, seq_length, prediction_window)\n\n# output\n\nBatch{:Transformer}(2, 3, 2)\n\nNote that here the batch is of type :Transformer.\n\n\n\n\n\n","category":"method"},{"location":"data_loader/data_loader/#GeometricMachineLearning.number_of_batches","page":"Routines","title":"GeometricMachineLearning.number_of_batches","text":"number_of_batches(dl, batch)\n\nCompute the number of batches.\n\nHere the distinction is between data that are time-series like and data that are autoencoder like.\n\nExamples\n\nusing GeometricMachineLearning\nusing GeometricMachineLearning: number_of_batches\nimport Random\n\nRandom.seed!(123)\n\ndat = [1, 2, 3, 4, 5]\ndl₁ = DataLoader(dat; autoencoder = false, suppress_info = true) # time series-like\ndl₂ = DataLoader(dat; autoencoder = true, suppress_info = true) # autoencoder-like\nbatch = Batch(3)\n\nnob₁ = number_of_batches(dl₁, batch)\nnob₂ = number_of_batches(dl₂, batch)\nprintln(stdout, \"Number of batches of dl₁: \", nob₁)\nprintln(stdout, \"Number of batches of dl₂: \", nob₂)\nprintln(stdout, batch(dl₁), \"\\n\", batch(dl₂))\n\n# output\n\nNumber of batches of dl₁: 2\nNumber of batches of dl₂: 2\n([(1, 1), (4, 1), (2, 1)], [(3, 1)])\n([(1, 3), (1, 2), (1, 4)], [(1, 1), (1, 5)])\n\nHere we see that in the autoencoder case that last minibatch has an additional element.\n\n\n\n\n\n","category":"function"},{"location":"data_loader/data_loader/#GeometricMachineLearning.convert_input_and_batch_indices_to_array","page":"Routines","title":"GeometricMachineLearning.convert_input_and_batch_indices_to_array","text":"convert_input_and_batch_indices_to_array(dl, batch, batch_indices)\n\nAssign batch data based on (i) input and (ii) batch indices.\n\nExamples\n\nusing GeometricMachineLearning\n\ndl = DataLoader([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]; suppress_info = true)\nbatch = Batch(3)\nbatch_indices = [(1, 1), (1, 3), (1, 5)]\n\nGeometricMachineLearning.convert_input_and_batch_indices_to_array(dl, batch, batch_indices)\n\n# output\n\n1×1×3 Array{Float64, 3}:\n[:, :, 1] =\n 0.1\n\n[:, :, 2] =\n 0.3\n\n[:, :, 3] =\n 0.5\n\n\n\n\n\n","category":"function"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"% This is a summary of the manifold chapter; this is only visible in the latex version\nIn this chapter we introduce basic concepts necessary to discuss manifolds and manifold optimization. We begin by discussing \\textit{topological vector spaces} and \\textit{topological metric spaces}, and several theorems important for developing a theory of manifolds such as the \\textit{implicit function theorem}. We then define manifolds and discuss the \\textit{preimage theorem} and the \\textit{immersion theorem} as tools to give general spaces the structure of a manifold. We then proceed with a discussion on \\textit{geodesics} and \\textit{Riemannian manifolds}. The chapter concludes with a presentation of \\textit{homogeneous spaces} and their \\textit{global tangent space representation} that will be crucial for generalizing neural network optimizers to the manifold setting.","category":"page"},{"location":"manifolds/basic_topology/#Basic-Concepts-from-General-Topology","page":"Concepts from General Topology","title":"Basic Concepts from General Topology","text":"","category":"section"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Here we discuss basic notions of topology that are necessary to define manifolds and work with them. Here we largely omit concrete examples and only define concepts that are necessary for defining a manifold[1], namely the properties of being Hausdorff and second countable. For a detailed discussion of the theory and for a wide range of examples that illustrate this theory see e.g. [14]. The here-presented concepts are also (rudimentarily) covered in most differential geometry textbooks such as [15, 16]. ","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"[1]: Some authors (see e.g. [15]) do not require these properties. But since they constitute very weak restrictions and are always satisfied by the manifolds relevant for our purposes we require them here. ","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"We now start by giving all the definitions, theorem and corresponding proofs that are needed to define manifolds. Every manifold is a topological space which is why we give this definition first: ","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.definition(raw\"A **topological space** is a set ``\\mathcal{M}`` for which we are given a collection of subsets of ``\\mathcal{M}``, which we denote by ``\\mathcal{T}`` and call the *open subsets*. ``\\mathcal{T}`` further has to satisfy the following three conditions:\n\" *\nMain.indentation * raw\"1. The empty set and ``\\mathcal{M}`` belong to ``\\mathcal{T}``.\n\" *\nMain.indentation * raw\"2. Any union of an arbitrary number of elements of ``\\mathcal{T}`` again belongs to ``\\mathcal{T}``.\n\" *\nMain.indentation * raw\"3. Any intersection of a finite number of elements of ``\\mathcal{T}`` again belongs to ``\\mathcal{T}``.\n\" *\nMain.indentation * \"So an arbitrary union of open sets is again open and a finite intersection of open sets is again open.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Based on this definition of a topological space we can now define what it means to be Hausdorff: ","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.definition(raw\"A topological space ``\\mathcal{M}`` is said to be **Hausdorff** if for any two points ``x,y\\in\\mathcal{M}`` we can find two open sets ``U_x,U_y\\in\\mathcal{T}`` s.t. ``x\\in{}U_x, y\\in{}U_y`` and ``U_x\\cap{}U_y=\\{\\}``.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"We now give the second definition that we need for defining manifolds, that of second countability:","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.definition(raw\"A topological space ``\\mathcal{M}`` is said to be **second-countable** if we can find a countable subcollection of ``\\mathcal{T}`` called ``\\mathcal{U}`` s.t. ``\\forall{}U\\in\\mathcal{T}`` and ``x\\in{}U`` we can find an element ``V\\in\\mathcal{U}`` for which ``x\\in{}V\\subset{}U``.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"We now give a few definitions and results that are needed for the inverse function theorem which is essential for practical applications of manifold theory. We start with the definition of continuity: ","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.definition(raw\"A mapping ``f`` between topological spaces ``\\mathcal{M}`` and ``\\mathcal{N}`` is called **continuous** if the preimage of every open set is again an open set, i.e. if ``f^{-1}\\{U\\}\\in\\mathcal{T}`` for ``U`` open in ``\\mathcal{N}`` and ``\\mathcal{T}`` the topology on ``\\mathcal{M}``.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Continuity can also be formulated in terms of closed sets instead of doing it with open sets. The definition of closed sets is given below:","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.definition(raw\"A **closed set** of a topological space ``\\mathcal{M}`` is one whose complement is an open set, i.e. ``F`` is closed if ``F^c\\in\\mathcal{T}``, where the superscript ``{}^c`` indicates the complement: ``F^c := \\{x\\in\\mathcal{M}:x\\not\\in{}F\\}.`` For closed sets we thus have the following three properties:\n\" *\nMain.indentation * raw\"1. The empty set and ``\\mathcal{M}`` are closed sets.\n\" *\nMain.indentation * raw\"2. Any union of a finite number of closed sets is again closed.\n\" *\nMain.indentation * raw\"3. Any intersection of an arbitrary number of closed sets is again closed.\n\" *\nMain.indentation * \"So a finite union of closed sets is again closed and an arbitrary intersection of closed sets is again closed.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"We now give the definition of continuity in terms of closed sets: ","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.theorem(raw\"The definition of continuity in terms of open sets is equivalent to the following, second definition: ``f:\\mathcal{M}\\to\\mathcal{N}`` is continuous if ``f^{-1}\\{F\\}\\subset\\mathcal{M}`` is a closed set for each closed set ``F\\subset\\mathcal{N}``.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.proof(raw\"First assume that ``f`` is continuous according to the first definition and not to the second. Then ``f^{-1}\\{F\\}`` is not closed but ``f^{-1}\\{F^c\\}`` is open. But ``f^{-1}\\{F^c\\} = \\{x\\in\\mathcal{M}:f(x)\\not\\in\\mathcal{N}\\} = (f^{-1}\\{F\\})^c`` cannot be open, else ``f^{-1}\\{F\\}`` would be closed. The implication of the first definition under assumption of the second can be shown analogously.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"The next theorem makes the rather abstract definition of closed sets more concrete; this definition is especially important for many practical proofs:","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.theorem(raw\"The property of a set ``F`` being closed is equivalent to the following statement: If a point ``y`` is such that for every open set ``U`` containing it we have ``U\\cap{}F\\ne\\{\\}`` then this point is contained in ``F``.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.proof(raw\"We first proof that if a set is closed then the statement holds. Consider a closed set ``F`` and a point ``y\\not\\in{}F`` s.t. every open set containing ``y`` has nonempty intersection with ``F``. But the complement ``F^c`` also is such a set, which is a clear contradiction. Now assume the above statement for a set ``F`` and further assume ``F`` is not closed. Its complement ``F^c`` is thus not open. Now consider the *interior* of this set: ``\\mathrm{int}(F^c):=\\cup\\{U:U\\subset{}F^c\\text{ and $U$ open}\\}``, i.e. the biggest open set contained within ``F^c``. Hence there must be a point ``y`` which is in ``F^c`` but is not in its interior, else ``F^c`` would be equal to its interior, i.e. would be open. We further must be able to find an open set ``U`` that contains ``y`` but is also contained in ``F^c``, else ``y`` would be an element of ``F``. A contradiction.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Next we define open covers, a concept that is very important in developing a theory of manifolds: ","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.definition(raw\"An **open cover** of a topological space ``\\mathcal{M}`` is a (not necessarily countable) collection of open sets ``\\{U_i\\}_{i\\mathcal{I}}`` s.t. their union contains ``\\mathcal{M}``. A **finite open cover** is a finite collection of open sets that cover ``\\mathcal{M}``. We say that an open cover is **reducible** to a finite cover if we can find a finite number of elements in the open cover whose union still contains ``\\mathcal{M}``.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"And connected to this definition we state what it means for a topological space to be compact. This is a rather strong property that some of the manifolds treated in here have, for example the Stiefel manifold.","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.definition(raw\"A topological space ``\\mathcal{M}`` is called **compact** if every open cover is reducible to a finite cover.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"A very important result from general topology is that continuous functions preserve compactness[2]: ","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"[2]: We also say that compactness is a topological property [14].","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.theorem(raw\"Consider a continuous function ``f:\\mathcal{M}\\to\\mathcal{N}`` and a compact set ``K\\in\\mathcal{M}``. Then ``f(K)`` is also compact.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.proof(raw\"Consider an open cover of ``f(K)``: ``\\{U_i\\}_{i\\in\\mathcal{I}}``. Then ``\\{f^{-1}\\{U_i\\}\\}_{i\\in\\mathcal{I}}`` is an open cover of ``K`` and hence reducible to a finite cover ``\\{f^{-1}\\{U_i\\}\\}_{i\\in\\{i_1,\\ldots,i_n\\}}``. But then ``\\{{U_i\\}_{i\\in\\{i_1,\\ldots,i_n}}`` also covers ``f(K)``.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Moreover compactness is a property that is inherited by closed subspaces:","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.theorem(raw\"A closed subset of a compact space is compact.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.proof(raw\"Call the closed set ``F`` and consider an open cover of this set: ``\\{U\\}_{i\\in\\mathcal{I}}``. Then this open cover combined with ``F^c`` is an open cover for the entire compact space, hence reducible to a finite cover.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"If a set is contained in a Hausdorff space and is also compact we have:","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.theorem(raw\"A compact subset of a Hausdorff space is closed.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.proof(raw\"Consider a compact subset ``K``. If ``K`` is not closed, then there has to be a point ``y\\not\\in{}K`` s.t. every open set containing ``y`` intersects ``K``. Because the surrounding space is Hausdorff we can now find the following two collections of open sets: ``\\{(U_z, U_{z,y}: U_z\\cap{}U_{z,y}=\\{\\})\\}_{z\\in{}K}``. The open cover ``\\{U_z\\}_{z\\in{}K}`` is then reducible to a finite cover ``\\{U_z\\}_{z\\in\\{z_1, \\ldots, z_n\\}}``. The intersection ``\\cap_{z\\in{z_1, \\ldots, z_n}}U_{z,y}`` is then an open set that contains ``y`` but has no intersection with ``K``. A contraction.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"This last theorem we will use in proofing the inverse function theorem:","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.theorem(raw\"If ``\\mathcal{M}`` is compact and ``\\mathcal{N}`` is Hausdorff, then the inverse of a continuous injective function ``f:\\mathcal{M}\\to\\mathcal{N}`` is again continuous, i.e. ``f(V)`` is an open set in ``\\mathcal{N}`` for ``V\\in\\mathcal{T}``.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.proof(raw\"We can equivalently show that every closed set is mapped to a closed set. First consider the set ``K\\in\\mathcal{M}``. Its image is again compact and hence closed because ``\\mathcal{N}`` is Hausdorff.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"We further define what it means for a set to be dense:","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"Main.definition(raw\"A set ``U`` is called **dense in ``D``**, where ``U\\subset{}D`` if the *closure of ``U``*, i.e. the smallest closed set containing ``U``, also contains ``D``.\")","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"We will come back to the notion of denseness when talking about the universal approximation theorem for SympNets.","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"\\begin{comment}","category":"page"},{"location":"manifolds/basic_topology/#References","page":"Concepts from General Topology","title":"References","text":"","category":"section"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"references = raw\"\"\"","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"S. Lipschutz. General Topology (McGraw-Hill Book Company, New York City, New York, 1965).\n\n\n\nS. Lang. Fundamentals of differential geometry. Vol. 191 (Springer Science & Business Media, 2012).\n\n\n\nS. I. Richard L. Bishop. Tensor Analysis on Manifolds (Dover Publications, Mineola, New York, 1980).\n\n\n\n","category":"page"},{"location":"manifolds/basic_topology/","page":"Concepts from General Topology","title":"Concepts from General Topology","text":"\\end{comment}","category":"page"},{"location":"layers/linear_symplectic_attention/#Linear-Symplectic-Attention","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"","category":"section"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"The attention layer introduced here can be seen as an extension of the SympNet gradient layer to the setting where we deal with time series data. Before we introduce the LinearSymplecticAttention layer we first define a notion of symplecticity for multi-step methods. ","category":"page"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"This definition is different from [58, 59], but similar to the definition of volume-preservation for product spaces in [4].","category":"page"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"Main.definition(raw\"\"\"\nA multi-step method ``\\varphi\\times_T\\mathbb{R}^{2n}\\to\\times_T\\mathbb{R}^{2n}`` is called **symplectic** if it preserves the the symplectic product structure, i.e. if ``\\hat{\\varphi}`` is symplectic.\"\"\")","category":"page"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"Main.remark(raw\"The **symplectic product structure** is the following skew-symmetric non-degenerate bilinear form: \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\hat{\\mathbb{J}}([z^{(1)}, \\ldots, z^{(T)}], [\\tilde{z}^{(1)}, \\ldots, \\tilde{z}^{(T)}]) := \\sum_{i=1}^T (z^{(i)})^T\\mathbb{J}_{2n}\\tilde{z}^{(i)}.\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"``\\hat{\\mathbb{J}}`` is defined through the isomorphism between the product space and the space of big vectors \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\hat{}: \\times_\\text{($T$ times)}\\mathbb{R}^{d}\\stackrel{\\approx}{\\longrightarrow}\\mathbb{R}^{dT},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"so we induce the symplectic structure on the product space through the pullback of this isomorphism.\")","category":"page"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"In order to construct a symplectic attention mechanism we extend the principle behind the SympNet gradient layer, i.e. we construct scalar functions that only depend on q^(1) ldots q^(T) or p^(1) ldots p^(T). The specific choice we make here is the following: ","category":"page"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"F(q^(1) ldots q^(T)) = frac12mathrmTr(QAQ^T)","category":"page"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"where Q = q^(1) ldots q^(T) is the concatenation of the vectors into a matrix. We therefore have for the gradient:","category":"page"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"nabla_Qf = frac12Q(A + A^T) = QbarA","category":"page"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"where barAinmathcalS_mathrmsym(T) is a symmetric matrix. So the map performs:","category":"page"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"q^(1) ldots q^(T) mapsto left sum_i=1^Ta_1iq^(i) ldots sum_i=1^Ta_Tiq^(i) right text for  a_ji = barA_ji","category":"page"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"Note that there is still a reweighting of the input vectors performed with this linear symplectic attention, like in standard attention and volume-preserving attention, but the crucial difference is that the coefficients a_ji here are fixed and not computed as the result of a softmax or a Cayley transform. We hence call this attention mechanism linear symplectic attention as it performs a linear reweighting of the input vectors. We distinguish it from the standard attention mechanism, which computes coefficients that depend on the input nonlinearly.","category":"page"},{"location":"layers/linear_symplectic_attention/#Library-Functions","page":"Linear Symplectic Attention","title":"Library Functions","text":"","category":"section"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"LinearSymplecticAttention\nLinearSymplecticAttentionQ\nLinearSymplecticAttentionP","category":"page"},{"location":"layers/linear_symplectic_attention/#GeometricMachineLearning.LinearSymplecticAttention","page":"Linear Symplectic Attention","title":"GeometricMachineLearning.LinearSymplecticAttention","text":"LinearSymplecticAttention\n\nImplements the linear symplectic attention layers. Analogous to GradientLayer it performs mappings that only change the Q or the P part.\n\nThis layer preserves symplecticity in the product-space sense.\n\nFor more information see LinearSymplecticAttentionQ and LinearSymplecticAttentionP.\n\nImplementation\n\nThe coefficients of a LinearSymplecticAttention layer is a SymmetricMatrix:\n\nusing GeometricMachineLearning\nusing GeometricMachineLearning: params\n\nl = LinearSymplecticAttentionQ(3, 5)\nps = params(NeuralNetwork(Chain(l))).L1\n\ntypeof(ps.A) <: SymmetricMatrix\n\n# output\n\ntrue\n\n\n\n\n\n","category":"type"},{"location":"layers/linear_symplectic_attention/#GeometricMachineLearning.LinearSymplecticAttentionQ","page":"Linear Symplectic Attention","title":"GeometricMachineLearning.LinearSymplecticAttentionQ","text":"LinearSymplecticAttentionQ(sys_dim, seq_length)\n\nMake an instance of LinearSymplecticAttentionQ for a specific dimension and sequence length.\n\nPerforms: \n\nbeginpmatrix Q  P endpmatrix mapsto beginpmatrix Q + nabla_PF  P endpmatrix\n\nwhere Q PinmathbbR^ntimesT and F(P) = frac12mathrmTr(P A P^T).\n\nThe parameters of this layer are barA = frac12(A + A^T)\n\n\n\n\n\n","category":"type"},{"location":"layers/linear_symplectic_attention/#GeometricMachineLearning.LinearSymplecticAttentionP","page":"Linear Symplectic Attention","title":"GeometricMachineLearning.LinearSymplecticAttentionP","text":"LinearSymplecticAttentionP(sys_dim, seq_length)\n\nMake an instance of LinearSymplecticAttentionP for a specific dimension and sequence length.\n\nPerforms: \n\nbeginpmatrix Q  P endpmatrix mapsto beginpmatrix Q  P + nabla_QF endpmatrix\n\nwhere Q PinmathbbR^ntimesT and F(Q) = frac12mathrmTr(Q A Q^T).\n\nThe parameters of this layer are barA = frac12(A + A^T)\n\n\n\n\n\n","category":"type"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"\\section*{Chapter Summary}\n\nIn this chapter we discussed various neural network layers and the corresponding application interface in \\texttt{GeometricMachineLearning}. Some of these layers constitute novel work (like the volume-preserving attention layer and the linear symplectic layer) and others were established before (such as SympNet layers and multihead attention). Volume-preserving attention and linear symplectic attention were designed as a modification of standard attention in order to imbue the corresponding neural network with structure (volume preservation and symplecticity respectively). Volume-preserving attention was achieved by exchanging the softmax activation function with a \\textit{Cayley activation function}, but otherwise keeping the operations the same, i.e. we still perform \\textit{multiplicative attention}. In order to make the attention layer symplectic we had to impose more severe restrictions on it: the attention mechanism does not compute a scalar product in this case. We therefore call it \\textit{linear}.\n\nIn the next chapter we use these neural network layers to build \\textit{neural network architectures}.","category":"page"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"<!--","category":"page"},{"location":"layers/linear_symplectic_attention/#References","page":"Linear Symplectic Attention","title":"References","text":"","category":"section"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"P. Jin, Z. Zhang, A. Zhu, Y. Tang and G. E. Karniadakis. SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems. Neural Networks 132, 166–179 (2020).\n\n\n\nD. Bahdanau, K. Cho and Y. Bengio. Neural machine translation by jointly learning to align and translate, arXiv preprint arXiv:1409.0473 (2014).\n\n\n\nM.-T. Luong, H. Pham and C. D. Manning. Effective approaches to attention-based neural machine translation, arXiv preprint arXiv:1508.04025 (2015).\n\n\n\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and I. Polosukhin. Attention is all you need. Advances in neural information processing systems 30 (2017).\n\n\n\n","category":"page"},{"location":"layers/linear_symplectic_attention/","page":"Linear Symplectic Attention","title":"Linear Symplectic Attention","text":"-->","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"% introductory remarks for the latex version\nA central topic of this dissertation is \\textit{structure preservation}. In this chapter we first discuss the notions of \\textit{symplecticity} and \\textit{volume preservation}. After this we introduce the concept of \\textit{structure-preserving neural networks}.","category":"page"},{"location":"structure_preservation/symplecticity/#Symplectic-Systems","page":"Symplecticity","title":"Symplectic Systems","text":"","category":"section"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"Symplectic systems are ODEs whose vector field has a specific structure that is very restrictive. Before we introduce symplectic vector fields on manifolds we first have to define what a symplectic structure is:","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"Main.definition(raw\"A **symplectic structure** or **symplectic 2-form** ``\\Omega`` assigns to each ``x\\in\\mathcal{M}`` a mapping \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\Omega_x:T_x\\mathcal{M}\\times{}T_x\\mathcal{M} \\to \\mathbb{R},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"such that ``\\Omega_x`` is skew-symmetric and nondegenerate, and ``\\Omega`` is closed.\")","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"We forego the precise definition of closedness because it would require us to introduce differential forms [16, 27]. This property is also closely related to the Jacobi identity [28, Chapter 4.4]. After having defined a symplectic structure, we can introduce Hamiltonian vector fields[1]:","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"[1]: Also compare this to the definition of the Riemannian gradient.","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"Main.definition(raw\"A **Hamiltonian vector field** at ``x\\in\\mathcal{M}`` corresponding to the function ``H:\\mathcal{M}\\to\\mathbb{R}`` (called **the Hamiltonian**) is a vector field that has the following property:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\Omega(X_H, \\dot{\\gamma}(0)) = \\frac{d}{dt}\\bigg|_{t = 0}H(\\gamma(t)),\n\" * Main.indentation * raw\"\\Omega(X_H, \\dot{\\gamma}(0)) = \\frac{d}{dt}\\bigg|_{t = 0}H(\\gamma(t)),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``\\gamma`` is a ``C^\\infty`` curve through ``x``.\")","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"Of particular importance for us here are canonical Hamiltonian systems:","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"Main.example(raw\"To obtain a canonical Hamiltonian system we take ``\\mathcal{M} = \\mathbb{R}^{2d}`` and \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\Omega_z = \\mathbb{J}_{2d}^T = \\begin{pmatrix} \\mathbb{O} & -\\mathbb{I} \\\\ \\mathbb{I} & \\mathbb{O} \\end{pmatrix}\n\" * Main.indentation * raw\"``` \n\" * Main.indentation * raw\"for all ``z``. We call ``\\mathbb{J}_{2d}`` the **Poisson tensor**. In this case the vector field can be written as:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"X_H(z) = \\mathbb{J}_{2d}\\nabla_z{}H,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``\\nabla{}H`` is the Euclidean gradient.\")","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"Typically we further split the z coordinates on mathbbR^2d into (q p) coordinates, where q and p are the first d components of z and the second d components of z respectively, i.e. ","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"z = beginpmatrix q  p endpmatrix","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"We can then reformulate a Hamiltonian vector field as two separate vector fields:","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"beginaligned\n    dotq  = fracpartialHpartialp quadtextand \n    dotp  = - fracpartialHpartialq\nendaligned","category":"page"},{"location":"structure_preservation/symplecticity/#Solution-of-Symplectic-Systems","page":"Symplecticity","title":"Solution of Symplectic Systems","text":"","category":"section"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"The flow of a Hamiltonian ODE has very restrictive properties, the most important one of these is called symplecticity [1]. This property dramatically restricts the dynamically accessible states of the flow map. For a canonical Hamiltonian system symplecticity is defined as follows:","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"Main.definition(raw\"A map ``\\phi:\\mathbb{R}^{2d}\\to\\mathbb{R}^{2d}`` is called **symplectic** on ``U\\subset\\mathbb{R}^{2d}`` if\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    (\\nabla_z\\phi)^T\\mathbb{J}_{2d}\\nabla_z\\phi = \\mathbb{J}_{2d},\n\" * Main.indentation * raw\"    (\\nabla_z\\phi)^T\\mathbb{J}_{2d}\\nabla_z\\phi = \\mathbb{J}_{2d},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"for all ``z\\in{}U.``\")","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"A similar definition of symplecticity holds for the general case of symplectic manifolds (mathcalM Omega) [27]. The following holds:","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"Main.theorem(raw\"The flow of a Hamiltonian system is symplectic\")","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"Main.proof(raw\"We proof this statement only for canonical Hamiltonian systems here. Consider the flow of a Hamiltonian ODE ``\\varphi^t:\\mathbb{R}\\to\\mathbb{R}``. For this we have:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\begin{aligned}\n\" * Main.indentation * raw\"    \\frac{d}{dt}\\left( (\\nabla\\varphi^t)^T\\mathbb{J}\\nabla\\varphi^t \\right)  & = (\\mathbb{J}\\nabla^2H)^T\\mathbb{J}\\nabla\\varphi^t + (\\nabla\\varphi^t)^T\\mathbb{J}\\mathbb{J}\\nabla^2H \\\\\n\" * Main.indentation * raw\"        & = \\nabla^2H\\nabla\\varphi^t - (\\nabla\\varphi^t)^T\\nabla^2H,\n\" * Main.indentation * raw\"\\end{aligned}\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"and this expression is zero at ``t=0.`` This computation holds for all points on the domain on which ``H`` is defined and ``\\varphi^t`` is symplectic.\")","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"The discipline of finding numerical approximations of flows varphi^t such that these numerical approximations also preserve certain properties of that flow (such as symplecticity) is referred to as structure-preserving numerical integration or geometric numerical integration [1]. The Julia library GeometricIntegrators [2] offers a wide array of such geometric numerical integrators for a broad class of systems (not just canonical Hamiltonian systems).","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"In addition to being symplectic, the flow of a Hamiltonian vector field is also energy-preserving:","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"Main.theorem(raw\"The flow of a Hamiltonian vector field preserves the Hamiltonian ``H,`` i.e. \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    H(\\varphi^t(z_0)) = H(z_0),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"for all ``t\\in[0, T].``\")","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"Main.proof(raw\"We differentiate ``H(\\varphi^t(z_0))`` with respect to ``t``:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\frac{d}{dt}H(\\varphi^t(z_0)) = (\\nabla_{\\varphi^t(z_0)}H)^T\\frac{d}{dt}\\varphi^t(z_0) = (\\nabla_{\\varphi^t(z_0)}H)^T\\mathbb{J}\\nabla_{\\varphi^t(z_0)}H = \\mathbb{O},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"because ``\\mathbb{J}`` is skew-symmetric.\")","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"It is important to note that symplecticity is a very strong property[2] that may not be achievable in some practical applications. If preservation of symplecticity is not achievable, it may however still be advantageous to consider weaker properties such as volume preservation.","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"[2]: Symplecticity imposes in general greater restrictions on the flow map than e.g. conservation of energy. The famous Ge-Marsden theorem [29] says that one cannot achieve preservation of energy and preservation of symplecticity at the same time, unless the numerical method is the exact integral of the Hamiltonian ODE. In practice we hence almost always choose a symplectic over an energy-preserving scheme.","category":"page"},{"location":"structure_preservation/symplecticity/#Library-Functions","page":"Symplecticity","title":"Library Functions","text":"","category":"section"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"PoissonTensor\nGeometricMachineLearning.QPT\nGeometricMachineLearning.QPTOAT","category":"page"},{"location":"structure_preservation/symplecticity/#GeometricMachineLearning.PoissonTensor","page":"Symplecticity","title":"GeometricMachineLearning.PoissonTensor","text":"PoissonTensor(n2)\n\nReturns a (canonical) Poisson tensor of size 2ntimes2n:\n\nmathbbJ_2n = beginpmatrix\nmathbbO  mathbbI_n \n-mathbbI_n  mathbbO \nendpmatrix\n\nArguments\n\nIt can also be called with a backend and a type:\n\nusing GeometricMachineLearning\n\nbackend = CPU()\nT = Float16\n\nPoissonTensor(backend, 4, T)\n\n# output\n\n4×4 PoissonTensor{Float16, Matrix{Float16}}:\n  0.0   0.0  1.0  0.0\n  0.0   0.0  0.0  1.0\n -1.0   0.0  0.0  0.0\n  0.0  -1.0  0.0  0.0\n\n\n\n\n\n","category":"type"},{"location":"structure_preservation/symplecticity/#GeometricMachineLearning.QPT","page":"Symplecticity","title":"GeometricMachineLearning.QPT","text":"QPT\n\nThe type for data in (q p) coordinates. It encompasses various array types.\n\nExamples\n\nusing GeometricMachineLearning: QPT\n\n# allocate two vectors\ndata1 = (q = rand(5), p = rand(5))\n\n# allocate two matrices\ndata2 = (q = rand(5, 4), p = rand(5, 4))\n\n# allocate two tensors\ndata3 = (q = rand(5, 4, 2), p = rand(5, 4, 2))\n\n(typeof(data1) <: QPT, typeof(data2) <: QPT, typeof(data3) <: QPT)\n\n# output\n\n(true, true, true)\n\nWe can also do:\n\nusing GeometricMachineLearning: QPT, PoissonTensor\n\n𝕁 = PoissonTensor(4)\nqp = (q = [1, 2], p = [3, 4])\n\n𝕁 * qp\n\n# output\n\n(q = [3, 4], p = [-1, -2])\n\n\n\n\n\n","category":"type"},{"location":"structure_preservation/symplecticity/#GeometricMachineLearning.QPTOAT","page":"Symplecticity","title":"GeometricMachineLearning.QPTOAT","text":"QPTOAT\n\nA union of two types:\n\nconst QPTOAT = Union{QPT, AbstractArray}\n\nThis could be data in (q p)inmathbbR^2d form or come from an arbitrary vector space.\n\n\n\n\n\n","category":"type"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"\\begin{comment}","category":"page"},{"location":"structure_preservation/symplecticity/#References","page":"Symplecticity","title":"References","text":"","category":"section"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"V. I. Arnold. Mathematical methods of classical mechanics. Vol. 60 of Graduate Texts in Mathematics (Springer Verlag, Berlin, 1978).\n\n\n\nS. I. Richard L. Bishop. Tensor Analysis on Manifolds (Dover Publications, Mineola, New York, 1980).\n\n\n\nE. Hairer, C. Lubich and G. Wanner. Geometric Numerical integration: structure-preserving algorithms for ordinary differential equations (Springer, Heidelberg, 2006).\n\n\n\n","category":"page"},{"location":"structure_preservation/symplecticity/","page":"Symplecticity","title":"Symplecticity","text":"\\end{comment}","category":"page"},{"location":"manifolds/riemannian_manifolds/#Riemannian-Manifolds","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"","category":"section"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"A Riemannian manifold is a manifold mathcalM that we endow with a mapping g that smoothly[1] assigns a metric g_x to each tangent space T_xmathcalM. By a slight abuse of notation we will also refer to this g as a metric.","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"[1]: Smooth here refers to the fact that gmathcalMtotext(Space of Metrics) has to be a smooth map. But in order to discuss this in detail we would have to define a topology on the space of metrics. A more detailed discussion can be found in [15, 16, 19].","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"After having defined a metric g we can associate a length to each curve gamma0 t to mathcalM through: ","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"L(gamma) = int_0^t sqrtg_gamma(s)(gamma(s) gamma(s))ds","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"This L turns mathcalM into a metric space:","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"Main.definition(raw\"The **metric on a Riemannian manifold** ``\\mathcal{M}`` is \n\" * \nMain.indentation * raw\"```math\n\" *\nMain.indentation * raw\"d(x, y) = \\inf_{\\substack{\\text{$\\gamma(0) = x$ and}\\\\\n    \\gamma(t) = y}}L(\\gamma),\n\" * \nMain.indentation * raw\"```\n\" *\nMain.indentation * raw\"where ``t`` can be chosen arbitrarily.\")","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"If a curve is minimal with respect to the function L we call it the shortest curve or a geodesic. So we say that a curve gamma0 ttomathcalM is a geodesic if there is no shorter curve that can connect two points in gamma(0 t), i.e. ","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"d(gamma(t_i) gamma(t_f)) = int_t_i^t_fsqrtg_gamma(s)(gamma(s) gamma(s))ds","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"for any t_i t_fin0 t.","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"An important result of Riemannian geometry states that there exists a vector field X on TmathcalM, called the geodesic spray, whose integral curves are derivatives of geodesics. We formalize this statement as a theorem in the next section.","category":"page"},{"location":"manifolds/riemannian_manifolds/#Geodesic-Sprays-and-the-Exponential-Map","page":"Riemannian Manifolds","title":"Geodesic Sprays and the Exponential Map","text":"","category":"section"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"To every Riemannian manifold we can naturally associate a vector field called the geodesic spray or geodesic equation. For our purposes it is enough to state that this vector field is unique and well-defined [19].","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"The important property of the geodesic spray is","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"Main.theorem(raw\"Given an initial point ``x`` and an initial velocity ``v_x``, an integral curve for the geodesic spray is of the form ``t \\mapsto (\\gamma_{v_x}(t), \\gamma_{v_x}'(t))`` where ``\\gamma_{v_x}`` is a geodesic. We further have the property that the integral curve for the geodesic spray for an initial point ``x`` and an initial velocity ``\\eta\\cdot{}v_x`` (where ``\\eta`` is a scalar) is of the form ``t \\mapsto (\\gamma_{\\eta\\cdot{}v_x}(t), \\gamma_{\\eta\\cdot{}v_x}'(t)) = (\\gamma_{v_x}(\\eta\\cdot{}t), \\eta\\cdot\\gamma_{v_x}'(\\eta\\cdot{}t)).``\")","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"It is therefore customary to introduce the exponential map expT_xmathcalMtomathcalM as","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"exp(v_x) = gamma_v_x(1)","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"and we see that gamma_v_x(t) = exp(tcdotv_x). In GeometricMachineLearning we denote the exponential map by geodesic to avoid confusion with the matrix exponential map[2] which is called as exp in Julia. So we use the definition:","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"[2]: The Riemannian exponential map and the matrix exponential map coincide for many matrix Lie groups.","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"    mathttgeodesic(x v_x) equiv exp(v_x)","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"We give an example of using this function here:","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"using GLMakie\n\ninclude(\"../../gl_makie_transparent_background_hack.jl\")","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"using GeometricMachineLearning # hide\nimport Random # hide\nRandom.seed!(123) # hide\n\nY = rand(StiefelManifold, 3, 1)\n\nv = 5 * rand(3, 1)\nΔ = v - Y * (v' * Y)\n\nmorange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nfunction set_up_plot(; theme = :dark) # hide\ntext_color = theme == :dark ? :white : :black # hide\nfig = Figure(; backgroundcolor = :transparent, size = (900, 675)) # hide\nax = Axis3(fig[1, 1]; # hide\n    backgroundcolor = (:tomato, .5), # hide\n    aspect = (1., 1., 1.), # hide\n    xlabel = L\"x_1\", # hide\n    ylabel = L\"x_2\", # hide\n    zlabel = L\"x_3\", # hide\n    xgridcolor = text_color, # hide\n    ygridcolor = text_color, # hide\n    zgridcolor = text_color, # hide\n    xtickcolor = text_color, # hide\n    ytickcolor = text_color, # hide\n    ztickcolor = text_color, # hide\n    xlabelcolor = text_color, # hide\n    ylabelcolor = text_color, # hide\n    zlabelcolor = text_color, # hide\n    xypanelcolor = :transparent, # hide\n    xzpanelcolor = :transparent, # hide\n    yzpanelcolor = :transparent, # hide\n    limits = ([-1, 1], [-1, 1], [-1, 1]), # hide\n    azimuth = π / 7, # hide\n    elevation = π / 7, # hide\n    # height = 75., # hide\n    ) # hide\n# plot a sphere with radius one and origin 0\nsurface!(ax, Main.sphere(1., [0., 0., 0.])...; alpha = .5, transparency = true)\n\npoint_vec = ([Y[1]], [Y[2]], [Y[3]])\nscatter!(ax, point_vec...; color = morange, marker = :star5, markersize = 30)\n\narrow_vec = ([Δ[1]], [Δ[2]], [Δ[3]])\narrows!(ax, point_vec..., arrow_vec...; color = mred, linewidth = .02)\n\nfig, ax # hide\nend # hide\n\nfig_light = set_up_plot(; theme = :light)[1] # hide\nfig_dark = set_up_plot(; theme = :dark)[1] # hide\n\nsave(\"sphere_with_tangent_vec_light.png\", alpha_colorbuffer(fig_light)) # hide\nsave(\"sphere_with_tangent_vec_dark.png\", alpha_colorbuffer(fig_dark)) # hide\n\nnothing # hide","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"(Image: ) (Image: )","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"We now solve the geodesic spray for etacdotDelta for eta = 01 02 ldots 55 with the function geodesic and plot the corresponding points:","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"Δ_increments = [Δ * η for η in 0.1 : 0.1 : 5.5]\n\nY_increments = [geodesic(Y, Δ_increment) for Δ_increment in Δ_increments]\n\nfunction make_plot_with_solution(; theme = :dark) # hide\nfig, ax = set_up_plot(; theme = theme) # hide\nfor Y_increment in Y_increments\n    scatter!(ax, [Y_increment[1]], [Y_increment[2]], [Y_increment[3]]; \n        color = mred)\nend\n\nfig # hide\nend # hide\n\nfig_light = make_plot_with_solution(; theme = :light) # hide\nfig_dark = make_plot_with_solution(; theme = :dark) # hide\n\nsave(\"sphere_with_tangent_vec_and_geodesic_light.png\", alpha_colorbuffer(fig_light)) # hide\nsave(\"sphere_with_tangent_vec_and_geodesic_dark.png\", alpha_colorbuffer(fig_dark)) # hide\n\nnothing # hide","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"(Image: ) (Image: )","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"A geodesic can be seen as the equivalent of a straight line on a manifold. Also note that we drew a random element form StiefelManifold here, and not from S^2. This is because the category of Stiefel manifolds is more general than the category of spheres S^n: St(1 3) simeq S^2.","category":"page"},{"location":"manifolds/riemannian_manifolds/#The-Riemannian-Gradient","page":"Riemannian Manifolds","title":"The Riemannian Gradient","text":"","category":"section"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"The Riemannian gradient is essential when talking about optimization on manifolds.","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"Main.definition(raw\"The Riemannian gradient of a function ``L:\\mathcal{M}\\to\\mathbb{R}`` is a vector field ``\\mathrm{grad}^gL`` (or simply ``\\mathrm{grad}L``) for which we have\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    g_x(\\mathrm{grad}^gL(x), v_x) = (\\nabla_{\\varphi_U(x)}(L\\circ\\varphi_U^{-1}))^T \\varphi_U'(v_x), \n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"for all ``v_x\\in{}T_x\\mathcal{M}.`` In the expression above ``\\varphi_U`` is some coordinate chart defined in a neighborhood ``U`` around ``x``.\")","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"In the definition above nabla indicates the Euclidean gradient:","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":" nabla_xf = beginpmatrix fracpartialfpartialx_1  cdots  fracpartialfpartialx_n endpmatrix","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"We can also describe the Riemannian gradient through differential curves:","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"Main.definition(raw\"The Riemannian gradient of ``L`` is a vector field ``\\mathrm{grad}^gL`` for which\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"g_x(\\mathrm{grad}^gL(x), \\dot{\\gamma}(0)) = \\frac{d}{dt}L(\\gamma(t)),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``\\gamma`` is a ``C^\\infty`` curve through ``x``.\")","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"By the non degeneracy of g the Riemannian gradient always exists [16]. In the following we will also write mathrmgrad^gL(x) = mathrmgrad^g_xL = mathrmgrad_xL We will give specific examples of this when discussing the Stiefel manifold and the Grassmann manifold. ","category":"page"},{"location":"manifolds/riemannian_manifolds/#Gradient-Flows-and-Riemannian-Optimization","page":"Riemannian Manifolds","title":"Gradient Flows and Riemannian Optimization","text":"","category":"section"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"In GeometricMachineLearning we can include weights in neural networks that are part of a manifold. Training such neural networks amounts to Riemannian optimization and hence solving the gradient flow equation. The gradient flow equation is given by","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"X(x) = - mathrmgrad_xL","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"Solving this gradient flow equation will then lead us to a local minimum on mathcalM. This will be elaborated on when talking about optimizers. In practice we cannot solve the gradient flow equation directly and have to rely on approximations. The most straightforward approximation (and one that serves as a basis for all the optimization algorithms in GeometricMachineLearning) is to take the point (x X(x)) as an initial condition for the geodesic spray and then solve the ODE for a small time step. Such an update rule, i.e. ","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"x^(t) leftarrow gamma_X(x^(t-1))(Deltat)text with Deltat the time step","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"we call the gradient optimization scheme.","category":"page"},{"location":"manifolds/riemannian_manifolds/#Library-Functions","page":"Riemannian Manifolds","title":"Library Functions","text":"","category":"section"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"geodesic(::Manifold{T}, ::AbstractMatrix{T}) where T","category":"page"},{"location":"manifolds/riemannian_manifolds/#GeometricMachineLearning.geodesic-Union{Tuple{T}, Tuple{Manifold{T}, AbstractMatrix{T}}} where T","page":"Riemannian Manifolds","title":"GeometricMachineLearning.geodesic","text":"geodesic(Y::Manifold, Δ)\n\nTake as input an element of a manifold Y and a tangent vector in Δ in the corresponding tangent space and compute the geodesic (exponential map).\n\nIn different notation: take as input an element x of mathcalM and an element of T_xmathcalM and return mathttgeodesic(x v_x) = exp(v_x)\n\nExamples\n\nusing GeometricMachineLearning\n\nY = StiefelManifold([1. 0. 0.;]' |> Matrix)\nΔ = [0. .5 0.;]' |> Matrix\nY₂ = geodesic(Y, Δ)\n\nY₂' * Y₂ ≈ [1.;]\n\n# output\n\ntrue\n\nImplementation\n\nInternally this geodesic method calls geodesic(::StiefelLieAlgHorMatrix).\n\n\n\n\n\n","category":"method"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"\\begin{comment}","category":"page"},{"location":"manifolds/riemannian_manifolds/#References","page":"Riemannian Manifolds","title":"References","text":"","category":"section"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"S. Lang. Fundamentals of differential geometry. Vol. 191 (Springer Science & Business Media, 2012).\n\n\n\nM. P. Do Carmo and J. Flaherty Francis. Riemannian geometry. Vol. 2 (Springer, 1992).\n\n\n\n","category":"page"},{"location":"manifolds/riemannian_manifolds/","page":"Riemannian Manifolds","title":"Riemannian Manifolds","text":"\\end{comment}","category":"page"},{"location":"architectures/transformer/#Standard-Transformer","page":"Standard Transformer","title":"Standard Transformer","text":"","category":"section"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"The transformer is a relatively modern neural network architecture [54] that has come to dominate the field of natural language processing (NLP, [87]) and replaced the previously dominant long-short term memory cells (LSTM, [83]). Its success is due to a variety of factors: ","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"unlike LSTMs it consists of very simple building blocks and hence is easier to interpret mathematically,\nit is very flexible in its application and the data it is fed with do not have to conform to a rigid pattern, \ntransformers utilize modern hardware (especially GPUs) very effectively. ","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"The transformer architecture is sketched below: ","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"(Image: Visualization of the standard transformer. It consists of two components: a mulithead attention layer and a feedforward neural network.) (Image: Visualization of the standard transformer. It consists of two components: a mulithead attention layer and a feedforward neural network.)","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"It is nothing more than a combination of a multihead attention layer and a residual neural network[1] (ResNet).","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"[1]: A layer of type GeometricMachineLearning.ResNetLayer is nothing more than a neural network to whose output we again add the input, i.e. every ResNet is of the form mathrmResNet(x) = x + mathcalNN(x).","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"As was explained when we talked about the attention module, the attention layer performs a convex reweighting of the input sequence:","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"mathrmAttention  Z equiv z^(1) ldots z^(T)   mapsto  sum_i=1^Tp^(1)_iz^(i) ldots sum_i=1^Tp^(T)_iz^(i) = mathrmAttention(Z)","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"where the coefficients p^(i) depend on Z and are learnable. In the case of multihead attention a greater number of these reweighting coefficients are learned, but it is otherwise not much more complicated than single-head attention.","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"The green arrow in the figure above indicates that this first add connection can be left out. This can be specified via the keyword argument add_connection in MultiHeadAttention layer and the StandardTransformerIntegrator.","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"We should also note that such transformers have been used for the online phase in reduced order modeling before [85].","category":"page"},{"location":"architectures/transformer/#Classification-Transformer","page":"Standard Transformer","title":"Classification Transformer","text":"","category":"section"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"Instead of using the transformer for integration, it can also be used as a image classifier. In this case it is often referred to as \"vision transformer\" [88]. In this case we append a ClassificationLayer to the output of the transformer. This will be used in one of the examples. ","category":"page"},{"location":"architectures/transformer/#The-Upscaling","page":"Standard Transformer","title":"The Upscaling","text":"","category":"section"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"When using the transformer one typically also benefits from defining a transformer_dim that is greater than the system dimension and a corresponding upscaling_activation (see the docstring of StandardTransformerIntegrator).","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"(Image: If the transformer dimension is not equal to the system dimension, then we add two more neural network layers. One that maps up to the space whose dimension is the transformer dimension and one that maps down again to the space whose dimension is the system dimension.) (Image: If the transformer dimension is not equal to the system dimension, then we add two more neural network layers. One that maps up to the space whose dimension is the transformer dimension and one that maps down again to the space whose dimension is the system dimension.)","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"In the figure above we call ","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"    Psi^mathrmupmathbbR^mathttsys_dimtomathbbR^mathtttransformer_dim","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"the upscaling layer and ","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"    Psi^mathrmdownmathbbR^mathtttransformer_dimtomathbbR^mathttsys_dim","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"the downscaling layer. Both of these layers are dense layers with the activation function for the downscaling layer being the identity (for better expressivity) and the activation function for the upscaling layer can be specified via the keyword upscaling_activation.","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"GeometricMachineLearning does not have an implementation of such an upscaling for the volume-preserving transformer and the linear symplectic transformer. Symplectic liftings have however recently been discussed to learn higher-dimensional Hamiltonian representations of given data [72].","category":"page"},{"location":"architectures/transformer/#Library-Functions","page":"Standard Transformer","title":"Library Functions","text":"","category":"section"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"StandardTransformerIntegrator\nTransformer\nClassificationTransformer\nGeometricMachineLearning.assign_output_estimate","category":"page"},{"location":"architectures/transformer/#GeometricMachineLearning.StandardTransformerIntegrator","page":"Standard Transformer","title":"GeometricMachineLearning.StandardTransformerIntegrator","text":"StandardTransformerIntegrator(sys_dim)\n\nMake an instance of StandardTransformerIntegrator for a specific system dimension.\n\nHere the standard transformer used as an integrator (see TransformerIntegrator). \n\nIt is a composition of MultiHeadAttention layers and ResNetLayer layers.\n\nArguments\n\nThe following are optional keyword arguments:\n\ntransformer_dim::Int = sys_dim: this is the dimension after the upscaling.\nn_blocks::Int = 1 : the number of ResNetLayer blocks.\nn_heads::Int = sys_dim: the number of heads in the multihead attention layer.\nL::Int = 2: the number of transformer blocks.\nupscaling_activation = identity: the activation used in the upscaling layer.\nresnet_activation = tanh: the activation used for the ResNetLayer.\nattention_activation = GeometricMachineLearning.VectorSoftmax() : the activation used for the MultiHeadAttention layer.\nadd_connection:Bool = true: specifies if the input should be added to the output.\n\n\n\n\n\n","category":"type"},{"location":"architectures/transformer/#GeometricMachineLearning.Transformer","page":"Standard Transformer","title":"GeometricMachineLearning.Transformer","text":"Transformer(dim, n_heads, L)\n\nMake an instance of the Transformer with n_heads for dimension dim and L blocks.\n\nArguments\n\nTransformer takes the following optional keyword arguments:\n\nactivation = tanh: the activation function used for the ResNetLayer.\nStiefel::Bool = false: if the matrices P^V, P^Q and P^K should live on a manifold.\nadd_connection::Bool = false: if the input should by added to the ouput after the MultiHeadAttention layer.\nuse_bias::Bool = true: Specifies if the ResNetLayer should use a bias.\n\n\n\n\n\n","category":"function"},{"location":"architectures/transformer/#GeometricMachineLearning.ClassificationTransformer","page":"Standard Transformer","title":"GeometricMachineLearning.ClassificationTransformer","text":"ClassificationTransformer(dl)\n\nMake an instance of the ClassificationTransformer based on an instance of DataLoader.\n\nThis is a transformer neural network for classification purposes. At the moment this is only used for training on MNIST, but can in theory be used for any classification problem.\n\nArguments\n\nThe optional keyword arguments are: \n\nn_heads::Int=7: The number of heads in the MultiHeadAttention (mha) layers.\nL::Int=16: The number of transformer blocks.\nactivation=softmax: The activation function.\nStiefel::Bool=true: Whether the matrices in the mha layers are on the StiefelManifold. \nadd_connection::Bool=true: Whether the input is appended to the output of the mha layer. (skip connection)\n\n\n\n\n\n","category":"type"},{"location":"architectures/transformer/#GeometricMachineLearning.assign_output_estimate","page":"Standard Transformer","title":"GeometricMachineLearning.assign_output_estimate","text":"assign_output_estimate(full_output, prediction_window)\n\nCrop the output to get the correct number of output vectors.\n\nThe function assign_output_estimate is closely related to the Transformer.  It takes the last prediction_window columns of the output and uses them for the prediction.\n\ni.e.\n\nmathbbR^NtimesTtomathbbR^Ntimesmathttpw \nbeginbmatrix \n    z^(1)_1                cdots  z^(T)_1  \n    cdots                   cdots  cdots     \n    z^(1)_n                cdots  z^(T)_n\n    endbmatrix mapsto \n    beginbmatrix \n    z^(T - mathttpw)_1  cdots       z^(T)_1  \n    cdots                   cdots       cdots  \n    z^(T - mathttpw)_n  cdots       z^(T)_nendbmatrix     \n\nIf prediction_window is equal to sequence_length, then this is not needed.\n\n\n\n\n\n","category":"function"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"\\begin{comment}","category":"page"},{"location":"architectures/transformer/#References","page":"Standard Transformer","title":"References","text":"","category":"section"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser and I. Polosukhin. Attention is all you need. Advances in neural information processing systems 30 (2017).\n\n\n\nA. Solera-Rico, C. S. Vila, M. Gómez, Y. Wang, A. Almashjary, S. Dawson and R. Vinuesa, beta-Variational autoencoders and transformers for reduced-order modelling of fluid flows, arXiv preprint arXiv:2304.03571 (2023).\n\n\n\n","category":"page"},{"location":"architectures/transformer/","page":"Standard Transformer","title":"Standard Transformer","text":"\\end{comment}","category":"page"},{"location":"arrays/tensors/#Tensors-in-GeometricMachineLearning","page":"Tensors","title":"Tensors in GeometricMachineLearning","text":"","category":"section"},{"location":"arrays/tensors/","page":"Tensors","title":"Tensors","text":"We typically store training data as tensors with three axes in GeometricMachineLearning. This allows for a parallel computation of matrix products, also for the special arrays such as LowerTriangular, UpperTriangular, SymmetricMatrix and SkewSymMatrix and objects of Manifold type such as the StiefelManifold. ","category":"page"},{"location":"arrays/tensors/#Library-Functions","page":"Tensors","title":"Library Functions","text":"","category":"section"},{"location":"arrays/tensors/","page":"Tensors","title":"Tensors","text":"GeometricMachineLearning.tensor_mat_mul(::AbstractArray{<:Number, 3}, ::AbstractMatrix)\nGeometricMachineLearning.tensor_mat_mul!(::AbstractArray{<:Number, 3}, ::AbstractArray{<:Number, 3}, ::AbstractMatrix)\nGeometricMachineLearning.tensor_mat_mul!(::AbstractArray{<:Number, 3}, ::AbstractArray{<:Number, 3}, ::SymmetricMatrix)\nGeometricMachineLearning.mat_tensor_mul(::AbstractMatrix, ::AbstractArray{<:Number, 3})\nGeometricMachineLearning.mat_tensor_mul!(::AbstractArray{<:Number, 3}, ::AbstractMatrix, ::AbstractArray{<:Number, 3})\nGeometricMachineLearning.mat_tensor_mul!(::AbstractArray{<:Number, 3}, ::LowerTriangular, ::AbstractArray{<:Number, 3})\nGeometricMachineLearning.mat_tensor_mul!(::AbstractArray{<:Number, 3}, ::UpperTriangular, ::AbstractArray{<:Number, 3})\nGeometricMachineLearning.mat_tensor_mul!(::AbstractArray{<:Number, 3}, ::SkewSymMatrix, ::AbstractArray{<:Number, 3})\nGeometricMachineLearning.mat_tensor_mul!(::AbstractArray{<:Number, 3}, ::SymmetricMatrix, ::AbstractArray{<:Number, 3})","category":"page"},{"location":"arrays/tensors/#GeometricMachineLearning.tensor_mat_mul-Tuple{AbstractArray{<:Number, 3}, AbstractMatrix}","page":"Tensors","title":"GeometricMachineLearning.tensor_mat_mul","text":"tensor_mat_mul(A::AbstractArray{<:Number, 3}, B::AbstractMatrix)\n\nMultipliy the matrix B onto the tensor A from the right. \n\nInternally this calls the inplace version tensor_mat_mul!.\n\nExamples\n\nusing GeometricMachineLearning: tensor_mat_mul\n\nA = [1 1 1; 1 1 1; 1 1 1;;; 2 2 2; 2 2 2; 2 2 2]\nB = [3 0 0; 0 2 0; 0 0 1]\n\ntensor_mat_mul(A, B)\n\n# output\n\n3×3×2 Array{Int64, 3}:\n[:, :, 1] =\n 3  2  1\n 3  2  1\n 3  2  1\n\n[:, :, 2] =\n 6  4  2\n 6  4  2\n 6  4  2\n\n\n\n\n\n","category":"method"},{"location":"arrays/tensors/#GeometricMachineLearning.tensor_mat_mul!-Tuple{AbstractArray{<:Number, 3}, AbstractArray{<:Number, 3}, AbstractMatrix}","page":"Tensors","title":"GeometricMachineLearning.tensor_mat_mul!","text":"tensor_mat_mul!(C, A, B)\n\nMultiply the matrix B onto the tensor A from the right and store the result in C.\n\nThe function tensor_mat_mul calls tensor_mat_mul! internally.\n\n\n\n\n\n","category":"method"},{"location":"arrays/tensors/#GeometricMachineLearning.tensor_mat_mul!-Tuple{AbstractArray{<:Number, 3}, AbstractArray{<:Number, 3}, SymmetricMatrix}","page":"Tensors","title":"GeometricMachineLearning.tensor_mat_mul!","text":"mat_tensor_mul!(C::AbstractArray{<:Number, 3}, B::AbstractArray{<:Number, 3}, A::SymmetricMatrix)\n\nMultiply the symmetric matrix A onto the tensor B from the right and store the result in C.\n\nThis performs an efficient multiplication based on the special structure of the symmetric matrix A.\n\n\n\n\n\n","category":"method"},{"location":"arrays/tensors/#GeometricMachineLearning.mat_tensor_mul-Tuple{AbstractMatrix, AbstractArray{<:Number, 3}}","page":"Tensors","title":"GeometricMachineLearning.mat_tensor_mul","text":"mat_tensor_mul(A, B)\n\nMultipliy the matrix A onto the tensor B from the left. \n\nInternally this calls the inplace version mat_tensor_mul!.\n\nExamples\n\nusing GeometricMachineLearning: mat_tensor_mul\n\nB = [1 1 1; 1 1 1; 1 1 1;;; 2 2 2; 2 2 2; 2 2 2]\nA = [3 0 0; 0 2 0; 0 0 1]\n\nmat_tensor_mul(A, B)\n\n# output\n\n3×3×2 Array{Int64, 3}:\n[:, :, 1] =\n 3  3  3\n 2  2  2\n 1  1  1\n\n[:, :, 2] =\n 6  6  6\n 4  4  4\n 2  2  2\n\n\n\n\n\n","category":"method"},{"location":"arrays/tensors/#GeometricMachineLearning.mat_tensor_mul!-Tuple{AbstractArray{<:Number, 3}, AbstractMatrix, AbstractArray{<:Number, 3}}","page":"Tensors","title":"GeometricMachineLearning.mat_tensor_mul!","text":"mat_tensor_mul!(C, A, B)\n\nMultiply the matrix A onto the tensor B from the left and store the result in C.\n\nThe function mat_tensor_mul calls mat_tensor_mul! internally.\n\n\n\n\n\n","category":"method"},{"location":"arrays/tensors/#GeometricMachineLearning.mat_tensor_mul!-Tuple{AbstractArray{<:Number, 3}, LowerTriangular, AbstractArray{<:Number, 3}}","page":"Tensors","title":"GeometricMachineLearning.mat_tensor_mul!","text":"mat_tensor_mul!(C, A::LowerTriangular, B)\n\nMultiply the lower-triangular matrix A onto the tensor B from the left and store the result in C.\n\nThis performs an efficient multiplication based on the special structure of the lower-triangular matrix A.\n\n\n\n\n\n","category":"method"},{"location":"arrays/tensors/#GeometricMachineLearning.mat_tensor_mul!-Tuple{AbstractArray{<:Number, 3}, UpperTriangular, AbstractArray{<:Number, 3}}","page":"Tensors","title":"GeometricMachineLearning.mat_tensor_mul!","text":"mat_tensor_mul!(C, A::UpperTriangular, B)\n\nMultiply the upper-triangular matrix A onto the tensor B from the left and store the result in C.\n\nThis performs an efficient multiplication based on the special structure of the upper-triangular matrix A.\n\n\n\n\n\n","category":"method"},{"location":"arrays/tensors/#GeometricMachineLearning.mat_tensor_mul!-Tuple{AbstractArray{<:Number, 3}, SkewSymMatrix, AbstractArray{<:Number, 3}}","page":"Tensors","title":"GeometricMachineLearning.mat_tensor_mul!","text":"mat_tensor_mul!(C, A::SkewSymMatrix, B)\n\nMultiply skew-symmetric the matrix A onto the tensor B from the left and store the result in C.\n\nThis performs an efficient multiplication based on the special structure of the skew-symmetric matrix A.\n\n\n\n\n\n","category":"method"},{"location":"arrays/tensors/#GeometricMachineLearning.mat_tensor_mul!-Tuple{AbstractArray{<:Number, 3}, SymmetricMatrix, AbstractArray{<:Number, 3}}","page":"Tensors","title":"GeometricMachineLearning.mat_tensor_mul!","text":"mat_tensor_mul!(C, A::SymmetricMatrix, B)\n\nMultiply the symmetric matrix A onto the tensor B from the left and store the result in C.\n\nThis performs an efficient multiplication based on the special structure of the symmetric matrix A.\n\n\n\n\n\n","category":"method"},{"location":"layers/symplectic_attention/#Symplectic-Attention","page":"Symplectic Attention","title":"Symplectic Attention","text":"","category":"section"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"There are two ways of constructing symplectic attention: with a matrix-like softmax and a classical softmax.","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"Both of these approaches are however based on taking the derivative of an expression with respect to its input arguments (similar to gradient layers).","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"We first start with singlehead attention[1]. We start with the correlation matrix:","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"[1]: The multihead attention case is in principle not much more difficult, but care has to be taken with regards to the projection spaces.","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"C = Z^TAZ implies C_mn = sum_kellZ_kmA_kellZ_elln","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"Its element-wise derivative is:","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"fracpartialC_mnpartialZ_ij = sum_ell(delta_jmA_iellZ_elln + delta_jnX_ellmA_elli)","category":"page"},{"location":"layers/symplectic_attention/#Matrix-Softmax","page":"Symplectic Attention","title":"Matrix Softmax","text":"","category":"section"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"Here we take as a staring point the expression:","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"Sigma(Z) = mathrmlog(1 + exp(sum_mnC_mn))","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"Its gradient (with respect to Z) is:","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"fracpartialSigma(Z)partialZ_ij = frac11 + summ nexp(C_mn)sum_mnexp(C_mn)sum_ell(delta_jmA_iellZ_elln + delta_jnX_ellmA_elli) = frac11 + sum_mnexp(C_mn)AXexp(C)^T_ij +  A^TXexp(C)_ij","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"Note that if A is a SymmetricMatrix the expression than simplifies to:","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"fracpartialSigma(Z)partialZ_ij = 2frac11 + sum_mnexp(C_mn)AXexp(C)^T_ij","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"or written in matrix notation:","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"nabla_ZSigma(Z) = 2frac11 + sum_mnexp(C_mn)AXexp(C)","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"Whether we use a SymmetricMatrix for A or not can be set with the keyword symmetric in SymplecticAttention.","category":"page"},{"location":"layers/symplectic_attention/#Vector-Softmax","page":"Symplectic Attention","title":"Vector Softmax","text":"","category":"section"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"Here we take as a staring point the expression:","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"Sigma(Z) = sum_nmathrmlog(1 + exp(sum_mC_mn))","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"We then get:","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"fracpartialSigma(Z)partialZ_ij = sum_nfracexp(C_jn)1 + sum_mexp(C_mn)AZ_in + frac11 + sum_mexp(C_mj)A^TZexp(C)_ij","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"The second term in this expression is equivalent to a standard attention step:","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"mathrmTermIIqquad A^TZmathrmsoftmax(C)","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"The first term is equivalent to:","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"mathrmTermIqquad sum_n AZ_inmathrmsoftmax(C)^T_nj equiv AZ(mathrmsoftmax(C))^T","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"If we again assume that the matrix A is a SymmetricMatrix then the expression simplifies to:","category":"page"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"nabla_ZSigma(Z) = AZmathrmsoftmax(C)","category":"page"},{"location":"layers/symplectic_attention/#Library-Functions","page":"Symplectic Attention","title":"Library Functions","text":"","category":"section"},{"location":"layers/symplectic_attention/","page":"Symplectic Attention","title":"Symplectic Attention","text":"VectorSoftmax\nMatrixSoftmax\nSymplecticAttention\nSymplecticAttentionQ\nSymplecticAttentionP","category":"page"},{"location":"layers/symplectic_attention/#GeometricMachineLearning.VectorSoftmax","page":"Symplectic Attention","title":"GeometricMachineLearning.VectorSoftmax","text":"VectorSoftmax <: AbstractSoftmax\n\nTurn an arbitrary vector into a probability vector with:\n\nmathrmsoftmax(a)_i = frace^a_isum_i=1^de^a_i \n\nThis is what is most often understood under the name \"softmax\". MatrixSoftmax is the matrix version.\n\n\n\n\n\n","category":"type"},{"location":"layers/symplectic_attention/#GeometricMachineLearning.MatrixSoftmax","page":"Symplectic Attention","title":"GeometricMachineLearning.MatrixSoftmax","text":"MatrixSoftmax\n\nLike VectorSoftmax but for matrices:\n\nmathrmsoftmax(A)_ij = frace^A_ijsum_i=1 j=1^dbarde^A_ij \n\n\n\n\n\n","category":"type"},{"location":"layers/symplectic_attention/#GeometricMachineLearning.SymplecticAttention","page":"Symplectic Attention","title":"GeometricMachineLearning.SymplecticAttention","text":"SymplecticAttention\n\nImplements the symplectic attention layers. See LinearSymplecticAttention.\n\nKeys\n\nIt stores the following key:\n\nactivation::AbstractSoftmax\n\nConstructors\n\nSee SymplecticAttentionQ and SymplecticAttentionP.\n\nImplementation\n\nSymplecticAttention is similar to MultiHeadAttention or VolumePreservingAttention in that it computes the scalar products of all vectors in a sequence of input vectors:\n\nC = Q^TAQ\n\nwhere Q is the q-part of an input Z (see QPT). The matrix A is a weighting that can either be symmetric or skew-symmetric (this can be adjusted with the key-word symmetric::Bool).\n\nExtended help\n\nThe symplectic attention mechanism is derived via computing the gradient of a separable Hamiltonian, as is also done in GSympNets.\n\n\n\n\n\n","category":"type"},{"location":"layers/symplectic_attention/#GeometricMachineLearning.SymplecticAttentionQ","page":"Symplectic Attention","title":"GeometricMachineLearning.SymplecticAttentionQ","text":"SymplecticAttentionQ\n\nA constant that is derived from SymplecticAttention. This only changes the q-part of the input.\n\nConstructor\n\nSymplecticAttentionQ(M; symmetric::Bool, activation)\n\nThe default for the keywords are true and MatrixSoftmax().\n\nYou may want to alter the activation function (either MatrixSoftmax or VectorSoftmax), but its almost always better to set the keyword symmetric to true.\n\n\n\n\n\n","category":"type"},{"location":"layers/symplectic_attention/#GeometricMachineLearning.SymplecticAttentionP","page":"Symplectic Attention","title":"GeometricMachineLearning.SymplecticAttentionP","text":"SymplecticAttentionP\n\nA constant that is derived from SymplecticAttention. This only changes the p-part of the input.\n\nConstructor\n\nSymplecticAttentionP(M; symmetric::Bool, activation)\n\nThe default for the keywords are true and MatrixSoftmax().\n\nYou may want to alter the activation function (either MatrixSoftmax or VectorSoftmax), but its almost always better to set the keyword symmetric to true.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/symplectic_transformer/#symplectic_transformer_tutorial","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"","category":"section"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"In this section we compare the symplectic transformer to the standard transformer. The example we treat here is the coupled harmonic oscillator:","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"(Image: Visualization of the coupled harmonic oscillator.) (Image: Visualization of the coupled harmonic oscillator.)","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"It is a Hamiltonian system with ","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"H(q_1 q_2 p_1 p_2) = fracq_1^22m_1 + fracq_2^22m_2 + k_1fracq_1^22 + k_2fracq_2^22 +  ksigma(q_1)frac(q_2 - q_1)^22","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"where sigma(x) = 1  (1 + e^-x) is the sigmoid activation function. The system parameters are:","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"k_1: spring constant belonging to m_1,\nk_2: spring constant belonging to m_2,\nm_1: mass 1,\nm_2: mass 2,\nk: coupling strength between the two masses. ","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"To demonstrate the efficacy of the symplectic transformer here we will leave the parameters fixed but alter the initial conditions[1]:","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"[1]: We here use the implementation of the coupled harmonic oscillator from GeometricProblems.","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"using GeometricMachineLearning # hide\nusing GeometricProblems.CoupledHarmonicOscillator: hodeensemble, default_parameters\nusing GeometricIntegrators: ImplicitMidpoint, integrate # hide\nusing LaTeXStrings # hide\nusing CairoMakie  # hide\nCairoMakie.activate!() # hide\nimport Random # hide\nRandom.seed!(123) # hide\n\nconst tstep = .3\nconst n_init_con = 5\n\n# ensemble problem\nep = hodeensemble([rand(2) for _ in 1:n_init_con], [rand(2) for _ in 1:n_init_con]; tstep = tstep)\ndl = DataLoader(integrate(ep, ImplicitMidpoint()); suppress_info = true)\n# dl = DataLoader(vcat(dl_nt.input.q, dl_nt.input.p))  # hide\n\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"We now define the architectures and train them: ","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"const seq_length = 4\nconst batch_size = 1024\nconst n_epochs = 2000\n\narch_standard = StandardTransformerIntegrator(dl.input_dim; n_heads = 2, \n                                                            L = 4, \n                                                            n_blocks = 2)\narch_symplectic = SymplecticTransformer(  dl.input_dim,; \n                                                n_sympnet = 2,\n                                                L = 4, \n                                                upscaling_dimension = 2 * dl.input_dim)\narch_sympnet = GSympNet(dl.input_dim;   n_layers = 4, \n                                        upscaling_dimension = 8 * dl.input_dim)\n\nnn_standard = NeuralNetwork(arch_standard)\nnn_symplectic = NeuralNetwork(arch_symplectic)\nnn_sympnet = NeuralNetwork(arch_sympnet)\n\no_method = AdamOptimizerWithDecay(n_epochs, Float64)\n\no_standard = Optimizer(o_method, nn_standard)\no_symplectic = Optimizer(o_method, nn_symplectic)\no_sympnet = Optimizer(o_method, nn_sympnet)\n\nbatch = Batch(batch_size, seq_length)\nbatch2 = Batch(batch_size)\n\nloss_array_standard = o_standard(nn_standard, dl, batch, n_epochs; show_progress = false)\nloss_array_symplectic = o_symplectic(nn_symplectic, dl, batch, n_epochs; show_progress = false)\nloss_array_sympnet = o_sympnet(nn_sympnet, dl, batch2, n_epochs; show_progress = false)\n\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"And the corresponding training losses look as follows:","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"morange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256) # hide\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256) # hide\nmgreen = RGBf(44 / 256, 160 / 256, 44 / 256) # hide\n\nfunction plot_training_losses(loss_array_standard, loss_array_symplectic, loss_array_sympnet; theme = :dark)\n    textcolor = theme == :dark ? :white : :black\n    fig = Figure(; backgroundcolor = :transparent)\n    ax = Axis(fig[1, 1]; \n        backgroundcolor = :transparent,\n        bottomspinecolor = textcolor, \n        topspinecolor = textcolor,\n        leftspinecolor = textcolor,\n        rightspinecolor = textcolor,\n        xtickcolor = textcolor, \n        ytickcolor = textcolor,\n        xticklabelcolor = textcolor,\n        yticklabelcolor = textcolor,\n        xlabel=\"Epoch\", \n        ylabel=\"Training loss\",\n        xlabelcolor = textcolor,\n        ylabelcolor = textcolor,\n        yscale = log10\n    )\n    lines!(ax, loss_array_standard, color = mpurple, label = \"ST\")\n    lines!(ax, loss_array_symplectic,  color = mred, label = \"SyT\")\n    lines!(ax, loss_array_sympnet, color = morange, label = \"SympNet\")\n    axislegend(; position = (.82, .75), backgroundcolor = :transparent, labelcolor = textcolor)\n\n    fig, ax\nend\n\nfig_dark, ax_dark = plot_training_losses(loss_array_standard, loss_array_symplectic, loss_array_sympnet; theme = :dark)\nfig_light, ax_light = plot_training_losses(loss_array_standard, loss_array_symplectic, loss_array_sympnet; theme = :light)\n\nsave(\"lst_dark.png\", fig_dark; px_per_unit = 1.2)\nsave(\"lst_light.png\", fig_light; px_per_unit = 1.2)\n\nnothing","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"(Image: ) (Image: )","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"We further evaluate a trajectory with the trained networks for thirty time steps: ","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"const index = 1\ninit_con = (q = dl.input.q[:, 1:seq_length, index], p = dl.input.p[:, 1:seq_length, index])\n# when we iterate with a feedforward neural network we only need a vector as input\ninit_con_ff = (q = dl.input.q[:, 1, index], p = dl.input.p[:, 1, index])\n\nconst n_steps = 30\n\nfunction make_validation_plot(n_steps = n_steps; theme = :dark)\n    textcolor = theme == :dark ? :white : :black\n    fig = Figure(; backgroundcolor = :transparent)\n    ax = Axis(fig[1, 1]; \n        backgroundcolor = :transparent,\n        bottomspinecolor = textcolor, \n        topspinecolor = textcolor,\n        leftspinecolor = textcolor,\n        rightspinecolor = textcolor,\n        xtickcolor = textcolor, \n        ytickcolor = textcolor,\n        xticklabelcolor = textcolor,\n        yticklabelcolor = textcolor,\n        xlabel=L\"t\", \n        ylabel=L\"q_1\",\n        xlabelcolor = textcolor,\n        ylabelcolor = textcolor,\n    )\n    prediction_standard = iterate(nn_standard, init_con; n_points = n_steps, prediction_window = seq_length)\n    prediction_symplectic = iterate(nn_symplectic, init_con; n_points = n_steps, prediction_window = seq_length)\n    prediction_sympnet = iterate(nn_sympnet, init_con_ff; n_points = n_steps)\n\n    # we use linewidth  = 2\n    lines!(ax, dl.input.q[1, 1:n_steps, index]; color = mblue, label = \"Implicit midpoint\", linewidth = 2)\n    lines!(ax, prediction_standard.q[1, :]; color = mpurple, label = \"ST\", linewidth = 2)\n    lines!(ax, prediction_symplectic.q[1, :]; color = mred, label = \"SyT\", linewidth = 2)\n    lines!(ax, prediction_sympnet.q[1, :]; color = morange, label = \"SympNet\", linewidth = 2)\n    axislegend(; position = (.55, .75), backgroundcolor = :transparent, labelcolor = textcolor)\n\n    fig, ax\nend\n\nfig_light, ax_light = make_validation_plot(n_steps; theme = :light)\nfig_dark, ax_dark = make_validation_plot(n_steps; theme = :dark)\nsave(\"lst_validation_light.png\", fig_light; px_per_unit = 1.2)\nsave(\"lst_validation_dark.png\", fig_dark; px_per_unit = 1.2)\n\nnothing","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"(Image: ) (Image: )","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"We can see that the standard transformer is not able to stay close to the trajectory coming from implicit midpoint very well. The linear symplectic transformer outperforms the standard transformer as well as the SympNet while needing fewer parameters than the standard transformer: ","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"parameterlength(nn_standard), parameterlength(nn_symplectic), parameterlength(nn_sympnet)","category":"page"},{"location":"tutorials/symplectic_transformer/","page":"Symplectic Transformer","title":"Symplectic Transformer","text":"It is also interesting to note that the training error for the SympNet gets lower than the one for the symplectic transformer, but it does not manage to outperform it when looking at the validation. ","category":"page"},{"location":"optimizers/bfgs_optimizer/#The-BFGS-Optimizer","page":"BFGS Optimizer","title":"The BFGS Optimizer","text":"","category":"section"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"The presentation shown here is largely taken from [44, chapters 3 and 6] with a derivation based on an online comment [45]. The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm is a second order optimizer that can be also be used to train a neural network.","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"It is a version of a quasi-Newton method and is therefore especially suited for convex problems. As is the case with any other (quasi-)Newton method[1] the BFGS algorithm approximates the objective with a quadratic function in each optimization step:","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"[1]: Various Newton methods and quasi-Newton methods differ in how they model the approximate Hessian.","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"m^(k)(x) = L(x^(k)) + (nabla_x^(k)L)^T(x - x^(k)) + frac12(x - x^(k))^TR^(k)(x - x^(k))","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"where R^(k) is referred to as the approximate Hessian. We further require R^(k) to be symmetric and positive definite. Differentiating the above expression and setting the derivative to zero gives us: ","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"nabla_xm^(k) = nabla_x^(k)L + R^(k)(x - x^(k)) = 0","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"or written differently: ","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"x - x^(k) = -(R^(k))^-1nabla_x^(k)L","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"This value we will from now on call p^(k) = -H^(k)nabla_x_kL with H^(k) = (R^(k))^-1 and refer to as the search direction. The new iterate then is: ","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"x^(k+1) = x^(k) + eta^(k)p^(k)","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"where eta^(k) is the step length. Techniques that describe how to pick an appropriate eta^(k) are called line-search methods and are discussed below. First we discuss what requirements we impose on R^(k). A first reasonable condition would be to require the gradient of m^(k) to be equal to that of L at the points x^(k-1) and x^(k): ","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"beginaligned\nnabla_x^(k)m^(k)   = nabla_x^(k)L + R^(k)(x^(k) - x^(k))   overset=  nabla_x^(k)L   text and  \nnabla_x^(k-1)m^(k)  = nabla_x^(k)L + R^(k)(x^(k-1) - x^(k))  overset=  nabla_x^(k-1)L  \nendaligned","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"The first one of these conditions is automatically satisfied. The second one can be rewritten as: ","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"x^(k) - x^(k-1) overset= H^(k)(nabla_x^(k)L - nabla_x^(k-1)L) ","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"The following notations are often used: ","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"s^(k-1) = eta^(k-1)p^(k-1) =  x^(k) - x^(k-1) quadtext and quad y^(k-1) = nabla_x^(k)L - nabla_x^(k-1)L","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"The condition mentioned above then becomes: ","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"s^(k-1) overset= H^(k)y^(k-1)","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"and we call it the secant equation. ","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"In order to pick the ideal H^(k) we solve the following problem: ","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"beginaligned\n min_H   H - H^(k-1)_W  \n textst   H = H^Tquadtextand\n             textand   s^(k-1) = Hy^(k-1)\nendaligned","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"where the first condition is symmetry and the second one is the secant equation. For the norm cdot_W we pick the weighted Frobenius norm:","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"A_W = W^12AW^12_F","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"where cdot_F is the usual Frobenius norm[2] and the matrix W=tildeR^(k-1) is the average Hessian:","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"tildeR^(k-1) = int_0^1 nabla^2f(x^(k-1) + taueta^(k-1)p^(k-1))dtau","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"[2]: The Frobenius norm is A_F^2 = sum_ija_ij^2.","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"We now state the solution to this minimization problem:","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"Main.theorem(raw\"The solution of the minimization problem is:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"H^{(k)} = (\\mathbb{I} - \\frac{1}{(s^{(k-1)})^Ty^{(k-1)}}s^{(k-1)}(y^{(k-1)})^T)H^{(k-1)}(\\mathbb{I} - \\frac{1}{(s^{k-1})^Ty^{(k-1)}}y^{(k-1)}(s^{(k-1)})^T) + \\\\ \\frac{1}{(s^{(k-1)})^Ty^{(k-1)}}s^{(k-1)}(s^{(k-1)})^T,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"with ``y^{(k-1)} = \\nabla_{x^{(k)}}L - \\nabla_{x^{(k-1)}}L`` and ``s^{(k-1)} = x^{(k)} - x^{(k-1)}`` as above.\")","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"Main.proof(raw\"In order to find the ideal ``H^{(k)}`` under the conditions described above, we introduce some notation: \n\" * Main.indentation * raw\"- ``\\tilde{H}^{(k-1)} := W^{1/2}H^{(k-1)}W^{1/2}``,\n\" * Main.indentation * raw\"- ``\\tilde{H} := W^{1/2}HW^{1/2}``, \n\" * Main.indentation * raw\"- ``\\tilde{y}^{(k-1)} := W^{-1/2}y^{(k-1)}``, \n\" * Main.indentation * raw\"- ``\\tilde{s}^{(k-1)} := W^{1/2}s^{(k-1)}``.\n\" * Main.indentation * raw\"\n\" * Main.indentation * raw\"With this notation we can rewrite the problem of finding ``H^{(k)}`` as: \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\begin{aligned}\n\" * Main.indentation * raw\"& \\min_{\\tilde{H}} & & ||\\tilde{H} - \\tilde{H}^{(k-1)}||_F  \\\\ \n\" * Main.indentation * raw\"& \\text{s.t.}\\quad & & \\tilde{H} = \\tilde{H}^T\\quad \\\\\n\" * Main.indentation * raw\"& \\text{and} & & \\tilde{s}^{(k-1)} = \\tilde{H}\\tilde{y}^{(k-1)}.\n\" * Main.indentation * raw\"\\end{aligned}\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"\n\" * Main.indentation * raw\"We further have ``y^{(k-1)} = Ws^{(k-1)}`` and hence ``\\tilde{y}^{(k-1)} = \\tilde{s}^{(k-1)}`` by a corollary of the mean value theorem: ``\\int_0^1 g'(\\xi_1 + \\tau(\\xi_2 - \\xi_1)) d\\tau (\\xi_2 - \\xi_1) = g(\\xi_2) - g(\\xi_1)`` for a vector-valued function ``g``.\n\" * Main.indentation * raw\"\n\" * Main.indentation * raw\"Now we rewrite ``H`` and ``H^{(k-1)}`` in a new basis ``U = [u|u_\\perp]``, where ``u := \\tilde{y}^{(k-1)}/||\\tilde{y}^{(k-1)}||`` and ``u_\\perp`` is an orthogonal complement of ``u`` (i.e. we have ``u^Tu_\\perp=0`` and ``u_\\perp^Tu_\\perp=\\mathbb{I}``):\n\" * Main.indentation * raw\"\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\begin{aligned}\n\" * Main.indentation * raw\"U^T\\tilde{H}^{(k-1)}U - U^T\\tilde{H}U = \\begin{bmatrix}  u^T \\\\ u_\\perp^T \\end{bmatrix}(\\tilde{H}^{(k-1)} - \\tilde{H})\\begin{bmatrix} u & u_\\perp \\end{bmatrix} = \\\\\n\" * Main.indentation * raw\"\\begin{bmatrix}\n\" * Main.indentation * raw\"    u^T\\tilde{H}^{(k-1)}u - 1 & u^T\\tilde{H}^{(k-1)}u_\\perp \\\\\n\" * Main.indentation * raw\"    u_\\perp^T\\tilde{H}^{(k-1)}u & u_\\perp^T(\\tilde{H}^{(k-1)}-\\tilde{H}^{(k)})u_\\perp\n\" * Main.indentation * raw\"\\end{bmatrix}.\n\" * Main.indentation * raw\"\\end{aligned}\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"By a property of the Frobenius norm we can consider the blocks independently: \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\begin{aligned}\n\" * Main.indentation * raw\"||\\tilde{H}^{(k-1)} - \\tilde{H}||^2_F & = ||U^T(\\tilde{H}^{(k-1)} - \\tilde{H})U||^2_F \\\\\n\" * Main.indentation * raw\"& = (u^T\\tilde{H}^{(k-1)}u -1)^2 + ||u^T\\tilde{H}^{(k-1)}u_\\perp||_F^2 + ||u_\\perp^T\\tilde{H}^{(k-1)}u||_F^2 + ||u_\\perp^T(\\tilde{H}^{(k-1)} - \\tilde{H})u_\\perp||_F^2.\n\" * Main.indentation * raw\"\\end{aligned}\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"We see that ``\\tilde{H}`` only appears in the last term, which should therefore be made zero, i.e. the projections of ``\\tilde{H}_{k-1}`` and ``\\tilde{H}`` onto the space spanned by ``u_\\perp`` should coincide. With the condition ``\\tilde{H}u \\overset{!}{=} u`` we hence get: \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\tilde{H} = U\\begin{bmatrix} 1 & 0 \\\\ 0 & u^T_\\perp\\tilde{H}^{(k-1)}u_\\perp \\end{bmatrix}U^T = uu^T + (\\mathbb{I}-uu^T)\\tilde{H}^{(k-1)}(\\mathbb{I}-uu^T).\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"If we now map back to the original coordinate system, the ideal solution for ``H^{(k)}`` is: \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"H^{(k)} = (\\mathbb{I} - \\frac{1}{(s^{(k-1)})^Ty^{(k-1)}}s^{(k-1)}(y^{(k-1)})^T)H^{(k-1)}(\\mathbb{I} - \\frac{1}{(s^{k-1})^Ty^{(k-1)}}y^{(k-1)}(s^{(k-1)})^T) + \\\\ \\frac{1}{(s^{(k-1)})^Ty^{(k-1)}}s^{(k-1)}(s^{(k-1)})^T,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"and the assertion is proved.\")","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"The cache and the parameters are updated with:","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"Compute the gradient nabla_x^(k)L,\nobtain a negative search direction p^(k) gets -H^(k)nabla_x^(k)L,\ncompute s^(k) = eta^(k)p^(k),\ncompute y^(k) gets nabla_x^(k)L - nabla_x^(k-1)L,\nupdate H^(k + 1) gets (mathbbI - frac1(s^(k))^Ty^(k)s^(k)(y^(k))^T)H^(k)(mathbbI - frac1s^(k)^Ty^(k)y^(k)(s^(k))^T) +  frac1(s^(k))^Ty^(k)s^(k)(s^(k))^T,\nupdate x^(k + 1) gets x^(k) + s^(k).","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"The cache of the BFGS algorithm thus consists of the matrix H^(cdot) for each weight x^(cdot) in the neural network and the gradient for the previous time step nabla_x^(k-1)L. s^(k) here is again the velocity that we use to update the neural network weights. ","category":"page"},{"location":"optimizers/bfgs_optimizer/#The-Riemannian-Version-of-the-BFGS-Algorithm","page":"BFGS Optimizer","title":"The Riemannian Version of the BFGS Algorithm","text":"","category":"section"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"Generalizing the BFGS algorithm to the setting of a Riemannian manifold is straightforward. All we have to do is replace Euclidean gradient by Riemannian ones (composed with a lift via global_rep): ","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"nabla_x^(k)L implies (Lambda^(k))^-1(Omega(x^(k) mathrmgrad_x^(k)L))Lambda^(k) = mathttglobal_rep(mathrmgrad_x^(k))","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"and addition by a retraction:","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"    x^(k+1) gets x^(k) + s^(k) implies x^(k+1) gets mathrmRetraction(s^(k))x^(k)","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"The Hessian for the manifold BFGS algorithm is of size tildeNtimestildeN where tildeN = mathrmdim(mathfrakg^mathrmhor). For the global tangent space belonging to the Stiefel manifold we have tildeN = (N - n)n + n(n - 1)div2.","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"In order to multiply the stored weights with the Hessian H we use the vectorization operation vec for all weights:","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"using GeometricMachineLearning # hide\nA = SkewSymMatrix([1, 2, 3], 3)\nB = [4 5 6; ]\nB̄ = StiefelLieAlgHorMatrix(A, B, 4, 3)\nB̄ |> vec","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"The H matrix in the cache is correspondingly initialized as:","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"@assert BFGSCache(B̄).H == [1  0  0  0  0  0; 0  1  0  0  0  0; 0  0  1  0  0  0; 0  0  0  1  0  0; 0  0  0  0  1  0; 0  0  0  0  0  1] # hide\nBFGSCache(B̄)","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"We see that barB is of dimension tildeN = (N - n)n + n(n - 1)div2 = 3 + 3 = 6 and H is of dimension tildeNtimestildeN = 6times6","category":"page"},{"location":"optimizers/bfgs_optimizer/#The-Curvature-Condition-and-the-Wolfe-Conditions","page":"BFGS Optimizer","title":"The Curvature Condition and the Wolfe Conditions","text":"","category":"section"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"In textbooks [44] an application of the BFGS algorithm typically further involves a line search subject to the Wolfe conditions. If these are satisfied the curvature condition usually also is.","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"A condition that is similar to the secant condition discussed before is that R^(k) has to be positive-definite at point s^(k-1):","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"(s^(k-1))^Ty^(k-1)  0","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"This is referred to as the standard curvature condition. If we impose the Wolfe conditions, the standard curvature condition holds automatically. The Wolfe conditions are stated with respect to the parameter eta^(k).","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"Main.definition(raw\"The **Wolfe conditions** are:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\begin{aligned}\n\" * Main.indentation * raw\"L(x^{(k)}+\\eta^{(k)}p^{(k)}) & \\leq{}L(x^{(k)}) + c_1\\eta^{(k)}(\\nabla_{x^{(k)}}L)^Tp^{(k)} & \\text{ for } & c_1\\in(0,1) \\quad\\text{and} \\\\\n\" * Main.indentation * raw\"(\\nabla_{(x^{(k)} + \\eta^{(k)}p^{(k)})}L)^Tp^{(k)} & \\geq c_2(\\nabla_{x^{(k)}}L)^Tp^{(k)} & \\text{ for } & c_2\\in(c_1,1).\n\" * Main.indentation * raw\"\\end{aligned}\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"The two Wolfe conditions above are respectively called the *sufficient decrease condition* and the *curvature condition*.\")","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"A possible choice for c_1 and c_2 are 10^-4 and 09 [44]. We further have:","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"Main.theorem(raw\"The second Wolfe condition, also called curvature condition, is stronger than the standard curvature condition under the assumption that the first Wolfe condition is true and ``L(x^{(k+1)}) < L(^{(x_k)})``.\")","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"Main.proof(raw\"We use the second Wolfe condition to write\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"(\\nabla_{x^{(k)}}L)^Tp^{(k-1)} - c_2(\\nabla_{x^{(k-1)}}L)^Tp^{(k-1)} = (y^{(k-1)})^Tp^{(k-1)} + (1 - c_2)(\\nabla_{x^{(k-1)}}L)^Tp^{(k-1)} \\geq 0,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"and we can apply the first Wolfe condition on the second term in this expression: \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"(1 - c_2)(\\nabla_{x^{(k-1)}}L)^Tp^{(k-1)}\\geq\\frac{1-c_2}{c_1\\eta^{(k-1)}}(L(x^{(k)}) - L(x^{(k-1)})),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"which is negative if the value of ``L`` is decreasing.\")","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"It is noteworthy that line search has not been used a lot in deep learning in the past. This is beginning to change however [46, 47]. We also note that the BFGS optimizer combined with the global tangent space representation offers a way of performing second order optimization on manifolds, this is however not the only way to do so [48, 49].","category":"page"},{"location":"optimizers/bfgs_optimizer/#Stability-of-the-Algorithm","page":"BFGS Optimizer","title":"Stability of the Algorithm","text":"","category":"section"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"Similar to the Adam optimizer we also add a delta term for stability to two of the terms appearing in the update rule of the BFGS algorithm in practice. ","category":"page"},{"location":"optimizers/bfgs_optimizer/#Library-Functions","page":"BFGS Optimizer","title":"Library Functions","text":"","category":"section"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"BFGSOptimizer\nBFGSCache\nupdate!(::Optimizer{<:BFGSOptimizer}, ::BFGSCache, ::AbstractArray)\nvec(::StiefelLieAlgHorMatrix)","category":"page"},{"location":"optimizers/bfgs_optimizer/#GeometricMachineLearning.BFGSOptimizer","page":"BFGS Optimizer","title":"GeometricMachineLearning.BFGSOptimizer","text":"BFGSOptimizer(η, δ)\n\nMake an instance of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) optimizer. \n\nη is the learning rate. δ is a stabilization parameter.\n\n\n\n\n\n","category":"type"},{"location":"optimizers/bfgs_optimizer/#GeometricMachineLearning.BFGSCache","page":"BFGS Optimizer","title":"GeometricMachineLearning.BFGSCache","text":"BFGSCache(B)\n\nMake the cache for the BFGS optimizer based on the array B.\n\nIt stores an array for the gradient of the previous time step B and the inverse of the Hessian matrix H.\n\nThe cache for the inverse of the Hessian is initialized with the idendity. The cache for the previous gradient information is initialized with the zero vector.\n\nNote that the cache for H is changed iteratively, whereas the cache for B is newly assigned at every time step.\n\n\n\n\n\n","category":"type"},{"location":"optimizers/bfgs_optimizer/#AbstractNeuralNetworks.update!-Tuple{Optimizer{<:BFGSOptimizer}, BFGSCache, AbstractArray}","page":"BFGS Optimizer","title":"AbstractNeuralNetworks.update!","text":"update!(o::Optimizer{<:BFGSOptimizer}, C, B)\n\nPeform an update with the BFGS optimizer. \n\nC is the cache, B contains the gradient information (the output of global_rep in general).\n\nFirst we compute the final velocity with\n\nvecS = -o.method.η * C.H * vec(B)\n\nand then we update H\n\nC.H .= (𝕀 - ρ * SY) * C.H * (𝕀 - ρ * SY') + ρ * vecS * vecS'\n\nwhere SY is vecS * Y' and 𝕀 is the idendity. \n\nImplementation\n\nFor stability we use δ for computing ρ:\n\nρ = 1. / (vecS' * Y + o.method.δ)\n\nThis is similar to the AdamOptimizer\n\nExtended help\n\nIf we have weights on a Manifold than the updates are slightly more difficult. In this case the vec operation has to be generalized to the corresponding global tangent space.\n\n\n\n\n\n","category":"method"},{"location":"optimizers/bfgs_optimizer/#Base.vec-Tuple{StiefelLieAlgHorMatrix}","page":"BFGS Optimizer","title":"Base.vec","text":"vec(A::StiefelLieAlgHorMatrix)\n\nVectorize A. \n\nExamples\n\nusing GeometricMachineLearning\n\nA = SkewSymMatrix([1, ], 2)\nB = [2 3; ]\nB̄ = StiefelLieAlgHorMatrix(A, B, 3, 2)\nB̄ |> vec\n\n# output\n\nvcat(1-element Vector{Int64}, 2-element Vector{Int64}):\n 1\n 2\n 3\n\nImplementation\n\nThis is using Vcat from the package LazyArrays.\n\n\n\n\n\n","category":"method"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"\\section*{Chapter Summary}\n\nIn this chapter we gave explicit examples of neural network optimizers and demonstrated the corresponding application interface; we referred to the related updating rules as \\textit{optimizer methods}. An important ingredient for all optimizers was the \\textit{optimizer cache}. In the simplest case (for the gradient optimizer) the cache stores nothing; it however contains \\textit{first moments} for the momentum optimizer, and \\textit{first} and \\textit{second moments} for the \\textit{Adam optimizer}. When using the more complex \\textit{BFGS optimizer} the cache is even more complicated (it stores an approximation to the inverse of the Hessian of the loss).\nThe cache further has to be parallel transported along the optimization trajectory; for vector spaces this is trivial and for homogeneous spaces this is done by utilizing the \\textit{global tangent space representation} which was developed in the previous chapter.\n\n\\begin{comment}","category":"page"},{"location":"optimizers/bfgs_optimizer/#References","page":"BFGS Optimizer","title":"References","text":"","category":"section"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"J. N. Stephen J. Wright. Numerical optimization (Springer Science+Business Media, New York, NY, 2006).\n\n\n\nA.Γ. (math.stackexchange user 253273). Quasi-newton methods: Understanding DFP updating formula, https://math.stackexchange.com/q/2279304 (2017). Accessed on September 19, 2024.\n\n\n\nW. Huang, P.-A. Absil and K. A. Gallivan. A Riemannian BFGS method for nonconvex optimization problems. In: Numerical Mathematics and Advanced Applications ENUMATH 2015 (Springer, 2016); pp. 627–634.\n\n\n\n","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"\\end{comment}","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"<!--","category":"page"},{"location":"optimizers/bfgs_optimizer/#References-2","page":"BFGS Optimizer","title":"References","text":"","category":"section"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"I. Goodfellow, Y. Bengio and A. Courville. Deep learning (MIT press, Cambridge, MA, 2016).\n\n\n\nJ. N. Stephen J. Wright. Numerical optimization (Springer Science+Business Media, New York, NY, 2006).\n\n\n\nA.Γ. (math.stackexchange user 253273). Quasi-newton methods: Understanding DFP updating formula, https://math.stackexchange.com/q/2279304 (2017). Accessed on September 19, 2024.\n\n\n\nW. Huang, P.-A. Absil and K. A. Gallivan. A Riemannian BFGS method for nonconvex optimization problems. In: Numerical Mathematics and Advanced Applications ENUMATH 2015 (Springer, 2016); pp. 627–634.\n\n\n\nB. Gao, N. T. Son and T. Stykel. Symplectic Stiefel manifold: tractable metrics, second-order geometry and Newton's methods, arXiv preprint arXiv:2406.14299 (2024).\n\n\n\n","category":"page"},{"location":"optimizers/bfgs_optimizer/","page":"BFGS Optimizer","title":"BFGS Optimizer","text":"-->","category":"page"},{"location":"manifolds/inverse_function_theorem/#Foundational-Theorems-for-Differential-Manifolds","page":"Foundations of Differential Manifolds","title":"Foundational Theorems for Differential Manifolds","text":"","category":"section"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"Here we state and proof all the theorems necessary to define differential manifolds. All these theorems (including proofs) can be found in e.g. [15].","category":"page"},{"location":"manifolds/inverse_function_theorem/#The-Fixed-Point-Theorem","page":"Foundations of Differential Manifolds","title":"The Fixed-Point Theorem","text":"","category":"section"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"The fixed-point theorem will be used in the proof of the inverse function theorem below and the existence-and-uniqueness theorem. ","category":"page"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"Main.theorem(raw\"A function ``f:U \\to U`` defined on an open subset ``U`` of a complete metric vector space ``\\mathcal{V} \\supset U`` that is contractive, i.e. ``|f(z) - f(y)| \\leq q|z - y|`` with ``q < 1``, has a unique fixed point ``y^*`` such that ``f(y^*) = y^*``. Further ``y^*`` can be found by taking any ``y\\in{}U`` through ``y^* = \\lim_{m\\to\\infty}f^m(y)``.\"; name = \"Banach Fixed-Point Theorem\")","category":"page"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"Main.proof(raw\"Fix a point ``y\\in{}U``. We proof that the sequence ``(f^m(y))_{m\\in\\mathbb{N}}`` is Cauchy and because ``\\mathcal{V}`` is a complete metric space, the limit of this sequence exists. Take ``\\tilde{m} > m`` and we have\n\" *\nMain.indentation * raw\"```math\n\" *\nMain.indentation * raw\"\\begin{aligned}\n\" *\nMain.indentation * raw\"|f^{\\tilde{m}}(y) - f^m(y)| & \\leq \\sum_{i = m}^{\\tilde{m} - 1}|f^{i+1}(y) - f^{i}(y)| \\\\\n\" *\nMain.indentation * raw\"                            & \\leq \\sum_{i = m}^{\\tilde{m} - 1}q^i|f(y) - y| \\\\ \n\" *\nMain.indentation * raw\"                            & \\leq \\sum_{i = m}^\\infty{}q^i|f(y) - y|  = (f(y) - y)\\left( \\frac{q}{1 - q} - \\sum_{i = 1}^{m-1}q^i \\right)\\\\\n\" *\nMain.indentation * raw\"                            & = (f(y) - y)\\left( \\frac{q}{1 - q} - \\frac{q - q^m}{q - 1} \\right) = (f(y) - y)\\frac{q^{m+1}}{1 - q}.\n\" *\nMain.indentation * raw\"\\end{aligned} \n\" *\nMain.indentation * raw\"```\n\" *\nMain.indentation * raw\"And the sequence is clearly Cauchy.\")","category":"page"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"Note that we stated the fixed-point theorem for arbitrary complete metric spaces here, not just for mathbbR^n. For the section on manifolds we only need the theorem for mathbbR^n, but for the existence-and-uniqueness theorem we need the statement for more general spaces. ","category":"page"},{"location":"manifolds/inverse_function_theorem/#The-Inverse-Function-Theorem","page":"Foundations of Differential Manifolds","title":"The Inverse Function Theorem","text":"","category":"section"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"The inverse function theorem gives a sufficient condition on a vector-valued function to be invertible in a neighborhood of a specific point. This theorem serves as a basis for the implicit function theorem and further for the preimage theorem and is critical in developing a theory of manifolds. Here we first state the theorem and then give a proof.","category":"page"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"Main.theorem(raw\"Consider a vector-valued differentiable function ``F:\\mathbb{R}^N\\to\\mathbb{R}^N`` and assume its Jacobian is non-degenerate at a point ``x\\in\\mathbb{R}^N``. Then there exists a neighborhood ``U`` that contains ``F(x)`` and on which ``F`` is invertible, i.e. ``\\exists{}H:U\\to\\mathbb{R}^N`` s.t. ``\\forall{}y\\in{}U,\\,F\\circ{}H(y) = y`` and ``H`` is differentiable.\"; name = \"Inverse function theorem\")","category":"page"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"Main.proof(raw\"Consider a mapping ``F:\\mathbb{R}^N\\to\\mathbb{R}^N`` and assume its Jacobian has full rank at point ``x``, i.e. ``\\det{}F'(x)\\neq0``. We further assume that ``F(x) = 0``, ``F'(x) = \\mathbb{I}`` and ``x = 0``. Now consider a ball around ``x`` whose radius ``r`` we do not yet fix and two points ``y`` and ``z`` in that ball: ``y,z\\in{}B(r)``. We further introduce the function ``G(y):=y-F(y)``. By the *mean value theorem* we have \n\" * Main.indentation * raw\"```math \n\" * Main.indentation * raw\"|G(y)| = |G(y) - x| = |G(y) - G(x)|\\leq|y-x|\\sup_{0<t<1}||G'(x + t(y-x))||,\n\" * Main.indentation * raw\"``` \n\" * Main.indentation * raw\"where ``||\\cdot||`` is the *operator norm*. Because ``t\\mapsto{}G'(x+t(y-x))`` is continuous and ``G'(x)=0`` there must exist an ``r`` s.t. ``\\forall{}t\\in[0,1],\\,||G'(x +t(y-x))||<1/2``. We have for any element ``y\\in{}B(r)``: ``|G(y) | \\leq ||G'(y)||\\cdot|y| < |y|/2``, so ``G(B(r))\\subset{}B(r/2)``. We further define ``G_z(y) := z + G(y)``; this map is contractive on ``B(r)`` (for ``z\\in{}B(r/2)``): ``|G_z(y)| \\leq |z| + |G(y) - x| < q < 1`` and therefore has a fixed point: ``y^* = G_z(y^*) = z + y^* - F(y^*)`` and we obtain ``z = F(y^*)``.  The inverse (which we call ``H:F(B(r/2))\\to{}B(r)``) is also continuous by the last theorem presented in the section on basic topological concepts. We now proof that the derivative of ``H`` at ``F(x) = 0`` exists and that it is equal to ``F'(H(z))^{-1}``. For this we let ``\\eta\\in{}F(B(r/2))`` go to zero. We further define ``\\xi = F(z)`` and ``h = H(\\xi + \\eta) - z``:\n\" *\nMain.indentation * raw\"```math\n\" *\nMain.indentation * raw\"\\begin{aligned}\n\" *\nMain.indentation * raw\"    |H(\\xi+\\eta) - H(\\xi) - F'(z)^{-1}\\eta| & = |h - F'(x)^{-1}\\xi| = |h - F'(z)^{-1}(F(z + h) - \\xi)| \\\\\n\" *\nMain.indentation * raw\"                                            & \\leq ||F'(z)^{-1}||\\cdot|F'(z)h - F(z + h) + \\xi| \\\\\n\" *\nMain.indentation * raw\"                                            & \\leq ||F'(z)^{-1}||\\cdot|h|\\cdot\\left| F'(z)\\frac{h}{|h|} - \\frac{F(z + h) - \\xi}{|h|} \\right|,\n\" *\nMain.indentation * raw\"\\end{aligned}\n\" *\nMain.indentation * raw\"```\n\" * \nMain.indentation * raw\"and the rightmost expression is bounded because of the mean value theorem: ``F(z + h) - F(z) \\leq sup_{0<t<1}|h| \\cdot ||F'(z + th)||``.\")","category":"page"},{"location":"manifolds/inverse_function_theorem/#The-Implicit-Function-Theorem","page":"Foundations of Differential Manifolds","title":"The Implicit Function Theorem","text":"","category":"section"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"This theorem is a direct consequence of the inverse function theorem. ","category":"page"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"Main.theorem(raw\"Given a function ``f:\\mathbb{R}^{n+m}\\to\\mathbb{R}^n`` whose derivative at ``x\\in\\mathbb{R}^{n+m}`` has full rank, we can find a map ``h:U\\to\\mathbb{R}^{n+m}`` for a neighborhood ``U\\ni(f(x), x_{n+1}, \\ldots, x_{n+m})`` such that ``f\\circ{}h`` is a projection onto the first factor, i.e. ``f(h(x_1, \\ldots, x_{n+m})) = (x_1, \\ldots, x_n).``\"; name = \"Implicit Function Theorem\")","category":"page"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"Main.proof(raw\"Consider the map ``x = (x_1, \\ldots, x_{n+m}) = (f(x), x_{n+1}, \\ldots, x_{n+m})``. The derivative of this map is clearly of full rank if ``f'(x)`` is of full rank and therefore invertible in a neighborhood around ``(f(x), x_{n+1}, \\ldots, x_{n+m})``. We call this inverse map ``h``. We then see that ``f\\circ{}h`` is a projection.\")","category":"page"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"The implicit function will be used to proof the preimage theorem which we use as a basis to construct all the manifolds in GeometricMachineLearning.","category":"page"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"\\begin{comment}","category":"page"},{"location":"manifolds/inverse_function_theorem/#References","page":"Foundations of Differential Manifolds","title":"References","text":"","category":"section"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"S. Lang. Fundamentals of differential geometry. Vol. 191 (Springer Science & Business Media, 2012).\n\n\n\n","category":"page"},{"location":"manifolds/inverse_function_theorem/","page":"Foundations of Differential Manifolds","title":"Foundations of Differential Manifolds","text":"\\end{comment}","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"In this chapter we use neural networks as symplectic integrators. We first show SympNets as an example of a neural network-based one-step method and then show linear symplectic transformers as an example of a symplectic multi-step method.","category":"page"},{"location":"tutorials/sympnet_tutorial/#SympNets-with-GeometricMachineLearning","page":"SympNets","title":"SympNets with GeometricMachineLearning","text":"","category":"section"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"This page serves as a short introduction into using SympNets with GeometricMachineLearning. ","category":"page"},{"location":"tutorials/sympnet_tutorial/#Loss-function","page":"SympNets","title":"Loss function","text":"","category":"section"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"The FeedForwardLoss is the default choice used in GeometricMachineLearning for training SympNets, this can however be changed or tweaked.","category":"page"},{"location":"tutorials/sympnet_tutorial/#Training-a-Harmonic-Oscillator","page":"SympNets","title":"Training a Harmonic Oscillator","text":"","category":"section"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"Here we begin with a simple example, the harmonic oscillator, the Hamiltonian of which is ","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"H(qp)inmathbbR^2 mapsto frac12p^2 + frac12q^2 in mathbbR","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"Here we take the ODE from GeometricProblems and integrate it with GeometricIntegrators [2]:","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"import GeometricProblems.HarmonicOscillator as ho\nusing GeometricIntegrators: ImplicitMidpoint, integrate\nimport Random # hide\nRandom.seed!(1234) # hide\n\n# the problem is the ODE of the harmonic oscillator\nho_problem = ho.hodeproblem(; tspan = 500)\n\n# integrate the system\nsolution = integrate(ho_problem, ImplicitMidpoint())\n\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"We call DataLoader in order to conveniently handle the data:","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"using GeometricMachineLearning # hide\ndl_raw = DataLoader(solution; suppress_info = true)\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"We have not yet specified the type and backend that we want to use. We do this now:","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"# specify the data type and the backend\ntype = Float16\nbackend = CPU()\n\n# we can then make a new instance of `DataLoader` with this backend and type.\ndl = DataLoader(dl_raw, backend, type)\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"Next we specify the architectures[1]: ","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"[1]: GeometricMachineLearning provides useful defaults for all parameters, but they can still be specified manually; which is what we are doing here. Details on these parameters can be found in the docstrings for GSympNet and LASympNet.","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"const upscaling_dimension = 2\nconst nhidden = 1\nconst activation = tanh\nconst n_layers = 4 # number of layers for the G-SympNet\nconst depth = 4 # number of layers in each linear block in the LA-SympNet\n\n# calling G-SympNet architecture \ngsympnet = GSympNet(dl; upscaling_dimension = upscaling_dimension, \n                        n_layers = n_layers, \n                        activation = activation)\n\n# calling LA-SympNet architecture \nlasympnet = LASympNet(dl;   nhidden = nhidden, \n                            activation = activation, \n                            depth = depth)\n\n# initialize the networks\nla_nn = NeuralNetwork(lasympnet, backend, type) \ng_nn = NeuralNetwork(gsympnet, backend, type)\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"If we want to obtain information on the number of parameters in a neural network, we can do that with the function parameterlength. For the LASympNet:","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"parameterlength(la_nn.model)","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"And for the GSympNet:","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"parameterlength(g_nn.model)","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"Main.remark(raw\"We can also specify whether we would like to start with a layer that changes the ``q``-component or one that changes the ``p``-component. This can be done via the keywords `init_upper` for the `GSympNet`, and `init_upper_linear` and `init_upper_act` for the `LASympNet`.\")","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"We have to define an optimizer which will be used in training of the SympNet. In this example we use Adam:","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"# set up optimizer; for this we first need to specify the optimization method\nopt_method = AdamOptimizer(type)\n# we then call the optimizer struct which allocates the cache\nla_opt = Optimizer(opt_method, la_nn)\ng_opt = Optimizer(opt_method, g_nn)\n\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"We can now perform the training of the neural networks:","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"# determine the batch size (the number of samples in one batch)\nconst batch_size = 16\n\nbatch = Batch(batch_size)\n\n# number of training epochs\nconst nepochs = 100\n\n# perform training (returns array that contains the total loss for each training step)\ng_loss_array = g_opt(g_nn, dl, batch, nepochs; show_progress = false)\nla_loss_array = la_opt(la_nn, dl, batch, nepochs; show_progress = false)\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"We plot the training errors against the epoch (here the y-axis is in log-scale):","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"using CairoMakie\nusing LaTeXStrings\n\nmorange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256) # hide\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256) # hide\n\nfunction make_error_plot(; theme = :dark) # hide\ntextcolor = theme == :dark ? :white : :black # hide\nfig = Figure(; backgroundcolor = :transparent)\nax = Axis(fig[1, 1]; \n    backgroundcolor = :transparent,\n    bottomspinecolor = textcolor, \n    topspinecolor = textcolor,\n    leftspinecolor = textcolor,\n    rightspinecolor = textcolor,\n    xtickcolor = textcolor, \n    ytickcolor = textcolor,\n    xticklabelcolor = textcolor,\n    yticklabelcolor = textcolor,\n    xlabel=\"Epoch\", \n    ylabel=\"Training loss\",\n    xlabelcolor = textcolor,\n    ylabelcolor = textcolor,\n    yscale = log10\n    )\n\nlines!(ax, g_loss_array, label=L\"$G$-SympNet\", color=morange)\nlines!(ax, la_loss_array, label=L\"$LA$-SympNet\", color=mpurple)\naxislegend(; position = (.82, .75), backgroundcolor = :transparent, labelcolor = textcolor) # hide\nfig_name = theme == :dark ? \"sympnet_training_loss_dark.png\" : \"sympnet_training_loss_light.png\" # hide\nsave(fig_name, fig; px_per_unit = 1.2) # hide\nend # hide\nmake_error_plot(; theme = :dark) # hide\nmake_error_plot(; theme = :light) # hide\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"(Image: ) (Image: )","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"Now we can make a prediction. We compare the initial data with a prediction starting from the same phase space point using the function GeometricMachineLearning.iterate:","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"ics = (q=dl.input.q[:, 1, 1], p=dl.input.p[:, 1, 1])\n\nsteps_to_plot = 200\n\n#predictions\nla_trajectory = iterate(la_nn, ics; n_points = steps_to_plot)\ng_trajectory =  iterate(g_nn, ics; n_points = steps_to_plot)\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"We now plot the result:","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"function make_prediction_plot(; theme = :dark) # hide\ntextcolor = theme == :dark ? :white : :black # hide\nfig = Figure(; backgroundcolor = :transparent)\n\nax = Axis(fig[1, 1]; \n    backgroundcolor = :transparent,\n    bottomspinecolor = textcolor, \n    topspinecolor = textcolor,\n    leftspinecolor = textcolor,\n    rightspinecolor = textcolor,\n    xtickcolor = textcolor, \n    ytickcolor = textcolor,\n    xticklabelcolor = textcolor,\n    yticklabelcolor = textcolor,\n    xlabel=L\"q\", \n    ylabel=L\"p\",\n    xlabelcolor = textcolor,\n    ylabelcolor = textcolor\n    )\n\nlines!(ax, dl.input.q[1, 1:steps_to_plot, 1], dl.input.p[1, 1:steps_to_plot, 1], label=\"training data\", color = mblue)\nlines!(ax, la_trajectory.q[1, :], la_trajectory.p[1, :], label=L\"$LA$-Sympnet\", color = mpurple)\nlines!(ax, g_trajectory.q[1, :], g_trajectory.p[1, :], label=L\"$G$-Sympnet\", color = morange)\naxislegend(; position = (.82, .45), backgroundcolor = :transparent, labelcolor = textcolor) # hide\nfig_name = theme == :dark ? \"sympnet_prediction_dark.png\" : \"sympnet_prediction_light.png\" # hide\nsave(fig_name, fig; px_per_unit = 1.2) # hide\nend # hide\nmake_prediction_plot(; theme = :dark) # hide\nmake_prediction_plot(; theme = :light) # hide\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"(Image: ) (Image: )","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"We see that GSympNet outperforms LASympNet on this problem; the blue line (reference) and the orange line (G-SympNet) are in fact almost indistinguishable.","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"Main.remark(raw\"We have actually never observed a scenario in which the ``LA``-SympNet can outperform the ``G``-SympNet. The ``G``-SympNet seems usually trains faster, is more accurate and less sensitive to the chosen hyperparameters and initialization of the weights. They are also more straightforward to interpret. We therefore use the ``G``-SympNet as a basis for the *linear symplectic transformer.*\")","category":"page"},{"location":"tutorials/sympnet_tutorial/#Comparison-with-a-ResNet","page":"SympNets","title":"Comparison with a ResNet","text":"","category":"section"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"We want to show the advantages of using a SympNet over a standard ResNet that is not symplectic. For this we make a ResNet with a similar size of parameters as the two SympNets have:","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"Random.seed!(1234) # hide\nresnet = ResNet(dl, n_layers ÷ 2; activation = activation)\n\nrn_nn = NeuralNetwork(resnet, backend, type)\n\nparameterlength(rn_nn)","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"We now train the network ldots","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"rn_opt = Optimizer(opt_method, rn_nn)\n\nrn_loss_array = rn_opt(rn_nn, dl, batch, nepochs; show_progress = false)\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"and plot the loss:","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"using CairoMakie\nusing LaTeXStrings\n\nmorange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256) # hide\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256) # hide\n\nfunction make_error_plot(; theme = :dark) # hide\ntextcolor = theme == :dark ? :white : :black # hide\nfig = Figure(; backgroundcolor = :transparent)\nax = Axis(fig[1, 1]; \n    backgroundcolor = :transparent,\n    bottomspinecolor = textcolor, \n    topspinecolor = textcolor,\n    leftspinecolor = textcolor,\n    rightspinecolor = textcolor,\n    xtickcolor = textcolor, \n    ytickcolor = textcolor,\n    xticklabelcolor = textcolor,\n    yticklabelcolor = textcolor,\n    xlabel=\"Epoch\", \n    ylabel=\"Training loss\",\n    xlabelcolor = textcolor,\n    ylabelcolor = textcolor,\n    yscale = log10\n    )\n\nlines!(ax, g_loss_array, label=L\"$G$-SympNet\", color=morange)\nlines!(ax, la_loss_array, label=L\"$LA$-SympNet\", color=mpurple)\nlines!(ax, rn_loss_array, label=\"ResNet\", color=mred)\naxislegend(; position = (.82, .75), backgroundcolor = :transparent, labelcolor = textcolor) # hide\nfig_name = theme == :dark ? \"sympnet_resnet_training_loss_dark.png\" : \"sympnet_resnet_training_loss_light.png\" # hide\nsave(fig_name, fig; px_per_unit = 1.2) # hide\nend # hide\nmake_error_plot(; theme = :dark) # hide\nmake_error_plot(; theme = :light) # hide\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"(Image: ) (Image: )","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"And we see that the loss is significantly lower than for the LA-SympNet, but slightly higher than for the G-SympNet. We can also plot the prediction:","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"rn_trajectory = iterate(rn_nn, ics; n_points = steps_to_plot)\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"function make_prediction_plot(; theme = :dark) # hide\ntextcolor = theme == :dark ? :white : :black # hide\nfig = Figure(; backgroundcolor = :transparent)\n\nax = Axis(fig[1, 1]; \n    backgroundcolor = :transparent,\n    bottomspinecolor = textcolor, \n    topspinecolor = textcolor,\n    leftspinecolor = textcolor,\n    rightspinecolor = textcolor,\n    xtickcolor = textcolor, \n    ytickcolor = textcolor,\n    xticklabelcolor = textcolor,\n    yticklabelcolor = textcolor,\n    xlabel=L\"q\", \n    ylabel=L\"p\",\n    xlabelcolor = textcolor,\n    ylabelcolor = textcolor\n    )\n\nlines!(ax, dl.input.q[1, 1:steps_to_plot, 1], dl.input.p[1, 1:steps_to_plot, 1], label=\"training data\", color = mblue)\nlines!(ax, la_trajectory.q[1, :], la_trajectory.p[1, :], label=L\"$LA$-Sympnet\", color = mpurple)\nlines!(ax, g_trajectory.q[1, :], g_trajectory.p[1, :], label=L\"$G$-Sympnet\", color = morange)\nlines!(ax, rn_trajectory.q[1, :], rn_trajectory.p[1, :], label=\"ResNet\", color = mred)\naxislegend(; position = (.82, .45), backgroundcolor = :transparent, labelcolor = textcolor) # hide\nfig_name = theme == :dark ? \"resnet_sympnet_prediction_dark.png\" : \"resnet_sympnet_prediction_light.png\" # hide\nsave(fig_name, fig; px_per_unit = 1.2) # hide\nend # hide\nmake_prediction_plot(; theme = :dark) # hide\nmake_prediction_plot(; theme = :light) # hide\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"(Image: ) (Image: )","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"We see that the ResNet is slowly gaining energy which consitutes unphysical behaviour. If we let this simulation run for even longer, this effect gets more pronounced:","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"steps_to_plot = 800\n\n#predictions\nla_trajectory = iterate(la_nn, ics; n_points = steps_to_plot)\ng_trajectory =  iterate(g_nn, ics; n_points = steps_to_plot)\nrn_trajectory = iterate(rn_nn, ics; n_points = steps_to_plot)\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"function make_prediction_plot(; theme = :dark) # hide\ntextcolor = theme == :dark ? :white : :black # hide\nfig = Figure(; backgroundcolor = :transparent)\n\nax = Axis(fig[1, 1]; \n    backgroundcolor = :transparent,\n    bottomspinecolor = textcolor, \n    topspinecolor = textcolor,\n    leftspinecolor = textcolor,\n    rightspinecolor = textcolor,\n    xtickcolor = textcolor, \n    ytickcolor = textcolor,\n    xticklabelcolor = textcolor,\n    yticklabelcolor = textcolor,\n    xlabel=L\"q\", \n    ylabel=L\"p\",\n    xlabelcolor = textcolor,\n    ylabelcolor = textcolor\n    )\n\nlines!(ax, dl.input.q[1, 1:steps_to_plot, 1], dl.input.p[1, 1:steps_to_plot, 1], label=\"training data\", color = mblue)\nlines!(ax, la_trajectory.q[1, :], la_trajectory.p[1, :], label=L\"$LA$-Sympnet\", color = mpurple)\nlines!(ax, g_trajectory.q[1, :], g_trajectory.p[1, :], label=L\"$G$-Sympnet\", color = morange)\nlines!(ax, rn_trajectory.q[1, :], rn_trajectory.p[1, :], label=\"ResNet\", color = mred)\naxislegend(; position = (.99, .9), backgroundcolor = :transparent, labelcolor = textcolor) # hide\nfig_name = theme == :dark ? \"resnet_sympnet_prediction_long_dark.png\" : \"resnet_sympnet_prediction_long_light.png\" # hide\nsave(fig_name, fig; px_per_unit = 1.2) # hide\nend # hide\nmake_prediction_plot(; theme = :dark) # hide\nmake_prediction_plot(; theme = :light) # hide\nnothing # hide","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"(Image: ) (Image: )","category":"page"},{"location":"tutorials/sympnet_tutorial/","page":"SympNets","title":"SympNets","text":"The behavior the ResNet exhibits is characteristic of integration schemes that do not preserve structure: the error in a single time step can be made very small, but for long-time simulations one typically has to consider symplecticity or other properties. Also note that the curves produced by the LA-SympNet and the G-SympNet are closed (or nearly closed). This is a property of symplectic maps in two dimensions that is preserved by construction [1].","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"In this chapter we show how to use symplectic autoencoders for the offline phase of reduced order modeling. The example we consider here is the \\textit{Toda lattice}. We further discuss the online phase and compare a standard integrator (implicit midpoint) with a transformer neural network.","category":"page"},{"location":"tutorials/symplectic_autoencoder/#Symplectic-Autoencoders-and-the-Toda-Lattice","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders and the Toda Lattice","text":"","category":"section"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"In this tutorial we use a symplectic autoencoder to approximate the solution of the Toda lattice with a lower-dimensional Hamiltonian model and compare it with standard proper symplectic decomposition.","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"Main.remark(raw\"As with any neural network we have to make the following choices:\n\" * Main.indentation * raw\"1. specify the *architecture*,\n\" * Main.indentation * raw\"2. specify the *type* and *backend*,\n\" * Main.indentation * raw\"3. pick an *optimizer* for training the network,\n\" * Main.indentation * raw\"4. specify how you want to perform *batching*,\n\" * Main.indentation * raw\"5. choose a *number of epochs*,\n\" * Main.indentation * raw\"where points 1 and 3 depend on a variable number of hyperparameters.\")","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"For the symplectic autoencoder point 1 is done by calling SymplecticAutoencoder, point 2 is done by calling NeuralNetwork, point 3 is done by calling Optimizer and point 4 is done by calling Batch.","category":"page"},{"location":"tutorials/symplectic_autoencoder/#The-system","page":"Symplectic Autoencoders","title":"The system","text":"","category":"section"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"The Toda lattice [89] is a prototypical example of a Hamiltonian PDE. It is described by ","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"    H(q p) = sum_ninmathbbZleft(  fracp_n^22 + alpha e^q_n - q_n+1 right)","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"Starting from this equation we further assume a finite number of particles N and impose periodic boundary conditions: ","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"beginaligned\n    q_n+N   equiv q_n  \n    p_n+N    equiv p_n\nendaligned","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"In this tutorial we want to reduce the dimension of the big system by a significant factor with (i) proper symplectic decomposition (PSD) and (ii) symplectic autoencoders (SAE). The first approach is strictly linear whereas the second one allows for more general mappings. ","category":"page"},{"location":"tutorials/symplectic_autoencoder/#Using-the-Toda-lattice-in-numerical-experiments","page":"Symplectic Autoencoders","title":"Using the Toda lattice in numerical experiments","text":"","category":"section"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"In order to use the Toda lattice in numerical experiments we have to pick suitable initial conditions. For this, consider the third-degree spline: ","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"h(s)  = begincases\n        1 - frac32s^2 + frac34s^3  textif  0 leq s leq 1  \n        frac14(2 - s)^3  textif  1  s leq 2  \n        0  textelse \nendcases","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"Plotted on the relevant domain it looks like this: ","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"(Image: ) (Image: )","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We end up with the following choice of parametrized initial conditions: ","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"u_0(mu)(omega) = h(s(omega mu)) quad s(omega mu) =  20 mu  omega + fracmu2","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"where the omega is an element of the domain Omega = -05 05 For the purposes of this tutorial we will use the default value for mu provided in GeometricProblems:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"import GeometricProblems.TodaLattice as tl\n\nN = tl.Ñ # hide\nΔx = 1. / (N - 1) # hide\nΩ = -0.5 : Δx : 0.5 # hide\ntl.μ","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We thus look at the displacement of N = 200 particles on the periodic domain Omega = -05 05 simeq S^1 where the equivalence relation  indicates that we associate the points -05 and 05 with each other.","category":"page"},{"location":"tutorials/symplectic_autoencoder/#Get-the-data","page":"Symplectic Autoencoders","title":"Get the data","text":"","category":"section"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"The training data can very easily be obtained by using the packages GeometricProblems and GeometricIntegrators:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"using GeometricIntegrators: integrate, ImplicitMidpoint\nusing GeometricMachineLearning \nimport Random # hide\n\npr = tl.hodeproblem(; tspan = (0.0, 800.))\nsol = integrate(pr, ImplicitMidpoint())\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We then put the format in the correct format by calling DataLoader[1]:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"[1]: For more information on DataLoader see the corresponding section.","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"dl_cpu = DataLoader(sol; autoencoder = true, suppress_info = true)\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"Also note that the keyword autoencoder was set to true when calling DataLoader. The keyword argument supress_info determines whether data loader provides some additional information on the data it is called on.","category":"page"},{"location":"tutorials/symplectic_autoencoder/#Train-the-network","page":"Symplectic Autoencoders","title":"Train the network","text":"","category":"section"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We now want to compare two different approaches: PSDArch and SymplecticAutoencoder. For this we first have to set up the networks: ","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"const reduced_dim = 2\n\nRandom.seed!(123) # hide\npsd_arch = PSDArch(dl_cpu.input_dim, reduced_dim)\nsae_arch = SymplecticAutoencoder(dl_cpu.input_dim, reduced_dim; n_encoder_blocks = 4, \n                                                                n_decoder_blocks = 4, \n                                                                n_encoder_layers = 2, \n                                                                n_decoder_layers = 2)\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"Training a neural network is usually done by calling an instance of Optimizer in GeometricMachineLearning. PSDArch however can be solved directly by using singular value decomposition and this is done by calling solve!:  ","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"psd_nn_cpu = NeuralNetwork(psd_arch, CPU(), eltype(dl_cpu))\n\nsolve!(psd_nn_cpu, dl_cpu)","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"The SymplecticAutoencoder we train with the AdamOptimizerWithDecay however[2]:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"[2]: It is not feasible to perform the training on CPU, which is why we use CUDA [11] here. We further perform the training in single precision.","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"using CUDA\n\nconst n_epochs = 262144\nconst batch_size = 4096\n\nbackend = CUDABackend()\ndl = DataLoader(dl_cpu, backend, Float32)\n\n\nsae_nn_gpu = NeuralNetwork(sae_arch, CUDADevice(), Float32)\no = Optimizer(sae_nn_gpu, AdamOptimizerWithDecay(integrator_train_epochs))\n\n# train the network\no(sae_nn_gpu, dl, Batch(batch_size), n_epochs)","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"After training we map the network parameters to cpu:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"const mtc = GeometricMachineLearning.map_to_cpu\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"sae_nn_cpu = mtc(sae_nn_gpu)","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"using JLD2\n\nsae_trained_parameters = load(\"sae_parameters.jld2\")[\"sae_parameters\"]\n_nnp(ps::Tuple) = NeuralNetworkParameters{Tuple(Symbol(\"L$(i)\") for i in 1:length(ps))}(ps)\nsae_nn_cpu = NeuralNetwork(sae_arch, Chain(sae_arch), _nnp(sae_trained_parameters), CPU())\n\nnothing  # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/#The-online-stage-with-a-standard-integrator","page":"Symplectic Autoencoders","title":"The online stage with a standard integrator","text":"","category":"section"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"After having trained our neural network we can now evaluate it in the online stage of reduced complexity modeling: ","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"psd_rs = HRedSys(pr, encoder(psd_nn_cpu), decoder(psd_nn_cpu); integrator = ImplicitMidpoint())\nsae_rs = HRedSys(pr, encoder(sae_nn_cpu), decoder(sae_nn_cpu); integrator = ImplicitMidpoint())\n\nnothing  # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We integrate the full system (again) as well as the two reduced systems[3]:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"[3]: All of this is done with ImplicitMidpoint as integrator.","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"integrate_full_system(psd_rs) # hide\nintegrate_reduced_system(psd_rs) # hide\nintegrate_reduced_system(sae_rs) # hide\n\n@time \"FOM + Implicit Midpoint\" sol_full = integrate_full_system(psd_rs) # hide\n@time \"PSD + Implicit Midpoint\" sol_psd_reduced = integrate_reduced_system(psd_rs) # hide\n@time \"SAE + Implicit Midpoint\" sol_sae_reduced = integrate_reduced_system(sae_rs) # hide\n\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"And plot the solutions for ","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"time_steps = (0, 300, 800)\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"using CairoMakie\n\nmorange = RGBf(255 / 256, 127 / 256, 14 / 256)\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) \nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256)\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256)\nmgreen = RGBf(44 / 256, 160 / 256, 44 / 256)\n\n# plot validation\nfunction plot_validation!(fig, coordinates::Tuple, t_steps::Integer=100; theme = :dark)\n    textcolor = theme == :dark ? :white : :black\n    ax_val = Axis(fig[coordinates[1], coordinates[2]]; backgroundcolor = :transparent,\n                                                                bottomspinecolor = textcolor, \n                                                                topspinecolor = textcolor,\n                                                                leftspinecolor = textcolor,\n                                                                rightspinecolor = textcolor,\n                                                                xtickcolor = textcolor, \n                                                                ytickcolor = textcolor,\n                                                                xticklabelcolor = textcolor,\n                                                                yticklabelcolor = textcolor,\n                                                                xlabel=L\"\\omega\", \n                                                                ylabel=L\"q\",\n                                                                xlabelcolor = textcolor,\n                                                                ylabelcolor = textcolor)\n    lines!(ax_val, Ω, sol_full.s.q[t_steps], label = rich(\"FOM + Implicit Midpoint\"; color = textcolor), color = mblue)\n    lines!(ax_val, Ω, psd_rs.decoder((q = sol_psd_reduced.s.q[t_steps], p = sol_psd_reduced.s.p[t_steps])).q, \n        label = rich(\"PSD + Implicit Midpoint\"; color = textcolor), color = morange)\n    lines!(ax_val, Ω, sae_rs.decoder((q = sol_sae_reduced.s.q[t_steps], p = sol_sae_reduced.s.p[t_steps])).q, \n        label = rich(\"SAE + Implicit Midpoint\"; color = textcolor), color = mgreen)\n\n    if t_steps == 0\n        axislegend(ax_val; position = (1.01, 1.5), backgroundcolor = theme == :dark ? :transparent : :white, color = textcolor, labelsize = 8)\n    end\n    nothing\nend\n\nfig_light = Figure(; backgroundcolor = :transparent)\n\nfig_dark = Figure(; backgroundcolor = :transparent)\n\nfor (i, time) in zip(1:length(time_steps), time_steps)\n    plot_validation!(fig_light, (i, 1), time; theme = :light)\n    plot_validation!(fig_dark, (i, 1), time; theme = :dark)\nend\n\n# axislegend(fig_light; position = (.82, .75), backgroundcolor = :transparent, color = :black)\n# axislegend(fig_dark;  position = (.82, .75), backgroundcolor = :transparent, color = :white)\n\nsave(\"sae_validation_light.png\", fig_light; px_per_unit = 1.2)\nsave(\"sae_validation_dark.png\", fig_dark; px_per_unit = 1.2)\n\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"(Image: Comparison between FOM (blue), PSD with implicit midpoint (orange) and SAE with implicit midpoint (green).) (Image: Comparison between FOM (blue), PSD with implicit midpoint (orange) and SAE with implicit midpoint (green).)","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We can see that the SAE has much more approximation capabilities than the PSD. But even though the SAE reasonably reproduces the full-order model (FOM), we see that the online stage of the SAE takes even longer than evaluating the FOM. In order to solve this problem we have to make the online stage more efficient.","category":"page"},{"location":"tutorials/symplectic_autoencoder/#The-online-stage-with-a-neural-network","page":"Symplectic Autoencoders","title":"The online stage with a neural network","text":"","category":"section"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"Instead of using a standard integrator we can also use a neural network that is trained on the reduced data. For this: ","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"backend = CPU() # hide\nconst integrator_train_epochs = 65536\nconst integrator_batch_size = 4096\nconst seq_length = 4\n\nintegrator_architecture = StandardTransformerIntegrator(reduced_dim; \n                                                                    transformer_dim = 20, \n                                                                    n_blocks = 3, \n                                                                    n_heads = 5, \n                                                                    L = 3,\n                                                                    upscaling_activation = tanh)\n\nintegrator_nn = NeuralNetwork(integrator_architecture, backend)\n\nintegrator_method = AdamOptimizerWithDecay(integrator_train_epochs)\n\no_integrator = Optimizer(integrator_method, integrator_nn)\n\ndl = dl_cpu # hide\n# map from autoencoder type to integrator type\ndl_integration = DataLoader(dl; autoencoder = false)\n\nintegrator_batch = Batch(integrator_batch_size, seq_length)\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"loss = GeometricMachineLearning.ReducedLoss(encoder(sae_nn_gpu), decoder(sae_nn_gpu))\n\ntrain_integrator_loss = o_integrator(   integrator_nn, \n                                        dl_integration, \n                                        integrator_batch, \n                                        integrator_train_epochs, \n                                        loss)","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We can now evaluate the solution:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"nn_integrator_parameters = load(\"integrator_parameters.jld2\")[\"integrator_parameters\"] # hide\nintegrator_nn = NeuralNetwork(integrator_architecture, Chain(integrator_architecture), _nnp(nn_integrator_parameters), backend) # hide\nics = encoder(sae_nn_cpu)((q = dl.input.q[:, 1:seq_length, 1], p = dl.input.p[:, 1:seq_length, 1])) # hide\niterate(mtc(integrator_nn), ics; n_points = length(sol.t), prediction_window = seq_length) # hide\n@time \"time stepping with transformer\" time_series = iterate(mtc(integrator_nn), ics; n_points = length(sol.t), prediction_window = seq_length)\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"# plot validation\nfunction plot_transformer_validation!(fig, coordinates, t_steps::Integer=100; theme = :dark)\n    textcolor = theme == :dark ? :white : :black\n    ax_val = Axis(fig[coordinates[1], coordinates[2]]; backgroundcolor = :transparent,\n                                                                bottomspinecolor = textcolor, \n                                                                topspinecolor = textcolor,\n                                                                leftspinecolor = textcolor,\n                                                                rightspinecolor = textcolor,\n                                                                xtickcolor = textcolor, \n                                                                ytickcolor = textcolor,\n                                                                xticklabelcolor = textcolor,\n                                                                yticklabelcolor = textcolor,\n                                                                xlabel = L\"\\omega\", \n                                                                ylabel = L\"q\",\n                                                                xlabelcolor = textcolor,\n                                                                ylabelcolor = textcolor)\n    lines!(ax_val, Ω, sol_full.s.q[t_steps], label = rich(\"FOM + Implicit Midpoint\"; color = textcolor), color = mblue)\n    lines!(ax_val, Ω, psd_rs.decoder((q = sol_psd_reduced.s.q[t_steps], p = sol_psd_reduced.s.p[t_steps])).q, \n        label = rich(\"PSD + Implicit Midpoint\"; color = textcolor), color = morange)\n    lines!(ax_val, Ω, sae_rs.decoder((q = sol_sae_reduced.s.q[t_steps], p = sol_sae_reduced.s.p[t_steps])).q, \n        label = rich(\"SAE + Implicit Midpoint\"; color = textcolor), color = mgreen)\n\n    time_series = iterate(mtc(integrator_nn), ics; n_points = t_steps, prediction_window = seq_length)\n    # prediction = (q = time_series.q[:, end], p = time_series.p[:, end])\n    prediction = (q = time_series.q[:, end], p = time_series.p[:, end])\n    sol = decoder(sae_nn_cpu)(prediction)\n\n    lines!(ax_val, Ω, sol.q; label = rich(\"SAE + Transformer\"; color = textcolor), color = mpurple)\n\n    if t_steps == 0\n        axislegend(ax_val; position = (1.01, .9), backgroundcolor = theme == :dark ? :transparent : :white, color = textcolor, labelsize = 8, nbanks = 2)\n    end\n    nothing\nend\n\nfig_light = Figure(; backgroundcolor = :transparent)\n\nfig_dark = Figure(; backgroundcolor = :transparent)\n\nfor (i, time) in zip(1:length(time_steps), time_steps)\n    plot_transformer_validation!(fig_light, (i, 1), time; theme = :light)\n    plot_transformer_validation!(fig_dark, (i, 1), time; theme = :dark)\nend\n\nsave(\"sae_integrator_validation_light.png\", fig_light; px_per_unit = 1.2)\nsave(\"sae_integrator_validation_dark.png\", fig_dark; px_per_unit = 1.2)\n\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"(Image: Comparison between FOM (blue), PSD with implicit midpoint (orange), SAE with implicit midpoint (green) and SAE with transformer (purple).) (Image: Comparison between FOM (blue), PSD with implicit midpoint (orange), SAE with implicit midpoint (green) and SAE with transformer (purple).)","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"Note that integration of the system with the transformer is orders of magnitudes faster than any comparable method and also leads to an improvement in accuracy over the case where we build the reduced space with the symplectic autoencoder and use implicit midpoint in the online phase.","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"Main.remark(raw\"While training the symplectic autoencoder we completely ignore the online phase, but only aim at finding a good low-dimensional approximation to the solution manifold. This is why we observe that the approximated solution differs somewhat form the actual one when using implicit midpoint for integrating the low-dimensional system (blue line vs. green line).\")","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"\\begin{comment}","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"Here we compared PSD with an SAE whith the same reduced dimension. One may argue that this is not entirely fair as the PSD has much fewer parameters than the SAE:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"(parameterlength(psd_nn_cpu), parameterlength(sae_nn_cpu))","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"and we also saw that evaluating PSD + Implicit Midpoint is much faster than SAE + Implicit Midpoint. We thus model the system with PSDs of higher reduced dimension:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"const reduced_dim2 = 8\n\nRandom.seed!(123) # hide\npsd_arch2 = PSDArch(dl_cpu.input_dim, reduced_dim2)\n\npsd_nn2 = NeuralNetwork(psd_arch2, CPU(), eltype(dl_cpu))\n\nsolve!(psd_nn2, dl_cpu)","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"And we see that the error is a lot lower than for the case reduced_dim = 2. We now proceed with building the reduced Hamiltonian system as before, again using HRedSys:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"psd_rs2 = HRedSys(pr, encoder(psd_nn2), decoder(psd_nn2); integrator = ImplicitMidpoint())\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We integrate this PSD to check how big the difference in performance is:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"integrate_reduced_system(psd_rs2) # hide\n@time \"PSD + Implicit Midpoint\" sol_psd_reduced2 = integrate_reduced_system(psd_rs2) # hide\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We can also plot the comparison with the FOM as before:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"morange = RGBf(255 / 256, 127 / 256, 14 / 256)\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) \nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256)\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256)\nmgreen = RGBf(44 / 256, 160 / 256, 44 / 256)\n\n# plot validation\nfunction plot_validation!(fig, coordinates::Tuple, t_steps::Integer=100; theme = :dark)\n    textcolor = theme == :dark ? :white : :black\n    ax_val = Axis(fig[coordinates[1], coordinates[2]]; backgroundcolor = :transparent,\n                                                                bottomspinecolor = textcolor, \n                                                                topspinecolor = textcolor,\n                                                                leftspinecolor = textcolor,\n                                                                rightspinecolor = textcolor,\n                                                                xtickcolor = textcolor, \n                                                                ytickcolor = textcolor,\n                                                                xticklabelcolor = textcolor,\n                                                                yticklabelcolor = textcolor,\n                                                                xlabel=L\"\\omega\", \n                                                                ylabel=L\"q\",\n                                                                xlabelcolor = textcolor,\n                                                                ylabelcolor = textcolor)\n    lines!(ax_val, Ω, sol_full.s.q[t_steps], label = rich(\"FOM + Implicit Midpoint\"; color = textcolor), color = mblue)\n    lines!(ax_val, Ω, psd_rs2.decoder((q = sol_psd_reduced2.s.q[t_steps], p = sol_psd_reduced2.s.p[t_steps])).q, \n        label = rich(\"PSD + Implicit Midpoint\"; color = textcolor), color = morange)\n\n    if t_steps == 0\n        axislegend(ax_val; position = (1.01, 1.5), backgroundcolor = theme == :dark ? :transparent : :white, color = textcolor, labelsize = 8)\n    end\n    nothing\nend\n\nfig_light = Figure(; backgroundcolor = :transparent)\n\nfig_dark = Figure(; backgroundcolor = :transparent)\n\nfor (i, time) in zip(1:length(time_steps), time_steps)\n    plot_validation!(fig_light, (i, 1), time; theme = :light)\n    plot_validation!(fig_dark, (i, 1), time; theme = :dark)\nend\n\n# axislegend(fig_light; position = (.82, .75), backgroundcolor = :transparent, color = :black)\n# axislegend(fig_dark;  position = (.82, .75), backgroundcolor = :transparent, color = :white)\n\nsave(\"psd_validation2_light.png\", fig_light; px_per_unit = 1.2)\nsave(\"psd_validation2_dark.png\", fig_dark; px_per_unit = 1.2)\n\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"(Image: Comparison between the FOM and the PSD with a bigger reduced dimension.) (Image: Comparison between the FOM and the PSD with a bigger reduced dimension.)","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We see that for a reduced dimension of 2n = 8 the PSD looks slightly better than the SAE for 2n = 2 As with the SAE we can also use a transformer to integrate the dynamics on the low-dimensional space:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"const integrator_architecture2 = StandardTransformerIntegrator(reduced_dim2; \n                                                                            transformer_dim = 20, \n                                                                            n_blocks = 3, \n                                                                            n_heads = 5, \n                                                                            L = 3, \n                                                                            upscaling_activation = tanh)\nintegrator_nn2 = NeuralNetwork(integrator_architecture2, backend)\nconst integrator_method2 = AdamOptimizerWithDecay(integrator_train_epochs)\nconst o_integrator2 = Optimizer(integrator_method2, integrator_nn2)\n\nloss2 = GeometricMachineLearning.ReducedLoss(encoder(psd_nn2), decoder(psd_nn2))\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"For training we leave dl_integration, integrator_batch and integrator_train_epochs unchanged:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"train_integrator_loss2 = o_integrator(integrator_nn2, dl_integration, integrator_batch, integrator_train_epochs, loss2)","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"nn_integrator_parameters2 = load(\"integrator_parameters_psd.jld2\")[\"integrator_parameters\"] # hide\nintegrator_nn2 = NeuralNetwork(integrator_architecture2, Chain(integrator_architecture2), _nnp(nn_integrator_parameters2), backend) # hide\nics = encoder(psd_nn2)((q = dl_cpu.input.q[:, 1:seq_length, 1], p = dl_cpu.input.p[:, 1:seq_length, 1])) # hide\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We again integrate the system and then plot the result:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"iterate(mtc(integrator_nn2), ics; n_points = length(sol.t), prediction_window = seq_length) # hide\n@time \"time stepping with transformer\" time_series2 = iterate(mtc(integrator_nn2), ics; n_points = length(sol.t), prediction_window = seq_length)\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"We see that using the transformer on the six-dimensional PSD-reduced system takes slightly longer than using the transformer on the two-dimensional SAE-reduced system. The accuracy is much worse however. Before we plotted the solution for:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"time_steps","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"Now we do so with:","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"time_steps = (0, 4, 5)\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"# plot validation\nfunction plot_validation!(fig, coordinates::Tuple, t_steps::Integer=100; theme = :dark)\n    textcolor = theme == :dark ? :white : :black\n    ax_val = Axis(fig[coordinates[1], coordinates[2]]; backgroundcolor = :transparent,\n                                                                bottomspinecolor = textcolor, \n                                                                topspinecolor = textcolor,\n                                                                leftspinecolor = textcolor,\n                                                                rightspinecolor = textcolor,\n                                                                xtickcolor = textcolor, \n                                                                ytickcolor = textcolor,\n                                                                xticklabelcolor = textcolor,\n                                                                yticklabelcolor = textcolor,\n                                                                xlabel=L\"\\omega\", \n                                                                ylabel=L\"q\",\n                                                                xlabelcolor = textcolor,\n                                                                ylabelcolor = textcolor)\n    lines!(ax_val, Ω, sol_full.s.q[t_steps], label = rich(\"FOM + Implicit Midpoint\"; color = textcolor), color = mblue)\n    lines!(ax_val, Ω, psd_rs2.decoder((q = sol_psd_reduced2.s.q[t_steps], p = sol_psd_reduced2.s.p[t_steps])).q, \n        label = rich(\"PSD + Implicit Midpoint\"; color = textcolor), color = morange)\n\n    time_series2 = iterate(mtc(integrator_nn2), ics; n_points = t_steps, prediction_window = seq_length)\n    # prediction = (q = time_series.q[:, end], p = time_series.p[:, end])\n    prediction2 = (q = time_series2.q[:, end], p = time_series2.p[:, end])\n    sol = decoder(psd_nn2)(prediction2)\n\n    lines!(ax_val, Ω, sol.q; label = rich(\"PSD + Transformer\"; color = textcolor), color = mred)\n\n    if t_steps == 0\n        axislegend(ax_val; position = (1.01, 1.5), backgroundcolor = theme == :dark ? :transparent : :white, color = textcolor, labelsize = 8)\n    end\n    nothing\nend\n\nfig_light = Figure(; backgroundcolor = :transparent)\n\nfig_dark = Figure(; backgroundcolor = :transparent)\n\nfor (i, time) in zip(1:length(time_steps), time_steps)\n    plot_validation!(fig_light, (i, 1), time; theme = :light)\n    plot_validation!(fig_dark, (i, 1), time; theme = :dark)\nend\n\n# axislegend(fig_light; position = (.82, .75), backgroundcolor = :transparent, color = :black)\n# axislegend(fig_dark;  position = (.82, .75), backgroundcolor = :transparent, color = :white)\n\nsave(\"psd_integrator_validation_light.png\", fig_light; px_per_unit = 1.2)\nsave(\"psd_integrator_validation_dark.png\", fig_dark; px_per_unit = 1.2)\n\nnothing # hide","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"(Image: Comparison between FOM (blue), PSD with implicit midpoint (orange), and PSD with transformer (red).) (Image: Comparison between FOM (blue), PSD with implicit midpoint (orange), and PSD with transformer (red).)","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"Here we however see a dramatic deterioration in the quality of the approximation. We assume that this because the transformer_dim was chosen to be 20 for the SAE and the PSD, but in the second case the reduced space is of dimension six, whereas it is of dimension two in the first case. This may mean that we need an even bigger transformer to find a good approximation of the reduced space.","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"\\end{comment}","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"\\section*{Chapter Summary}\nWe showed that for the Toda lattice we can achieve a very good approximation to the full-order system by using a two-dimensional reduced space; we also showed that for such a small space proper symplectic decomposition utterly fails.\n\nWe however also saw that when we use standard integrators in the online phase, the very low dimension of the reduced space may not mean fast computation. But by using a transformer as a neural network-based integrator we could circumvent this problem and achieve a speed-up of a factor of approximately 1000.\n\n\\begin{comment}","category":"page"},{"location":"tutorials/symplectic_autoencoder/#References","page":"Symplectic Autoencoders","title":"References","text":"","category":"section"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"L. Peng and K. Mohseni. Symplectic model reduction of Hamiltonian systems. SIAM Journal on Scientific Computing 38, A1–A27 (2016).\n\n\n\nC. Greif and K. Urban. Decay of the Kolmogorov N-width for wave problems. Applied Mathematics Letters 96, 216–222 (2019).\n\n\n\nP. Buchfink, S. Glas and B. Haasdonk. Symplectic model reduction of Hamiltonian systems on nonlinear manifolds and approximation with weakly symplectic autoencoder. SIAM Journal on Scientific Computing 45, A289–A311 (2023).\n\n\n\nB. Brantner and M. Kraus. Symplectic autoencoders for Model Reduction of Hamiltonian Systems, arXiv preprint arXiv:2312.10004 (2023).\n\n\n\n","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"\\end{comment}","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"<!--","category":"page"},{"location":"tutorials/symplectic_autoencoder/#References-2","page":"Symplectic Autoencoders","title":"References","text":"","category":"section"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"L. Peng and K. Mohseni. Symplectic model reduction of Hamiltonian systems. SIAM Journal on Scientific Computing 38, A1–A27 (2016).\n\n\n\nC. Greif and K. Urban. Decay of the Kolmogorov N-width for wave problems. Applied Mathematics Letters 96, 216–222 (2019).\n\n\n\nP. Buchfink, S. Glas and B. Haasdonk. Symplectic model reduction of Hamiltonian systems on nonlinear manifolds and approximation with weakly symplectic autoencoder. SIAM Journal on Scientific Computing 45, A289–A311 (2023).\n\n\n\nB. Brantner and M. Kraus. Symplectic autoencoders for Model Reduction of Hamiltonian Systems, arXiv preprint arXiv:2312.10004 (2023).\n\n\n\n","category":"page"},{"location":"tutorials/symplectic_autoencoder/","page":"Symplectic Autoencoders","title":"Symplectic Autoencoders","text":"-->","category":"page"},{"location":"optimizers/manifold_related/retractions/#Retractions","page":"Retractions","title":"Retractions","text":"","category":"section"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"In practice we usually do not solve the geodesic equation exactly in each optimization step (even though this is possible and computationally feasible), but prefer approximations that are called \"retractions\" [22] for numerical stability. The definition of a retraction in GeometricMachineLearning is slightly different from how it is usually defined in textbooks [1, 22]. We discuss these differences here.","category":"page"},{"location":"optimizers/manifold_related/retractions/#Classical-Retractions","page":"Retractions","title":"Classical Retractions","text":"","category":"section"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"By \"classical retraction\" we here mean the textbook definition. ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Main.definition(raw\"A **classical retraction** is a smooth map\n\" * Main.indentation * raw\"```math \n\" * Main.indentation * raw\"R: T\\mathcal{M}\\to\\mathcal{M}:(x,v)\\mapsto{}R_x(v),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"such that each curve ``c(t) := R_x(tv)`` is a local approximation of a geodesic, i.e. the following two conditions hold:\n\" * Main.indentation * raw\"1. ``c(0) = x`` and \n\" * Main.indentation * raw\"2. ``c'(0) = v.``\n\")","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Perhaps the most common example for matrix manifolds is the Cayley retraction. It is a retraction for many matrix Lie groups [1, 37, 38].","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Main.example(raw\"The **Cayley retraction** for ``V\\in{}T_\\mathbb{I}G\\equiv\\mathfrak{g}`` is defined as\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\mathrm{Cayley}(V) = \\left(\\mathbb{I} - \\frac{1}{2}V\\right)^{-1}\\left(\\mathbb{I} +\\frac{1}{2}V\\right).\n\" * Main.indentation * raw\"```\")","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"We show that the Cayley transform is a retraction for G = SO(N) at mathbbIinSO(N):","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Main.proof(raw\"The Cayley transform trivially satisfies ``\\mathrm{Cayley}(\\mathbb{O}) = \\mathbb{I}``. So what we have to show is the second condition for a retraction and that ``\\mathrm{Cayley}(V)\\in{}SO(N)``. For this take ``V\\in\\mathfrak{so}(N).`` We then have\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\frac{d}{dt}\\bigg|_{t = 0}\\mathrm{Cayley}(tV) = \\frac{d}{dt}\\bigg|_{t = 0}\\left(\\mathbb{I} - \\frac{1}{2}tV\\right)^{-1}\\left(\\mathbb{I} +\\frac{1}{2}tV\\right) = \\frac{1}{2}V - \\frac{1}{2}V^T = V,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"which satisfies the second condition. We further have\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\frac{d}{dt}\\bigg|_{t = 0}(\\mathrm{Cayley}(tV))^T\\mathrm{Cayley}(tV) = (\\frac{1}{2}V - \\frac{1}{2}V^T)^T + \\frac{1}{2}V - \\frac{1}{2}V^T = 0.\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"This proofs that the Cayley transform maps to ``SO(N)``.\")","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"We should mention that the factor frac12 is sometimes left out in the definition of the Cayley transform when used in different contexts. But it is necessary for defining a retraction as without it the second condition is not satisfied.","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Main.remark(raw\"We can also use the Cayley retraction at a different point than the identity ``\\mathbb{I}.`` For this consider ``\\bar{A}\\in{}SO(N)`` and ``\\bar{B}\\in{}T_{\\bar{A}}SO(N) = \\{\\bar{B}\\in\\mathbb{R}^{N\\times{}N}: \\bar{A}^T\\bar{B} + \\bar{B}^T\\bar{A} = \\mathbb{O}\\}``. We then have ``\\bar{A}^T\\bar{B}\\in\\mathfrak{so}(N)`` and \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"    \\overline{\\mathrm{Cayley}}: T_{\\bar{A}}SO(N) \\to SO(N), \\bar{B} \\mapsto \\bar{A}\\mathrm{Cayley}(\\bar{A}^T\\bar{B}),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"is a retraction ``\\forall{}\\bar{A}\\in{}SO(N)``.\")","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"As a retraction is always an approximation of the geodesic map, we now compare the cayley retraction for the example we introduced along Riemannian manifolds:","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"using GLMakie\n\ninclude(\"../../../gl_makie_transparent_background_hack.jl\")","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"using GeometricMachineLearning\nimport Random # hide\nRandom.seed!(123) # hide\n\nY = rand(StiefelManifold, 3, 1)\n\nv = 5 * rand(3, 1)\nΔ = v - Y * (v' * Y)\n\nfunction do_setup(; theme=:light)\n    text_color = theme == :dark ? :white : :black # hide\n    fig = Figure(; backgroundcolor = :transparent, size = (900, 675)) # hide\n    ax = Axis3(fig[1, 1]; # hide\n        backgroundcolor = (:tomato, .5), # hide\n        aspect = (1., 1., 1.), # hide\n        xlabel = L\"x_1\", # hide\n        ylabel = L\"x_2\", # hide\n        zlabel = L\"x_3\", # hide\n        xgridcolor = text_color, # hide\n        ygridcolor = text_color, # hide\n        zgridcolor = text_color, # hide\n        xtickcolor = text_color, # hide\n        ytickcolor = text_color, # hide\n        ztickcolor = text_color, # hide\n        xlabelcolor = text_color, # hide\n        ylabelcolor = text_color, # hide\n        zlabelcolor = text_color, # hide\n        xypanelcolor = :transparent, # hide\n        xzpanelcolor = :transparent, # hide\n        yzpanelcolor = :transparent, # hide\n        limits = ([-1, 1], [-1, 1], [-1, 1]),\n        azimuth = π / 7, # hide\n        elevation = π / 7, # hide\n        # height = 75.,\n        ) # hide\n\n    # plot a sphere with radius one and origin 0\n    surface!(ax, Main.sphere(1., [0., 0., 0.])...; alpha = .5, transparency = true)\n\n    morange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\n    point_vec = ([Y[1]], [Y[2]], [Y[3]])\n    scatter!(ax, point_vec...; color = morange, marker = :star5, markersize = 30)\n\n    fig, ax, point_vec\nend\n\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256)\n\nnothing","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"η_increments = 0.2 : 0.2 : 5.4\nΔ_increments = [Δ * η for η in η_increments]\n\nY_increments_geodesic = [geodesic(Y, Δ_increment) for Δ_increment in Δ_increments]\nY_increments_cayley = [cayley(Y, Δ_increment) for Δ_increment in Δ_increments]\nnothing # hide","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"function make_plot(; theme=:light) # hide\n\ntext_color = theme == :light ? :black : :white # hide\n\nfig, ax, point_vec = do_setup(; theme = theme) # hide\n\nY_zeros = zeros(length(Y_increments_geodesic))\nY_geodesic_reshaped = [copy(Y_zeros), copy(Y_zeros), copy(Y_zeros)]\nY_cayley_reshaped = [copy(Y_zeros), copy(Y_zeros), copy(Y_zeros)]\n\nzip_ob = zip(Y_increments_geodesic, Y_increments_cayley, axes(Y_increments_geodesic, 1))\n\nfor (Y_increment_geodesic, Y_increment_cayley, i) in zip_ob\n    for d in (1, 2, 3)\n        Y_geodesic_reshaped[d][i] = Y_increment_geodesic[d]\n\n        Y_cayley_reshaped[d][i] = Y_increment_cayley[d]\n    end\nend\n\nscatter!(ax, Y_geodesic_reshaped...; \n        color = mred, label = rich(\"geodesic retraction\"; color = text_color), markersize = 15)\n\nscatter!(ax, Y_cayley_reshaped...; \n        color = mblue, label = rich(\"Cayley retraction\"; color = text_color), markersize = 15)\n\narrow_vec = ([Δ[1]], [Δ[2]], [Δ[3]]) # hide\narrows!(ax, point_vec..., arrow_vec...; color = mred, linewidth = .02) # hide\nbackgroundcolor = theme == :light ? :white : :transparent\naxislegend(; position = (.82, .75), backgroundcolor = backgroundcolor, color = text_color) # hide\n\nfig, ax, zip_ob, Y_increments_geodesic, Y_increments_cayley # hide\nend # hide\n\nfig_light = make_plot(; theme = :light)[1] # hide\nfig_dark = make_plot(; theme = :dark)[1] # hide\n\nsave(\"retraction_comparison_light.png\",        alpha_colorbuffer(fig_light)) # hide\nsave(\"retraction_comparison_dark.png\",   alpha_colorbuffer(fig_dark)) # hide\n\nnothing","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"(Image: Comparison between the geodesic and the Cayley retraction.) (Image: Comparison between the geodesic and the Cayley retraction.)","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"We see that for small Delta increments the Cayley retraction seems to match the geodesic retraction very well, but for larger values there is a notable discrepancy. We can plot this discrepancy directly: ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"using CairoMakie\n\nCairoMakie.activate!()\nfunction plot_discrepancies(discrepancies; theme = :light)\n    fig = Figure(; backgroundcolor = :transparent) # hide\n    text_color = theme == :dark ? :white : :black # hide\n    ax = Axis(fig[1, 1]; # hide\n            backgroundcolor = :transparent, # hide\n            xlabel = rich(\"η\", font = :italic, color = text_color), # hide\n            ylabel = rich(\"discrepancy\", color = text_color), # hide\n            ) # hide\n    lines!(η_increments, discrepancies; label = rich(\"Discrepancies between geodesic and Cayley retraction\", color = text_color), \n        linewidth = 2, color = mblue)\n\n    axislegend(; position = (.22, .9), backgroundcolor = :transparent, color = text_color) # hide\n\n    fig, ax\nend","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"using LinearAlgebra: norm # hide\nzip_ob = zip(Y_increments_geodesic, Y_increments_cayley, axes(Y_increments_geodesic, 1))\n_, __, zip_ob, Y_increments_geodesic, Y_increments_cayley = make_plot() # hide\ndiscrepancies = [norm(Y_geo_inc - Y_cay_inc) for (Y_geo_inc, Y_cay_inc, _) in zip_ob]\nfig_light = plot_discrepancies(discrepancies; theme = :light)[1] # hide\nfig_dark = plot_discrepancies(discrepancies; theme = :dark)[1] # hide\nsave(\"retraction_discrepancy_light.png\",        fig_light; px_per_unit = 1.3) # hide\nsave(\"retraction_discrepancy_dark.png\",   fig_dark; px_per_unit = 1.3) # hide\nnothing","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"(Image: Discrepancy between the geodesic and the Cayley retraction.) (Image: Discrepancy between the geodesic and the Cayley retraction.)","category":"page"},{"location":"optimizers/manifold_related/retractions/#In-GeometricMachineLearning","page":"Retractions","title":"In GeometricMachineLearning","text":"","category":"section"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"The way we use retractions[1] in GeometricMachineLearning is slightly different from their classical definition:","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"[1]: Classical retractions are also defined in GeometricMachineLearning under the same name, i.e. there is e.g. a method cayley(::StiefelLieAlgHorMatrix) and a method cayley(::StiefelManifold, ::AbstractMatrix) (the latter being the classical retraction); but the user is strongly discouraged from using classical retractions as these are computationally inefficient.","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Main.definition(raw\"Given a section ``\\lambda:\\mathcal{M}\\to{}G,`` where ``\\mathcal{M}`` is a homogeneous space, a **retraction** is a map ``\\mathrm{Retraction}:\\mathfrak{g}^\\mathrm{hor}\\to{}G`` such that \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\Delta \\mapsto \\lambda(Y)\\mathrm{Retraction}(\\lambda(Y)^{-1}\\Omega(\\Delta)\\lambda(Y))E,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"is a classical retraction.\")","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"This map mathrmRetraction is also what was visualized in the figure on the general optimization framework. We now discuss how the geodesic retraction (exponential map) and the Cayley retraction are implemented in GeometricMachineLearning.","category":"page"},{"location":"optimizers/manifold_related/retractions/#Retractions-for-Homogeneous-Spaces","page":"Retractions","title":"Retractions for Homogeneous Spaces","text":"","category":"section"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Here we harness special properties of homogeneous spaces to obtain computationally efficient retractions for the Stiefel manifold and the Grassmann manifold. This is also discussed in e.g. [23, 37].","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"The geodesic retraction is a retraction whose associated curve is also the unique geodesic. For many matrix Lie groups (including SO(N)) geodesics are obtained by simply evaluating the exponential map [22, 39]:","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Main.theorem(raw\"The geodesic on a compact matrix Lie group ``G`` with bi-invariant metric for ``\\bar{B}\\in{}T_{\\bar{A}}G`` is simply\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\gamma(t) = \\exp(t\\cdot{}\\bar{B}\\bar{A}^{-1})\\bar{A} = A\\exp(t\\cdot{}\\bar{A}^{-1}\\bar{B}^n),\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``\\exp:\\mathfrak{g}\\to{}G`` is the matrix exponential map.\")","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"The last equality in the equation above is a result of:","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"beginaligned\nexp(barA^-1hatBbarA) = sum_k=1^inftyfrac1k(barA^-1hatBbarA)^k  = sum_k=1^infty frac1kunderbrace(barA^-1hatBbarA)cdots(A^-1hatBbarA)_textk times   = sum_k=1^infty frac1k barA^-1 hatB^k barA = barA^-1exp(hatB)barA\nendaligned","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Because SO(N) is compact and we furnish it with the canonical metric, i.e. ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"    gT_barAGtimesT_barAG to mathbbR (B_1 B_2) mapsto mathrmTr(B_1^TB_2) = mathrmTr((B_1barA^-1)^T(B_2barA^-1))","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"its geodesics are thus equivalent to the exponential maps. We now use this observation to obtain an expression for the geodesics on the Stiefel manifold. We use the following theorem from [39, Proposition 25.7]:","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Main.theorem(raw\"The geodesics for a naturally reductive homogeneous space ``\\mathcal{M}`` starting at ``Y`` are given by:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\gamma_{\\Delta}(t) = \\exp(t\\cdot\\Omega(\\Delta))Y,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where the ``\\exp`` is the exponential map for the Lie group ``G`` corresponding to ``\\mathcal{M}``.\")","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"The theorem requires the homogeneous space to be naturally reductive: ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Main.definition(raw\"A homogeneous space is called **naturally reductive** if the following two conditions hold:\n\" * Main.indentation * raw\"1. ``\\bar{A}^{-1}\\bar{B}\\bar{A}\\in\\mathfrak{g}^\\mathrm{hor}`` for every ``\\bar{B}\\in\\mathfrak{g}^\\mathrm{hor}`` and ``\\bar{A}\\in\\exp(\\mathfrak{g}^\\mathrm{ver}``),\n\" * Main.indentation * raw\"2. ``g([X, Y]^\\mathrm{hor}, Z) = g(X, [Y, Z]^\\mathrm{hor})`` for all ``X, Y, Z \\in \\mathfrak{g}^\\mathrm{hor}``,\n\" * Main.indentation * raw\"where ``[X, Y]^\\mathrm{hor} = \\Omega(XYE - YXE)``. If only the first condition holds the homogeneous space is called **reductive** (but not **naturally reductive**).\")","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"We state here without proof that the Stiefel manifold and the Grassmann manifold are naturally reductive. We can however provide empirical evidence here:","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"using GeometricMachineLearning # hide\nimport Random # hide\nRandom.seed!(123) # hide\nB̄ = rand(SkewSymMatrix, 6) # ∈ 𝔤\nĀ = exp(B̄ - StiefelLieAlgHorMatrix(B̄, 3)) # ∈ exp(𝔤ᵛᵉʳ)\n\nX = rand(StiefelLieAlgHorMatrix, 6, 3) # ∈ 𝔤ʰᵒʳ\nY = rand(StiefelLieAlgHorMatrix, 6, 3) # ∈ 𝔤ʰᵒʳ\nZ = rand(StiefelLieAlgHorMatrix, 6, 3) # ∈ 𝔤ʰᵒʳ\n\n@assert StiefelLieAlgHorMatrix(Ā' * X * Ā, 3) ≈ Ā' * X * Ā # hide\nĀ' * X * Ā # this has to be in 𝔤ʰᵒʳ for St(3, 6) to be reductive","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"verifies the first property and","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"using LinearAlgebra: tr # hide\nadʰᵒʳ(X, Y) = StiefelLieAlgHorMatrix(X * Y - Y * X, 3)\n\n@assert tr(adʰᵒʳ(X, Y)' * Z) ≈ tr(X' * adʰᵒʳ(Y, Z)) # hide\ntr(adʰᵒʳ(X, Y)' * Z) ≈ tr(X' * adʰᵒʳ(Y, Z))","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"verifies the second.","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"In GeometricMachineLearning we always work with elements in mathfrakg^mathrmhor and the Lie group G is always SO(N). We hence use:","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"    gamma_Delta(t) = exp(lambda(Y)lambda(Y)^-1Omega(Delta)lambda(Y)lambda(Y)^-1)Y = lambda(Y)exp(lambda(Y)^-1Omega(Delta)lambda(Y))E","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Based on this we define the maps: ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"mathttgeodesic mathfrakg^mathrmhor to G barB mapsto exp(barB)","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"and","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"mathttcayley mathfrakg^mathrmhor to G barB mapsto mathrmCayley(barB)","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"where barB = lambda(Y)^-1Omega(Delta)lambda(Y). These expressions for geodesic and cayley are the ones that we typically use in GeometricMachineLearning for computational reasons. We show how we can utilize the sparse structure of mathfrakg^mathrmhor for computing the geodesic retraction and the Cayley retraction (i.e. the expressions exp(barB) and mathrmCayley(barB) for barBinmathfrakg^mathrmhor). Similar derivations can be found in [37, 40, 41].","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Main.remark(raw\"Further note that, even though the global section ``\\lambda:\\mathcal{M} \\to G`` is not unique, the final geodesic ``\\gamma_\\Delta(t) = \\lambda(Y)\\exp(\\lambda(Y)^{-1}\\Omega(\\Delta)\\lambda(Y))E`` does not depend on the particular section we choose.\")","category":"page"},{"location":"optimizers/manifold_related/retractions/#The-Geodesic-Retraction","page":"Retractions","title":"The Geodesic Retraction","text":"","category":"section"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"An element barB of mathfrakg^mathrmhor can be written as:","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"barB = beginbmatrix\n    A  -B^T  \n    B  mathbbO\nendbmatrix = beginbmatrix  frac12A  mathbbI  B  mathbbO endbmatrix beginbmatrix  mathbbI  mathbbO  frac12A  -B^T  endbmatrix = B(B)^T","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"where we exploit the sparse structure of the array, i.e. it is a multiplication of a Ntimes2n with a 2ntimesN matrix.","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"We further use the following: ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"    beginaligned\n    exp(B(B)^T)  = sum_n=0^infty frac1n (B(B)^T)^n = mathbbI + sum_n=1^infty frac1n B((B)^TB)^n-1(B)^T \n     = mathbbI + Bleft( sum_n=1^infty frac1n ((B)^TB)^n-1 right)B = mathbbI + BmathfrakA(B B)B\n    endaligned","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"where we defined mathfrakA(B B) = sum_n=1^infty frac1n ((B)^TB)^n-1 Note that evaluating mathfrakA relies on computing products of small matrices of size 2ntimes2n We do this by relying on a simple Taylor expansion (see the docstring for GeometricMachineLearning.𝔄). ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"The final expression we obtain is: ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"exp(barB) = mathbbI + B mathfrakA(B B)  (B)^T","category":"page"},{"location":"optimizers/manifold_related/retractions/#The-Cayley-Retraction","page":"Retractions","title":"The Cayley Retraction","text":"","category":"section"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"For the Cayley retraction we leverage the decomposition of barB = B(B)^Tinmathfrakg^mathrmhor through the Sherman-Morrison-Woodbury formula:","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"(mathbbI - frac12B(B)^T)^-1 = mathbbI + frac12B(mathbbI - frac12B(B)^T)^-1(B)^T","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"So what we have to compute the inverse of:","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"mathbbI - frac12beginbmatrix  mathbbI  mathbbO  frac12A  -B^T  endbmatrixbeginbmatrix  frac12A  mathbbI  B  mathbbO endbmatrix = \nbeginbmatrix  mathbbI - frac14A  - frac12mathbbI  frac12B^TB - frac18A^2  mathbbI - frac14A  endbmatrix","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"By leveraging the sparse structure of the matrices in mathfrakg^mathrmhor we arrive at the following expression for the Cayley retraction (similar to the case of the geodesic retraction):","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"mathrmCayley(barB) = mathbbI + frac12 B left(mathbbI_2n - frac12 (B)^T Bright)^-1 (B)^T left(mathbbI + frac12 barBright)","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"where we have abbreviated mathbbI = mathbbI_N We conclude with a remark:","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Main.remark(raw\"As mentioned previously the Lie group ``SO(N)``, i.e. the one corresponding to the Stiefel manifold and the Grassmann manifold, has a bi-invariant Riemannian metric associated with it: ``(B_1,B_2)\\mapsto \\mathrm{Tr}(B_1^TB_2)``. For other Lie groups (e.g. the symplectic group) the situation is slightly more difficult.\")","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"One of such Lie groups is the group of symplectic matrices [37]; for this group the expressions presented here are more complicated.","category":"page"},{"location":"optimizers/manifold_related/retractions/#Library-Functions","page":"Retractions","title":"Library Functions","text":"","category":"section"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"GeometricMachineLearning.geodesic(::StiefelLieAlgHorMatrix)\nGeometricMachineLearning.geodesic(::GrassmannLieAlgHorMatrix)\nGeometricMachineLearning.cayley(::StiefelLieAlgHorMatrix)\nGeometricMachineLearning.cayley(::GrassmannLieAlgHorMatrix)\nGeometricMachineLearning.cayley(::Manifold{T}, ::AbstractMatrix{T}) where T\nGeometricMachineLearning.𝔄(::AbstractMatrix)\nGeometricMachineLearning.𝔄(::AbstractMatrix, ::AbstractMatrix)","category":"page"},{"location":"optimizers/manifold_related/retractions/#GeometricMachineLearning.geodesic-Tuple{StiefelLieAlgHorMatrix}","page":"Retractions","title":"GeometricMachineLearning.geodesic","text":"geodesic(B̄::StiefelLieAlgHorMatrix)\n\nCompute the geodesic of an element in StiefelLieAlgHorMatrix.\n\nImplementation\n\nInternally this is using:\n\nmathbbI + BmathfrakA(B B)B\n\nwith \n\nbarB = beginbmatrix\n    A  -B^T  \n    B  mathbbO\nendbmatrix = beginbmatrix  frac12A  mathbbI  B  mathbbO endbmatrix beginbmatrix  mathbbI  mathbbO  frac12A  -B^T  endbmatrix = B(B)^T\n\nThis is using a computationally efficient version of the matrix exponential mathfrakA. \n\nSee GeometricMachineLearning.𝔄.\n\n\n\n\n\n","category":"method"},{"location":"optimizers/manifold_related/retractions/#GeometricMachineLearning.geodesic-Tuple{GrassmannLieAlgHorMatrix}","page":"Retractions","title":"GeometricMachineLearning.geodesic","text":"geodesic(B̄::GrassmannLieAlgHorMatrix)\n\nCompute the geodesic of an element in GrassmannLieAlgHorMatrix.\n\nThis is equivalent to the method of geodesic for StiefelLieAlgHorMatrix.\n\nSee geodesic(::StiefelLieAlgHorMatrix).\n\n\n\n\n\n","category":"method"},{"location":"optimizers/manifold_related/retractions/#GeometricMachineLearning.cayley-Tuple{StiefelLieAlgHorMatrix}","page":"Retractions","title":"GeometricMachineLearning.cayley","text":"cayley(B̄::StiefelLieAlgHorMatrix)\n\nCompute the Cayley retraction of B.\n\nImplementation\n\nInternally this is using \n\nmathrmCayley(barB) = mathbbI + frac12 B (mathbbI_2n - frac12 (B)^T B)^-1 (B)^T (mathbbI + frac12 B)\n\nwith\n\nbarB = beginbmatrix\n    A  -B^T  \n    B  mathbbO\nendbmatrix = beginbmatrix  frac12A  mathbbI  B  mathbbO endbmatrix beginbmatrix  mathbbI  mathbbO  frac12A  -B^T  endbmatrix = B(B)^T\n\ni.e. barB is expressed as a product of two Ntimes2n matrices.\n\n\n\n\n\n","category":"method"},{"location":"optimizers/manifold_related/retractions/#GeometricMachineLearning.cayley-Tuple{GrassmannLieAlgHorMatrix}","page":"Retractions","title":"GeometricMachineLearning.cayley","text":"cayley(B̄::GrassmannLieAlgHorMatrix)\n\nCompute the Cayley retraction of B.\n\nThis is equivalent to the method of cayley for StiefelLieAlgHorMatrix.\n\nSee cayley(::StiefelLieAlgHorMatrix).\n\n\n\n\n\n","category":"method"},{"location":"optimizers/manifold_related/retractions/#GeometricMachineLearning.cayley-Union{Tuple{T}, Tuple{Manifold{T}, AbstractMatrix{T}}} where T","page":"Retractions","title":"GeometricMachineLearning.cayley","text":"cayley(Y::Manifold, Δ)\n\nTake as input an element of a manifold Y and a tangent vector in Δ in the corresponding tangent space and compute the Cayley retraction.\n\nIn different notation: take as input an element x of mathcalM and an element of T_xmathcalM and return mathrmCayley(v_x) \n\nExamples\n\nusing GeometricMachineLearning\n\nY = StiefelManifold([1. 0. 0.;]' |> Matrix)\nΔ = [0. .5 0.;]' |> Matrix\nY₂ = cayley(Y, Δ)\n\nY₂' * Y₂ ≈ [1.;]\n\n# output\n\ntrue\n\nSee the example in [geodesic(::Manifold{T}, ::AbstractMatrix{T}) where T].\n\n\n\n\n\n","category":"method"},{"location":"optimizers/manifold_related/retractions/#GeometricMachineLearning.𝔄-Tuple{AbstractMatrix}","page":"Retractions","title":"GeometricMachineLearning.𝔄","text":"𝔄(A)\n\nCompute mathfrakA(A) = sum_n=1^infty frac1n (A)^n-1\n\nImplementation\n\nThis uses a Taylor expansion that iteratively adds terms with\n\nwhile norm(Aⁿ) > ε\nmul!(A_temp, Aⁿ, A)\nAⁿ .= A_temp\nrmul!(Aⁿ, T(inv(n)))\n\n𝔄A += Aⁿ\nn += 1 \nend\n\nuntil the norm of Aⁿ becomes smaller than machine precision.  The counter n in the above algorithm is initialized as 2 The matrices Aⁿ and 𝔄 are initialized as the identity matrix.\n\n\n\n\n\n","category":"method"},{"location":"optimizers/manifold_related/retractions/#GeometricMachineLearning.𝔄-Tuple{AbstractMatrix, AbstractMatrix}","page":"Retractions","title":"GeometricMachineLearning.𝔄","text":"𝔄(B̂, B̄)\n\nCompute mathfrakA(B B) = sum_n=1^infty frac1n ((B)^TB)^n-1\n\nThis expression has the property mathbbI +  BmathfrakA(B B)(B)^T = exp(B(B)^T)\n\nExamples\n\nusing GeometricMachineLearning\nusing GeometricMachineLearning: 𝔄\nimport Random\nRandom.seed!(123)\n\nB = rand(StiefelLieAlgHorMatrix, 10, 2)\nB̂ = hcat(vcat(.5 * B.A, B.B), vcat(one(B.A), zero(B.B)))\nB̄ = hcat(vcat(one(B.A), zero(B.B)), vcat(-.5 * B.A, -B.B))\n\none(B̂ * B̄') + B̂ * 𝔄(B̂, B̄) * B̄' ≈ exp(Matrix(B))\n\n# output\n\ntrue\n\n\n\n\n\n","category":"method"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"\\begin{comment}","category":"page"},{"location":"optimizers/manifold_related/retractions/#References","page":"Retractions","title":"References","text":"","category":"section"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"P.-A. Absil, R. Mahony and R. Sepulchre. Optimization algorithms on matrix manifolds (Princeton University Press, Princeton, New Jersey, 2008).\n\n\n\nT. Bendokat and R. Zimmermann. The real symplectic Stiefel and Grassmann manifolds: metrics, geodesics and applications, arXiv preprint arXiv:2108.12447 (2021).\n\n\n\nB. O'neill. Semi-Riemannian geometry with applications to relativity (Academic press, New York City, New York, 1983).\n\n\n\n","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"\\end{comment}","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"\\texttt{GeometricMachineLearning} has custom versions of matrices such as the symmetric and the skew-symmetric matrix implemented. These are important ingredients in e.g. SympNets and volume-preserving transformers and it is therefore important that those implementations also run efficiently on GPU. We also show how to build custom pullbacks for specific functions in \\texttt{Julia}.","category":"page"},{"location":"arrays/skew_symmetric_matrix/#Symmetric,-Skew-Symmetric-and-Triangular-Matrices.","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric, Skew-Symmetric and Triangular Matrices.","text":"","category":"section"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"Among the special arrays implemented in GeometricMachineLearning SymmetricMatrix, SkewSymMatrix, UpperTriangular and LowerTriangular are the most common ones and similar implementations can also be found in other libraries; LinearAlgebra.jl has an implementation of a symmetric matrix called Symmetric for example. The versions of these matrices in GeometricMachineLearning are however more memory efficient as they only store as many parameters as are necessary, i.e. n(n+1)2 for the symmetric matrix and n(n-1)2 for the other three. In addition operations such as matrix and tensor multiplication are implemented for these matrices to work in parallel on GPU via GeometricMachineLearning.tensor_mat_mul for example. We here give an overview of elementary custom matrices that are implemented in GeometricMachineLearning. More involved matrices are the so-called global tangent spaces.","category":"page"},{"location":"arrays/skew_symmetric_matrix/#Custom-Matrices","page":"Symmetric and Skew-Symmetric Matrices","title":"Custom Matrices","text":"","category":"section"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"GeometricMachineLearning has two types of triangular matrices. The first one is UpperTriangular:","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"U = beginpmatrix\n     0  a_12  cdots  a_1n      \n     0  ddots          a_2n \n     vdots  ddots  ddots  vdots \n     0  cdots  0       0 \nendpmatrix","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"And the second one is LowerTriangular:","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"L = beginpmatrix\n     0  0  cdots  0      \n     a_21  ddots          vdots \n     vdots  ddots  ddots  vdots \n     a_n1  cdots  a_n(n-1)       0 \nendpmatrix","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"An instance of SkewSymMatrix can be written as A = L - L^T or A = U^T - U:","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"A = beginpmatrix\n     0  - a_21  cdots  - a_n1     \n     a_21  ddots          vdots \n     vdots  ddots  ddots  vdots \n     a_n1  cdots  a_n(n-1)       0 \nendpmatrix","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"And lastly a SymmetricMatrix:","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"B = beginpmatrix\n     a_11  a_21  cdots  a_n1      \n     a_21  ddots          vdots \n     vdots  ddots  ddots  vdots \n     a_n1  cdots  a_n(n-1)       a_nn\nendpmatrix","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"Note that any matrix MinmathbbR^ntimesn can be written","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"M = frac12(M - M^T) + frac12(M + M^T)","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"where the first part of this matrix is skew-symmetric and the second part is symmetric. This is also how the constructors for SkewSymMatrix and SymmetricMatrix are designed. Consider an arbitrary matrix:","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"using GeometricMachineLearning  # hide\n\nM = [1; 2; 3;; 4; 5; 6;; 7; 8; 9]","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"Calling SkewSymMatrix on M is equivalent to doing M to frac12(M - M^T):","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"A = SkewSymMatrix(M)","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"And calling SymmetricMatrix on M is equivalent to doing M to frac12(M + M^T):","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"B = SymmetricMatrix(M)","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"We can further confirm the identity above:","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"@assert M  ≈ A + B # hide\nM  ≈ A + B","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"Note that for LowerTriangular and UpperTriangular no projection step is involved, which means that if we start with a matrix of type AbstractMatrix{Int64} we will end up with a matrix that is also of type AbstractMatrix{Int64}. The type changes however when we call SkewSymMatrix and SymmetricMatrix:","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"@assert (typeof(A) <: AbstractMatrix{Int64}) == false # hide\n@assert (typeof(B) <: AbstractMatrix{Int64}) == false # hide\n(typeof(A) <: AbstractMatrix{Int64}, typeof(B) <: AbstractMatrix{Int64})","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"For the triangular matrices:","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"U = UpperTriangular(M)\nL = LowerTriangular(M)\n@assert (typeof(U) <: AbstractMatrix{Int64}) == true # hide\n@assert (typeof(L) <: AbstractMatrix{Int64}) == true # hide\n(typeof(U) <: AbstractMatrix{Int64}, typeof(L) <: AbstractMatrix{Int64})","category":"page"},{"location":"arrays/skew_symmetric_matrix/#How-are-Special-Matrices-Stored?","page":"Symmetric and Skew-Symmetric Matrices","title":"How are Special Matrices Stored?","text":"","category":"section"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"The following image demonstrates how a skew-symmetric matrix is stored in GeometricMachineLearning:","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"(Image: The elements of a skew-symmetric matrix (and other special matrices) are stored as a vector. The elements of the big vector are the entries on the lower left of the matrix, stored row-wise.) (Image: The elements of a skew-symmetric matrix (and other special matrices) are stored as a vector. The elements of the big vector are the entries on the lower left of the matrix, stored row-wise.)","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"So what is stored internally is a vector of size n(n-1)2 for the skew-symmetric matrix and the triangular matrices, and a vector of size n(n+1)2 for the symmetric matrix. ","category":"page"},{"location":"arrays/skew_symmetric_matrix/#Sample-Random-Matrices","page":"Symmetric and Skew-Symmetric Matrices","title":"Sample Random Matrices","text":"","category":"section"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"We can sample a random skew-symmetric matrix: ","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"using GeometricMachineLearning # hide\nimport Random # hide\nRandom.seed!(123) # hide\n\nA = rand(SkewSymMatrix, 3)","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"and then access the vector:","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"A.S ","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"This is equivalent to sampling a vector and then assigning a matrix[1]:","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"[1]: We fixed the seed to the same value in both these examples.","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"using GeometricMachineLearning # hide\nimport Random # hide\nRandom.seed!(123) # hide\n\nS = rand(3 * (3 - 1) ÷ 2)\n@assert A == SkewSymMatrix(S, 3) # hide\nSkewSymMatrix(S, 3)","category":"page"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"These special matrices are important for SympNets, volume-preserving transformers and linear symplectic transformers.","category":"page"},{"location":"arrays/skew_symmetric_matrix/#Parallel-Computation","page":"Symmetric and Skew-Symmetric Matrices","title":"Parallel Computation","text":"","category":"section"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"The functions GeometricMachineLearning.mat_tensor_mul and GeometricMachineLearning.tensor_mat_mul are also implemented for these matrices for efficient parallel computations. This is elaborated on when we take about tensors.","category":"page"},{"location":"arrays/skew_symmetric_matrix/#Library-Functions","page":"Symmetric and Skew-Symmetric Matrices","title":"Library Functions","text":"","category":"section"},{"location":"arrays/skew_symmetric_matrix/","page":"Symmetric and Skew-Symmetric Matrices","title":"Symmetric and Skew-Symmetric Matrices","text":"GeometricMachineLearning.AbstractTriangular\nUpperTriangular\nUpperTriangular(::AbstractMatrix)\nLowerTriangular\nLowerTriangular(::AbstractMatrix)\nvec(::GeometricMachineLearning.AbstractTriangular)\nSkewSymMatrix\nSkewSymMatrix(::AbstractMatrix)\nSymmetricMatrix\nSymmetricMatrix(::AbstractMatrix)\nvec(::SkewSymMatrix)","category":"page"},{"location":"arrays/skew_symmetric_matrix/#GeometricMachineLearning.AbstractTriangular","page":"Symmetric and Skew-Symmetric Matrices","title":"GeometricMachineLearning.AbstractTriangular","text":"AbstractTriangular\n\nSee UpperTriangular and LowerTriangular.\n\n\n\n\n\n","category":"type"},{"location":"arrays/skew_symmetric_matrix/#GeometricMachineLearning.UpperTriangular","page":"Symmetric and Skew-Symmetric Matrices","title":"GeometricMachineLearning.UpperTriangular","text":"UpperTriangular(S::AbstractVector, n::Int)\n\nBuild an upper-triangular matrix from a vector.\n\nAn upper-triangular matrix is an ntimesn matrix that has zeros on the diagonal and on the lower triangular.\n\nThe data are stored in a vector S similarly to other matrices. See LowerTriangular, SkewSymMatrix and SymmetricMatrix.\n\nThe struct two fields: S and n. The first stores all the entries of the matrix in a sparse fashion (in a vector) and the second is the dimension n for AinmathbbR^ntimesn.\n\nExamples\n\nusing GeometricMachineLearning\nS = [1, 2, 3, 4, 5, 6]\nUpperTriangular(S, 4)\n\n# output\n\n4×4 UpperTriangular{Int64, Vector{Int64}}:\n 0  1  2  4\n 0  0  3  5\n 0  0  0  6\n 0  0  0  0\n\n\n\n\n\n","category":"type"},{"location":"arrays/skew_symmetric_matrix/#GeometricMachineLearning.UpperTriangular-Tuple{AbstractMatrix}","page":"Symmetric and Skew-Symmetric Matrices","title":"GeometricMachineLearning.UpperTriangular","text":"UpperTriangular(A::AbstractMatrix)\n\nBuild an upper-triangular matrix from a matrix.\n\nThis is done by taking the upper right of that matrix.\n\nExamples\n\nusing GeometricMachineLearning\nM = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]\nUpperTriangular(M)\n\n# output\n\n4×4 UpperTriangular{Int64, Vector{Int64}}:\n 0  2  3   4\n 0  0  7   8\n 0  0  0  12\n 0  0  0   0\n\n\n\n\n\n","category":"method"},{"location":"arrays/skew_symmetric_matrix/#GeometricMachineLearning.LowerTriangular","page":"Symmetric and Skew-Symmetric Matrices","title":"GeometricMachineLearning.LowerTriangular","text":"LowerTriangular(S::AbstractVector, n::Int)\n\nBuild a lower-triangular matrix from a vector.\n\nA lower-triangular matrix is an ntimesn matrix that has zeros on the diagonal and on the upper triangular.\n\nThe data are stored in a vector S similarly to other matrices. See UpperTriangular, SkewSymMatrix and SymmetricMatrix.\n\nThe struct two fields: S and n. The first stores all the entries of the matrix in a sparse fashion (in a vector) and the second is the dimension n for AinmathbbR^ntimesn.\n\nExamples\n\nusing GeometricMachineLearning\nS = [1, 2, 3, 4, 5, 6]\nLowerTriangular(S, 4)\n\n# output\n\n4×4 LowerTriangular{Int64, Vector{Int64}}:\n 0  0  0  0\n 1  0  0  0\n 2  3  0  0\n 4  5  6  0\n\n\n\n\n\n","category":"type"},{"location":"arrays/skew_symmetric_matrix/#GeometricMachineLearning.LowerTriangular-Tuple{AbstractMatrix}","page":"Symmetric and Skew-Symmetric Matrices","title":"GeometricMachineLearning.LowerTriangular","text":"LowerTriangular(A::AbstractMatrix)\n\nBuild a lower-triangular matrix from a matrix.\n\nThis is done by taking the lower left of that matrix.\n\nExamples\n\nusing GeometricMachineLearning\nM = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]\nLowerTriangular(M)\n\n# output\n\n4×4 LowerTriangular{Int64, Vector{Int64}}:\n  0   0   0  0\n  5   0   0  0\n  9  10   0  0\n 13  14  15  0\n\n\n\n\n\n","category":"method"},{"location":"arrays/skew_symmetric_matrix/#Base.vec-Tuple{GeometricMachineLearning.AbstractTriangular}","page":"Symmetric and Skew-Symmetric Matrices","title":"Base.vec","text":"vec(A::AbstractTriangular)\n\nReturn the associated vector to A.\n\nExamples\n\nusing GeometricMachineLearning\n\nM = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]\nLowerTriangular(M) |> vec\n\n# output\n\n6-element Vector{Int64}:\n  5\n  9\n 10\n 13\n 14\n 15\n\n\n\n\n\n","category":"method"},{"location":"arrays/skew_symmetric_matrix/#GeometricMachineLearning.SkewSymMatrix","page":"Symmetric and Skew-Symmetric Matrices","title":"GeometricMachineLearning.SkewSymMatrix","text":"SkewSymMatrix(S::AbstractVector, n::Integer)\n\nInstantiate a skew-symmetric matrix with information stored in vector S.\n\nA skew-symmetric matrix A is a matrix A^T = -A.\n\nInternally the struct saves a vector S of size n(n-1)div2. The conversion is done the following way: \n\nA_ij = begincases 0                              textif i=j \n                         S( (i-2) (i-1) ) div 2 + j  textif ij \n                         S( (j-2) (j-1) ) div 2 + i  textelse endcases\n\nSo S stores a string of vectors taken from A: S = tildea_1 tildea_2 ldots tildea_n with tildea_i = A_i1A_i2ldotsA_i(i-1).\n\nAlso see SymmetricMatrix, LowerTriangular and UpperTriangular.\n\nExamples\n\nusing GeometricMachineLearning\nS = [1, 2, 3, 4, 5, 6]\nSkewSymMatrix(S, 4)\n\n# output\n\n4×4 SkewSymMatrix{Int64, Vector{Int64}}:\n 0  -1  -2  -4\n 1   0  -3  -5\n 2   3   0  -6\n 4   5   6   0\n\n\n\n\n\n","category":"type"},{"location":"arrays/skew_symmetric_matrix/#GeometricMachineLearning.SkewSymMatrix-Tuple{AbstractMatrix}","page":"Symmetric and Skew-Symmetric Matrices","title":"GeometricMachineLearning.SkewSymMatrix","text":"SkewSymMatrix(A::AbstractMatrix)\n\nPerform 0.5 * (A - A') and store the matrix in an efficient way (as a vector with n(n-1)2 entries).\n\nIf the constructor is called with a matrix as input it returns a skew-symmetric matrix via the projection:\n\nA mapsto frac12(A - A^T)\n\nExamples\n\nusing GeometricMachineLearning\nM = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]\nSkewSymMatrix(M)\n\n# output\n\n4×4 SkewSymMatrix{Float64, Vector{Float64}}:\n 0.0  -1.5  -3.0  -4.5\n 1.5   0.0  -1.5  -3.0\n 3.0   1.5   0.0  -1.5\n 4.5   3.0   1.5   0.0\n\nExtended help\n\nNote that the constructor is designed in such a way that it always returns matrices of type SkewSymMatrix{<:AbstractFloat} when called with a matrix, even if this matrix is of type AbstractMatrix{<:Integer}.\n\nIf the user wishes to allocate a matrix SkewSymMatrix{<:Integer} then call:\n\nSkewSymMatrix(::AbstractVector, n::Integer)\n\nNote that this is different from LowerTriangular and UpperTriangular as no porjection takes place there.\n\n\n\n\n\n","category":"method"},{"location":"arrays/skew_symmetric_matrix/#GeometricMachineLearning.SymmetricMatrix","page":"Symmetric and Skew-Symmetric Matrices","title":"GeometricMachineLearning.SymmetricMatrix","text":"SymmetricMatrix(S::AbstractVector, n::Integer)\n\nInstantiate a symmetric matrix with information stored in vector S.\n\nA SymmetricMatrix A is a matrix A^T = A.\n\nInternally the struct saves a vector S of size n(n+1)div2. The conversion is done the following way: \n\nA_ij = begincases S( (i-1) i ) div 2 + j  textif igeqj \n                         S( (j-1) j ) div 2 + i  textelse endcases\n\nSo S stores a string of vectors taken from A: S = tildea_1 tildea_2 ldots tildea_n with tildea_i = A_i1A_i2ldotsA_ii.\n\nAlso see SkewSymMatrix, LowerTriangular and UpperTriangular.\n\nExamples\n\nusing GeometricMachineLearning\nS = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nSymmetricMatrix(S, 4)\n\n# output\n\n4×4 SymmetricMatrix{Int64, Vector{Int64}}:\n 1  2  4   7\n 2  3  5   8\n 4  5  6   9\n 7  8  9  10\n\n\n\n\n\n","category":"type"},{"location":"arrays/skew_symmetric_matrix/#GeometricMachineLearning.SymmetricMatrix-Tuple{AbstractMatrix}","page":"Symmetric and Skew-Symmetric Matrices","title":"GeometricMachineLearning.SymmetricMatrix","text":"SymmetricMatrix(A::AbstractMatrix)\n\nPerform a projection and store the matrix in an efficient way (as a vector with n(n+1)2 entries).\n\nIf the constructor is called with a matrix as input it returns a symmetric matrix via the projection:\n\nA mapsto frac12(A + A^T)\n\nExamples\n\nusing GeometricMachineLearning\nM = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]\nSymmetricMatrix(M)\n\n# output\n\n4×4 SymmetricMatrix{Float64, Vector{Float64}}:\n 1.0   3.5   6.0   8.5\n 3.5   6.0   8.5  11.0\n 6.0   8.5  11.0  13.5\n 8.5  11.0  13.5  16.0\n\nExtended help\n\nNote that the constructor is designed in such a way that it always returns matrices of type SymmetricMatrix{<:AbstractFloat} when called with a matrix, even if this matrix is of type AbstractMatrix{<:Integer}.\n\nIf the user wishes to allocate a matrix SymmetricMatrix{<:Integer} then call\n\njulia SymmetricMatrix(AbstractVector nInteger)`\n\nNote that this is different from LowerTriangular and UpperTriangular as no porjection takes place there.\n\n\n\n\n\n","category":"method"},{"location":"arrays/skew_symmetric_matrix/#Base.vec-Tuple{SkewSymMatrix}","page":"Symmetric and Skew-Symmetric Matrices","title":"Base.vec","text":"vec(A)\n\nOutput the associated vector of A.\n\nExamples\n\nusing GeometricMachineLearning\n\nM = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]\nSkewSymMatrix(M) |> vec\n\n# output\n\n6-element Vector{Float64}:\n 1.5\n 3.0\n 1.5\n 4.5\n 3.0\n 1.5\n\n\n\n\n\n","category":"method"},{"location":"pullbacks/computation_of_pullbacks/#Pullbacks-and-Automatic-Differentiation","page":"Pullbacks","title":"Pullbacks and Automatic Differentiation","text":"","category":"section"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"Automatic Differentiation is an important part of modern machine learning. It is essentially a tool to compute the gradient of a loss function with respect to its input arguments, i.e. given a function LThetatomathbbR an AD routine computes:","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"    mathrmAD theta mapsto nabla_thetaL","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"When we train a neural network the function L is the composition of a neural network ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"    mathcalNN_theta(x) = l^(n)_theta_ncircl^(1)_theta_1(x)","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"and a loss function, so we have:","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"    L(theta) = mathttloss(l^(n)_theta_ncircl^(1)_theta_1(x) mathttminibatch)","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"where theta = (theta_1 ldots theta_n) and L further depends on a specific mini batch here. So what we need to do is compute the pullback[1] of every single layer l^(i)_theta_i. For this consider a function[2] f mathbbR^m to mathbbR^n and xinmathbbR^m. The pullback of f is a function that, depending on x, provides a recipe to map an element from mathbbR^n to an element of mathbbR^m:","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"[1]: The term pullback originally comes from differential geometry [16]. This motivation is discussed below.","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"[2]: mathbbR^m can be interpreted as the space of the neural network parameters Theta here. ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"    mathrmpullbak(f)xmathbbR^m simeq T^*_f(x)mathbbR^m to T^*_xmathbbR^n simeq mathbbR^n","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"where T^*_xmathcalV is the cotangent space of mathcalV at x. ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/#How-to-Compute-Pullbacks","page":"Pullbacks","title":"How to Compute Pullbacks","text":"","category":"section"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"GeometricMachineLearning has many pullbacks for custom array types and other operations implemented. The need for this essentially comes from the fact that we cannot trivially differentiate custom GPU kernels[3]. Implemented custom pullback comprise parallel multiplications with tensors.","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"[3]: This may change in the future if the package Enzyme [24] reaches maturity.","category":"page"},{"location":"pullbacks/computation_of_pullbacks/#What-is-a-Pullback?","page":"Pullbacks","title":"What is a Pullback?","text":"","category":"section"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"Here we first explain the principle of a pullback with the example of a vector-valued function. The generalization to matrices and higher-order tensors is straight-forward. ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"The pullback of a vector-valued function fmathbbR^ntomathbbR^m can be interpreted as the sensitivities in the input space mathbbR^n with respect to variations in the output space mathbbR^m via the function f: ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"leftmathrmpullback(f)ainmathbbR^n dbinmathbbR^mright_i = sum_j=1^mfracpartialf_jpartiala_idb_j","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"This principle can easily be generalized to matrices. For this consider the function gmathbbR^n_1timesn_2tomathbbR^m_1timesm_2. We then have: ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"leftmathrmpullback(g)AinmathbbR^n_1timesn_2 dBinmathbbR^m_1timesm_2right_(i_1 i_2) = sum_j_1=1^m_1sum_j_2=1^m_2fracpartialf_(j_1 j_2)partiala_(i_1 i_2)db_(j_1 j_2)","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"The generalization to higher-order tensors is again straight-forward.","category":"page"},{"location":"pullbacks/computation_of_pullbacks/#Illustrative-example","page":"Pullbacks","title":"Illustrative example","text":"","category":"section"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"Consider the matrix inverse mathrminv mathbbR^ntimesntomathbbR^ntimesn as an example. This fits into the above framework where inv is a matrix-valued function from mathbbR^ntimesn to mathbbR^ntimesn. We here write B = A^-1 = mathrminv(A). We thus have to compute: ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"leftmathrmpullback(mathrminv)AinmathbbR^ntimesn dBinmathbbR^ntimesnright_(i j) = sum_k=1^nsum_ell=1^nfracpartialb_k ellpartiala_i jdb_k ell","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"For a matrix A that depends on a parameter varepsilon we have: ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"fracpartialpartialvarepsilonB = -Bleft( fracpartialpartialvarepsilon A right) B","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"This can easily be checked: ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"mathbbO = fracpartialpartialvarepsilonmathbbI = fracpartialpartialvarepsilon(AB) = AfracpartialpartialvarepsilonB + left(fracpartialpartialvarepsilonAright)B","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"We can then write: ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"beginaligned\nsum_kellleft( fracpartialpartiala_ij b_kell right) db_kell   = sum_kellleft fracpartialpartiala_ij B right_kell db_kell  \n = - sum_kellleftB left(fracpartialpartiala_ij Aright) B right_kell db_kell  \n = - sum_kellmnb_km left(fracpartiala_mnpartiala_ijright) b_nell db_kell  \n = - sum_kellmnb_km delta_imdelta_jn b_nell db_kell  \n = - sum_kellb_ki b_jell db_kell  \n equiv - B^TcdotdBcdotB^T \nendaligned","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"We use this expression to differentiate the VolumePreservingAttention layer. ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/#Motivation-from-a-differential-geometric-perspective","page":"Pullbacks","title":"Motivation from a differential-geometric perspective","text":"","category":"section"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"The notion of a pullback in automatic differentiation is motivated by the concept of pullback in differential geometry [25, 26]. In both cases we want to compute, based on a mapping ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"fmathcalVtomathcalW a mapsto f(a) = b ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"a map of differentials db mapsto da. In the differential geometry case db and da are part of the associated cotangent spaces, i.e. dbinT^*_bmathcalW and dainT^*_amathcalV; in AD we (mostly) deal with spaces of arrays, i.e. vector spaces, which means that T^*_bmathcalW simeq mathcalW and T^*_amathcalV simeq mathcalV. If we have neural network weights on manifolds however, then we have to map weights from T^*_amathcalV (the result of an AD routine) to T_amathcalV before we can apply a retraction. The mapping ","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"T^*_amathcalV to T_amathcalV","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"is equivalent to applying the Riemannian gradient.","category":"page"},{"location":"pullbacks/computation_of_pullbacks/#Library-Functions","page":"Pullbacks","title":"Library Functions","text":"","category":"section"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"GeometricMachineLearning.ZygotePullback","category":"page"},{"location":"pullbacks/computation_of_pullbacks/#GeometricMachineLearning.ZygotePullback","page":"Pullbacks","title":"GeometricMachineLearning.ZygotePullback","text":"ZygotePullback <: AbstractPullback\n\nThe pullback based on the Zygote backend.\n\n\n\n\n\n","category":"type"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"\\begin{comment}","category":"page"},{"location":"pullbacks/computation_of_pullbacks/#References","page":"Pullbacks","title":"References","text":"","category":"section"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"\\end{comment}","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"<!--","category":"page"},{"location":"pullbacks/computation_of_pullbacks/#References-2","page":"Pullbacks","title":"References","text":"","category":"section"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"-->","category":"page"},{"location":"pullbacks/computation_of_pullbacks/","page":"Pullbacks","title":"Pullbacks","text":"M. Betancourt. A geometric theory of higher-order automatic differentiation, arXiv preprint arXiv:1812.11592 (2018).\n\n\n\nJ. Bolte and E. Pauwels. A mathematical model for automatic differentiation in machine learning. Advances in Neural Information Processing Systems 33, 10809–10819 (2020).\n\n\n\n","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"A neural network framework can be seen as a collection of (i) a neural network architecture, (ii) a loss function and (iii) an optimization procedure. In this dissertation we focused on points (i) and (iii) when designing neural networks. \\texttt{GeometricMachineLearning} however also offers the possibility to change the loss function. We show how to do this here.","category":"page"},{"location":"tutorials/adjusting_the_loss_function/#Adjusting-the-Loss-Function","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"","category":"section"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"GeometricMachineLearning provides a few standard loss functions that are used as defaults for specific neural networks.","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"If these standard losses do not satisfy the user's needs, it is very easy to implement custom loss functions. Adding terms to the loss function is standard practice in machine learning to either increase stability [43] or to inform the network about physical properties[1] [71].","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"[1]: Note however that we discourage using so-called physics-informed neural networks as they do not preserve any physical properties but only give a potential improvement on stability in the region where we have training data.","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"We again consider training a SympNet on the data coming from a harmonic oscillator:","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"using GeometricMachineLearning  # hide\nusing GeometricMachineLearning: params # hide\nusing GeometricIntegrators: integrate, ImplicitMidpoint  # hide\nusing GeometricProblems.HarmonicOscillator: hodeproblem\nimport Random # hide\nRandom.seed!(123) # hide\n\nsol = integrate(hodeproblem(; tspan = 100), ImplicitMidpoint()) \ndata = DataLoader(sol; suppress_info = true)\n\nnn = NeuralNetwork(GSympNet(2))\n\n# train the network\no = Optimizer(AdamOptimizer(), nn)\nbatch = Batch(32)\nn_epochs = 30\nloss = FeedForwardLoss()\nloss_array = o(nn, data, batch, n_epochs, loss; show_progress = false)\nprint(loss_array[end])","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"And we see that the loss goes down to a very low value. But the user might want to constrain the norm of the network parameters:","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"using LinearAlgebra: norm  # hide\n\n# norm of parameters for single layer\nnetwork_parameter_norm(params::NamedTuple) = sum([norm(params[i]) for i in 1:length(params)])\n# norm of parameters for entire network\nfunction network_parameter_norm(params::NeuralNetworkParameters)\n    sum([network_parameter_norm(params[key]) for key in keys(params)])\nend\n\nnetwork_parameter_norm(params(nn))","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"We now implement a custom loss such that:","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"    mathrmloss_mathcalNN^mathrmcustom(mathrminput mathrmoutput) = mathrmloss_mathcalNN^mathrmfeedforward + lambda mathrmnorm(mathcalNNmathttparams)","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"struct CustomLoss <: GeometricMachineLearning.NetworkLoss end\n\nconst λ = .1\nfunction (loss::CustomLoss)(model::Chain, params::NeuralNetworkParameters, input::CT, output::CT) where {\n                                                            T,\n                                                            AT<:AbstractArray{T, 3}, \n                                                            CT<:@NamedTuple{q::AT, p::AT}\n                                                            }\n    FeedForwardLoss()(model, params, input, output) + λ * network_parameter_norm(params)\nend\nnothing # hide","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"And we train the same network with this new loss:","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"loss = CustomLoss()\nnn_custom = NeuralNetwork(GSympNet(2))\nloss_array = o(nn_custom, data, batch, n_epochs, loss; show_progress = false)\nprint(loss_array[end])","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"We see that the norm of the parameters is lower:","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"network_parameter_norm(params(nn_custom))","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"We can also compare the solutions of the two networks:","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"using CairoMakie\n\nfunction make_fig(; theme = :dark) # hide\ntextcolor = theme == :dark ? :white : :black # hide\nfig = Figure(; backgroundcolor = :transparent)\nax = Axis(fig[1, 1]; backgroundcolor = :transparent, \n    bottomspinecolor = textcolor, \n    topspinecolor = textcolor,\n    leftspinecolor = textcolor,\n    rightspinecolor = textcolor,\n    xtickcolor = textcolor, \n    ytickcolor = textcolor,\n    xticklabelcolor = textcolor,\n    yticklabelcolor = textcolor)\n\ninit_con = [0.5 0.]\nn_time_steps = 100\nprediction1 = zeros(2, n_time_steps + 1)\nprediction2 = zeros(2, n_time_steps + 1)\nprediction1[:, 1] = init_con\nprediction2[:, 1] = init_con\n\nfor i in 2:(n_time_steps + 1)\n    prediction1[:, i] = nn(prediction1[:, i - 1])\n    prediction2[:, i] = nn_custom(prediction2[:, i - 1])\nend\n\nlines!(ax, data.input.q[:], data.input.p[:], label = rich(\"Training Data\"; color = textcolor), linewidth = 3)\nlines!(ax, prediction1[1, :], prediction1[2, :], label = rich(\"FeedForwardLoss\"; color = textcolor), linewidth = 3)\nlines!(ax, prediction2[1, :], prediction2[2, :], label = rich(\"CustomLoss\"; color = textcolor), linewidth = 3)\naxislegend(; position = (.82, .75), backgroundcolor = :transparent) # hide\n\nfig\nend # hide\n # hide\nsave(\"compare_losses_light.png\", make_fig(; theme = :light); px_per_unit = 1.2) # hide\nsave(\"compare_losses_dark.png\", make_fig(; theme = :dark); px_per_unit = 1.2) # hide\n\nnothing","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"(Image: Here we trained the same network with two different losses.) (Image: Here we trained the same network with two different losses.)","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"Wit the second loss function, for which the norm of the resulting network parameters has lower value, the network still performs well, albeit slightly worse than the network trained with the first loss.","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"\\begin{comment}","category":"page"},{"location":"tutorials/adjusting_the_loss_function/#References","page":"Adjusting the Loss Function","title":"References","text":"","category":"section"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"\\end{comment}","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"<!--","category":"page"},{"location":"tutorials/adjusting_the_loss_function/#References-2","page":"Adjusting the Loss Function","title":"References","text":"","category":"section"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"-->","category":"page"},{"location":"tutorials/adjusting_the_loss_function/","page":"Adjusting the Loss Function","title":"Adjusting the Loss Function","text":"M. Raissi, P. Perdikaris and G. E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics 378, 686–707 (2019).\n\n\n\n","category":"page"},{"location":"tutorials/optimizer_comparison/#Comparison-of-Optimizers","page":"Comparing Optimizers","title":"Comparison of Optimizers","text":"","category":"section"},{"location":"tutorials/optimizer_comparison/","page":"Comparing Optimizers","title":"Comparing Optimizers","text":"using GeometricMachineLearning\nusing GLMakie\nimport Random\nRandom.seed!(123)\n\nf(x::Number, y::Number) = x ^ 2 + y ^ 2\nfunction make_surface()\n    n = 100\n    r = √2\n    u = range(-π, π; length = n)\n    v = range(0, π; length = n)\n    x = r * cos.(u) * sin.(v)'\n    y = r * sin.(u) * sin.(v)'\n    z = f.(x, y)\n    x, y, z\nend\n\nfig = Figure()\nax = Axis3(fig[1, 1])\nsurface!(ax, make_surface()...; alpha = .3, transparency = true)\n\ninit_con = rand(2, 1)\ninit_cont = Tuple(init_con)\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nscatter!(ax, init_cont..., f(init_cont...); color = mred, marker = :star5)\n\nweights = (xy = init_con, )\nη = 1e-3\nmethod1 = GradientOptimizer(η)\nmethod2 = AdamOptimizer(η)\nmethod3 = BFGSOptimizer(η)\noptimizer1 = Optimizer(method1, weights)\noptimizer2 = Optimizer(method2, weights)","category":"page"},{"location":"reduced_order_modeling/losses/#Losses-and-Errors","page":"Losses and Errors","title":"Losses and Errors","text":"","category":"section"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"In general we distinguish between losses that are used during training of a neural network and errors that arise in the context of reduced order modeling. ","category":"page"},{"location":"reduced_order_modeling/losses/#Different-Neural-Network-Losses","page":"Losses and Errors","title":"Different Neural Network Losses","text":"","category":"section"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"GeometricMachineLearning has a number of loss functions implemented that can be called standard losses. Those are the FeedForwardLoss, the TransformerLoss, the AutoEncoderLoss and the ReducedLoss. How to implement custom losses is shown in a tutorial.","category":"page"},{"location":"reduced_order_modeling/losses/#A-Note-on-Physics-Informed-Neural-Networks","page":"Losses and Errors","title":"A Note on Physics-Informed Neural Networks","text":"","category":"section"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"A popular trend in recent years has been considering known physical properties of the differential equation, or the entire differential equation, through the loss function [71]. This is one way of considering physical properties, and GeometricMachineLearning allows for a flexible implementation of custom losses, but this is nonetheless discouraged. In general a neural networks consists of three ingredients:","category":"page"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"(Image: ) (Image: )","category":"page"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"Instead of considering certain properties through the loss function, we instead do so by enforcing them strongly through the network architecture and the optimizer; the latter pertains to manifold optimization. The advantages of this approach are the strong enforcement of properties that we know our network should have and much easier training because we do not have to tune hyperparameters. ","category":"page"},{"location":"reduced_order_modeling/losses/#Projection-and-Reduction-Errors-of-Reduced-Models","page":"Losses and Errors","title":"Projection and Reduction Errors of Reduced Models","text":"","category":"section"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"Two errors that are of very big importance in reduced order modeling are the projection and the reduction error. During training one typically aims at minimizing the projection error, but for the actual application of the model the reduction error is often more important.","category":"page"},{"location":"reduced_order_modeling/losses/#Projection-Error","page":"Losses and Errors","title":"Projection Error","text":"","category":"section"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"The projection error computes how well a reduced basis, represented by the reduction mathcalP and the reconstruction mathcalR, can represent the data with which it is build. In mathematical terms: ","category":"page"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"    e_mathrmproj(mu) = frac mathcalRcircmathcalP(M) - M  M ","category":"page"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"where cdot is the Frobenius norm (one could also optimize for different norms). The corresponding function in GeometricMachineLearning is projection_error. The projection error is equivalent to AutoEncoderLoss and is used for training under that name.","category":"page"},{"location":"reduced_order_modeling/losses/#Reduction-Error","page":"Losses and Errors","title":"Reduction Error","text":"","category":"section"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"The reduction error measures how far the reduced system diverges from the full-order system during integration (online stage). In mathematical terms (and for a single initial condition): ","category":"page"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"e_mathrmred(mu) = sqrt\n    fracsum_t=0^K mathbfx^(t)(mu) - mathcalR(mathbfx^(t)_r(mu)) ^2sum_t=0^K mathbfx^(t)(mu) ^2\n","category":"page"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"where mathbfx^(t) is the solution of the FOM at point t and mathbfx^(t)_r is the solution of the ROM (in the reduced basis) at point t. The reduction error, as opposed to the projection error, not only measures how well the solution manifold is represented by the reduced basis, but also measures how well the FOM dynamics are approximated by the ROM dynamics (via the induced vector field on the reduced basis). The corresponding function in GeometricMachineLearning is reduction_error. The reduction error is, in contract to the projection error, typically not used during training (even though some authors are using a similar error to do so [62]).","category":"page"},{"location":"reduced_order_modeling/losses/#Library-Functions","page":"Losses and Errors","title":"Library Functions","text":"","category":"section"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"FeedForwardLoss\nTransformerLoss\nAutoEncoderLoss\nReducedLoss\nprojection_error\nreduction_error","category":"page"},{"location":"reduced_order_modeling/losses/#GeometricMachineLearning.FeedForwardLoss","page":"Losses and Errors","title":"GeometricMachineLearning.FeedForwardLoss","text":"FeedForwardLoss()\n\nMake an instance of a loss for feedforward neural networks.\n\nThis should be used together with a neural network of type NeuralNetworkIntegrator.\n\nExample\n\nFeedForwardLoss applies a neural network to an input and compares it to the output via an L_2 norm:\n\nusing GeometricMachineLearning\nusing LinearAlgebra: norm\nimport Random\nRandom.seed!(123)\n\nconst d = 2\narch = GSympNet(d)\nnn = NeuralNetwork(arch)\n\ninput_vec =  [1., 2.]\noutput_vec = [3., 4.]\nloss = FeedForwardLoss()\n\nloss(nn, input_vec, output_vec) ≈ norm(output_vec - nn(input_vec)) / norm(output_vec)\n\n# output\n\ntrue\n\nSo FeedForwardLoss simply does:\n\n    mathttloss(mathcalNN mathttinput mathttoutput) =  mathcalNN(mathttinput) - mathttoutput    mathttoutput\n\nwhere cdot is the L_2 norm. \n\nParameters\n\nThis loss does not have any parameters.\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/losses/#GeometricMachineLearning.TransformerLoss","page":"Losses and Errors","title":"GeometricMachineLearning.TransformerLoss","text":"TransformerLoss(seq_length, prediction_window)\n\nMake an instance of the transformer loss. \n\nThis should be used together with a neural network of type TransformerIntegrator.\n\nExample\n\nTransformerLoss applies a neural network to an input and compares it to the output via an L_2 norm:\n\nusing GeometricMachineLearning\nusing LinearAlgebra: norm\nimport Random\n\nconst d = 2\nconst seq_length = 3\nconst prediction_window = 2\n\nRandom.seed!(123)\narch = StandardTransformerIntegrator(d)\nnn = NeuralNetwork(arch)\n\ninput_mat =  [1. 2. 3.; 4. 5. 6.]\noutput_mat = [1. 2.; 3. 4.]\nloss = TransformerLoss(seq_length, prediction_window)\n\n# start of prediction\nconst sop = seq_length - prediction_window + 1\nloss(nn, input_mat, output_mat) ≈ norm(output_mat - nn(input_mat)[:, sop:end]) / norm(output_mat)\n\n# output\n\ntrue\n\nSo TransformerLoss simply does:\n\n    mathttloss(mathcalNN mathttinput mathttoutput) =  mathcalNN(mathttinput)(mathttsl - mathttpw + 1)mathttend - mathttoutput    mathttoutput \n\nwhere cdot is the L_2 norm. \n\nParameters\n\nThe prediction_window specifies how many time steps are predicted into the future. It defaults to the value specified for seq_length.\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/losses/#GeometricMachineLearning.AutoEncoderLoss","page":"Losses and Errors","title":"GeometricMachineLearning.AutoEncoderLoss","text":"AutoEncoderLoss()\n\nMake an instance of AutoEncoderLoss.\n\nThis loss should always be used together with a neural network of type AutoEncoder (and it is also the default for training such a network). \n\nExample\n\nAutoEncoderLoss applies a neural network to an input and compares it to the output via an L_2 norm:\n\nusing GeometricMachineLearning\nusing LinearAlgebra: norm\nimport Random\nRandom.seed!(123)\n\nconst N = 4\nconst n = 1\narch = SymplecticAutoencoder(2*N, 2*n)\nnn = NeuralNetwork(arch)\n\ninput_vec =  [1., 2., 3., 4., 5., 6., 7., 8.]\nloss = AutoEncoderLoss()\n\nloss(nn, input_vec) ≈ norm(input_vec - nn(input_vec)) / norm(input_vec)\n\n# output\n\ntrue\n\nSo AutoEncoderLoss simply does:\n\n    mathttloss(mathcalNN mathttinput) =  mathcalNN(mathttinput) - mathttinput    mathttinput \n\nwhere cdot is the L_2 norm. \n\nParameters\n\nThis loss does not have any parameters.\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/losses/#GeometricMachineLearning.ReducedLoss","page":"Losses and Errors","title":"GeometricMachineLearning.ReducedLoss","text":"ReducedLoss(encoder, decoder)\n\nMake an instance of ReducedLoss based on an Encoder and a Decoder.\n\nThis loss should be used together with a NeuralNetworkIntegrator or TransformerIntegrator.\n\nExample\n\nReducedLoss applies the encoder, integrator and decoder neural networks in this order to an input and compares it to the output via an L_2 norm:\n\nusing GeometricMachineLearning\nusing LinearAlgebra: norm\nimport Random\nRandom.seed!(123)\n\nconst N = 4\nconst n = 1\n\nΨᵉ = NeuralNetwork(Chain(Dense(N, n), Dense(n, n))) |> encoder\nΨᵈ = NeuralNetwork(Chain(Dense(n, n), Dense(n, N))) |> decoder\ntransformer = NeuralNetwork(StandardTransformerIntegrator(n))\n\ninput_mat =  [1.  2.;  3.  4.;  5.  6.;  7.  8.]\noutput_mat = [9. 10.; 11. 12.; 13. 14.; 15. 16.]\nloss = ReducedLoss(Ψᵉ, Ψᵈ)\n\noutput_prediction = Ψᵈ(transformer(Ψᵉ(input_mat)))\nloss(transformer, input_mat, output_mat) ≈ norm(output_mat - output_prediction) / norm(output_mat)\n\n# output\n\ntrue\n\nSo the loss computes: \n\nmathrmloss_mathcalE mathcalD(mathcalNN mathrminput mathrmoutput) = mathcalD(mathcalNN(mathcalE(mathrminput))) - mathrmoutput\n\nwhere mathcalE is the Encoder, mathcalD is the Decoder. mathcalNN is the neural network we compute the loss of.\n\n\n\n\n\n","category":"type"},{"location":"reduced_order_modeling/losses/#GeometricMachineLearning.projection_error","page":"Losses and Errors","title":"GeometricMachineLearning.projection_error","text":"projection_error(rs)\n\nCompute the projection error for a HRedSys.\n\nArguments\n\nIf the full system has already been integrated, then the projection error can be computed quicker:\n\nprojection_error(rs, sol_full)\n\nThis saves the cost of again integrating the systems.\n\n\n\n\n\n","category":"function"},{"location":"reduced_order_modeling/losses/#GeometricMachineLearning.reduction_error","page":"Losses and Errors","title":"GeometricMachineLearning.reduction_error","text":"reduction_error(rs)\n\nCompute the reduction error for a HRedSys.\n\nArguments\n\nIf the full system and the reduced system have already been integrated, then the reduction error can be computed quicker:\n\nreduction_error(rs, sol_full, sol_reduced)\n\nThis saves the cost of again integrating the respective systems.\n\n\n\n\n\n","category":"function"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"\\begin{comment}","category":"page"},{"location":"reduced_order_modeling/losses/#References","page":"Losses and Errors","title":"References","text":"","category":"section"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"K. Lee and K. T. Carlberg. Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders. Journal of Computational Physics 404, 108973 (2020).\n\n\n\nM. Raissi, P. Perdikaris and G. E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. Journal of Computational physics 378, 686–707 (2019).\n\n\n\n","category":"page"},{"location":"reduced_order_modeling/losses/","page":"Losses and Errors","title":"Losses and Errors","text":"\\end{comment}","category":"page"},{"location":"manifolds/existence_and_uniqueness_theorem/#The-Existence-And-Uniqueness-Theorem","page":"Differential Equations and the EAU theorem","title":"The Existence-And-Uniqueness Theorem","text":"","category":"section"},{"location":"manifolds/existence_and_uniqueness_theorem/","page":"Differential Equations and the EAU theorem","title":"Differential Equations and the EAU theorem","text":"The existence-and-uniqueness theorem, also known as the Picard-Lindelöf theorem, Picard's existence theorem or the Cauchy-Lipschitz theorem gives a proof of the existence of solutions for ODEs. Here we state the existence-and-uniqueness theorem for manifolds as vector spaces are just a special case of this. Its proof relies on the Banach fixed-point theorem[1].","category":"page"},{"location":"manifolds/existence_and_uniqueness_theorem/","page":"Differential Equations and the EAU theorem","title":"Differential Equations and the EAU theorem","text":"[1]: It has to be noted that the proof given here is not entirely self-contained. The proof of the fundamental theorem of calculus, i.e. the proof of the existence of an antiderivative of a continuous function [17], is omitted for example. ","category":"page"},{"location":"manifolds/existence_and_uniqueness_theorem/","page":"Differential Equations and the EAU theorem","title":"Differential Equations and the EAU theorem","text":"Main.theorem(raw\"Let ``X`` a vector field on the manifold ``\\mathcal{M}`` that is differentiable at ``x``. Then we can find an ``\\epsilon>0`` and a unique curve ``\\gamma:(-\\epsilon, \\epsilon)\\to\\mathcal{M}`` such that ``\\gamma'(t) = X(\\gamma(t))``.\"; name = \"Existence-And-Uniqueness Theorem\")","category":"page"},{"location":"manifolds/existence_and_uniqueness_theorem/","page":"Differential Equations and the EAU theorem","title":"Differential Equations and the EAU theorem","text":"Main.proof(raw\"We consider a ball around a point ``x\\in\\mathcal{M}`` with radius ``r`` that we pick such that the ball ``B(x, r)`` fits into the ``U`` of some coordinate chart ``\\varphi_U``; we further use ``X`` and ``\\varphi'\\circ{}X\\circ\\varphi^{-1}`` interchangeably in this proof. We then define ``L := \\mathrm{sup}_{y,z\\in{}B(x,r)}|X(y) - X(z)|/|y - z|.`` Note that this ``L`` is always finite because ``X`` is bounded and differentiable. We now define the map ``\\Gamma: C^\\infty((-\\epsilon, \\epsilon), \\mathbb{R}^n)\\to{}C^\\infty((-\\epsilon, \\epsilon), \\mathbb{R}^n)`` (for some ``\\epsilon`` that we do not yet fix) as \n\" * \nMain.indentation * raw\"```math\n\" * \nMain.indentation * raw\"\\Gamma\\gamma(t) = x + \\int_0^tX(\\gamma(s))ds,\n\" * \nMain.indentation * raw\"```\n\" * \nMain.indentation * raw\"i.e. ``\\Gamma`` maps ``C^\\infty`` curves through ``x`` into ``C^\\infty`` curves through ``x``. We further have with the norm ``||\\gamma||_\\infty = \\mathrm{sup}_{t \\in (-\\epsilon, \\epsilon)}|\\gamma(t)|``:\n\" * \nMain.indentation * raw\"```math\n\" *\nMain.indentation * raw\"\\begin{aligned} \n\" * \nMain.indentation * raw\"||\\Gamma(\\gamma_1 - \\gamma_2)||_\\infty & = \\mathrm{sup}_{t \\in (-\\epsilon, \\epsilon)}\\left| \\int_0^t (X(\\gamma_1(s)) - X(\\gamma_2(s)))ds \\right| \\\\\n\" * \nMain.indentation * raw\"& \\leq \\mathrm{sup}_{t \\in (-\\epsilon, \\epsilon)}\\int_0^t | X(\\gamma_1(s)) - X(\\gamma_2(s)) | ds \\\\\n\" * \nMain.indentation * raw\"& \\leq \\mathrm{sup}_{t \\in (-\\epsilon, \\epsilon)}\\int_0^t L |\\gamma_1(s) - \\gamma_2(s)| ds \\\\\n\" * \nMain.indentation * raw\"& \\leq \\epsilon{}L \\cdot \\mathrm{sup}_{t \\in (-\\epsilon, \\epsilon)}|\\gamma_1(t) - \\gamma_2(t)|,\n\" * \nMain.indentation * raw\"\\end{aligned}\n\" * \nMain.indentation * raw\"```\n\" * \nMain.indentation * raw\"and we see that ``\\Gamma`` is a contractive mapping if we pick ``\\epsilon`` small enough and we can hence apply the fixed-point theorem. So there has to exist a ``C^\\infty`` curve through ``x`` that we call ``\\gamma^*`` such that \n\" * \nMain.indentation * raw\"```math\n\" * \nMain.indentation * raw\"\\gamma^*(t) = \\int_0^tX(\\gamma^*(s))ds,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"and this ``\\gamma^*`` is the curve we were looking for. Its uniqueness is guaranteed by the fixed-point theorem.\")","category":"page"},{"location":"manifolds/existence_and_uniqueness_theorem/","page":"Differential Equations and the EAU theorem","title":"Differential Equations and the EAU theorem","text":"For all the problems we discuss here we can extend the integral curves of X from the finite interval (-epsilon epsilon) to all of mathbbR. The solution gamma we call an integral curve or flow of the vector field (ODE).","category":"page"},{"location":"manifolds/existence_and_uniqueness_theorem/#Time-Dependent-Vector-Fields","page":"Differential Equations and the EAU theorem","title":"Time-Dependent Vector Fields","text":"","category":"section"},{"location":"manifolds/existence_and_uniqueness_theorem/","page":"Differential Equations and the EAU theorem","title":"Differential Equations and the EAU theorem","text":"We proved the theorem above for a time-independent vector field X, but it also holds for time-dependent vector fields, i.e. for mappings of the form: ","category":"page"},{"location":"manifolds/existence_and_uniqueness_theorem/","page":"Differential Equations and the EAU theorem","title":"Differential Equations and the EAU theorem","text":"X 0TtimesmathcalMtoTM","category":"page"},{"location":"manifolds/existence_and_uniqueness_theorem/","page":"Differential Equations and the EAU theorem","title":"Differential Equations and the EAU theorem","text":"The proof for this case proceeds analogously to the case of the time-independent vector field; to apply the proof we simply have to extend the vector field to (here written for a specific coordinate chart varphi_U): ","category":"page"},{"location":"manifolds/existence_and_uniqueness_theorem/","page":"Differential Equations and the EAU theorem","title":"Differential Equations and the EAU theorem","text":"barX 0 TtimesmathbbR^ntomathbbR^n+1 (t x_1 ldots x_n) mapsto (1 X(x_1 ldots x_n))","category":"page"},{"location":"manifolds/existence_and_uniqueness_theorem/","page":"Differential Equations and the EAU theorem","title":"Differential Equations and the EAU theorem","text":"More details on this can be found in e.g. [15]. For GeometricMachineLearning time-dependent vector fields are important because many of the optimizers we are using (such as the Adam optimizer) can be seen as approximating the flow of a time-dependent vector field.","category":"page"},{"location":"manifolds/existence_and_uniqueness_theorem/","page":"Differential Equations and the EAU theorem","title":"Differential Equations and the EAU theorem","text":"\\begin{comment}","category":"page"},{"location":"manifolds/existence_and_uniqueness_theorem/#References","page":"Differential Equations and the EAU theorem","title":"References","text":"","category":"section"},{"location":"manifolds/existence_and_uniqueness_theorem/","page":"Differential Equations and the EAU theorem","title":"Differential Equations and the EAU theorem","text":"S. Lang. Real and functional analysis. Vol. 142 (Springer Science & Business Media, 2012).\n\n\n\nS. Lang. Fundamentals of differential geometry. Vol. 191 (Springer Science & Business Media, 2012).\n\n\n\n","category":"page"},{"location":"manifolds/existence_and_uniqueness_theorem/","page":"Differential Equations and the EAU theorem","title":"Differential Equations and the EAU theorem","text":"\\end{comment}","category":"page"},{"location":"arrays/global_tangent_spaces/#Global-Tangent-Spaces","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"","category":"section"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"In GeometricMachineLearning standard neural network optimizers are generalized to homogeneous spaces by leveraging the special structure of the tangent spaces of this class of manifolds. When we introduced homogeneous spaces we already talked about that every tangent space to a homogeneous space T_YmathcalM is of the form: ","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"    T_YmathcalM = mathfrakg cdot Y = AY Ainmathfrakg","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"We then have a decomposition of mathfrakg into a vertical part mathfrakg^mathrmver Y and a horizontal part mathfrakg^mathrmhor Y and the horizontal part is isomorphic to T_YmathcalM via:","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"    mathfrakg^mathrmhor Y = Omega(Delta) DeltainT_YmathcalM ","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"We now identify a special element E in mathcalM and designate the horizontal component mathfrakg^mathrmhor E as our global tangent space. We will refer to this global tangent space by mathfrakg^mathrmhor. We can now find a transformation from any mathfrakg^mathrmhor Y to mathfrakg^mathrmhor and vice-versa (these spaces are isomorphic).","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"Main.theorem(raw\"Let ``A\\in{}G`` an element such that ``AE = Y``. Then we have\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"A^{-1}\\cdot\\mathfrak{g}^{\\mathrm{hor},Y}\\cdot{}A = \\mathfrak{g}^\\mathrm{hor},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"i.e. for every element ``B\\in\\mathfrak{g}^\\mathrm{hor}`` we can find a ``B^Y \\in \\mathfrak{g}^{\\mathrm{hor},Y}`` s.t. ``B = A^{-1}B^YA`` (and vice-versa).\")","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"Main.proof(raw\"We first show that for every ``B^Y\\in\\mathfrak{g}^{\\mathrm{hor},Y}`` the element ``A^{-1}B^YA`` is in ``\\mathfrak{g}^{\\mathrm{hor}}``. First note that ``A^{-1}B^YA\\in\\mathfrak{g}`` by a fundamental theorem of Lie group theory (closedness of the Lie algebra under adjoint action). Now assume that ``A^{-1}B^YA`` is not fully contained in ``\\mathfrak{g}^\\mathrm{hor}``, i.e. it also has a vertical component. So we would lose information when performing ``A^{-1}B^YA \\mapsto A^{-1}B^YAE = A^{-1}B^YY``, but this contradicts the fact that ``B^Y\\in\\mathfrak{g}^{\\mathrm{hor},Y}.`` We now have to proof that for every ``B\\in\\mathfrak{g}^\\mathrm{hor}`` we can find an element in ``\\mathfrak{g}^{\\mathrm{hor}, Y}`` such that this element is mapped to ``B``. By a argument similar to the one above we can show that ``ABA^{-1}\\in\\mathfrak{g}^\\mathrm{hor, Y}`` and this element maps to ``B``. Proofing that the map is injective is now trivial.\")","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"We should note that we have written all Lie group and Lie algebra actions as simple matrix multiplications, like AE = Y. For some Lie groups and Lie algebras, as the Lie group of isomorphisms on some domain mathcalD, this notation may not be appropriate [21]. These Lie groups are however not relevant for what we use in GeometricMachineLearning and we will stick to regular matrix notation.","category":"page"},{"location":"arrays/global_tangent_spaces/#Global-Sections","page":"Global Tangent Spaces","title":"Global Sections","text":"","category":"section"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"Note that the theorem above requires us to find an element AinG such that AE = Y. We will call such a mapping lambdamathcalMtoG a global section[1]. ","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"[1]: Global sections are also crucial for parallel transport in GeometricMachineLearning. A global section is first updated, i.e. Lambda^(t) gets mathrmupdate(Lambda^(t-1)) and on the basis of this we then update the element of the manifold YinmathcalM and the tangent vector DeltainTmathcalM.","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"Main.definition(raw\"We call a mapping ``\\lambda`` from a homogeneous space ``\\mathcal{M}`` to its associated Lie group ``G`` a **global section** if ``\\forall{}Y\\in\\mathcal{M}`` it satisfies:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\lambda(Y)E = Y,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"where ``E`` is the distinct element of the homogeneous space.\")","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"Note that in general global sections are not unique because the rank of G is in general greater than that of mathcalM. We give an example of how to construct such a global section for the Stiefel and the Grassmann manifolds below. ","category":"page"},{"location":"arrays/global_tangent_spaces/#The-Global-Tangent-Space-for-the-Stiefel-Manifold","page":"Global Tangent Spaces","title":"The Global Tangent Space for the Stiefel Manifold","text":"","category":"section"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"We now discuss the specific form of the global tangent space for the Stiefel manifold. We pick as distinct element E (which build by calling StiefelProjection):","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"E = beginbmatrix\nmathbbI_n  \nmathbbO\nendbmatrixinSt(n N)","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"Based on this, elements of the vector space mathfrakg^mathrmhor E = mathfrakg^mathrmhor are: ","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"barB = beginpmatrix\nA  B^T  B  mathbbO\nendpmatrix","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"where A is a skew-symmetric matrix of size ntimesn and B is an arbitrary matrix of size (N - n)timesn. Arrays of type mathfrakg^mathrmhor E equiv mathfrakg^mathrmhor are implemented in GeometricMachineLearning under the name StiefelLieAlgHorMatrix.","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"We can call this with e.g. a skew-symmetric matrix A and an arbitrary matrix B:","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"using GeometricMachineLearning # hide\n\nN, n = 5, 2\n\nA = rand(SkewSymMatrix, n)","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"B = rand(N - n, n)","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"The constructor is then called as follows:","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"B̄ = StiefelLieAlgHorMatrix(A, B, N, n)","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"We can also call it with a matrix of shape NtimesN:","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"B̄₂ = Matrix(B̄) # note that this does not have any special structure\n\nStiefelLieAlgHorMatrix(B̄₂, n)","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"Or we can call it on T_EmathcalMsubsetmathbbR^Ntimesn i.e. a matrix of shape Ntimesn:","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"E = StiefelProjection(N, n)","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"B̄₃ = B̄ * E\n\nStiefelLieAlgHorMatrix(B̄₃, n)","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"We now demonstrate how to map from an element of mathfrakg^mathrmhor Y to an element of mathfrakg^mathrmhor:","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"using GeometricMachineLearning # hide\nusing GeometricMachineLearning: Ω\n\nN, n = 5, 2 # hide\nY = rand(StiefelManifold, N, n)\nΔ = rgrad(Y, rand(N, n))\nΩΔ = Ω(Y, Δ)\nλY = GlobalSection(Y) \n\nλY_mat = Matrix(λY)\n\nround.(λY_mat' * ΩΔ * λY_mat; digits = 3)","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"Performing this computation directly is computationally very inefficient however and the user is strongly discouraged to call Matrix on an instance of GlobalSection. The better option is calling global_rep:","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"using GeometricMachineLearning: _round # hide\n\n_round(global_rep(λY, Δ); digits = 3)","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"Internally GlobalSection calls the function GeometricMachineLearning.global_section which does the following for the Stiefel manifold: ","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"A = randn(N, N - n) # or the gpu equivalent\nA = A - Y * (Y' * A)\nY⟂ = qr(A).Q[1:N, 1:(N - n)]","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"So we draw (N - n) new columns randomly, subtract the part that is spanned by the columns of Y and then perform a QR composition on the resulting matrix. The Q part of the decomposition is a matrix of (N - n) columns that is orthogonal to Y and is typically referred to as Y_perp  [20, 22, 23]. We can easily check that this Y_perp is indeed orthogonal to Y.","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"Main.theorem(raw\"The matrix ``Y_\\perp`` constructed with the algorithm above satisfies\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"Y^TY_\\perp = \\mathbb{O}_{n\\times{}n},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"and\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"(Y_\\perp)^TY_\\perp = \\mathbb{I}_n,\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"i.e. all the columns in the big matrix ``[Y, Y_\\perp]\\in\\mathbb{R}^{N\\times{}N}`` are mutually orthonormal and it therefore is an element of ``SO(N)``.\")","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"Main.proof(raw\"The second property is trivially satisfied because the ``Q`` component of a ``QR`` decomposition is an orthogonal matrix. For the first property note that ``Y^TQR = \\mathbb{O}`` is zero because we have subtracted the ``Y`` component from the matrix ``QR``. The matrix ``R\\in\\mathbb{R}^{N\\times{}(N-n)}`` further has the property ``[R]_{ij} = 0`` for ``i > j`` and we have that \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"(Y^TQ)R = [r_{11}(Y^TQ)_{1\\bullet}, r_{12}(Y^TQ)_{1\\bullet} + r_{22}(Y^TQ)_{2\\bullet}, \\ldots, \\sum_{i=1}^{N-n}r_{i(N-n)}(Y^TQ)_{i\\bullet}].\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"Now all the coefficients ``r_{ii}`` are non-zero because the matrix we performed the ``QR`` decomposition on has full rank and we can see that if ``(Y^TQ)R`` is zero ``Y^TQ`` also has to be zero.\")","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"The function global_rep furthermore makes use of the following:","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"    mathttglobal_rep(Y) = lambda(Y)^TOmega(YDelta)lambda(Y) = EY^TDeltaE^T + beginbmatrix mathbbO  barlambda^TDeltaE^T endbmatrix - beginbmatrix mathbbO  EDelta^Tbarlambda endbmatrix","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"where lambda(Y) = Y barlambda","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"Main.proof(raw\"We derive the expression above: \n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\"\\begin{aligned}\n\" * Main.indentation * raw\"\\lambda(Y)^T\\Omega(Y,\\Delta)\\lambda(Y)  & = \\lambda(Y)^T[(\\mathbb{I} - \\frac{1}{2}YY^T)\\Delta{}Y^T - Y\\Delta^T(\\mathbb{I} - \\frac{1}{2}YY^T)]\\lambda(Y) \\\\\n\" * Main.indentation * raw\"                                        & = \\lambda(Y)^T[(\\mathbb{I} - \\frac{1}{2}YY^T)\\Delta{}E^T - Y\\Delta^T(\\lambda(Y) - \\frac{1}{2}YE^T)] \\\\\n\" * Main.indentation * raw\"                                        & = \\lambda(Y)^T\\Delta{}E^T - \\frac{1}{2}EY^T\\Delta{}E^T - E\\Delta^T\\lambda(Y) + \\frac{1}{2}E\\Delta^TYE^T \\\\ \n\" * Main.indentation * raw\"                                        & = \\begin{bmatrix} Y^T\\Delta{}E^T \\\\ \\bar{\\lambda}\\Delta{}E^T \\end{bmatrix} - \\frac{1}{2}EY^T\\Delta{}E - \\begin{bmatrix} E\\Delta^TY & E\\Delta^T\\bar{\\lambda} \\end{bmatrix} + \\frac{1}{2}E\\Delta^TYE^T \\\\\n\" * Main.indentation * raw\"                                        & = \\begin{bmatrix} Y^T\\Delta{}E^T \\\\ \\bar{\\lambda}\\Delta{}E^T \\end{bmatrix} + E\\Delta^TYE^T - \\begin{bmatrix}E\\Delta^TY & E\\Delta^T\\bar{\\lambda} \\end{bmatrix} \\\\\n\" * Main.indentation * raw\"                                                & = EY^T\\Delta{}E^T + E\\Delta^TYE^T - E\\Delta^TYE^T + \\begin{bmatrix} \\mathbb{O} \\\\ \\bar{\\lambda}\\Delta{}E^T \\end{bmatrix} - \\begin{bmatrix} \\mathbb{O} & E\\Delta^T\\bar{\\lambda} \\end{bmatrix} \\\\\n\" * Main.indentation * raw\"                                        & = EY^T\\Delta{}E^T + \\begin{bmatrix} \\mathbb{O}_{n\\times{}N} \\\\ \\bar{\\lambda}\\Delta{}E^T \\end{bmatrix} - \\begin{bmatrix} \\mathbb{O}_{N\\times{}n} & E\\Delta^T\\bar{\\lambda} \\end{bmatrix},\n\" * Main.indentation * raw\"\\end{aligned}\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"which proofs our assertion.\")","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"This expression of global_rep means we only need Y^TDelta and barlambda^TDelta and this is what is used internally.","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"We now discuss the global tangent space for the Grassmann manifold. This is similar to the Stiefel case.","category":"page"},{"location":"arrays/global_tangent_spaces/#Global-Tangent-Space-for-the-Grassmann-Manifold","page":"Global Tangent Spaces","title":"Global Tangent Space for the Grassmann Manifold","text":"","category":"section"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"In the case of the Grassmann manifold we construct the global tangent space with respect to the distinct element mathcalE=mathrmspan(E)inGr(nN), where E is again the same matrix.","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"The tangent tangent space T_mathcalEGr(nN) can be represented through matrices: ","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"beginpmatrix\n    0  cdots  0 \n    cdots  cdots  cdots  \n    0  cdots  0 \n    b_11  cdots  b_1n \n    cdots  cdots  cdots  \n    b_(N-n)1  cdots  b_(N-n)n\nendpmatrix","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"This representation is based on the identification T_mathcalEGr(nN)toT_EmathcalS_E that was discussed in the section on the Grassmann manifold[2]. We use the following notation:","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"[2]: We derived the following expression for the Riemannian gradient of the Grassmann manifold: mathrmgrad_mathcalY^GrL = nabla_YL - YY^Tnabla_YL. The tangent space to the element mathcalE can thus be written as barB - EE^TbarB where BinmathbbR^Ntimesn and the matrices in this tangent space have the desired form. ","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"mathfrakg^mathrmhor = mathfrakg^mathrmhormathcalE = leftbeginpmatrix 0  -B^T  B  0 endpmatrix textBinmathbbR^(N-n)timesn is arbitraryright","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"This is equivalent to the horizontal component of mathfrakg for the Stiefel manifold for the case when A is zero. This is a reflection of the rotational invariance of the Grassmann manifold: the skew-symmetric matrices A are connected to the group of rotations O(n) which is factored out in the Grassmann manifold Gr(nN)simeqSt(nN)O(n). In GeometricMachineLearning we thus treat the Grassmann manifold as being embedded in the Stiefel manifold. In [23] viewing the Grassmann manifold as a quotient space of the Stiefel manifold is important for \"feasibility\" in \"practical computations\". ","category":"page"},{"location":"arrays/global_tangent_spaces/#Library-Functions","page":"Global Tangent Spaces","title":"Library Functions","text":"","category":"section"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"GeometricMachineLearning.AbstractLieAlgHorMatrix\nStiefelLieAlgHorMatrix\nStiefelLieAlgHorMatrix(::AbstractMatrix, ::Int)\nGrassmannLieAlgHorMatrix\nGrassmannLieAlgHorMatrix(::AbstractMatrix, ::Int)\nGlobalSection\nMatrix(::GlobalSection)\napply_section\napply_section!\n*(::GlobalSection, ::Manifold)\nGeometricMachineLearning.global_section(::StiefelManifold{T}) where T\nGeometricMachineLearning.global_section(::GrassmannManifold{T}) where T\nglobal_rep","category":"page"},{"location":"arrays/global_tangent_spaces/#GeometricMachineLearning.AbstractLieAlgHorMatrix","page":"Global Tangent Spaces","title":"GeometricMachineLearning.AbstractLieAlgHorMatrix","text":"AbstractLieAlgHorMatrix <: AbstractMatrix\n\nAbstractLieAlgHorMatrix is a supertype for various horizontal components of Lie algebras. We usually call this mathfrakg^mathrmhor.\n\nSee StiefelLieAlgHorMatrix and GrassmannLieAlgHorMatrix for concrete examples.\n\n\n\n\n\n","category":"type"},{"location":"arrays/global_tangent_spaces/#GeometricMachineLearning.StiefelLieAlgHorMatrix","page":"Global Tangent Spaces","title":"GeometricMachineLearning.StiefelLieAlgHorMatrix","text":"StiefelLieAlgHorMatrix(A::SkewSymMatrix, B::AbstractMatrix, N::Integer, n::Integer)\n\nBuild an instance of StiefelLieAlgHorMatrix based on a skew-symmetric matrix A and an arbitrary matrix B.\n\nAn element of StiefelLieAlgMatrix takes the form: \n\nbeginpmatrix\nA  B^T  B  mathbbO\nendpmatrix\n\nwhere A is skew-symmetric (this is SkewSymMatrix in GeometricMachineLearning).\n\nAlso see GrassmannLieAlgHorMatrix.\n\nExtended help\n\nStiefelLieAlgHorMatrix is the horizontal component of the Lie algebra of skew-symmetric matrices (with respect to the canonical metric).\n\nThe projection here is: piS to SE where \n\nE = beginbmatrix mathbbI_n  mathbbO_(N-n)timesn  endbmatrix\n\nThe matrix E is implemented under StiefelProjection in GeometricMachineLearning.\n\n\n\n\n\n","category":"type"},{"location":"arrays/global_tangent_spaces/#GeometricMachineLearning.StiefelLieAlgHorMatrix-Tuple{AbstractMatrix, Int64}","page":"Global Tangent Spaces","title":"GeometricMachineLearning.StiefelLieAlgHorMatrix","text":"StiefelLieAlgHorMatrix(D::AbstractMatrix, n::Integer)\n\nTake a big matrix as input and build an instance of StiefelLieAlgHorMatrix.\n\nThe integer N in St(n N) is the number of rows of D.\n\nExtended help\n\nIf the constructor is called with a big NtimesN matrix, then the projection is performed the following way: \n\nbeginpmatrix\nA  B_1  \nB_2  D\nendpmatrix mapsto \nbeginpmatrix\nmathrmskew(A)  -B_2^T  \nB_2  mathbbO\nendpmatrix\n\nThe operation mathrmskewmathbbR^ntimesntomathcalS_mathrmskew(n) is the skew-symmetrization operation. This is equivalent to calling of SkewSymMatrix with an ntimesn matrix.\n\nThis can also be seen as the operation:\n\nD mapsto Omega(E DE) = mathrmskewleft(2 left(mathbbI - frac12 E E^T right) DE E^Tright)\n\nAlso see GeometricMachineLearning.Ω.\n\n\n\n\n\n","category":"method"},{"location":"arrays/global_tangent_spaces/#GeometricMachineLearning.GrassmannLieAlgHorMatrix","page":"Global Tangent Spaces","title":"GeometricMachineLearning.GrassmannLieAlgHorMatrix","text":"GrassmannLieAlgHorMatrix(B::AbstractMatrix, N::Integer, n::Integer)\n\nBuild an instance of GrassmannLieAlgHorMatrix based on an arbitrary matrix B of size (N-n)timesn.\n\nGrassmannLieAlgHorMatrix is the horizontal component of the Lie algebra of skew-symmetric matrices (with respect to the canonical metric).\n\nExtended help\n\nThe projection here is: piS to SEsim where \n\nE = beginbmatrix mathbbI_n  mathbbO_(N-n)timesn  endbmatrix\n\nand the equivalence relation is \n\nV_1 sim V_2 iff exists AinmathcalS_mathrmskew(n) text such that  V_2 = V_1 + beginbmatrix A  mathbbO endbmatrix\n\nAn element of GrassmannLieAlgMatrix takes the form: \n\nbeginpmatrix\nbarmathbbO  B^T  B  mathbbO\nendpmatrix\n\nwhere barmathbbOinmathbbR^ntimesn and mathbbOinmathbbR^(N - n)times(N-n)\n\n\n\n\n\n","category":"type"},{"location":"arrays/global_tangent_spaces/#GeometricMachineLearning.GrassmannLieAlgHorMatrix-Tuple{AbstractMatrix, Int64}","page":"Global Tangent Spaces","title":"GeometricMachineLearning.GrassmannLieAlgHorMatrix","text":"GrassmannLieAlgHorMatrix(D::AbstractMatrix, n::Integer)\n\nTake a big matrix as input and build an instance of GrassmannLieAlgHorMatrix.\n\nThe integer N in Gr(n N) here is the number of rows of D.\n\nExtended help\n\nIf the constructor is called with a big NtimesN matrix, then the projection is performed the following way: \n\nbeginpmatrix\nA  B_1  \nB_2  D\nendpmatrix mapsto \nbeginpmatrix\nbarmathbbO  -B_2^T  \nB_2  mathbbO\nendpmatrix\n\nThis can also be seen as the operation:\n\nD mapsto Omega(E DE - EE^TDE)\n\nwhere Omega is the horizontal lift GeometricMachineLearning.Ω.\n\n\n\n\n\n","category":"method"},{"location":"arrays/global_tangent_spaces/#GeometricMachineLearning.GlobalSection","page":"Global Tangent Spaces","title":"GeometricMachineLearning.GlobalSection","text":"GlobalSection(Y)\n\nConstruct a global section for Y.  \n\nA global section lambda is a mapping from a homogeneous space mathcalM to the corresponding Lie group G such that \n\nlambda(Y)E = Y\n\nAlso see apply_section and global_rep.\n\nImplementation\n\nFor an implementation of GlobalSection for a custom array (especially manifolds), the function global_section has to be generalized.\n\n\n\n\n\n","category":"type"},{"location":"arrays/global_tangent_spaces/#Base.Matrix-Tuple{GlobalSection}","page":"Global Tangent Spaces","title":"Base.Matrix","text":"Matrix(λY::GlobalSection)\n\nPut λY into matrix form. \n\nThis is not recommended if speed is important!\n\nUse apply_section and global_rep instead!\n\n\n\n\n\n","category":"method"},{"location":"arrays/global_tangent_spaces/#GeometricMachineLearning.apply_section","page":"Global Tangent Spaces","title":"GeometricMachineLearning.apply_section","text":"apply_section(λY::GlobalSection{T, AT}, Y₂::AT) where {T, AT <: StiefelManifold{T}}\n\nApply λY to Y₂.\n\nMathematically this is the group action of the element lambdaYinG on the element Y_2 of the homogeneous space mathcalM.\n\nInternally it calls apply_section!.\n\n\n\n\n\n","category":"function"},{"location":"arrays/global_tangent_spaces/#GeometricMachineLearning.apply_section!","page":"Global Tangent Spaces","title":"GeometricMachineLearning.apply_section!","text":"apply_section!(Y::AT, λY::GlobalSection{T, AT}, Y₂::AT) where {T, AT<:StiefelManifold{T}}\n\nApply λY to Y₂ and store the result in Y.\n\nThis is the inplace version of apply_section.\n\n\n\n\n\n","category":"function"},{"location":"arrays/global_tangent_spaces/#Base.:*-Tuple{GlobalSection, Manifold}","page":"Global Tangent Spaces","title":"Base.:*","text":"λY * Y\n\nApply the element λY onto Y.\n\nHere λY is an element of a Lie group and Y is an element of a homogeneous space.\n\n\n\n\n\n","category":"method"},{"location":"arrays/global_tangent_spaces/#GeometricMachineLearning.global_section-Union{Tuple{StiefelManifold{T, AT} where AT<:AbstractMatrix{T}}, Tuple{T}} where T","page":"Global Tangent Spaces","title":"GeometricMachineLearning.global_section","text":"global_section(Y::StiefelManifold)\n\nCompute a matrix of size Ntimes(N-n) whose columns are orthogonal to the columns in Y.\n\nThis matrix is also called Y_perp [20, 22, 23].\n\nExamples\n\nusing GeometricMachineLearning\nusing GeometricMachineLearning: global_section\nimport Random\n\nRandom.seed!(123)\n\nY = StiefelManifold([1. 0.; 0. 1.; 0. 0.; 0. 0.])\n\nround.(Matrix(global_section(Y)); digits = 3)\n\n# output\n\n4×2 Matrix{Float64}:\n 0.0    -0.0\n 0.0     0.0\n 0.936  -0.353\n 0.353   0.936\n\nFurther note that we convert the QRCompactWYQ object to a Matrix before we display it.\n\nImplementation\n\nThe implementation is done with a QR decomposition (LinearAlgebra.qr!). Internally we do: \n\nA = randn(N, N - n) # or the gpu equivalent\nA = A - Y.A * (Y.A' * A)\nqr!(A).Q\n\n\n\n\n\nglobal_section(Y::GrassmannManifold)\n\nCompute a matrix of size Ntimes(N-n) whose columns are orthogonal to the columns in Y.\n\nThe method global_section for the Grassmann manifold is equivalent to that for the StiefelManifold (we represent the Grassmann manifold as an embedding in the Stiefel manifold). \n\nSee the documentation for global_section(Y::StiefelManifold{T}) where T. \n\n\n\n\n\n","category":"method"},{"location":"arrays/global_tangent_spaces/#GeometricMachineLearning.global_section-Union{Tuple{GrassmannManifold{T, AT} where AT<:AbstractMatrix{T}}, Tuple{T}} where T","page":"Global Tangent Spaces","title":"GeometricMachineLearning.global_section","text":"global_section(Y::StiefelManifold)\n\nCompute a matrix of size Ntimes(N-n) whose columns are orthogonal to the columns in Y.\n\nThis matrix is also called Y_perp [20, 22, 23].\n\nExamples\n\nusing GeometricMachineLearning\nusing GeometricMachineLearning: global_section\nimport Random\n\nRandom.seed!(123)\n\nY = StiefelManifold([1. 0.; 0. 1.; 0. 0.; 0. 0.])\n\nround.(Matrix(global_section(Y)); digits = 3)\n\n# output\n\n4×2 Matrix{Float64}:\n 0.0    -0.0\n 0.0     0.0\n 0.936  -0.353\n 0.353   0.936\n\nFurther note that we convert the QRCompactWYQ object to a Matrix before we display it.\n\nImplementation\n\nThe implementation is done with a QR decomposition (LinearAlgebra.qr!). Internally we do: \n\nA = randn(N, N - n) # or the gpu equivalent\nA = A - Y.A * (Y.A' * A)\nqr!(A).Q\n\n\n\n\n\nglobal_section(Y::GrassmannManifold)\n\nCompute a matrix of size Ntimes(N-n) whose columns are orthogonal to the columns in Y.\n\nThe method global_section for the Grassmann manifold is equivalent to that for the StiefelManifold (we represent the Grassmann manifold as an embedding in the Stiefel manifold). \n\nSee the documentation for global_section(Y::StiefelManifold{T}) where T. \n\n\n\n\n\n","category":"method"},{"location":"arrays/global_tangent_spaces/#GeometricMachineLearning.global_rep","page":"Global Tangent Spaces","title":"GeometricMachineLearning.global_rep","text":"global_rep(λY::GlobalSection{T, AT}, Δ::AbstractMatrix{T}) where {T, AT<:StiefelManifold{T}}\n\nExpress Δ (an the tangent space of Y) as an instance of StiefelLieAlgHorMatrix.\n\nThis maps an element from T_YmathcalM to an element of mathfrakg^mathrmhor. \n\nThese two spaces are isomorphic where the isomorphism where the isomorphism is established through lambda(Y)inG via:\n\nT_YmathcalM to mathfrakg^mathrmhor Delta mapsto lambda(Y)^-1Omega(Y Delta)lambda(Y)\n\nAlso see GeometricMachineLearning.Ω.\n\nExamples\n\nusing GeometricMachineLearning\nusing GeometricMachineLearning: _round\nimport Random \n\nRandom.seed!(123)\n\nY = rand(StiefelManifold, 6, 3)\nΔ = rgrad(Y, randn(6, 3))\nλY = GlobalSection(Y)\n\n_round(global_rep(λY, Δ); digits = 3)\n\n# output\n\n6×6 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:\n  0.0     0.679   1.925   0.981  -2.058   0.4\n -0.679   0.0     0.298  -0.424   0.733  -0.919\n -1.925  -0.298   0.0    -1.815   1.409   1.085\n -0.981   0.424   1.815   0.0     0.0     0.0\n  2.058  -0.733  -1.409   0.0     0.0     0.0\n -0.4     0.919  -1.085   0.0     0.0     0.0\n\nImplementation\n\nThe function global_rep does in fact not perform the entire map lambda(Y)^-1Omega(Y Delta)lambda(Y) but only\n\nDelta mapsto mathrmskew(Y^TDelta)\n\nto get the small skew-symmetric matrix AinmathcalS_mathrmskew(n) and \n\nDelta mapsto (lambda(Y)_1N nN^T Delta)_1(N-n) 1n\n\nto get the arbitrary matrix BinmathbbR^(N-n)timesn.\n\n\n\n\n\nglobal_rep(λY::GlobalSection{T, AT}, Δ::AbstractMatrix{T}) where {T, AT<:GrassmannManifold{T}}\n\nExpress Δ (an element of the tangent space of Y) as an instance of GrassmannLieAlgHorMatrix.\n\nThe method global_rep for GrassmannManifold is similar to that for StiefelManifold.\n\nExamples\n\nusing GeometricMachineLearning\nusing GeometricMachineLearning: _round\nimport Random \n\nRandom.seed!(123)\n\nY = rand(GrassmannManifold, 6, 3)\nΔ = rgrad(Y, randn(6, 3))\nλY = GlobalSection(Y)\n\n_round(global_rep(λY, Δ); digits = 3)\n\n# output\n\n6×6 GrassmannLieAlgHorMatrix{Float64, Matrix{Float64}}:\n  0.0     0.0     0.0     0.981  -2.058   0.4\n  0.0     0.0     0.0    -0.424   0.733  -0.919\n  0.0     0.0     0.0    -1.815   1.409   1.085\n -0.981   0.424   1.815   0.0     0.0     0.0\n  2.058  -0.733  -1.409   0.0     0.0     0.0\n -0.4     0.919  -1.085   0.0     0.0     0.0\n\n\n\n\n\n","category":"function"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"\\section*{Chapter Summary}\n\nIn this chapter we discussed mathematical core concepts in this dissertation: aspects from general topology and analysis. We furthermore discussed Riemannian geometry and parallel transport which are crucial concepts for manifold optimization. Homogeneous spaces were introduced as an important subcategory of manifolds and we showed how to find a global tangent space representation for them. This was done by identifying a distinct element $E\\in\\mathcal{M}$ and then considering the action of the Lie algebra $\\mathfrak{g}$ on this element. We have presented our basic application user interface (API) for the \\texttt{Julia} package \\texttt{GeometricMachineLearning}. This API will be extended in Part II with a general optimizer framework. ","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"\\begin{comment}","category":"page"},{"location":"arrays/global_tangent_spaces/#References","page":"Global Tangent Spaces","title":"References","text":"","category":"section"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"P.-A. Absil, R. Mahony and R. Sepulchre. Riemannian geometry of Grassmann manifolds with a view on algorithmic computation. Acta Applicandae Mathematica 80, 199–220 (2004).\n\n\n\nP.-A. Absil, R. Mahony and R. Sepulchre. Optimization algorithms on matrix manifolds (Princeton University Press, Princeton, New Jersey, 2008).\n\n\n\nT. Bendokat, R. Zimmermann and P.-A. Absil. A Grassmann manifold handbook: Basic geometry and computational aspects, arXiv preprint arXiv:2011.13699 (2020).\n\n\n\nB. Brantner. Generalizing Adam To Manifolds For Efficiently Training Transformers, arXiv preprint arXiv:2305.16901 (2023).\n\n\n\nT. Frankel. The geometry of physics: an introduction (Cambridge university press, Cambridge, UK, 2011).\n\n\n\n","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"\\end{comment}","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"<!--","category":"page"},{"location":"arrays/global_tangent_spaces/#References-2","page":"Global Tangent Spaces","title":"References","text":"","category":"section"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"S. Lipschutz. General Topology (McGraw-Hill Book Company, New York City, New York, 1965).\n\n\n\nS. Lang. Real and functional analysis. Vol. 142 (Springer Science & Business Media, 2012).\n\n\n\nS. Lang. Fundamentals of differential geometry. Vol. 191 (Springer Science & Business Media, 2012).\n\n\n\nS. I. Richard L. Bishop. Tensor Analysis on Manifolds (Dover Publications, Mineola, New York, 1980).\n\n\n\nM. P. Do Carmo and J. Flaherty Francis. Riemannian geometry. Vol. 2 (Springer, 1992).\n\n\n\nP.-A. Absil, R. Mahony and R. Sepulchre. Riemannian geometry of Grassmann manifolds with a view on algorithmic computation. Acta Applicandae Mathematica 80, 199–220 (2004).\n\n\n\nT. Bendokat, R. Zimmermann and P.-A. Absil. A Grassmann manifold handbook: Basic geometry and computational aspects, arXiv preprint arXiv:2011.13699 (2020).\n\n\n\nT. Frankel. The geometry of physics: an introduction (Cambridge university press, Cambridge, UK, 2011).\n\n\n\nB. O'neill. Semi-Riemannian geometry with applications to relativity (Academic press, New York City, New York, 1983).\n\n\n\nT. Bendokat and R. Zimmermann. The real symplectic Stiefel and Grassmann manifolds: metrics, geodesics and applications, arXiv preprint arXiv:2108.12447 (2021).\n\n\n\n","category":"page"},{"location":"arrays/global_tangent_spaces/","page":"Global Tangent Spaces","title":"Global Tangent Spaces","text":"-->","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"In this chapter we give another example of using the new neural network optimizer framework for manifolds, but this time for the \\textit{Grassmann manifold}. Here we suppose we are given data on a nonlinear subspace of $\\mathbb{R}^N$ and want to sample additional data from this nonlinear subspace. We also utilize the Wasserstein distance and its gradient in this task.","category":"page"},{"location":"tutorials/grassmann_layer/#Example-of-a-Neural-Network-with-a-Grassmann-Layer","page":"Grassmann Manifold","title":"Example of a Neural Network with a Grassmann Layer","text":"","category":"section"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"Here we show how to implement a neural network that contains a layer whose weight is an element of the Grassmann manifold and where this is useful. Recall that the Grassmann manifold Gr(n N) is the set of vector spaces of dimension n embedded in mathbbR^N. So if we optimize on the Grassmann manifold, we optimize for an ideal n-dimensional vector space in the bigger space mathbbR^N. ","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"We visualize this:","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"(Image: We can build a neural network that creates new samples from an unknown distribution.) (Image: We can build a neural network that creates new samples from an unknown distribution.)","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"So assume that we are given data on a nonlinear manifold:","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"beginpmatrix x_1^(1)  x_2^(1)  vdots  x_N^(1) endpmatrix  beginpmatrix x_1^(2)  x_2^(2)  vdots  x_N^(2) endpmatrix cdots beginpmatrix x_1^(mathttnop)  x_2^(mathttnop)  vdots  x_N^(mathttnop) endpmatrix = mathcalD subset mathcalMsubsetmathbbR^N","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"and mathrmdim(mathcalM) = n We want to obtain additional data on this manifold by sampling from an n-dimensional distribution. Here we do not want to identify an isomorphism mathbbR^n oversetapproxto mathcalM (as is the case with autoencoders for example), but find a mapping from a distribution on mathbbR^n to a distribution on mathcalM. This is where the Grassmann manifold is useful: each element V of the Grassmann manifold is an n-dimensional subspace of mathbbR^N from which we can easily sample. We can then construct a mapping from this space V onto a space that contains the data points mathcalD. ","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"Main.remark(raw\"This example for learning weights on a Grassmann manifold also shows how `GeometricMachineLearning` can be used together with other packages. Here we use the *Wasserstein distance* from the package `BrenierTwoFluid` for example.\")","category":"page"},{"location":"tutorials/grassmann_layer/#Graph-of-Rosenbrock-Function-as-Example","page":"Grassmann Manifold","title":"Graph of Rosenbrock Function as Example","text":"","category":"section"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"Consider the following toy example: we want to sample from the graph of the (scaled) Rosenbrock function ","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"f(xy) = ((1 - x)^2 + 100(y - x^2)^2)1000","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"without using the explicit form of the function during sampling. We show the graph of f for (x y)in-15 15^2 in the following picture:","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"using GLMakie, LaTeXStrings\nGLMakie.activate!() # hide\ninclude(\"../../gl_makie_transparent_background_hack.jl\")\n\nrosenbrock(x::Vector) = ((1.0 - x[1]) ^ 2 + 100.0 * (x[2] - x[1] ^ 2) ^ 2) / 1000\nx, y = -1.5:0.1:1.5, -1.5:0.1:1.5\nz = [rosenbrock([x,y]) for x in x, y in y]\nfunction make_rosenbrock(; theme = :dark, alpha = .7) # hide\ntextcolor = theme == :dark ? :white : :black\nfig = Figure(; backgroundcolor = :transparent, size = (900, 675))\nax = Axis3(fig[1, 1];\n                     limits = ((-1.5, 1.5), (-1.5, 1.5), (0.0, rosenbrock([-1.5, -1.5]))),\n                     azimuth = π / 6,\n                     elevation = π / 8,\n                     backgroundcolor = (:tomato, .5), # hide\n                     xgridcolor = textcolor, \n                     ygridcolor = textcolor, \n                     zgridcolor = textcolor,\n                     xtickcolor = textcolor, \n                     ytickcolor = textcolor,\n                     ztickcolor = textcolor,\n                     xticklabelcolor = textcolor,\n                     yticklabelcolor = textcolor,\n                     zticklabelcolor = textcolor,\n                     xypanelcolor = :transparent,\n                     xzpanelcolor = :transparent,\n                     yzpanelcolor = :transparent,\n                     xlabel = L\"x\", \n                     ylabel = L\"y\",\n                     zlabel = L\"z\",\n                     xlabelcolor = textcolor,\n                     ylabelcolor = textcolor,\n                     zlabelcolor = textcolor)\nsurface!(ax, x, y, z; alpha = alpha, transparency = true)\n\nfig, ax\nend # hide\n\nfig_dark, ax_dark = make_rosenbrock(; theme = :dark, alpha = .85)\nfig_light, ax_light = make_rosenbrock(; theme = :light)\nsave(\"rosenbrock_dark.png\", alpha_colorbuffer(fig_dark))\nsave(\"rosenbrock_light.png\", alpha_colorbuffer(fig_light))\n\nnothing # hide","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"(Image: Graph of the Rosenbrock function.) (Image: Graph of the Rosenbrock function.)","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"We now build a neural network whose task it is to map a product of two Gaussians mathcalN(01)timesmathcalN(01) onto the graph of the Rosenbrock function:","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"    Psi mathcalN(01)timesmathcalN(01) to mathcalW((x y z) (x y)in-15 15times-15 15 z = f(x y)) = tildemathcalW","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"where mathcalW is a distribution on the graph of f For computing the loss between the two distributions, i.e. Psi(mathcalN(01)timesmathcalN(01)) and mathcalW(f(-1515 -1515)) we use the Wasserstein distance[2]. The Wasserstein distance can  compute the distance between mathcalN(01)timesmathcalN(01) and tildemathcalW For more details confer [91, 92].","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"[2]: The implementation of the Wasserstein distance is taken from [93].","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"Before we can use the Wasserstein distance however to train the neural network we need to set it up. It consists of three layers:","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"using GeometricMachineLearning # hide\nusing GeometricMachineLearning: params # hide\nusing Zygote, BrenierTwoFluid\nusing LinearAlgebra: norm # hide\nimport Random # hide\nRandom.seed!(1234) # hide\n\nmodel = Chain(GrassmannLayer(2,3), Dense(3, 8, tanh), Dense(8, 3, identity))\n\nnn = NeuralNetwork(model, CPU(), Float64)\n\nnothing # hide","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"We then lift the neural network parameters via GlobalSection.","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"λY = GlobalSection(params(nn))\nnothing # hide","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"As the cost function c for the Wasserstein loss[3] we simply use:","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"[3]: For each Wasserstein loss we need to define such a cost function. For this particular choice of c the Wasserstein distance corresponds to a W_2 loss.","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"# this computes the cost that is associated to the Wasserstein distance\nc = (x,y) -> .5 * norm(x - y)^2\n∇c = (x,y) -> x - y\n\nnothing # hide","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"We define a function compute_wasserstein_gradient; this is the gradient of the Wasserstein distance with respect to one of the probability distributions it is supplied with (this is further explained below). The function is defined as: ","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"const ε = 0.1                 # entropic regularization. √ε is a length.  # hide\nconst q = 1.0                 # annealing parameter                       # hide\nconst Δ = 1.0                 # characteristic domain size                # hide\nconst s = ε                   # current scale: no annealing -> equals ε   # hide\nconst tol = 1e-6              # marginal condition tolerance              # hide\nconst crit_it = 20            # acceleration inference                    # hide\nconst p_η = 2                                                             # hide\n\nfunction compute_wasserstein_gradient(ensemble1::AT, ensemble2::AT) where AT<:AbstractArray\n    number_of_particles1 = size(ensemble1, 2)\n    number_of_particles2 = size(ensemble2, 2)\n    V = SinkhornVariable(copy(ensemble1'), ones(number_of_particles1) / number_of_particles1)\n    W = SinkhornVariable(copy(ensemble2'), ones(number_of_particles2) / number_of_particles2)\n    params = SinkhornParameters(; ε=ε,q=1.0,Δ=1.0,s=s,tol=tol,crit_it=crit_it,p_η=p_η,sym=false,acc=true) # hide\n    S = SinkhornDivergence(V, W, c, params; islog = true)\n    initialize_potentials!(S)\n    compute!(S)\n    value(S), x_gradient!(S, ∇c)'\nend\n\nnothing # hide","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"This function associates particles in two point clouds with each other. As an illustrative example we will compare the following two point clouds: ","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"    mathcalD_1 = (x y z) (x y) in mathtt-150115^2 z = f(x y z)","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"and","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"    mathcalD_2 = left  beginpmatrix 2  2  2 endpmatrix + x x sim mathttrand right","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"where x sim mathttrand means that we draw x with the function rand. In code the two sets are:","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"xyz_points = hcat([[x, y, rosenbrock([x,y])] for x in x for y in y]...)\nnothing # hide","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"and ","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"point_cloud = rand(size(xyz_points)...) .+ [2., 2., 2.]\nnothing # hide","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"We then compute the Wasserstein gradients and plot 30 of those (picked at random):","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"morange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256) # hide\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256) # hide\n\nloss, grads = compute_wasserstein_gradient(point_cloud, xyz_points)\n\nfunction make_point_cloud_arrows(; theme = :dark)\ntextcolor = theme == :dark ? :white : :black\nfig = Figure(; backgroundcolor = :transparent, size = (900, 675))\nax = Axis3(fig[1, 1];\n                     limits = ((-1.5, 1.5), (-1.5, 1.5), (0.0, rosenbrock([-1.5, -1.5]))),\n                     azimuth = π / 6,\n                     elevation = π / 8,\n                     backgroundcolor = (:tomato, .5), # hide\n                     xgridcolor = textcolor, \n                     ygridcolor = textcolor, \n                     zgridcolor = textcolor,\n                     xtickcolor = textcolor, \n                     ytickcolor = textcolor,\n                     ztickcolor = textcolor,\n                     xticklabelcolor = textcolor,\n                     yticklabelcolor = textcolor,\n                     zticklabelcolor = textcolor,\n                     xypanelcolor = :transparent,\n                     xzpanelcolor = :transparent,\n                     yzpanelcolor = :transparent,\n                     xlabel = L\"x\", \n                     ylabel = L\"y\",\n                     zlabel = L\"z\",\n                     xlabelcolor = textcolor,\n                     ylabelcolor = textcolor,\n                     zlabelcolor = textcolor)\n\nscatter!(ax, point_cloud; color = mred, alpha = .6, label = L\"\\mathcal{D}_1\")\nscatter!(ax, xyz_points; color = mblue, alpha = .6, label = L\"\\mathcal{D}_2\")\n\nnumber_arrows_drawn = 30\nindices = Int.(ceil.(size(point_cloud, 2) * rand(number_arrows_drawn)))\narrows!(ax, point_cloud[1, indices], point_cloud[2, indices], point_cloud[3, indices], \n            - grads[1, indices],     - grads[2, indices],     - grads[3, indices]; \n            color = mred, \n            linewidth = .01, \n            alpha = .01, \n            arrowsize = .04,\n            transparency = true\n            )\naxislegend(; position = (.92, .75), backgroundcolor = :transparent, labelcolor = textcolor) # hide\nfig, ax\nend\nfig_light, ax_light = make_point_cloud_arrows(; theme = :light)\nfig_dark, ax_dark = make_point_cloud_arrows(; theme = :dark)\n\nsave(\"point_cloud_arrows_light.png\", alpha_colorbuffer(fig_light))\nsave(\"point_cloud_arrows_dark.png\", alpha_colorbuffer(fig_dark))\n\nnothing # hide","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"(Image: ) (Image: )","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"We now want to train a neural network based on this Wasserstein loss. The loss function is:","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"L_mathcalNN(theta) = W_2(mathcalNN_theta(x^(1) ldots x^(mathttnp)) mathcalD_2)","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"where np is the number of points in mathcalD_2 and W_2 is the Wasserstein distance. We then have","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"nabla_thetaL_mathcalNN = (nablaW_2)nabla_thetamathcalNN","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"where nablaW_2 is equivalent to the function compute_wasserstein_gradient.","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"function compute_gradient(ps::NeuralNetworkParameters)\n    samples = randn(2, size(xyz_points, 2))\n    estimate, nn_pullback = Zygote.pullback(ps -> model(samples, ps), ps)\n\n    valS, wasserstein_gradient = compute_wasserstein_gradient(estimate, xyz_points)\n    valS, nn_pullback(wasserstein_gradient)[1]\nend\n\n# note the very high value for the learning rate\noptimizer = Optimizer(nn, AdamOptimizer(1e-1))\n\nnothing # hide","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"So we use Zygote [94] to compute nabla_thetamathcalNN and we use compute_wasserstein_gradient to obtain nablaW_2. We can now train our network:","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"import CairoMakie # hide\nCairoMakie.activate!() # hide\nusing GeometricMachineLearning: params # hide\n# note the small number of training steps\nconst training_steps = 80\nloss_array = zeros(training_steps)\nfor i in 1:training_steps\n    val, dp = compute_gradient(params(nn))\n    loss_array[i] = val\n    optimization_step!(optimizer, λY, params(nn), dp.params)\nend","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"and plot the training loss:","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"function make_error_plot(; theme = :dark) # hide\ntextcolor = theme == :dark ? :white : :black # hide\nfig = CairoMakie.Figure(; backgroundcolor = :transparent)\nax = CairoMakie.Axis(fig[1, 1]; \n    backgroundcolor = :transparent,\n    bottomspinecolor = textcolor, \n    topspinecolor = textcolor,\n    leftspinecolor = textcolor,\n    rightspinecolor = textcolor,\n    xtickcolor = textcolor, \n    ytickcolor = textcolor,\n    xticklabelcolor = textcolor,\n    yticklabelcolor = textcolor,\n    xlabel=\"Epoch\", \n    ylabel=\"Training error\",\n    xlabelcolor = textcolor,\n    ylabelcolor = textcolor,\n    yscale = log10\n    )\n\nCairoMakie.lines!(ax, loss_array, label = \"training loss\", color = mblue)\nCairoMakie.axislegend(; position = (.82, .75), backgroundcolor = :transparent, labelcolor = textcolor) # hide\nfig_name = theme == :dark ? \"training_loss_dark.png\" : \"training_loss_light.png\" # hide\nCairoMakie.save(fig_name, fig; px_per_unit = 1.2) # hide\nend # hide\nmake_error_plot(; theme = :dark) # hide\nmake_error_plot(; theme = :light) # hide\n\nnothing # hide","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"(Image: ) (Image: )","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"Now we plot a few points to check how well they match the graph:","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"using GLMakie # hide\nGLMakie.activate!() # hide\n\nRandom.seed!(124)  # hide\nconst number_of_points = 35\ncoordinates = nn(randn(2, number_of_points))\nnothing # hide","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"for theme in (:dark, :light) # hide\nfig, ax = make_rosenbrock(; theme = theme) # hide\nscatter!(ax, coordinates[1, :], coordinates[2, :], coordinates[3, :]; \n            alpha = .9, \n            color = mblue, \n            label=\"mapped points\")\ntextcolor = theme == :dark ? :white : :black # hide\naxislegend(; position = (.82, .75), backgroundcolor = :transparent, labelcolor = textcolor) # hide\nfile_name = \"mapped_points\" * (theme == :dark ? \"_dark.png\" : \"_light.png\") # hide\nsave(file_name, alpha_colorbuffer(fig)) # hide\nend # hide","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"(Image: The blue points were obtained with the neural network sampler.) (Image: The blue points were obtained with the neural network sampler.)","category":"page"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"If points appear in darker color this means that they lie behind the graph of the Rosenbrock function.","category":"page"},{"location":"tutorials/grassmann_layer/#Library-Functions","page":"Grassmann Manifold","title":"Library Functions","text":"","category":"section"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"GrassmannLayer","category":"page"},{"location":"tutorials/grassmann_layer/#GeometricMachineLearning.GrassmannLayer","page":"Grassmann Manifold","title":"GeometricMachineLearning.GrassmannLayer","text":"GrassmannLayer(n, N)\n\nMake an instance of GrassmannLayer.\n\nThis layer performs simple multiplication with an element of the Grassmann manifold, i.e.\n\n    mathttGrassmannLayer x mapsto Ax\n\nwhere A is a representation of an element in the Grassmann manifold, i.e. mathcalA = mathrmspan(A).\n\n\n\n\n\n","category":"type"},{"location":"tutorials/grassmann_layer/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"\\section*{Chapter Summary}\n\nIn this chapter we used a neural network, which has some of its weights lie on the \\textit{Grassmann manifold}, to sample from a distribution from which we have some data available. In order to accomplish this task we combined the gradient of a Wasserstein distance with the pullback of an automatic differentiation routine. At the end we saw that the we could sample new data which were close to the original, given ones.","category":"page"},{"location":"architectures/linear_symplectic_transformer/#Linear-Symplectic-Transformer","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"","category":"section"},{"location":"architectures/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"The linear symplectic transformer consists of a combination of linear symplectic attention and gradient layers and is visualized below:","category":"page"},{"location":"architectures/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"(Image: Visualization of the linear symplectic transformer architecutre. In this figure the number of SympNet layers per transformer block is two.) (Image: Visualization of the linear symplectic transformer architecutre. In this figure the number of SympNet layers per transformer block is two.)","category":"page"},{"location":"architectures/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"In this picture we also visualize the keywords n_sympnet and L for LinearSymplecticTransformer.","category":"page"},{"location":"architectures/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"What we discussed for the volume-preserving transformer also applies here: the attention mechanism acts on all the input vectors at once and is designed such that it preserves the product structure (here this is the symplectic product structure). The attention mechanism serves as a preprocessing step after which we apply a regular feedforward neural network; here this is a SympNet.","category":"page"},{"location":"architectures/linear_symplectic_transformer/#Why-use-Transformers-for-Model-Order-Reduction","page":"Linear Symplectic Transformer","title":"Why use Transformers for Model Order Reduction","text":"","category":"section"},{"location":"architectures/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"The standard transformer, the volume-preserving transformer and the linear symplectic transformer are suitable for model order reduction for a number of reasons. Besides their improved accuracy [85] their ability to resolve time series data also makes it possible to deal with data that come from multiple parameters. For this consider the following two trajectories:","category":"page"},{"location":"architectures/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"(Image: Two trajectories of a parameter-dependent ODE with the same initial condition.) (Image: Two trajectories of a parameter-dependent ODE with the same initial condition.)","category":"page"},{"location":"architectures/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"The trajectories come from a parameter-dependent ODE in two dimensions. As initial condition we take AinmathbbR^2 and we look at two different parameter instances: mu_1 and mu_2. As we can see the curves tildez_mu_1 and tildez_mu_2 both start out at A then go into different directions but cross again at D If we used a standard feedforward neural network to treat this system it would not be able to resolve those training data as the information would be ambiguous at points A and D i.e. the network would not know what it should predict. If we however consider the information coming from points three points, either (A B D) or (A C D) then the network can learn to predict the next time step. We will elaborate more on this in the tutorial section.","category":"page"},{"location":"architectures/linear_symplectic_transformer/#Library-Functions","page":"Linear Symplectic Transformer","title":"Library Functions","text":"","category":"section"},{"location":"architectures/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"LinearSymplecticTransformer","category":"page"},{"location":"architectures/linear_symplectic_transformer/#GeometricMachineLearning.LinearSymplecticTransformer","page":"Linear Symplectic Transformer","title":"GeometricMachineLearning.LinearSymplecticTransformer","text":"LinearSymplecticTransformer(sys_dim, seq_length)\n\nMake an instance of LinearSymplecticTransformer for a specific system dimension and sequence length.\n\nArguments\n\nYou can provide the additional optional keyword arguments:\n\nn_sympnet::Int = (2): The number of sympnet layers in the transformer.\nupscaling_dimension::Int = 2*dim: The upscaling that is done by the gradient layer. \nL::Int = 1: The number of transformer units. \nactivation = tanh: The activation function for the SympNet layers. \ninit_upper::Bool=true: Specifies if the first layer is a q-type layer (init_upper=true) or if it is a p-type layer (init_upper=false).\n\nThe number of SympNet layers in the network is 2n_sympnet, i.e. for n_sympnet = 1 we have one GradientLayerQ and one GradientLayerP.\n\n\n\n\n\n","category":"type"},{"location":"architectures/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"\\section*{Chapter Summary}\n\nIn this chapter we discussed various neural network architectures and presented the corresponding application interface in \\texttt{GeometricMachineLearning}. Of the presented architectures the \\textit{symplectic autoencoder}, the \\textit{volume-preserving transformer} and the \\textit{linear symplectic transformer} constitute novel architectures. We construced the symplectic autoencoder as a composition of \\textit{PSD-like layers} and \\textit{SympNet} layers. For optimizing this architecture we need to resort to manifold optimization. The volume-preserving transformer is a composition of \\textit{volume-preserving attention layers} and \\textit{volume-preserving feedforward layers}; this is similar for the liner symplectic transformer: it is a composition of \\textit{linear symplectic attention layers} and \\textit{SympNets}.\n\nWe discussed how all three transformer neural networks can be seen as \\textit{multi-step integrators}. This was elaborated on in an extra section on \\textit{neural network-based integrators}.","category":"page"},{"location":"architectures/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"<!--","category":"page"},{"location":"architectures/linear_symplectic_transformer/#References","page":"Linear Symplectic Transformer","title":"References","text":"","category":"section"},{"location":"architectures/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"B. Brantner and M. Kraus. Symplectic autoencoders for Model Reduction of Hamiltonian Systems, arXiv preprint arXiv:2312.10004 (2023).\n\n\n\nM. Kraus. GeometricIntegrators.jl: Geometric Numerical Integration in Julia, https://github.com/JuliaGNI/GeometricIntegrators.jl (2020).\n\n\n\nK. Feng. The step-transition operators for multi-step methods of ODE's. Journal of Computational Mathematics, 193–202 (1998).\n\n\n\nB. Brantner, G. de Romemont, M. Kraus and Z. Li. Volume-Preserving Transformers for Learning Time Series Data with Structure, arXiv preprint arXiv:2312:11166v2 (2024).\n\n\n\n","category":"page"},{"location":"architectures/linear_symplectic_transformer/","page":"Linear Symplectic Transformer","title":"Linear Symplectic Transformer","text":"-->","category":"page"},{"location":"abstract/#Summary","page":"Summary","title":"Summary","text":"","category":"section"},{"location":"abstract/","page":"Summary","title":"Summary","text":"Scientific computing has become an indispensible tool for many disciplines like biology, engineering and physics. It is, for example, used to (i) establishing connections between theories and experiments, (ii) making predictions and (iii) aiding in optimizing parameters in the design of large-scale projects such as fusion reactors. ","category":"page"},{"location":"abstract/","page":"Summary","title":"Summary","text":"In practice scientific computing involves solving partial differential equations, usually on supercomputers; and this can be very expensive. Scientists have long tried to reduce the cost required to solve these equations by various methods. In this work we focus on data-driven reduced order modeling to do so. This approach in practice often means employing machine learning techniques such as neural networks to process data that come from simulations.","category":"page"},{"location":"abstract/","page":"Summary","title":"Summary","text":"Neural networks have facilitated breakthroughs in various fields, but their application in reduced order modeling is still in its infancy. A largely neglected aspect is the adherence to properties in their algorithms that have been observed to be important in traditional scientific computing. These properties often pertain to the structure of the solution of the differential equations.","category":"page"},{"location":"abstract/","page":"Summary","title":"Summary","text":"From theoretical and empirical results we know such structure is often indispensible when performing simulations and should be accounted for, and structure preservation serves as the main motivation of this dissertation. To this end we design new neural network architectures that preserve structure. Geometric has traditionally been used as a synonym for structure-preserving and we therefore adopt the name geometric machine learning as an umbrella term for the ideas introduced in this work. ","category":"page"},{"location":"abstract/","page":"Summary","title":"Summary","text":"Geometric machine learning and neural networks are not an entirely novel concept and other researches have proposed architectures that preserve structure before. There are however many new instances of geometric neural networks presented here that are original work. This dissertation is structured into four main parts.","category":"page"},{"location":"abstract/","page":"Summary","title":"Summary","text":"In Part I we give background information that does not constitute novel work, but lays the foundation for the coming chapters. This first part includes a basic introduction into the theory of Riemannian manifolds, a basic discussion of structure preservation and a short explanation of data-driven reduced order modeling.","category":"page"},{"location":"abstract/","page":"Summary","title":"Summary","text":"Part II discusses a novel optimization framework that generalizes existing neural network optimizers, such as the Adam optimizer and the BFGS optimizer, to manifolds. These new optimizers were necessary to enable the training of a new neural network architecture which we call symplectic autoencoders (SAEs).","category":"page"},{"location":"abstract/","page":"Summary","title":"Summary","text":"Part III finally introduces various special neural network layers and architectures. Some of them, like SympNets and the multihead attention layer, are well-known, but others like SAEs, volume-preserving attention and the linear symplectic transformer are new.","category":"page"},{"location":"abstract/","page":"Summary","title":"Summary","text":"In Part IV we give some results based on the new architectures and show how they compare to existing approaches. Most of these applications pertain to applications from physics; but to demonstrate the efficacy of the new optimizers we resort to a classical problem from image classification to show that geometric machine learning can also find applications in fields outside of scientific computing. For all examples that we show, our new architectures exhibit a clear improvement in terms of speed or accuracy over existing architectures. We show one example where we obtain a speed-up of a factor of 1000 with SAEs and transformers.","category":"page"},{"location":"abstract/","page":"Summary","title":"Summary","text":"\\clearpage","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/#The-Volume-Preserving-Transformer-for-the-Rigid-Body","page":"Volume-Preserving Transformer for the Rigid Body","title":"The Volume-Preserving Transformer for the Rigid Body","text":"","category":"section"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"Here we train a volume-preserving feedforward neural network, a standard transformer and a volume-preserving transformer on a rigid body [1, 27]. These are also the results presented in [4]. The ODE that describes the rigid body is: ","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"fracddtbeginpmatrix z_1  z_2  z_3 endpmatrix = beginpmatrix Az_2z_3  Bz_1z_3  Cz_1z_2 endpmatrix","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"In the following we use A = 1 B = 12 and C = -12 For a derivation of this equation see [4]. ","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"We first generate the data. The initial conditions that we use are:","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"mathttics = left beginpmatrix sin(alpha)  0  cos(alpha) endpmatrix beginpmatrix 0  sin(alpha)  cos(alpha) endpmatrix alpha = 01mathtt001mathtt2pi right","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"We build these initial conditions by concatenating mathttics_1 and mathttics_2:","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"const ics₁ = [[sin(val), 0., cos(val)] for val in .1:.01:(2*π)]\nconst ics₂ = [[0., sin(val), cos(val)] for val in .1:.01:(2*π)]\nconst ics = [ics₁..., ics₂...]\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"We now generate the data by integrating with:","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"const tstep = .2\nconst tspan = (0., 20.)\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"The rigid body is implemented in GeometricProblems:","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"using GeometricMachineLearning # hide\nusing GeometricIntegrators: integrate, ImplicitMidpoint\nusing GeometricProblems.RigidBody: odeproblem, odeensemble, default_parameters\n\nensemble_problem = odeensemble(ics; tspan = tspan, tstep = tstep, parameters = default_parameters)\nensemble_solution = integrate(ensemble_problem, ImplicitMidpoint())\n\ndl_cpu = DataLoader(ensemble_solution; suppress_info = true)\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"We plot the trajectories for some of the initial conditions to get and idea of what the data look like:","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"import Random # hide\nRandom.seed!(123456) # hide\nconst n_trajectories_to_plot = 5\nindices = Int.(ceil.(size(dl_cpu.input, 3) * rand(n_trajectories_to_plot)))\n\ntrajectories = [dl_cpu.input[:, :, index] for index in indices]\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"using GLMakie\ninclude(\"../../gl_makie_transparent_background_hack.jl\")\nGLMakie.activate!() # hide\n\nmorange = RGBf(255 / 256, 127 / 256, 14 / 256) # hide\nmred = RGBf(214 / 256, 39 / 256, 40 / 256) # hide\nmpurple = RGBf(148 / 256, 103 / 256, 189 / 256) # hide\nmblue = RGBf(31 / 256, 119 / 256, 180 / 256) # hide\nmgreen = RGBf(44 / 256, 160 / 256, 44 / 256)\ncolors = (morange, mred, mpurple, mblue, mgreen)\nfunction set_up_plot(; theme = :dark) # hide\ntext_color = theme == :dark ? :white : :black # hide\nfig = Figure(; backgroundcolor = :transparent, size = (900, 675)) # hide\nax = Axis3(fig[1, 1]; # hide\n    backgroundcolor = (:tomato, .5), # hide\n    aspect = (1., 1., 1.), # hide\n    xlabel = L\"z_1\", # hide\n    ylabel = L\"z_2\", # hide\n    zlabel = L\"z_3\", # hide\n    xgridcolor = text_color, # hide\n    ygridcolor = text_color, # hide\n    zgridcolor = text_color, # hide\n    xtickcolor = text_color, # hide\n    ytickcolor = text_color, # hide\n    ztickcolor = text_color, # hide\n    xlabelcolor = text_color, # hide\n    ylabelcolor = text_color, # hide\n    zlabelcolor = text_color, # hide\n    xypanelcolor = :transparent, # hide\n    xzpanelcolor = :transparent, # hide\n    yzpanelcolor = :transparent, # hide\n    limits = ([-1, 1], [-1, 1], [-1, 1]), # hide\n    azimuth = π / 7, # hide\n    elevation = π / 7, # hide\n    # height = 75., # hide\n    ) # hide\n# plot a sphere with radius one and origin 0\nsurface!(ax, Main.sphere(1., [0., 0., 0.])...; alpha = .45, transparency = true)\n\nfor (trajectory, color, i) in zip(trajectories, colors, 1:length(trajectories))\n    lines!(ax, trajectory[1, :], trajectory[2, :], trajectory[3, :]; color = color, linewidth = 4, label = \"Trajectory $(i)\")\nend\n\naxislegend(; position = (.82, .75), backgroundcolor = theme == :dark ? :transparent : :white, labelcolor = text_color) # hide\nfig, ax # hide\nend # hide\n\nfig_light = set_up_plot(; theme = :light)[1] # hide\nfig_dark = set_up_plot(; theme = :dark)[1] # hide\n\nsave(\"rigid_body_trajectories_light.png\", alpha_colorbuffer(fig_light)) # hide\nsave(\"rigid_body_trajectories_dark.png\", alpha_colorbuffer(fig_dark)) # hide\n\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"(Image: A sample of rigid body trajectories. This system has two conserved quantities.) (Image: A sample of rigid body trajectories. This system has two conserved quantities.)","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"The rigid body has two conserved quantities:","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"one conserved quantity is the Hamiltonian of the system: H(z_1 z_2 z_3) = frac12left( fracz_1^2I_1 + fracz_2^2I_2 + fracz_3^2I_3 right)\nthe second one is the quadratic invariant: I(z_1 z_2 z_3) = z_1^2 + z_2^2 + z_3^2","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"The coefficients I_1 I_2 and I_3 can be obtained through","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"beginaligned\nA = fracI_2 - I_3I_2I_3  \nB = fracI_3 - I_1I_3I_1  \nC = fracI_1 - I_2I_1I_2\nendaligned","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"The second conserved invariant I(cdot cdot cdot) is visualized through the sphere in the figure above. The conserved Hamiltonian is the reason for why the curves are closed.","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"The rigid body has Poisson structure [1], but does not have canonical Hamiltonian structure. We can thus not use SympNets or symplectic transformers here, but the ODE is clearly divergence-free. We use this to demonstrate the efficacy of the volume-preserving transformer. We set up our networks:","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"# hyperparameters concerning the architectures \nconst sys_dim = size(dl_cpu.input, 1)\nconst n_heads = 1\nconst L = 3 # transformer blocks \nconst activation = tanh\nconst resnet_activation = tanh\nconst n_linear = 1\nconst n_blocks = 2\nconst skew_sym = false\nconst seq_length = 3\n\narch_vpff = VolumePreservingFeedForward(sys_dim, n_blocks * L, n_linear, resnet_activation)\narch_vpt = VolumePreservingTransformer(sys_dim, seq_length; \n                                            n_blocks = n_blocks, \n                                            n_linear = n_linear, \n                                            L = L, \n                                            activation = resnet_activation, \n                                            skew_sym = skew_sym)\narch_st = StandardTransformerIntegrator(sys_dim;    n_heads = n_heads, \n                                                    transformer_dim = sys_dim, \n                                                    n_blocks = n_blocks, \n                                                    L = L, \n                                                    resnet_activation = resnet_activation, \n                                                    add_connection = false)\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"Note that we set the keyword skew_sym to false here. This is different from what we did in [4], where it was set to true[1]. We allocate the networks on GPU:","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"[1]: A detailed discussion of the consequences of setting this keyword is presented as a separate example.","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"using CUDA\nbackend = CUDABackend()","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"T = Float32\nbackend = CPU() # hide\n\ndl = DataLoader(dl_cpu, backend, T)\nnn_vpff = NeuralNetwork(arch_vpff, backend, T)\nnn_vpt = NeuralNetwork(arch_vpt, backend, T)\nnn_st = NeuralNetwork(arch_st, backend, T)\n\n(parameterlength(nn_vpff), parameterlength(nn_vpt), parameterlength(nn_st))","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"We now train the various networks. For this we use AdamOptimizerWithDecay:","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"const n_epochs = 500000\nconst batch_size = 16384\nconst feedforward_batch = Batch(batch_size)\nconst transformer_batch = Batch(batch_size, seq_length, seq_length)\nconst opt_method = AdamOptimizerWithDecay(n_epochs, T; η₁ = 1e-2, η₂ = 1e-6)\n\no_vpff = Optimizer(opt_method, nn_vpff)\no_vpt = Optimizer(opt_method, nn_vpt)\no_st = Optimizer(opt_method, nn_st)\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"o_vpff(nn_vpff, dl, feedforward_batch, n_epochs)\no_vpt(nn_vpt, dl, transformer_batch, n_epochs)\no_st(nn_st, dl, transformer_batch, n_epochs)","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"After the networks have been trained we map the parameters to cpu:","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"const mtc = GeometricMachineLearning.map_to_cpu\nnn_vpff = mtc(nn_vpff)\nnn_vpt = mtc(nn_vpt)\nnn_st = mtc(nn_st)\nusing JLD2 # hide\n# get correct parameters from jld2 file # hide\nf = load(\"transformer_rigid_body.jld2\")  # hide\n_nnp(ps::Tuple) = NeuralNetworkParameters{Tuple(Symbol(\"L$(i)\") for i in 1:length(ps))}(ps) # hide\nnn_vpff = NeuralNetwork(nn_vpff.architecture, nn_vpff.model, _nnp(f[\"nn_vpff_params\"]), nn_vpff.backend) # hide\nnn_vpt = NeuralNetwork(nn_vpt.architecture, nn_vpt.model, _nnp(f[\"nn_vpt_arb_params\"]), nn_vpt.backend) # hide\nnn_st = NeuralNetwork(nn_st.architecture, nn_st.model, _nnp(f[\"nn_st_params\"]), nn_st.backend) # hide\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"After this we use iterate to obtain predicted orbits:","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"ics_val₁ = [0., sin(0.9), cos(0.9 + π)]\nics_val₂ = [0., sin(1.1), cos(1.1)]\nconst t_validation = 120\n\nfunction produce_trajectory(ics_val)\n    problem = odeproblem(ics_val;   tspan = (0, t_validation), \n                                    tstep = tstep, \n                                    parameters = default_parameters)\n    solution = integrate(problem, ImplicitMidpoint())\n    trajectory = Float32.(DataLoader(solution; suppress_info = true).input)\n    nn_vpff_solution = iterate(nn_vpff, trajectory[:, 1]; \n                                            n_points = Int(floor(t_validation / tstep)) + 1)\n    nn_vpt_solution = iterate(nn_vpt, trajectory[:, 1:seq_length];\n                                            n_points = Int(floor(t_validation / tstep)) + 1)\n    nn_st_solution = iterate(nn_st, trajectory[:, 1:seq_length];\n                                            n_points = Int(floor(t_validation / tstep)) + 1)\n    trajectory, nn_vpff_solution, nn_vpt_solution, nn_st_solution\nend\n\ntrajectory₁, nn_vpff_solution₁, nn_vpt_solution₁, nn_st_solution₁ = produce_trajectory(ics_val₁)\ntrajectory₂, nn_vpff_solution₂, nn_vpt_solution₂, nn_st_solution₂ = produce_trajectory(ics_val₂)\nnothing # hide","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"function evaluate_plot!(fig, coordinate1, coordinate2, trajectory1, trajectory2; theme = :dark, label = \"\", color = mgreen) # hide\ntext_color = theme == :dark ? :white : :black # hide\nax = Axis3(fig[coordinate1, coordinate2]; # hide\n    backgroundcolor = (:tomato, .5), # hide\n    aspect = (1., 1., 1.), # hide\n    xlabel = L\"z_1\", # hide\n    ylabel = L\"z_2\", # hide\n    zlabel = L\"z_3\", # hide\n    xgridcolor = text_color, # hide\n    ygridcolor = text_color, # hide\n    zgridcolor = text_color, # hide\n    xtickcolor = text_color, # hide\n    ytickcolor = text_color, # hide\n    ztickcolor = text_color, # hide\n    xlabelcolor = text_color, # hide\n    ylabelcolor = text_color, # hide\n    zlabelcolor = text_color, # hide\n    xypanelcolor = :transparent, # hide\n    xzpanelcolor = :transparent, # hide\n    yzpanelcolor = :transparent, # hide\n    limits = ([-1, 1], [-1, 1], [-1, 1]), # hide\n    azimuth = π / 7, # hide\n    elevation = π / 7, # hide\n    # this has to be fixed manually for now: https://discourse.julialang.org/t/makie-unwanted-whitespace-to-right-of-3d-subplots-and-dont-fully-understand-behaviour-of-colgap-with-3d-plots/108636\n    protrusions = (50, 0, 0, 12)\n    # height = 75., # hide\n    ) # hide\n# plot a sphere with radius one and origin 0\nsurface!(ax, Main.sphere(1., [0., 0., 0.])...; alpha = .45, transparency = true)\n\nlines!(ax, trajectory₁[1, :], trajectory₁[2, :], trajectory₁[3, :], color = mblue, label = \"Implicit Midpoint\", linewidth = 2)\nlines!(ax, trajectory₂[1, :], trajectory₂[2, :], trajectory₂[3, :], color = mblue, linewidth = 2)\nlines!(ax, trajectory1[1, :], trajectory1[2, :], trajectory1[3, :], color = color, label = label, linewidth = 2)\nlines!(ax, trajectory2[1, :], trajectory2[2, :], trajectory2[3, :], color = color, linewidth = 2)\naxislegend(ax; position = (.82, 1.15), backgroundcolor = theme == :dark ? :transparent : :white, labelcolor = text_color) # hide\nax # hide\nend # hide\n\nfig_dark = Figure(; backgroundcolor = :transparent, size = (900, 675)) # hide\nfig_light = Figure(; backgroundcolor = :transparent, size = (900, 675)) # hide\n\nax_dark_vpff = evaluate_plot!(fig_dark, 1, 1, nn_vpff_solution₁, nn_vpff_solution₂; theme = :dark, label = \"VPFF\", color = mgreen)\nax_dark_vpff = evaluate_plot!(fig_dark, 1, 2, nn_vpt_solution₁, nn_vpt_solution₂; theme = :dark, label = \"VPT\", color = mpurple)\nax_dark_vpff = evaluate_plot!(fig_dark, 1, 3, nn_st_solution₁, nn_st_solution₂; theme = :dark, label = \"ST\", color = morange)\n\nax_light_vpff = evaluate_plot!(fig_light, 1, 1, nn_vpff_solution₁, nn_vpff_solution₂; theme = :light, label = \"VPFF\", color = mgreen)\nax_light_vpff = evaluate_plot!(fig_light, 1, 2, nn_vpt_solution₁, nn_vpt_solution₂; theme = :light, label = \"VPT\", color = mpurple)\nax_light_vpff = evaluate_plot!(fig_light, 1, 3, nn_st_solution₁, nn_st_solution₂; theme = :light, label = \"ST\", color = morange)\n\nrowsize!(fig_light.layout, 1, Aspect(1, 1.))\nrowsize!(fig_dark.layout, 1, Aspect(1, 1.))\nresize_to_layout!(fig_light)\nresize_to_layout!(fig_dark)\nsave(\"rigid_body_evaluation_light.png\", alpha_colorbuffer(fig_light))\nsave(\"rigid_body_evaluation_dark.png\", alpha_colorbuffer(fig_dark))\nnothing","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"(Image: Evaluation of the three different networks for an interval (0, 120).) (Image: Evaluation of the three different networks for an interval (0, 120).)","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"We can see that the volume-preserving transformer performs much better than the volume-preserving feedforward neural network and the standard transformer. It is especially noteworthy that its curves stick to the sphere at all times, which is not the case for the standard transformer. We also see that the standard transformer seems to perform better for one of the curves shown, but completely fails for the other. Why this is should be further investigated. ","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"We also compare the times it takes to integrate the system with (i) implicit midpoint, (ii) the volume-preserving transformer and (iii) the standard transformer:","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"function timing() # hide\nproblem = odeproblem(ics_val₁; tspan = (0, t_validation), tstep = tstep, parameters = default_parameters) # hide\nsolution = integrate(problem, ImplicitMidpoint()) # hide\n@time \"Implicit Midpoint\" solution = integrate(problem, ImplicitMidpoint())\ntrajectory = Float32.(DataLoader(solution; suppress_info = true).input)\niterate(nn_vpt, trajectory[:, 1:seq_length]; n_points = Int(floor(t_validation / tstep)) + 1) # hide\n@time \"VPT\" iterate(nn_vpt, trajectory[:, 1:seq_length]; n_points = Int(floor(t_validation / tstep)) + 1)\niterate(nn_st, trajectory[:, 1:seq_length]; n_points = Int(floor(t_validation / tstep)) + 1) # hide\n@time \"ST\" iterate(nn_st, trajectory[:, 1:seq_length]; n_points = Int(floor(t_validation / tstep)) + 1)\nnothing # hide\nend # hide\ntiming() # hide","category":"page"},{"location":"tutorials/volume_preserving_transformer_rigid_body/","page":"Volume-Preserving Transformer for the Rigid Body","title":"Volume-Preserving Transformer for the Rigid Body","text":"Here we see that standard transformer is the fastest, followed by the volume-preserving transformers. Both neural network integrators are however faster than implicit midpoint (as evaluating them is completely explicit). The ODE we treated here is a very simple one. For more complicated cases the speed-up we gain by using neural networks can be up to a factor 1000.","category":"page"},{"location":"architectures/abstract_neural_networks/","page":"Using Architectures with NeuralNetwork","title":"Using Architectures with NeuralNetwork","text":"In this chapter we build, starting from the neural network layers introduced in the previous chapter, \\textit{neural network architectures}. Here these are always understood as a composition of neural network layers. We start by shortly explaining the application interface for neural networks used in \\texttt{GeometricMachineLearning} and then introduce symplectic autoencoders, SympNets, volume-preserving feedforward neural networks, standard transformers, volume-preserving transformers and linear symplectic transformers. All of these architectures, except SympNets and standard transformers (and arguably volume-preserving feedforward neural networks), constitute novel work. As we will see all of them, except symplectic autoencoders, can be interpreted as \\textit{neural network-based integrators}.","category":"page"},{"location":"architectures/abstract_neural_networks/#NeuralNetworks-in-GeometricMachineLearning","page":"Using Architectures with NeuralNetwork","title":"NeuralNetworks in GeometricMachineLearning","text":"","category":"section"},{"location":"architectures/abstract_neural_networks/","page":"Using Architectures with NeuralNetwork","title":"Using Architectures with NeuralNetwork","text":"GeometricMachineLearning inherits some functionality from another Julia package called AbstractNeuralNetworks. How these two packages interact is shown in the figure below for the example of the SympNet[1]:","category":"page"},{"location":"architectures/abstract_neural_networks/","page":"Using Architectures with NeuralNetwork","title":"Using Architectures with NeuralNetwork","text":"[1]: The section on SympNets also contains an explanation of all the structs and types described in this section here.","category":"page"},{"location":"architectures/abstract_neural_networks/","page":"Using Architectures with NeuralNetwork","title":"Using Architectures with NeuralNetwork","text":"(Image: Visualization of how the packages interact.) (Image: Visualization of how the packages interact.)","category":"page"},{"location":"architectures/abstract_neural_networks/","page":"Using Architectures with NeuralNetwork","title":"Using Architectures with NeuralNetwork","text":"The red color indicates an abstract type, blue indicates a struct and orange indicates a const (derived from a struct). Solid black arrows indicate direct dependencies, i.e. we have","category":"page"},{"location":"architectures/abstract_neural_networks/","page":"Using Architectures with NeuralNetwork","title":"Using Architectures with NeuralNetwork","text":"using GeometricMachineLearning # hide\nusing GeometricMachineLearning: GradientLayer, Architecture, SympNetLayer, AbstractExplicitLayer # hide\n@assert GradientLayer <: SympNetLayer <: AbstractExplicitLayer # hide\nGradientLayer <: SympNetLayer <: AbstractExplicitLayer\n@assert GSympNet <: SympNet <: Architecture # hide\nGSympNet <: SympNet <: Architecture\nnothing # hide","category":"page"},{"location":"architectures/abstract_neural_networks/","page":"Using Architectures with NeuralNetwork","title":"Using Architectures with NeuralNetwork","text":"Dashed black arrows indicate a derived neural network architecture. A GSympNet (which is an Architecture) is derived from GradientLayerQ and GradientLayerP (which are AbstractExplicitLayers) for example. An Architecture can be turned into a NeuralNetwork by calling the associated constructor","category":"page"},{"location":"architectures/abstract_neural_networks/","page":"Using Architectures with NeuralNetwork","title":"Using Architectures with NeuralNetwork","text":"arch = GSympNet(3)\nnn = NeuralNetwork(arch, CPU(), Float64)\nnothing # hide","category":"page"},{"location":"architectures/abstract_neural_networks/","page":"Using Architectures with NeuralNetwork","title":"Using Architectures with NeuralNetwork","text":"Such a neural network has four fields:","category":"page"},{"location":"architectures/abstract_neural_networks/","page":"Using Architectures with NeuralNetwork","title":"Using Architectures with NeuralNetwork","text":"architecture: the Architecture we supplied the constructor with,\nmodel: a translation of the supplied architecture into specific neural network layers,\nparams: the neural network parameters,\nbackend: this indicates on which device we allocate the neural network parameters. In this case it is CPU().","category":"page"},{"location":"architectures/abstract_neural_networks/","page":"Using Architectures with NeuralNetwork","title":"Using Architectures with NeuralNetwork","text":"We can get the associated model to GSympNet by calling:","category":"page"},{"location":"architectures/abstract_neural_networks/","page":"Using Architectures with NeuralNetwork","title":"Using Architectures with NeuralNetwork","text":"nn.model.layers","category":"page"},{"location":"architectures/abstract_neural_networks/","page":"Using Architectures with NeuralNetwork","title":"Using Architectures with NeuralNetwork","text":"and we see that it consists of two layers: a GradientLayerQ and a GradientLayerP.","category":"page"},{"location":"layers/volume_preserving_feedforward/#Volume-Preserving-Feedforward-Layer","page":"Volume-Preserving Layers","title":"Volume-Preserving Feedforward Layer","text":"","category":"section"},{"location":"layers/volume_preserving_feedforward/","page":"Volume-Preserving Layers","title":"Volume-Preserving Layers","text":"The volume-preserving feedforward layers in GeometricMachineLearning are closely related to SympNet layers. We note that [50] proposes more complicated volume-preserving feedforward neural networks than the ones presented here. These are motivated by a classical theorem [51]; but for reasons of simplicity and their similarity to SympNet layers we resort to simpler ones.","category":"page"},{"location":"layers/volume_preserving_feedforward/","page":"Volume-Preserving Layers","title":"Volume-Preserving Layers","text":"Volume-preserving feedforward layers in GeometricMachineLearning are a special type of ResNet layer for which we restrict the weight matrices to be of a particular form. Each layer computes: ","category":"page"},{"location":"layers/volume_preserving_feedforward/","page":"Volume-Preserving Layers","title":"Volume-Preserving Layers","text":"mathttVPFF_A b x mapsto x + sigma(Ax + b)","category":"page"},{"location":"layers/volume_preserving_feedforward/","page":"Volume-Preserving Layers","title":"Volume-Preserving Layers","text":"where sigma is a nonlinearity, A is the weight and b is the bias. The matrix A is either a LowerTriangular matrix L or an UpperTriangular matrix U. We demonstrate volume-preservation of these layers by considering the case A = L. The matrix looks as follows:","category":"page"},{"location":"layers/volume_preserving_feedforward/","page":"Volume-Preserving Layers","title":"Volume-Preserving Layers","text":"L = beginpmatrix\n     0  0  cdots  0      \n     a_21  ddots          vdots \n     vdots  ddots  ddots  vdots \n     a_n1  cdots  a_n(n-1)       0 \nendpmatrix","category":"page"},{"location":"layers/volume_preserving_feedforward/","page":"Volume-Preserving Layers","title":"Volume-Preserving Layers","text":"For the jacobian we then have:","category":"page"},{"location":"layers/volume_preserving_feedforward/","page":"Volume-Preserving Layers","title":"Volume-Preserving Layers","text":"J = nablamathttVPFF_L b = beginpmatrix\n     1  0  cdots  0      \n     b_21  ddots          vdots \n     vdots  ddots  ddots  vdots \n     b_n1  cdots  b_n(n-1)       1 \nendpmatrix","category":"page"},{"location":"layers/volume_preserving_feedforward/","page":"Volume-Preserving Layers","title":"Volume-Preserving Layers","text":"and the determinant of J is 1, i.e. the map is volume-preserving. A similar statement holds if the matrix A is UpperTriangular instead of LowerTriangular.","category":"page"},{"location":"layers/volume_preserving_feedforward/#Library-Functions","page":"Volume-Preserving Layers","title":"Library Functions","text":"","category":"section"},{"location":"layers/volume_preserving_feedforward/","page":"Volume-Preserving Layers","title":"Volume-Preserving Layers","text":"VolumePreservingFeedForwardLayer\nVolumePreservingLowerLayer\nVolumePreservingUpperLayer","category":"page"},{"location":"layers/volume_preserving_feedforward/#GeometricMachineLearning.VolumePreservingFeedForwardLayer","page":"Volume-Preserving Layers","title":"GeometricMachineLearning.VolumePreservingFeedForwardLayer","text":"VolumePreservingFeedForwardLayer <: AbstractExplicitLayer\n\nSuper-type of VolumePreservingLowerLayer and VolumePreservingUpperLayer. The layers do the following: \n\nx mapsto begincases sigma(Lx + b)  textwhere L is mathttLowerTriangular  sigma(Ux + b)  textwhere U is mathttUpperTriangular endcases\n\nThe functor can be applied to a vector, a matrix or a tensor. The special matrices are implemented as LowerTriangular and UpperTriangular.\n\n\n\n\n\n","category":"type"},{"location":"layers/volume_preserving_feedforward/#GeometricMachineLearning.VolumePreservingLowerLayer","page":"Volume-Preserving Layers","title":"GeometricMachineLearning.VolumePreservingLowerLayer","text":"VolumePreservingLowerLayer(dim)\n\nMake an instance of VolumePreservingLowerLayer for a specific system dimension.\n\nSee the documentation for VolumePreservingFeedForwardLayer.\n\nExamples\n\nWe apply the VolumePreservingLowerLayer with matrix:\n\nL = beginpmatrix 0  0  1  0 endpmatrix\n\nto a vector:\n\nx = beginpmatrix 1  1 endpmatrix\n\nusing GeometricMachineLearning\n\nl = VolumePreservingLowerLayer(2, identity; use_bias=false)\nA = LowerTriangular([1,], 2)\nps = (weight = A, )\nx = ones(eltype(A), 2)\n\nl(x, ps)\n\n# output\n\n2×1 Matrix{Int64}:\n 1\n 2\n\nArguments\n\nThe constructor can be called with the optional arguments:\n\nactivation=tanh: the activation function. \nuse_bias::Bool=true (keyword argument): specifies whether a bias should be used. \n\n\n\n\n\n","category":"type"},{"location":"layers/volume_preserving_feedforward/#GeometricMachineLearning.VolumePreservingUpperLayer","page":"Volume-Preserving Layers","title":"GeometricMachineLearning.VolumePreservingUpperLayer","text":"VolumePreservingUpperLayer(dim)\n\nMake an instance of VolumePreservingUpperLayer for a specific system dimension.\n\nSee the documentation for VolumePreservingFeedForwardLayer.\n\nExamples\n\nWe apply the VolumePreservingUpperLayer with matrix:\n\nU = beginpmatrix 0  1  0  0 endpmatrix\n\nto a vector:\n\nx = beginpmatrix 1  1 endpmatrix\n\nusing GeometricMachineLearning\n\nl = VolumePreservingUpperLayer(2, identity; use_bias=false)\nA = UpperTriangular([1,], 2)\nps = (weight = A, )\nx = ones(eltype(A), 2)\n\nl(x, ps)\n\n# output\n\n2×1 Matrix{Int64}:\n 2\n 1\n\nArguments\n\nThe constructor can be called with the optional arguments:\n\nactivation=tanh: the activation function. \nuse_bias::Bool=true (keyword argument): specifies whether a bias should be used. \n\n\n\n\n\n","category":"type"},{"location":"architectures/sympnet/#SympNet-Architecture","page":"SympNet","title":"SympNet Architecture","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"This section discusses the symplectic neural network (SympNet) architecture and its implementation in GeometricMachineLearning.","category":"page"},{"location":"architectures/sympnet/#Principle","page":"SympNet","title":"Principle","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"SympNets [5] are a type of neural network that can model the trajectory of a canonical Hamiltonian system in phase space. Take (q^Tp^T)^T=(q_1ldotsq_dp_1ldotsp_d)^Tin mathbbR^2d as the coordinates in phase space, where q=(q_1 ldots q_d)^Tin mathbbR^d is refered to as the position and p=(p_1 ldots p_d)^Tin mathbbR^d the momentum. Given a point ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"  beginpmatrix q^(m)  p^(m) endpmatrix in mathbbR^2d","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"the SympNet aims to compute the next position","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"  mathrmSympNetleft( beginpmatrix q^(m)  p^(m) endpmatrix right) = beginpmatrix tildeq^(m+1)  tildep^(m+1) endpmatrix in mathbbR^2d","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"and thus predicts the trajectory while preserving the symplectic structure of the system. SympNets are enforcing symplecticity strongly, meaning that this property is hard-coded into the network architecture. The layers are reminiscent of traditional neural network feedforward layers, but have a strong restriction imposed on them in order to be symplectic.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"SympNets can be viewed as a symplectic integrator or symplectic one-step method[1] [1, 82]. Their goal is to predict, based on an initial condition ((q^(0))^T(p^(0))^T)^T, a sequence of points in phase space that fit the training data as well as possible:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"[1]: Symplectic multi-step methods can be modeled with transformers.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"beginpmatrix q^(0)  p^(0) endpmatrix beginpmatrix tildeq^(1)  tildep^(1) endpmatrix cdots beginpmatrix tildeq^(n)  tildep^(n) endpmatrix","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The tilde in the above equation indicates predicted data. With standard SympNets[2] the time step between predictions is not a parameter we can choose but is related to the temporal frequency of the training data. This means that if data is recorded in an interval of e.g. 0.1 seconds, then this will be the time step of our integrator.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"[2]: Recently an approach [35] has been proposed that makes explicitly specifying the time step possible by viewing SympNets as a subclass of so-called \"Generalized Hamiltonian Neural Networks\".","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"SympNets preserve symplecticity by exploiting the (q p) structure of the system. This is visualized below:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"(Image: Visualization of the SympNet architecture.) (Image: Visualization of the SympNet architecture.)","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"In the figure above we see that an update for q is based on data coming from p and an update for p is based on data coming from q. T_imathbbR^dtomathbbR^d is an operation that changes p when i is even and changes q when odd. It has the special property that its Jacobian is a symmetric matrix. There are two types of SympNet architectures: LA-SympNets and G-SympNets. ","category":"page"},{"location":"architectures/sympnet/#LA-SympNet","page":"SympNet","title":"LA-SympNet","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The first type of SympNets, LA-SympNets, are obtained from composing two types of layers: symplectic linear layers and symplectic activation layers.  For a given integer w, the linear part of an LA-SympNet is","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"mathcalL^wmathrmup\nbeginpmatrix\n q \n p \nendpmatrix\n =  \nbeginpmatrix \n mathbbI  A^wmathbbO \n mathbbOA^w  mathbbI \nendpmatrix\n cdots \nbeginpmatrix \n mathbbI  mathbbO \n A^2  mathbbI \nendpmatrix\nbeginpmatrix \n mathbbI  A^1 \n mathbbO  mathbbI \nendpmatrix\nbeginpmatrix\n q \n p \nendpmatrix\n+ b ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"or ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"mathcalL^wmathrmlow\nbeginpmatrix  q    \n p  endpmatrix =  \n  beginpmatrix \n mathbbI  mathbbOA^w   \n A^wmathbbO  mathbbI\n endpmatrix cdots \n  beginpmatrix \n mathbbI  A^2   \n mathbbO  mathbbI\n endpmatrix\n beginpmatrix \n mathbbI  mathbbO   \n A^1  mathbbI\n endpmatrix\n beginpmatrix  q    \n p  endpmatrix\n  + b  ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The superscripts mathrmup and mathrmlow indicate whether the q or the p part is first changed[3]. The learnable parameters are the symmetric matrices A^iinmathcalS_mathrmsym(d) and the bias binmathbbR^2d. The integer w is the number of linear layers in one block. It can be shown that five of these layers, i.e. wgeq5, can represent any linear symplectic map [86], so w need not be larger than five. We denote the set of symplectic linear layers by mathcalM^L.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"[3]: \"up\" means we first change the q part and \"low\" means we first change the p part. This can be set via the keyword init_upper_linear in LASympNet. ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The second type of layer needed for LA-SympNets are activation layers.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"An LA-SympNet is a mapping of the form Psi=l_k circ a_k circ l_k-1 circ cdots circ a_1 circ l_0 where (l_i)_0leq ileq k subset mathcalM^L and (a_i)_1leq ileq k subset mathcalM^A. We will refer to k as the number of hidden layers of the SympNet[4] and the number w above as the depth of the linear layer.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"[4]: Note that if k=0 then the LA-SympNet consists of only one linear layer.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"We give an example of calling LA-SympNet:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"using GeometricMachineLearning\n\nk = 1\nw = 2\narch = LASympNet(4; \n                    nhidden = k, \n                    depth = 2, \n                    init_upper_linear = true, \n                    init_upper_act = true, \n                    activation = tanh)\n\nmodel = Chain(arch).layers","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The keywords init_upper_linear and init_upper_act indicate whether the first linear (respectively activation) layer is of q type[5].","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"[5]: Similarly to init_upper_linear, if init_upper_act = true then the first activation layer is of q type, i.e. changes the q component and leaves the p component unchanged. ","category":"page"},{"location":"architectures/sympnet/#G-SympNets","page":"SympNet","title":"G-SympNets","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"G-SympNets are an alternative to LA-SympNets. They are built with only one kind of layer, the gradient layer. If we denote by mathcalM^G the set of gradient layers, a G-SympNet is a function of the form Psi=g_k circ g_k-1 circ cdots circ g_1 where (g_i)_1leq ileq k subset mathcalM^G. The index k here is the number of layers in the SympNet.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"using GeometricMachineLearning\n\nk = 2\nn = 10\narch = GSympNet(4; upscaling_dimension = n, n_layers = k, init_upper = true, activation = tanh)\n\nmodel = Chain(arch).layers","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The keyword init_upper for GSympNet is similar as in the case for LASympNet. The keyword upscaling_dimension is explained in the section on the SympNet gradient layer.","category":"page"},{"location":"architectures/sympnet/#Universal-Approximation-Theorems","page":"SympNet","title":"Universal Approximation Theorems","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"In order to state the universal approximation theorem for both architectures we first need a few definitions:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Let U be an open set of mathbbR^2d, and let us denote by mathcalSP^r(U) the set of C^r smooth symplectic maps on U. We now define a topology on C^r(K mathbbR^2d), the set of C^r-smooth maps from a compact set KsubsetU to mathbbR^2d through the norm","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"f_C^r(KmathbbR^2d) = sum_alphaleq r underset1leq i leq 2dmax hspace2mm undersetxin Ksup D^alpha f_i(x)","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"where the differential operator D^alpha is defined by ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"D^alpha f = fracpartial^alpha fpartial x_1^alpha_1x_n^alpha_n","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"with alpha = alpha_1 ++ alpha_2d. We impose the following condition (r finiteness) on the activation function:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Main.definition(raw\"``\\sigma`` is **``r``-finite** if ``\\sigma\\in C^r(\\mathbb{R},\\mathbb{R})`` and ``\\int |D^r\\sigma(x)|dx <\\infty``.\")","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"We further consider the topology on C^r(U mathbbR^d) induced by cdot _C^r(cdot mathbbR^d) and the associated notion of denseness:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Main.definition(raw\"Let ``m,d,r\\in \\mathbb{N}`` with ``m,d>0`` be given, ``U`` an open subset of ``\\mathbb{R}^m``, and ``I,J\\subset C^r(U,\\mathbb{R}^d)``. We say ``J`` is **``r``-uniformly dense on compacta in ``I``** if ``J \\subset I`` and for any ``f\\in I``, ``\\epsilon>0``, and any compact ``K\\subset U``, there exists ``g\\in J`` such that ``||f-g||_{C^r(K,\\mathbb{R}^{d})} < \\epsilon``.\")","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Main.remark(raw\"The associated topology to this notion of denseness is the **compact-open topology**. It is generated by the following sets:\n\" * Main.indentation * raw\"```math\n\" * Main.indentation * raw\" V(K, U) := \\{f\\in{}C^r(\\mathbb{R}^m, \\mathbb{R}^d): \\text{ such that $f(K)\\subset{}U$}\\},\n\" * Main.indentation * raw\"```\n\" * Main.indentation * raw\"i.e. the *compact-open topology* is the smallest topology that contains all sets of the form ``V(K, U)``.\")","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"We can now state the universal approximation theorems:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Main.theorem(raw\"For any positive integer ``r>0`` and open set ``U\\in \\mathbb{R}^{2d}``, the set of ``LA``-SympNet is ``r``-uniformly dense on compacta in ``SP^r(U)`` if the activation function ``\\sigma`` is ``r``-finite.\"; name = raw\"Approximation theorem for LA-SympNets\")","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"and","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Main.theorem(raw\"For any positive integer ``r>0`` and open set ``U\\in \\mathbb{R}^{2d}``, the set of ``G``-SympNet is ``r``-uniformly dense on compacta in ``SP^r(U)`` if the activation function ``\\sigma`` is ``r``-finite.\"; name = raw\"Approximation theorem for G-SympNets\")","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"There are many r-finite activation functions commonly used in neural networks, for example:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The sigmoid activation function: sigma(x) = 1  (1+e^-x), \nThe hyperbolic tangent function: tanh(x) = (e^x-e^-x)  (e^x+e^-x). ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The universal approximation theorems state that we can, in principle, get arbitrarily close to any symplectomorphism defined on mathbbR^2d. But this does not tell us anything about how to optimize the network. This is can be done with any common neural network optimizer and these neural network optimizers always rely on a corresponding loss function.  ","category":"page"},{"location":"architectures/sympnet/#Loss-function","page":"SympNet","title":"Loss function","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"To train the SympNet, one needs data along a trajectory such that the model is trained to perform an integration. The loss function is defined as[6]:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"[6]: This loss function is implemented as FeedForwardLoss in GeometricMachineLearning.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"mathrmloss(z^mathrmc z^mathrmp) = frac z^mathrmc - z^mathrmp  z^mathrmc ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"where ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"z^mathrmc = beginpmatrix q^mathrmc  p^mathrmc endpmatrix","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"is the current state and ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"z^mathrmp = beginpmatrix q^mathrmp  p^mathrmp endpmatrix","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"is the predicted state. In the example section we show how to use SympNets in GeometricMachineLearning.jl and how to modify the loss function.","category":"page"},{"location":"architectures/sympnet/#Library-Functions","page":"SympNet","title":"Library Functions","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"SympNet\nLASympNet\nGSympNet","category":"page"},{"location":"architectures/sympnet/#GeometricMachineLearning.SympNet","page":"SympNet","title":"GeometricMachineLearning.SympNet","text":"SympNet <: NeuralNetworkIntegrator\n\nThe SympNet type encompasses GSympNets and LASympNets.  SympNets [5] are universal approximators of canonical symplectic flows. This means that for every map \n\n    varphimathbbR^2ntomathbbR^2n text with  (nablavarphi)^TmathbbJnablavarphi = mathbbJ\n\nwe can find a SympNet that approximates varphi arbitrarily well.\n\n\n\n\n\n","category":"type"},{"location":"architectures/sympnet/#GeometricMachineLearning.LASympNet","page":"SympNet","title":"GeometricMachineLearning.LASympNet","text":"LASympNet(d)\n\nMake an LA-SympNet with dimension d\n\nThere exists an additional constructor that can be called by supplying an instance of DataLoader.\n\nExamples\n\nusing GeometricMachineLearning\ndl = DataLoader(rand(2, 20); suppress_info = true)\nLASympNet(dl)\n\n# output\n\nLASympNet{typeof(tanh), true, true}(2, 5, 1, tanh)\n\nArguments\n\nKeyword arguments are: \n\ndepth::Int = 5: The number of linear layers that are applied. \nnhidden::Int = 1: The number of hidden layers (i.e. layers that are not output layers).\nactivation = tanh: The activation function that is applied.\ninit_upper_linear::Bool = true: Initialize the linear layer so that it first modifies the q-component. \ninit_upper_act::Bool = true: Initialize the activation layer so that it first modifies the q-component.\n\n\n\n\n\n","category":"type"},{"location":"architectures/sympnet/#GeometricMachineLearning.GSympNet","page":"SympNet","title":"GeometricMachineLearning.GSympNet","text":"GSympNet(d)\n\nMake a G-SympNet with dimension d\n\nThere exists an additional constructor that can be called by supplying an instance of DataLoader (see LASympNet for an example of using this constructor).\n\nArguments\n\nKeyword arguments are:\n\nupscaling_dimension::Int = 2d: The upscaling dimension of the gradient layer. See the documentation for GradientLayerQ and GradientLayerP for further explanation.\nn_layers::Int2: The number of layers (i.e. the total number of GradientLayerQ and GradientLayerP).\nactivationtanh: The activation function that is applied.\ninit_upper::Booltrue: Initialize the gradient layer so that it first modifies the q-component.\n\n\n\n\n\n","category":"type"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"\\begin{comment}","category":"page"},{"location":"architectures/sympnet/#References","page":"SympNet","title":"References","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"P. Jin, Z. Zhang, A. Zhu, Y. Tang and G. E. Karniadakis. SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems. Neural Networks 132, 166–179 (2020).\n\n\n\n","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"\\end{comment}","category":"page"}]
}
