var documenterSearchIndex = {"docs":
[{"location":"manifolds/stiefel_manifold/#Stiefel-manifold","page":"Stiefel","title":"Stiefel manifold","text":"","category":"section"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"The Stiefel manifold St(n N) is the space of all orthonormal frames in mathbbR^Ntimesn, i.e. matrices YinmathbbR^Ntimesn s.t. Y^TY = I. It can also be seen as the orthonormal group O(N) modulo an equivalence relation: AsimBiffexistsCtext st AC = B for ","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"C = beginpmatrix\n    I  0 \n    0  Q \nendpmatrix","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"and QinO(N-n), so the first n columns of A and B are equivalent.","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"The tangent space to the element YinSt(nN) can easily be determined: T_YSt(nN)=DeltaDelta^TY + Y^TDelta = 0. St(n N) is furthermore a homogeneous space because O(N) acts transitively on it, hence its tangent space can be described through mathfrakg, the Lie algebra of O(N) via T_YSt(nN) = mathfrakgcdotY. Based on the element Y, mathfrakg can be split into a vertical and a horizontal component: mathfrakg = mathfrakg^mathrmverYoplusmathfrakg^mathrmhorY, with mathfrakg^mathrmverY = VinmathfrakgVY = 0 and the horizontal component is computed according to the canonical metric on mathfrakg, i.e. is the orthogonal complement to mathfrakg^mathrmverY.","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"The function rgrad is a mapping that takes an element of St(nN) and a \"Euclidean gradient\" and produces an element inT_YSt(nN). This mapping has the property: mathrmTr((nablaf)^TDelta) = g_Y(mathttrgrad(Y nablaf) Delta) forallDeltainT_YSt(nN) and g is the Riemannian metric.","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"What we use for optimizing on the Stiefel manifold (especially regarding the generalization of the Adam optimizer) is the tangent space to E=e_1ldotse_n. This consists of elements: ","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"T_ESt(nN) = leftbeginpmatrix A  B endpmatrix Atext skew-sym and B arbitraryright","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"Further: ","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"mathfrakg^mathrmhor = mathfrakg^mathrmhorE = leftbeginpmatrix A  -B^T  B  0 endpmatrix Atext skew-sym and B arbitraryright","category":"page"},{"location":"library/","page":"Library","title":"Library","text":"CurrentModule = GeometricMachineLearning","category":"page"},{"location":"library/#GeometricMachineLearning-Library-Functions","page":"Library","title":"GeometricMachineLearning Library Functions","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"Modules = [GeometricMachineLearning]","category":"page"},{"location":"library/#GeometricMachineLearning.AbstractCache","page":"Library","title":"GeometricMachineLearning.AbstractCache","text":"AbstractCache has subtypes:  AdamCache MomentumCache GradientCache\n\nAll of them can be initialized with providing an array (also supporting manifold types).\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.AbstractRetraction","page":"Library","title":"GeometricMachineLearning.AbstractRetraction","text":"AbstractRetraction is a type that comprises all retraction methods for manifolds. For every manifold layer one has to specify a retraction method that takes the layer and elements of the (global) tangent space.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.AdamOptimizer","page":"Library","title":"GeometricMachineLearning.AdamOptimizer","text":"Defines the Adam Optimizer. Algorithm and suggested defaults are taken from (Goodfellow et al., 2016, page 301), except for Œ¥, because single precision is used!\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.BlockIdentityLowerMatrix","page":"Library","title":"GeometricMachineLearning.BlockIdentityLowerMatrix","text":"A BlockIdentityLowerMatrix is a matrix with blocks | 1  0 | | S  1 | Currently, it only implements a custom mul! method, exploiting this structure.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.BlockIdentityUpperMatrix","page":"Library","title":"GeometricMachineLearning.BlockIdentityUpperMatrix","text":"A BlockIdentityUpperMatrix is a matrix with blocks | 1  S | | 0  1 | Currently, it only implements a custom mul! method, exploiting this structure.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.Gradient","page":"Library","title":"GeometricMachineLearning.Gradient","text":"The gradient layer from the SympNet paper (https://www.sciencedirect.com/science/article/abs/pii/S0893608020303063).  Its components are of the form:  $ \\begin{pmatrix}         I & \\nabla{}V \\ 0 & I  \\end{pmatrix}, $ with V(p) = sum_ia_iSigma(sum_jk_ijp_j+b_i), where Sigma is the antiderivative of the activation function sigma. Such layers are by construction symplectic.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.GradientOptimizer","page":"Library","title":"GeometricMachineLearning.GradientOptimizer","text":"Define the Gradient optimizer, i.e. W ‚Üê W - Œ∑*‚àáf(W) Or the riemannian manifold equivalent, if applicable.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.GrassmannLayer","page":"Library","title":"GeometricMachineLearning.GrassmannLayer","text":"Defines a layer that performs simple multiplication with an element of the Grassmann manifold.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.GrassmannManifold","page":"Library","title":"GeometricMachineLearning.GrassmannManifold","text":"maybe consider dividing the output in the check functions by n! TODO: Implement sampling procedures!!\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.ManifoldLayer","page":"Library","title":"GeometricMachineLearning.ManifoldLayer","text":"This defines a manifold layer that only has one matrix-valued manifold A associated with it does xmapstoAx. \n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.MomentumOptimizer","page":"Library","title":"GeometricMachineLearning.MomentumOptimizer","text":"Define the Momentum optimizer, i.e.  V ‚Üê Œ±V - ‚àáf(W) W ‚Üê W + Œ∑V Or the riemannian manifold equivalent, if applicable.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.MultiHeadAttention","page":"Library","title":"GeometricMachineLearning.MultiHeadAttention","text":"MultiHeadAttention (MHA) serves as a preprocessing step in the transformer. It reweights the input vectors bases on correlations within those data. \n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.Optimizer","page":"Library","title":"GeometricMachineLearning.Optimizer","text":"Optimizer struct that stores the 'method' (i.e. Adam with corresponding hyperparameters), the cache and the optimization step.\n\nIt takes as input an optimization method and the parameters of a network. \n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.StiefelLayer","page":"Library","title":"GeometricMachineLearning.StiefelLayer","text":"Defines a layer that performs simple multiplication with an element of the Stiefel manifold.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.SymplecticHouseholderDecom","page":"Library","title":"GeometricMachineLearning.SymplecticHouseholderDecom","text":"this algorithm is taken (and adjusted) from https://doi.org/10.1016/j.laa.2008.02.029\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.Transformer-Tuple{Integer, Integer, Integer}","page":"Library","title":"GeometricMachineLearning.Transformer","text":"The architecture for a \"transformer encoder\" is essentially taken from arXiv:2010.11929, but with the difference that ùêßùê® layer normalization is employed.     This is because we still need to find a generalization of layer normalization to manifolds. \n\n\n\n\n\n","category":"method"},{"location":"library/#GeometricMachineLearning.init_optimizer_cache-Tuple{GradientOptimizer, Any}","page":"Library","title":"GeometricMachineLearning.init_optimizer_cache","text":"Wrapper for the functions setupadamcache, setupmomentumcache, setupgradientcache. These appear outside of optimizer_caches.jl because the OptimizerMethods first have to be defined.\n\n\n\n\n\n","category":"method"},{"location":"library/#GeometricMachineLearning.train!","page":"Library","title":"GeometricMachineLearning.train!","text":"train!(...)\n\nPerform a training of a neural networks on data using given method a training Method\n\nDifferent ways of use:\n\ntrain!(neuralnetwork, data, optimizer = GradientOptimizer(1e-2), training_method; nruns = 1000, batch_size = default(data, type), showprogress = false )\n\nArguments\n\nneuralnetwork::LuxNeuralNetwork : the neural net work using LuxBackend\ndata : the data (see TrainingData)\noptimizer = GradientOptimizer: the optimization method (see Optimizer)\ntraining_method : specify the loss function used \nnruns : number of iteration through the process with default value \nbatch_size : size of batch of data used for each step\n\n\n\n\n\n","category":"function"},{"location":"library/#GeometricMachineLearning.train!-Tuple{NeuralNetwork{<:AbstractNeuralNetworks.Architecture}, AbstractTrainingData, TrainingParameters}","page":"Library","title":"GeometricMachineLearning.train!","text":"train!(neuralnetwork, data, optimizer, training_method; nruns = 1000, batch_size, showprogress = false )\n\nArguments\n\nneuralnetwork::LuxNeuralNetwork : the neural net work using LuxBackend\ndata::AbstractTrainingData : the data\n``\n\n\n\n\n\n","category":"method"},{"location":"architectures/sympnet/#SympNet","page":"SympNet","title":"SympNet","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"This page documents the SympNet architecture and its implementation in GeometricMachineLearning.jl.","category":"page"},{"location":"architectures/sympnet/#Quick-overview-of-the-theory-of-SympNet","page":"SympNet","title":"Quick overview of the theory of SympNet","text":"","category":"section"},{"location":"architectures/sympnet/#Principle","page":"SympNet","title":"Principle","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"SympNets is a new type of neural network proposing a new approach to compute the trajectory of an Hamiltonian system in phase space. Let us denote by (qp)=(q_1q_dp_1p_d)in mathbbR^2d the phase space with qin mathbbR^d the generalized position and  pin mathbbR^d the generalized momentum. Given a physical problem, SympNets takes a phase space element (qp) and aims to compute the next position (qp) of the trajectory in phase space a time step later while preserving the well known symplectic structure of Hamiltonian systems. The way SympNet preserve the symplectic structure is really specific and characterizes it as this preserving is intrinsic of the neural network. Indeed, SympNet is not made with traditional layers but with symplectic layers (described later) modifying the traditional universal approximation theorem into a symplectic one : SympNet is able to approach any symplectic function providing conditions on an activation function.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"SympNet (noted Phi in the following) is so an integrator from mathbbR^d times mathbbR^d to mathbbR^d times mathbbR^d preserving symplecticity which can compute, from an initial condition (q_0p_0), a sequence of phase space elements of a trajectory (q_np_n)=Phi(q_n-1p_n-1)==Phi^n(q_0p_0). The time step between predictions is not a parameter we can choose but is related to the temporal frequency of the training data. SympNet can handle both  temporally regular data, i.e with a fix time step between data, and temporally irregular data, i.e with variable time step. ","category":"page"},{"location":"architectures/sympnet/#Architecture-of-SympNets","page":"SympNet","title":"Architecture of SympNets","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"With GeometricMachineLearning.jl, it is possible to implement two types of architecture which are LA-SympNet and G-SympNet. ","category":"page"},{"location":"architectures/sympnet/#LA-SympNet","page":"SympNet","title":"LA-SympNet","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"(Image: )","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"LA-SympNets are made of the alternation of two types of layers, symplectic linear layers and symplectic activation layers.  For a given integer n, a symplectic linear layer is defined by","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"mathcalL^nup\nbeginpmatrix\n q \n p \nendpmatrix\n =  \nbeginpmatrix \n I  S^n0 \n 0S^n  I \nendpmatrix\n cdots \nbeginpmatrix \n I  0 \n S^2  I \nendpmatrix\nbeginpmatrix \n I  S^1 \n 0  I \nendpmatrix\nbeginpmatrix\n q \n p \nendpmatrix\n+ b ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"or ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"mathcalL^nlow\nbeginpmatrix  q    \n p  endpmatrix =  \n  beginpmatrix \n I  0S^n   \n S^n0  I\n endpmatrix cdots \n  beginpmatrix \n I  S^2   \n 0  I\n endpmatrix\n beginpmatrix \n I  0   \n S^1  I\n endpmatrix\n beginpmatrix  q    \n p  endpmatrix\n  + b  ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The parameters to learn are the symmetric matrices S^iinmathbbR^dtimes d and the bias binmathbbR^2d. The integer n is the width of the symplectic linear layer. If ngeq9, we know that the symplectic linear layers represent any linear symplectic map so that n need not be larger than 9. We note the set of symplectic linear layers mathcalM^L. This type of layers plays the role of standard linear layers. ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"For a given activation function sigma, a symplectic activation layer is defined by","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":" mathcalA^up  beginpmatrix  q    \n p  endpmatrix =  \n  beginbmatrix \n Ihatsigma^a   \n 0I\n endbmatrix beginpmatrix  q    \n p  endpmatrix =\n beginpmatrix \n  mathrmdiag(a)sigma(p)+q  \n  p\n endpmatrix","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"or","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":" mathcalA^low  beginpmatrix  q    \n p  endpmatrix =  \n  beginbmatrix \n I0   \n hatsigma^aI\n endbmatrix beginpmatrix  q    \n p  endpmatrix\n =\n beginpmatrix \n q  \n mathrmdiag(a)sigma(q)+p\n endpmatrix","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The parameters to learn are the weights ainmathbbR^d. This type of layers plays the role of standard activation layers layers. We note the set of symplectic activation layers mathcalM^A. ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"A LA-SympNet is a function of the form Psi=l_k+1 circ a_k circ v_k circ cdots circ a_1 circ l_1 where (l_i)_1leq ileq k+1 subset (mathcalM^L)^k+1 and  ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"(a_i)_1leq ileq k subset (mathcalM^A)^k","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":".","category":"page"},{"location":"architectures/sympnet/#G-SympNet","page":"SympNet","title":"G-SympNet","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"G-SympNets are an alternative to LA-SympNet. They are constituated with only one kind of layers called gradient layers. For a given activation function sigma and an integer ngeq d, a gradient layers is a symplectic map from mathbbR^2d to mathbbR^2d defined by","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":" mathcalG^up  beginpmatrix  q    \n p  endpmatrix =  \n  beginbmatrix \n Ihatsigma^Kab   \n 0I\n endbmatrix beginpmatrix  q    \n p  endpmatrix =\n beginpmatrix \n  K^T mathrmdiag(a)sigma(Kp+b)+q  \n  p\n endpmatrix","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"or","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":" mathcalG^low  beginpmatrix  q    \n p  endpmatrix =  \n  beginbmatrix \n I0   \n hatsigma^KabI\n endbmatrix beginpmatrix  q    \n p  endpmatrix\n =\n beginpmatrix \n q  \n K^T mathrmdiag(a)sigma(Kq+b)+p\n endpmatrix","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The parameters of this layer are the scale matrix KinmathbbR^ntimes d, the bias binmathbbR^n and the vector of weights ainmathbbR^n. The idea is that hatsigma^Kab can approximate any function of the form nabla V, hence the name of this layer. The integer n is called the width of the gradient layer.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"If we note by mathcalM^G the set of gradient layers, a G-SympNet is a function of the form Psi=g_k circ g_k-1 circ cdots circ g_1 where (g_i)_1leq ileq k subset (mathcalM^G)^k.","category":"page"},{"location":"architectures/sympnet/#Universal-approximation-theorems","page":"SympNet","title":"Universal approximation theorems","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"We give now properly the universal approximation for both architectures. But let us give few definitions before. ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Let U be an open set of mathbbR^2d, and let us note by SP^r(U) the set of C^r smooth symplectic maps on U. Let us give a topology on the  set of C^r smooth maps from a compact K of mathbbR^n to mathbbR^n for any positive integers n through the norm","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"f_C^r(KmathbbR^n) = undersetalphaleq rsum underset1leq i leq nmaxundersetxin Ksup D^alpha f_i(x)","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"where the differential operator D^alpha is defined for any map of C^r(mathbbR^nmathbbR) by ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"D^alpha f = fracpartial^alpha fpartial x_1^alpha_1x_n^alpha_n","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"with alpha = alpha_1 ++ alpha_n. ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Definition Let sigma a real map and rin mathbbN. sigma is r-finite if sigmain C^r(mathbbRmathbbR) and int D^rsigma(x)dx +infty.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Definition Let mnrin mathbbN with mn0 be given, U an open set of mathbbR^m, and IJsubset C^r(UmathbbR^n. We say J is r-uniformly dense on compacta in I if J subset I and for any fin I, epsilon0, and any compact Ksubset U, there exists gin J such that f-g_C^r(KmathbbR^n)  epsilon.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"We can now gives the theorems.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Theorem (Approximation theorem for LA-SympNet) For any positive integer r0 and open set Uin mathbbR^2d, the set of LA-SympNet is r-uniformly dense on compacta in SP^r(U) if the activation function sigma is r-finite.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Theorem (Approximation theorem for G-SympNet) For any positive integer r0 and open set Uin mathbbR^2d, the set of G-SympNet is r-uniformly dense on compacta in SP^r(U) if the activation function sigma is r-finite.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"These two theorems are at odds with the well-foundedness of the SympNets. ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Example of r-finite functions","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"sigmoid sigma(x)=frac11+e^-x for any positive integer r, \ntanh tanh(x)=frace^x-e^-xe^x+e^-x for any positive integer r. ","category":"page"},{"location":"architectures/sympnet/#SympNet-with-GeometricMachineLearning.jl-a-id\"SympNet*with*GeometricMachineLearning\"/a","page":"SympNet","title":"SympNet with GeometricMachineLearning.jl <a id=\"SympNetwithGeometricMachineLearning\"></a>","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"With GeometricMachineLearning.jl, it is really easy to implement and train a SympNet. The steps are the following :","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Create the architecture in one line with the function GSympNet or LASympNet,\nCreate the neural networks depending a backend (e.g. with Lux),\nCreate an optimizer for the training step,\nTrain the neural networks with the train!function.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Both LA-SympNet and G-SympNet architectures can be generated in one line with GeometricMachineLearning.jl.","category":"page"},{"location":"architectures/sympnet/#LA-SympNet-2","page":"SympNet","title":"LA-SympNet","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"To create a LA-SympNet, one needs to write","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"lasympnet = LASympNet(dim; width=9, nhidden=1, activation=tanh, init_uplow_linear=[true,false], \n            init_uplow_act=[true,false],init_sym_matrices=Lux.glorot_uniform, init_bias=Lux.zeros32, \n            init_weight=Lux.glorot_uniform) ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"LASympNet takes one obligatory argument:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"dim : the dimension of the phase space,","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"and several keywords argument :","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"width : the width for all the symplectic linear layers with default value set to 9 (if width>9, width is set to 9),\nnhidden : the number of pairs of symplectic linear and activation layers with default value set to 0 (i.e LA-SympNet is a single symplectic linear layer),\nactivation : the activation function for all the symplectic activations layers with default value set to tanh,\ninituplowlinear : a vector of boolean whose the ith coordinate is true only if all the symplectic linear layers in (i mod length(init_uplow_linear))-th position is up (for example the default value is [true,false] which represents an alternation of up and low symplectic linear layers),\ninituplowact : a vector of boolean whose the ith coordinate is true only if all the symplectic activation layers in (i mod length(init_uplow_act))-th position is up (for example the default value is [true,false] which represents an alternation of up and low symplectic activation layers),\ninitsymmatrices: the function which gives the way to initialize the symmetric matrices S^i of symplectic linear layers,\ninit_bias: the function which gives the way to initialize the vector of bias b,\ninit_weight: the function which gives the way to initialize the weight a.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The default value of the last three keyword arguments uses Lux functions.","category":"page"},{"location":"architectures/sympnet/#G-SympNet-2","page":"SympNet","title":"G-SympNet","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"To create a G-SympNet, one needs to write","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"gsympnet = GSympNet(dim; width=dim, nhidden=1, activation=tanh, init_uplow=[true,false], init_weight=Lux.glorot_uniform, \ninit_bias=Lux.zeros32, init_scale=Lux.glorot_uniform) ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"GSympNet takes one obligatory argument:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"dim : the dimension of the phase space,","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"and severals keywords argument :","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"width : the width for all the gradients layers with default value set to dim to have widthgeqdim,\nnhidden : the number of gradient layers with default value set to 1,\nactivation : the activation function for all the gradients layers with default value set to tanh,\ninit_uplow: a vector of boolean whose the ith coordinate is true only if all the gradient layers in (i mod length(init_uplow))-th position is up (for example the default value is [true,false] which represents an alternation of up and low gradient layers),\ninit_weight: the function which gives the way to initialize the vector of weights a,\ninit_bias: the function which gives the way to initialize the vector of bias b,\ninit_scale: the function which gives the way to initialize the scale matrix K.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The default value of the last three keyword arguments uses Lux functions.","category":"page"},{"location":"architectures/sympnet/#Loss-function","page":"SympNet","title":"Loss function","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"To train the SympNet, one need data along a trajectory such that the model is trained to perform an integration. These data are (QP) where Qij (respectively Pij) is the real number q_j(t_i) (respectively pij) which is the j-th coordinates of the generalized position (respectively momentum) at the i-th time step. One also need a loss function defined as :","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Loss(QP) = undersetisum d(Phi(Qi-Pi-) Qi- Pi-^T)","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"where d is a distance on mathbbR^d.","category":"page"},{"location":"architectures/sympnet/#Examples","page":"SympNet","title":"Examples","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Let us see how to use it on several examples.","category":"page"},{"location":"architectures/sympnet/#Example-of-a-pendulum-with-G-SympNet","page":"SympNet","title":"Example of a pendulum with G-SympNet","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Let us begin with a simple example, the pendulum system, the Hamiltonian of which is ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"H(qp)inmathbbR^2 mapsto frac12p^2-cos(q) in mathbbR","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The first thing to do is to create an architecture, in this example a G-SympNet.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"# number of inputs/dimension of system\nconst ninput = 2\n# layer dimension for gradient module \nconst ld = 10 \n# hidden layers\nconst ln = 4\n# activation function\nconst act = tanh\n\n# Creation of a G-SympNet architecture \ngsympnet = GSympNet(ninput, width=ld, nhidden=ln, activation=act)\n\n# Creation of a LA-SympNet architecture \nlasympnet = LASympNet(ninput, nhidden=ln, activation=act)","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Then we can create the neural networks depending on the backend. Here we will use Lux:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"# create Lux network\nnn = NeuralNetwork(gsympnet, LuxBackend())","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"We have to define an optimizer which will be use in the training of the SympNet. For more details on optimizer, please see the corresponding documentation Optimizer.md. For example, let us use a momentum optimizer :","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"# Optimiser\nopt = MomentumOptimizer(1e-2, 0.5)","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"We can now perform the training of the neural networks. The syntax is the following :","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"# number of training runs\nconst nruns = 10000\n# Batchsize used to compute the gradient of the loss function with respect to the parameters of the neural networks.\nconst nbatch = 10\n\n# perform training (returns array that contains the total loss for each training step)\ntotal_loss = train!(nn, opt, data_q, data_p; ntraining = nruns, batch_size = nbatch)","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The train function will change the parameters of the neural networks and gives an a vector containing the evolution of the value of the loss function during the training. Default values for the arguments ntraining and batch_size are respectively 1000 and 10.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The trainings data data_q and data_p must be matrices of mathbbR^ntimes d where n is the length of data and d is the half of the dimension of the system, i.e data_q[i,j] is q_j(t_i) where (t_1t_n) are the corresponding time of the training data.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Then we can make prediction. Let's compare the initial data with a prediction starting from the same phase space point using the provided function Iterate_Sympnet:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"#predictions\nq_learned, p_learned = Iterate_Sympnet(nn, q0, p0; n_points = size(data_q,1))","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"(Image: )","category":"page"},{"location":"Optimizer/#Optimizer","page":"Optimizer","title":"Optimizer","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = GeometricMachineLearning","category":"page"},{"location":"#Geometric-Machine-Learning","page":"Home","title":"Geometric Machine Learning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GeometricMachineLearning.jl implements various scientific machine learning models that aim at learning dynamical systems with geometric structure, such as Hamiltonian (symplectic) or Lagrangian (variational) systems.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GeometricMachineLearning.jl and all of its dependencies can be installed via the Julia REPL by typing ","category":"page"},{"location":"","page":"Home","title":"Home","text":"]add GeometricMachineLearning","category":"page"},{"location":"#Architectures","page":"Home","title":"Architectures","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"architectures/sympnet.md\",\n]","category":"page"},{"location":"#Manifolds","page":"Home","title":"Manifolds","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"manifolds/grassmann_manifold.md\",\n    \"manifolds/stiefel_manifold.md\",\n]","category":"page"},{"location":"manifolds/grassmann_manifold/#Grassmann-Manifold","page":"Grassmann","title":"Grassmann Manifold","text":"","category":"section"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann","title":"Grassmann","text":"(The description of the Grassmann manifold is based on that of the Stiefel manifold, so this should be read first.)","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann","title":"Grassmann","text":"An element of the Grassmann manifold G(nN) is a vector subspace subsetmathbbR^N of dimension n, and each such subspace can be represented by a full-rank matrix AinmathbbR^Ntimesn and the full space takes the form G(nN) = mathbbR^Ntimesnsim where the equivalence relation is AsimB iff existsCinmathbbR^ntimesntext st AC = B. One can find a parametrization of the manifold the following way: Because the matrix A has full rank, there have to be n independent columns in it: i_1 ldots i_n. For simplicity assume that i_1 = 1 i_2=2 ldots i_n=n and call the matrix made up by these columns C. Then the mapping to the coordinate chart is: AC^-1 and the last N-n columns are the coordinates. ","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann","title":"Grassmann","text":"The tangent space for this element can then be represented through matrices: ","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann","title":"Grassmann","text":"beginpmatrix\n    0  cdots  0 \n    cdots  cdots  cdots  \n    0  cdots  0 \n    a_11  cdots  a_1n \n    cdots  cdots  cdots  \n    a_(N-n)1  cdots  a_(N-n)n\nendpmatrix","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann","title":"Grassmann","text":"The Grassmann manifold can also be seen as the Stiefel manifold modulo an equivalence class. This leads to the following (which is used for optimization):","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann","title":"Grassmann","text":"mathfrakg^mathrmhor = mathfrakg^mathrmhorE = leftbeginpmatrix 0  -B^T  B  0 endpmatrix textB arbitraryright","category":"page"}]
}
