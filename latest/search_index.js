var documenterSearchIndex = {"docs":
[{"location":"manifolds/stiefel_manifold/#Stiefel-manifold","page":"Stiefel manifold","title":"Stiefel manifold","text":"","category":"section"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel manifold","title":"Stiefel manifold","text":"The Stiefel manifold St(n N) is the space of all orhtonormal frames in mathbbR^Ntimesn, i.e. matrices YinmathbbR^Ntimesn s.t. Y^TY = I. It can also be seen as the orthonormal group O(N) modulo an equivalence relation: AsimBiffexistsCtext st AC = B for ","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel manifold","title":"Stiefel manifold","text":"$","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel manifold","title":"Stiefel manifold","text":"C = \\begin{pmatrix}     I & 0 \\\n    0 & Q  \\end{pmatrix} $","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel manifold","title":"Stiefel manifold","text":"and QinO(N-n), so the first n columns of A and B are equivalent.","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel manifold","title":"Stiefel manifold","text":"The tangent space to the element YinSt(nN) can easily be determined: T_YSt(nN)=DeltaDelta^TY + Y^TDelta = 0. St(n N) is furthermore a homogeneous space because O(N) acts transitively on it, hence its tangent space can be described through mathfrakg, the Lie algebra of O(N) via T_YSt(nN) = mathfrakgcdotY. Based on the element Y, mathfrakg can be split into a vertical and a horizontal component: mathfrakg = mathfrakg^mathrmverYoplusmathfrakg^mathrmhorY, with mathfrakg^mathrmverY = VinmathfrakgVY = 0 and the horizontal component is computed according to the canonical metric on mathfrakg, i.e. is the orthogonal complement to mathfrakg^mathrmverY.","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel manifold","title":"Stiefel manifold","text":"The function rgrad is a mapping that takes an element of St(nN) and a \"Euclidean gradient\" and produces an element inT_YSt(nN). This mapping has the property: mathrmTr((nablaf)^TDelta) = g_Y(mathttrgrad(Y nablaf) Delta) forallDeltainT_YSt(nN) and g is the Riemannian metric.","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel manifold","title":"Stiefel manifold","text":"What we use for optimizing on the Stiefel manifold (especially regarding the generalization of the Adam optimizer) is the tangent space to E=e_1ldotse_n. This consists of elements: ","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel manifold","title":"Stiefel manifold","text":"$","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel manifold","title":"Stiefel manifold","text":"T_ESt(n,N) = \\left{\\begin{pmatrix} A \\ B \\end{pmatrix}: A\\text{ skew-sym. and B arbitrary}\\right}. $","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel manifold","title":"Stiefel manifold","text":"Further: ","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel manifold","title":"Stiefel manifold","text":"$","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel manifold","title":"Stiefel manifold","text":"\\mathfrak{g}^\\mathrm{hor} = \\mathfrak{g}^{\\mathrm{hor},E} = \\left{\\begin{pmatrix} A & -B^T \\ B & 0 \\end{pmatrix}: A\\text{ skew-sym. and B arbitrary}\\right}. $","category":"page"},{"location":"Sympnet/#SympNet-Documentation","page":"SympNet Documentation","title":"SympNet Documentation","text":"","category":"section"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Here is the documentation about the SympNets architecture that the package GeometricMachineLearning.jl offers. ","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Quick overview of the theory of SympNet\nPrinciple\nArchitecture of SympNets\nUniversal approximation theorems\nSympNet with GeometricMachineLearning.jl","category":"page"},{"location":"Sympnet/#Quick-overview-of-the-theory-of-SympNet-a-name\"Quick*overview*of*the*theory*of*SympNet\"/a","page":"SympNet Documentation","title":"Quick overview of the theory of SympNet <a name=\"QuickoverviewofthetheoryofSympNet\"></a>","text":"","category":"section"},{"location":"Sympnet/#Principle-a-name\"Principle\"/a","page":"SympNet Documentation","title":"Principle <a name=\"Principle\"></a>","text":"","category":"section"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"SympNets is a new type of neural network proposing a new approach to compute the trajectory of an Hamiltonian system in phase space. Let us denote by (qp)=(q_1q_dp_1p_d)in mathbbR^2d the phase space with qin mathbbR^d the gereralized position and  pin mathbbR^d the generalized momentum. Given a physical problem, SympNets takes a phase space element (qp) and aims to compute the next position (qp) of the trajectory in phase space a time step later while preserving the well known symplectic structure of Hamiltonian systems. The way SympNet preserve the symplectic structure is really specific and characterizes it as this preseving is intrinsic of the neural network. Indeed, SympNet is not made with traditional layers but with symplectic layers (decribe later) modifyng the traditional universal approximation theorem into a symplectic one : SympNet is able to approach any symplectic function providing conditions on an activation function.","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"SympNet (noted Phi in the following) is so an integrator from mathbbR^d times mathbbR^d to mathbbR^d times mathbbR^d preserving symplecticity wich can compute, from an initial condition (q_0p_0), a sequence of phase space elements of a trajectory (q_np_n)=Phi(q_n-1p_n-1)==Phi^n(q_0p_0). The time step between predictions is not a parameter we can choose but is related to the temporal frequency of the training data. SympNet can handle both  temporally regular data, i.e with a fix time step between data, and temporally irregular data, i.e with variable time step. ","category":"page"},{"location":"Sympnet/#Architecture-of-SympNets-a-name\"Architecture\"/a","page":"SympNet Documentation","title":"Architecture of SympNets <a name=\"Architecture\"></a>","text":"","category":"section"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"With GeometricMachineLearning.jl, it is possible to implement two types of architecture which are LA-SympNet and G-SympNet. ","category":"page"},{"location":"Sympnet/#LA-SympNet","page":"SympNet Documentation","title":"LA-SympNet","text":"","category":"section"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"LA-SympNets are made of the alternation of two types of layers, sy (Image: )mplectic linear layers and symplectic activation layers.  For a given integer n, a symplectic linear layer is defined by","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"mathcalL^nup  beginpmatrix  q      p  endpmatrix =     beginpmatrix   IS^n0     0S^nI  endpmatrix cdots    beginpmatrix   I0     S^2I  endpmatrix    beginpmatrix   IS^1     0I  endpmatrix  beginpmatrix  q      p  endpmatrix + b","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"or ","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"mathcalL^nlow  beginpmatrix  q      p  endpmatrix =     beginpmatrix   I0S^n     S^n0I  endpmatrix cdots    beginpmatrix   IS^2     0I  endpmatrix  beginpmatrix   I0     S^1I  endpmatrix  beginpmatrix  q      p  endpmatrix + b","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"The parameters to learn are the symmetric matrices S^iinmathbbR^dtimes d and the bias binmathbbR^2d. The integer n is the width of the symplectic linear layer. If ngeq9, we know that the symplectic linear layers represent any linear symplectic map so that n need not be larger than 9. We note the set of symplectic linear layers mathcalM^L. This type of layers plays the role of standard linear layers. ","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"For a given activation function sigma, a symplectic activation layer is defined by","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"mathcalA^up  beginpmatrix  q      p  endpmatrix =     beginbmatrix   Ihatsigma^a     0I  endbmatrix beginpmatrix  q      p  endpmatrix =  beginpmatrix    mathrmdiag(a)sigma(p)+q     p  endpmatrix","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"or","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"mathcalA^low  beginpmatrix  q      p  endpmatrix =     beginbmatrix   I0     hatsigma^aI  endbmatrix beginpmatrix  q      p  endpmatrix  =  beginpmatrix   q    mathrmdiag(a)sigma(q)+p  endpmatrix","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"The parameters to learn are the weights ainmathbbR^d. This type of layers plays the role of standard activation layers layers. We note the set of symplectic activation layers mathcalM^A. ","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"A LA-SympNet is a function of the form Psi=l_k+1 circ a_k circ v_k circ cdots circ a_1 circ l_1 where (l_i)_1leq ileq k+1 subset (mathcalM^L)^k+1 and  ","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"(a_i)_1leq ileq k subset (mathcalM^A)^k.","category":"page"},{"location":"Sympnet/#G-SympNet","page":"SympNet Documentation","title":"G-SympNet","text":"","category":"section"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"G-SympNets are an alternative to LA-SympNet. They are constituated with only one kind of layers called gradient layers. For a given activation function sigma and an interger ngeq d, a gradient layers is a symplectic map from mathbbR^2d to mathbbR^2d defined by","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"mathcalG^up  beginpmatrix  q      p  endpmatrix =     beginbmatrix   Ihatsigma^Kab     0I  endbmatrix beginpmatrix  q      p  endpmatrix =  beginpmatrix    K^T mathrmdiag(a)sigma(Kp+b)+q     p  endpmatrix","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"or","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"mathcalG^low  beginpmatrix  q      p  endpmatrix =     beginbmatrix   I0     hatsigma^KabI  endbmatrix beginpmatrix  q      p  endpmatrix  =  beginpmatrix   q    K^T mathrmdiag(a)sigma(Kq+b)+p  endpmatrix","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"The parameters of this layer are the scale matrix KinmathbbR^ntimes d, the bias binmathbbR^n and the vector of weights ainmathbbR^n. The idea is that hatsigma^Kab can approximate any function of the form nabla V, hence the name of this layer. The integer n is called the width of the gradient layer.","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"If we note by mathcalM^G the set of gradient layers, a G-SympNet is a function of the form Psi=g_k circ g_k-1 circ cdots circ g_1 where (g_i)_1leq ileq k subset (mathcalM^G)^k.","category":"page"},{"location":"Sympnet/#Universal-approximation-theorems-a-name\"Theorems\"/a","page":"SympNet Documentation","title":"Universal approximation theorems <a name=\"Theorems\"></a>","text":"","category":"section"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"We give now properly the universal approximation for both architectures. But let us give few definitions before. ","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Let U be an open set of mathbbR^2d, and let us note by SP^r(U) the set of C^r smooth symplectic maps on U. Let us give a topology on the  set of C^r smooth maps from a compacta K of mathbbR^n to mathbbR^n for any positive intergers n through the norm","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"f_C^r(KmathbbR^n) = undersetalphaleq rsum underset1leq i leq nmaxundersetxin Ksup D^alpha f_i(x)","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"where the differential operator D^alpha is defined for any map of C^r(mathbbR^nmathbbR) by  D^alpha f = fracpartial^alpha fpartial x_1^alpha_1x_n^alpha_n with alpha = alpha_1 ++ alpha_n. ","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Definition Let sigma a real map and rin mathbbN. sigma is r-finite if sigmain C^r(mathbbRmathbbR) and int D^rsigma(x)dx +infty.","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Definition Let mnrin mathbbN with mn0 be given, U an open set of mathbbR^m, and IJsubset C^r(UmathbbR^n. We say J is r-uniformly dense on compacta in I if J subset I and for any fin I, epsilon0, and any compacta Ksubset U, there exists gin J such that f-g_C^r(KmathbbR^n)  epsilon.","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"We can now gives the theorems.","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Theorem (Approximation theorem for LA-SympNet) For any positive interger r0 and open set Uin mathbbR^2d, the set of LA-SympNet is r-uniformly dense on compacta in SP^r(U) if the activation function sigma is r-finite.","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Theorem (Approximation theorem for G-SympNet) For any positive interger r0 and open set Uin mathbbR^2d, the set of G-SympNet is r-uniformly dense on compacta in SP^r(U) if the activation function sigma is r-finite.","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"These two theorems are at odds with the well-foundedness of the SympNets. ","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Example of r-finite functions","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"sigmoid sigma(x)=frac11+e^-x for any positve interger r, \ntanh tanh(x)=frace^x-e^-xe^x+e^-x for any positve interger r. ","category":"page"},{"location":"Sympnet/#SympNet-with-GeometricMachineLearning.jl-a-id\"SympNet*with*GeometricMachineLearning\"/a","page":"SympNet Documentation","title":"SympNet with GeometricMachineLearning.jl <a id=\"SympNetwithGeometricMachineLearning\"></a>","text":"","category":"section"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"With GeometricMachineLearning.jl, it is really easy to implement and train a SympNet. The steps are the following :","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Create the architecture in one line with the function GSympNet or LASympNet,\nCreate the neural networks depending a backend (e.g. with Lux),\nCreate an optimizer for the training step,\nTrain the neural networks with the train!function.","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Both LA-SympNet and G-SympNet architectures can be generated in one line with GeometricMachineLearning.jl.","category":"page"},{"location":"Sympnet/#LA-SympNet-2","page":"SympNet Documentation","title":"LA-SympNet","text":"","category":"section"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"To create a LA-SympNet, one needs to write","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"lasympnet = LASympNet(dim; width=9, nhidden=1, activation=tanh, init_uplow_linear=[true,false], \n            init_uplow_act=[true,false],init_sym_matrices=Lux.glorot_uniform, init_bias=Lux.zeros32, \n            init_weight=Lux.glorot_uniform) ","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"LASympNet takes one obligatory argument:","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"dim : the dimensiom of the phase space,","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"and severals keywords argument :","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"width : the width for all the symplectic linear layers with default value set to 9 (if width>9, width is set to 9),\nnhidden : the number of pairs of symplectic linear and activation layers with default value set to 0 (i.e LA-SympNet is a single symplectic linear layer),\nactivation : the activation function for all the symplectic activations layers with default value set to tanh,\ninituplowlinear : a vector of boolean whose the ith coordinate is true only if all the symplectic linear layers in (i mod length(init_uplow_linear))-th position is up (for example the default value is [true,false] which represents an alternation of up and low symplectic linear layers),\ninituplowact : a vector of boolean whose the ith coordinate is true only if all the symplectic activation layers in (i mod length(init_uplow_act))-th position is up (for example the default value is [true,false] which represents an alternation of up and low symplectic activation layers),\ninitsymmatrices: the function which gives the way to initialize the symmetric matrices S^i of symplectic linear layers,\ninit_bias: the function which gives the way to initialize the vector of bias b,\ninit_weight: the function which gives the way to initialize the weight a.","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"The default value of the last three keyword arguments uses Lux functions.","category":"page"},{"location":"Sympnet/#G-SympNet-2","page":"SympNet Documentation","title":"G-SympNet","text":"","category":"section"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"To create a G-SympNet, one needs to write","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"gsympnet = GSympNet(dim; width=dim, nhidden=1, activation=tanh, init_uplow=[true,false], init_weight=Lux.glorot_uniform, \ninit_bias=Lux.zeros32, init_scale=Lux.glorot_uniform) ","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"GSympNet takes one obligatory argument:","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"dim : the dimensiom of the phase space,","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"and severals keywords argument :","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"width : the width for all the gradients layers with default value set to dim to have widthgeqdim,\nnhidden : the number of gradient layers with default value set to 1,\nactivation : the activation function for all the gradients layers with default value set to tanh,\ninit_uplow: a vector of boolean whose the ith coordinate is true only if all the gradient layers in (i mod length(init_uplow))-th position is up (for example the default value is [true,false] which represents an alternation of up and low gradient layers),\ninit_weight: the function which gives the way to initialize the vector of weights a,\ninit_bias: the function which gives the way to initialize the vector of bias b,\ninit_scale: the function which gives the way to initialize the scale matrix K.","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"The default value of the last three keyword arguments uses Lux functions.","category":"page"},{"location":"Sympnet/#Loss-function","page":"SympNet Documentation","title":"Loss function","text":"","category":"section"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"To train the SympNet, one need data along a trajectory such that the model is trained to perform an integration. These datas are (QP) where Qij (respectively Pij) is the real number q_j(t_i) (respectively pij) which is the j-th cordinates of the generalized position (respicitvely momentum) at the i-th time step. One also need a loss function defined as :","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Loss(QP) = undersetisum d(Phi(Qi-Pi-) Qi- Pi-^T)","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"where d is a distance on mathbbR^d.","category":"page"},{"location":"Sympnet/#Examples","page":"SympNet Documentation","title":"Examples","text":"","category":"section"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Let us see how to use it on severals examples.","category":"page"},{"location":"Sympnet/#Example-of-a-pendulum-with-G-SympNet","page":"SympNet Documentation","title":"Example of a pendulum with G-SympNet","text":"","category":"section"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Let us begin with an esay example, the pendulum system, the Hamiltonian of which is H(qp)inmathbbR^2 mapsto frac12p^2-cos(q) in mathbbR","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"The first thing to do is to create an architecture, in this example a G-SympNet.","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"# number of inputs/dimension of system\nconst ninput = 2\n# layer dimension for gradient module \nconst ld = 10 \n# hidden layers\nconst ln = 4\n# activation function\nconst act = tanh\n\n# Creation of a G-SympNet architecture \ngsympnet = GSympNet(ninput, width=ld, nhidden=ln, activation=act)\n\n# Creation of a LA-SympNet architecture \nlasympnet = LASympNet(ninput, nhidden=ln, activation=act)","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Then we can create the neraul networks depending on the backend. Here we will use Lux :","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"# create Lux network\nnn = NeuralNetwork(gsympnet, LuxBackend())","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"We have to define an optimizer wich will be use in the training of the SympNet. For more details on optimizer, please see the corresponding documentation Optimizer.md. For exemple, let us use a momentum optimizer :","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"# Optimiser\nopt = MomentumOptimizer(1e-2, 0.5)","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"We can now perform the training of the neural networks. The syntax is the following :","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"# number of training runs\nconst nruns = 10000\n# Batchsize used to compute the gradient of the loss function with respect to the parameters of the neural networks.\nconst nbatch = 10\n\n# perform training (returns array that contains the total loss for each training step)\ntotal_loss = train!(nn, opt, data_q, data_p; ntraining = nruns, batch_size = nbatch)","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"The train function will change the parameters of the neural networks and gives an a vector containing the evolution of the value of the loss function during the training. Default values for the arguments ntraining and batch_size are respectively 1000 and 10.","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"The trainings data data_q and data_p must be matrices of mathbbR^ntimes d where n is the lenght of data and d is the half of the dimension of the system, i.e data_q[i,j] is q_j(t_i) where (t_1t_n) are the corresponding time of the training data.","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"Then we can make prediction. Let's compare the initial datas with a prediction starting from the same phase space point using the provided function Iterate_Sympnet:","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"#predictions\nq_learned, p_learned = Iterate_Sympnet(nn, q0, p0; n_points = size(data_q,1))","category":"page"},{"location":"Sympnet/","page":"SympNet Documentation","title":"SympNet Documentation","text":"(Image: )","category":"page"},{"location":"library/","page":"Library","title":"Library","text":"CurrentModule = GeometricMachineLearning","category":"page"},{"location":"library/#GeometricMachineLearning-Library-Functions","page":"Library","title":"GeometricMachineLearning Library Functions","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"Modules = [GeometricMachineLearning]","category":"page"},{"location":"library/#GeometricMachineLearning.AbstractCache","page":"Library","title":"GeometricMachineLearning.AbstractCache","text":"AbstractCache has subtypes:  AdamCache MomentumCache GradientCache\n\nAll of them can be initialized with providing an array (also supporting manifold types).\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.AbstractRetraction","page":"Library","title":"GeometricMachineLearning.AbstractRetraction","text":"AbstractRetraction is a type that comprises all retraction methods for manifolds. For every manifold layer one has to specify a retraction method that takes the layer and elements of the (global) tangent space.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.AdamOptimizer","page":"Library","title":"GeometricMachineLearning.AdamOptimizer","text":"Defines the Adam Optimizer. Algorithm and suggested defaults are taken from (Goodfellow et al., 2016, page 301), except for δ, because single precision is used!\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.BlockIdentityLowerMatrix","page":"Library","title":"GeometricMachineLearning.BlockIdentityLowerMatrix","text":"A BlockIdentityLowerMatrix is a matrix with blocks | 1  0 | | S  1 | Currently, it only implements a custom mul! method, exploiting this structure.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.BlockIdentityUpperMatrix","page":"Library","title":"GeometricMachineLearning.BlockIdentityUpperMatrix","text":"A BlockIdentityUpperMatrix is a matrix with blocks | 1  S | | 0  1 | Currently, it only implements a custom mul! method, exploiting this structure.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.Gradient","page":"Library","title":"GeometricMachineLearning.Gradient","text":"The gradient layer from the SympNet paper (https://www.sciencedirect.com/science/article/abs/pii/S0893608020303063).  Its components are of the form:  $ \\begin{pmatrix}         I & \\nabla{}V \\ 0 & I  \\end{pmatrix}, $ with V(p) = sum_ia_iSigma(sum_jk_ijp_j+b_i), where Sigma is the antiderivative of the activation function sigma. Such layers are by construction symplectic.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.GradientOptimizer","page":"Library","title":"GeometricMachineLearning.GradientOptimizer","text":"Define the Gradient optimizer, i.e. W ← W - η*∇f(W) Or the riemannian manifold equivalent, if applicable.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.GrassmannLayer","page":"Library","title":"GeometricMachineLearning.GrassmannLayer","text":"Defines a layer that performs simple multiplication with an element of the Grassmann manifold.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.GrassmannManifold","page":"Library","title":"GeometricMachineLearning.GrassmannManifold","text":"maybe consider dividing the output in the check functions by n! TODO: Implement sampling procedures!!\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.ManifoldLayer","page":"Library","title":"GeometricMachineLearning.ManifoldLayer","text":"This defines a manifold layer that only has one matrix-valued manifold A associated with it does xmapstoAx. \n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.MomentumOptimizer","page":"Library","title":"GeometricMachineLearning.MomentumOptimizer","text":"Define the Momentum optimizer, i.e.  V ← αV - ∇f(W) W ← W + ηV Or the riemannian manifold equivalent, if applicable.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.MultiHeadAttention","page":"Library","title":"GeometricMachineLearning.MultiHeadAttention","text":"MultiHeadAttention (MHA) serves as a preprocessing step in the transformer. It reweights the input vectors bases on correlations within those data. \n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.Optimizer","page":"Library","title":"GeometricMachineLearning.Optimizer","text":"Optimizer struct that stores the 'method' (i.e. Adam with corresponding hyperparameters), the cache and the optimization step.\n\nIt takes as input an optimization method and the parameters of a network. \n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.StiefelLayer","page":"Library","title":"GeometricMachineLearning.StiefelLayer","text":"Defines a layer that performs simple multiplication with an element of the Stiefel manifold.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.SymplecticHouseholderDecom","page":"Library","title":"GeometricMachineLearning.SymplecticHouseholderDecom","text":"this algorithm is taken (and adjusted) from https://doi.org/10.1016/j.laa.2008.02.029\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.Transformer-Tuple{Integer, Integer, Integer}","page":"Library","title":"GeometricMachineLearning.Transformer","text":"The architecture for a \"transformer encoder\" is essentially taken from arXiv:2010.11929, but with the difference that 𝐧𝐨 layer normalization is employed.     This is because we still need to find a generalization of layer normalization to manifolds. \n\n\n\n\n\n","category":"method"},{"location":"library/#GeometricMachineLearning.init_optimizer_cache-Tuple{GradientOptimizer, Any}","page":"Library","title":"GeometricMachineLearning.init_optimizer_cache","text":"Wrapper for the functions setupadamcache, setupmomentumcache, setupgradientcache. These appear outside of optimizer_caches.jl because the OptimizerMethods first have to be defined.\n\n\n\n\n\n","category":"method"},{"location":"library/#GeometricMachineLearning.train!","page":"Library","title":"GeometricMachineLearning.train!","text":"train!(...)\n\nPerform a training of a neural networks on data using given method a training Method\n\nDifferent ways of use:\n\ntrain!(neuralnetwork, data, optimizer = GradientOptimizer(1e-2), training_method; nruns = 1000, batch_size = default(data, type), showprogress = false )\n\nArguments\n\nneuralnetwork::LuxNeuralNetwork : the neural net work using LuxBackend\ndata : the data (see TrainingData)\noptimizer = GradientOptimizer: the optimization method (see Optimizer)\ntraining_method : specify the loss function used \nnruns : number of iteration through the process with default value \nbatch_size : size of batch of data used for each step\n\n\n\n\n\n","category":"function"},{"location":"library/#GeometricMachineLearning.train!-Tuple{NeuralNetwork{<:AbstractNeuralNetworks.Architecture}, AbstractTrainingData, TrainingParameters}","page":"Library","title":"GeometricMachineLearning.train!","text":"train!(neuralnetwork, data, optimizer, training_method; nruns = 1000, batch_size, showprogress = false )\n\nArguments\n\nneuralnetwork::LuxNeuralNetwork : the neural net work using LuxBackend\ndata::AbstractTrainingData : the data\n``\n\n\n\n\n\n","category":"method"},{"location":"Optimizer/#Optimizer","page":"Optimizer","title":"Optimizer","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = GeometricMachineLearning","category":"page"},{"location":"#Geometric-Machine-Learning","page":"Home","title":"Geometric Machine Learning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GeometricMachineLearning.jl implements various scientific machine learning models that aim at learning dynamical systems with geometric structure, such as Hamiltonian (symplectic) or Lagrangian (variational) systems.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GeometricMachineLearning.jl and all of its dependencies can be installed via the Julia REPL by typing ","category":"page"},{"location":"","page":"Home","title":"Home","text":"]add GeometricMachineLearning","category":"page"},{"location":"manifolds/grassmann_manifold/#Grassmann-Manifold","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"","category":"section"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"(The description of the Grassmann manifold is based on that of the Stiefel manifold, so this should be read first.)","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"An element of the Grassmann manifold G(nN) is a vector subspace subsetmathbbR^N of dimension n, and each such subspace can be represented by a full-rank matrix AinmathbbR^Ntimesn and the full space takes the form G(nN) = mathbbR^Ntimesnsim where the equivalence relation is AsimB iff existsCinmathbbR^ntimesntext st AC = B. One can find a parametrization of the manifold the following way: Because the matrix A has full rank, there have to be n independent columns in it: i_1 ldots i_n. For simplicity assume that i_1 = 1 i_2=2 ldots i_n=n and call the matrix made up by these columns C. Then the mapping to the coordinate chart is: AC^-1 and the last N-n columns are the coordinates. ","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"The tangent space for this element can then be represented through matrices: ","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"$","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"\\begin{pmatrix}     0 & \\cdots & 0 \\\n    \\cdots & \\cdots & \\cdots \\      0 & \\cdots & 0 \\\n    a{11} & \\cdots & a{1n} \\\n    \\cdots & \\cdots & \\cdots \\      a{(N-n)1} & \\cdots & a{(N-n)n} \\end{pmatrix}. $","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"The Grassmann manifold can also be seen as the Stiefel manifold modulo an equivalence class. This leads to the following (which is used for optimization):","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"$","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann Manifold","title":"Grassmann Manifold","text":"\\mathfrak{g}^\\mathrm{hor} = \\mathfrak{g}^{\\mathrm{hor},E} = \\left{\\begin{pmatrix} 0 & -B^T \\ B & 0 \\end{pmatrix}: \\text{B arbitrary}\\right}. $","category":"page"}]
}
