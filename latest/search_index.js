var documenterSearchIndex = {"docs":
[{"location":"manifolds/stiefel_manifold/#Stiefel-manifold","page":"Stiefel","title":"Stiefel manifold","text":"","category":"section"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"The Stiefel manifold St(n N) is the space of all orthonormal frames in mathbbR^Ntimesn, i.e. matrices YinmathbbR^Ntimesn s.t. Y^TY = I. It can also be seen as the orthonormal group O(N) modulo an equivalence relation: AsimBiffexistsCtext st AC = B for ","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"C = beginpmatrix\n    I  0 \n    0  Q \nendpmatrix","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"and QinO(N-n), so the first n columns of A and B are equivalent.","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"The tangent space to the element YinSt(nN) can easily be determined: ","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"T_YSt(nN)=DeltaDelta^TY + Y^TDelta = 0","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"St(n N)","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"is furthermore a homogeneous space because O(N) acts transitively on it, hence its tangent space can be described through mathfrakg, the Lie algebra of O(N) via T_YSt(nN) = mathfrakgcdotY. Based on the element Y, mathfrakg can be split into a vertical and a horizontal component: mathfrakg = mathfrakg^mathrmverYoplusmathfrakg^mathrmhorY, with mathfrakg^mathrmverY = VinmathfrakgVY = 0 and the horizontal component is computed according to the canonical metric on mathfrakg, i.e. is the orthogonal complement to mathfrakg^mathrmverY.","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"The function rgrad is a mapping that takes an element of St(nN) and a \"Euclidean gradient\" and produces an element inT_YSt(nN). This mapping has the property: mathrmTr((nablaf)^TDelta) = g_Y(mathttrgrad(Y nablaf) Delta) forallDeltainT_YSt(nN) and g is the Riemannian metric.","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"What we use for optimizing on the Stiefel manifold (especially regarding the generalization of the Adam optimizer) is the tangent space to E=e_1ldotse_n. This consists of elements: ","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"T_ESt(nN) = leftbeginpmatrix A  B endpmatrix Atext skew-sym and B arbitraryright","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"Further: ","category":"page"},{"location":"manifolds/stiefel_manifold/","page":"Stiefel","title":"Stiefel","text":"mathfrakg^mathrmhor = mathfrakg^mathrmhorE = leftbeginpmatrix A  -B^T  B  0 endpmatrix Atext skew-sym and B arbitraryright","category":"page"},{"location":"optimizers/manifold_related/retractions/#Retractions","page":"Retractions","title":"Retractions","text":"","category":"section"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Retractions are a map from the horizontal part of the Lie algebra mathfrakg^mathrmhor to the respective manifold (homogeneous space).","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Homogeneous spaces (i.e. the all the manifolds treated in GeometricMachineLearning) have the structure mathcalM = Gsim, i.e. are a Lie group modulu an equivalence relation.  For us this equivalence relation is: two elements A_1 and A_2 are equivalent (A_1 sim A_2) iff their application to the canonical element EinmathcalM is the same, i.e. A_1E = A_2E. ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"For the Stiefel manifold St(nN) this canonical element is ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"E = beginbmatrix\n    mathbbI  \n    mathbbO\nendbmatrix","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"where the matrices in the first row are inmathbbR^ntimesn and the matrices in the second row are inmathbbR^(N-n)timesn.","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"For optimization in neural networks (almost always first order) we solve a gradient flow equation dotW = -etacdotmathrmgrad_WL, where mathrmgrad_WL is the Riemannian gradient of the loss function L evaluated at position W.","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"If we deal with Euclidean spaces (vector spaces), then this gradient is just the result of an AD routine and we do not have to do anything else. ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"For manifolds, after we obtained the Riemannian gradient (see e.g. the section on Stiefel manifold for how this is done there), we have to solve a geodesic equation. This is a canonical ODE associated with any Riemannian manifold. ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"The general theory of Riemannian manifolds is rather complicated, but for the neural networks treated in GeometricMachineLearning, we only rely on optimization of matrix Lie groups and homogeneous spaces, which is much simpler. ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"For Lie groups each tangent space is isomorphic to its Lie algebra mathfrakgequivT_mathbbIG. The geodesic map from mathfrakg to G, for matrix Lie groups with bi-invariant Riemannian metric, is simply the application of the matrix exponential exp. Alternatively this can be replaced by the Cayley transform (see (Absil et al, 2008).)","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Starting from this basic map expmathfrakgtoG we can build mappings for more complicated cases: ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"General tangent space to a Lie group T_AG: The geodesic map for an element VinT_AG is simply Aexp(A^-1V).\nSpecial tangent space to a homogeneous space T_EmathcalM: For V=BEinT_EmathcalM the exponential map is simply exp(B)E. \nGeneral tangent space to a homogeneous space T_YmathcalM for Y = AE: For V=ABEinT_YmathcalM with Y = AE the exponential map is simply Aexp(B)E. This is the general case which we deal with.  ","category":"page"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"The function retraction in GeometricMachineLearning performs mathfrakg^mathrmhortomathcalM, which is the second of the above points. To get the third from the second point, we simply have to multiply with a matrix from the left. ","category":"page"},{"location":"optimizers/manifold_related/retractions/#Word-of-caution","page":"Retractions","title":"Word of caution","text":"","category":"section"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Note that the O(N), the Lie group corresponding to the Stiefel manifold, has a bi-invariant Riemannian metric associated with it: (B_1B_2)mapsto mathrmTr(B_1^TB_2). For other Lie groups (e.g. the symplectic group) the situation is slightly more difficult (see (Bendokat et al, 2021).)","category":"page"},{"location":"optimizers/manifold_related/retractions/#References","page":"Retractions","title":"References","text":"","category":"section"},{"location":"optimizers/manifold_related/retractions/","page":"Retractions","title":"Retractions","text":"Absil P A, Mahony R, Sepulchre R. Optimization algorithms on matrix manifolds[M]. Princeton University Press, 2008.\nBendokat T, Zimmermann R. The real symplectic Stiefel and Grassmann manifolds: metrics, geodesics and applications[J]. arXiv preprint arXiv:2108.12447, 2021.\nBrantner B. Generalizing Adam To Manifolds For Efficiently Training Transformers[J]. arXiv preprint arXiv:2305.16901, 2023.","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/#Horizontal-component-of-the-Lie-algebra-\\mathfrak{g}","page":"Global Tangent Space","title":"Horizontal component of the Lie algebra mathfrakg","text":"","category":"section"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"What we use to optimize Adam (and other algorithms) to manifolds is a global tangent space representation of the homogeneous spaces. ","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"For the Stiefel manifold, the homogeneous space takes a simple form: ","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"B = beginbmatrix\n    A  -B^T  \n    B  mathbbO\nendbmatrix","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/#Theoretical-foundations-of-global-tangent-space-representation","page":"Global Tangent Space","title":"Theoretical foundations of global tangent space representation","text":"","category":"section"},{"location":"arrays/stiefel_lie_alg_horizontal/#Vertical-and-horizontal-components","page":"Global Tangent Space","title":"Vertical and horizontal components","text":"","category":"section"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"The Stiefel manifold is a homogeneous space obtained from SO(N) by setting two matrices, whose first n columns conincide, equivalent.  Another way of expressing this is: ","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"A_1 sim A_2 iff A_1E = A_2E","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"for ","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"E = beginbmatrix mathbbI  mathbbOendbmatrix","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"The tangent space T_ESt(nN) can also be expressed that way:","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"T_ESt(nN) = mathfrakgcdotE = BEBinmathfrakg","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"The kernel of the mapping mathfrakgtoT_ESt(nN) BmapstoBE is referred to as mathfrakg^mathrmverE, the vertical component of the Lie algebra at E. It is clear that elements belonging to mathfrakg^mathrmverE are of the following form: ","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"beginpmatrix\nhatmathbbO  mathbbO^T  \nmathbbO  C\nendpmatrix","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"where hatmathbbOinmathbbR^ntimesn is a \"small\" matrix and mathbbOinmathbbR^Ntimesn is a \"big\" one. CinmathbbR^NtimesN is a skew-symmetric matrix. ","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"We can then take the orthogonal complement of this matrix (with respect to the canonical metric). We will denote this by mathfrakg^mathrmhorEequivmathfrakg^mathrmhor. Its alements are of the form: ","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"beginpmatrix\nA  -B^T  \nB  tildemathbbO\nendpmatrix","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"where AinmathbbR^ntimesn is skew-symmetric and BinmathbbR^Ntimesn is arbitrary. This is what the struct StiefelLieAlgHorMatrix implements. ","category":"page"},{"location":"arrays/stiefel_lie_alg_horizontal/#Special-functions","page":"Global Tangent Space","title":"Special functions","text":"","category":"section"},{"location":"arrays/stiefel_lie_alg_horizontal/","page":"Global Tangent Space","title":"Global Tangent Space","text":"You can also draw random elements from mathfrakg^mathrmhor through e.g. rand(CUDADevice(), StiefelLieAlgHorMatrix{Float32}, 10, 5). Where N=10 and n=5 in this example.","category":"page"},{"location":"library/","page":"Library","title":"Library","text":"CurrentModule = GeometricMachineLearning","category":"page"},{"location":"library/#GeometricMachineLearning-Library-Functions","page":"Library","title":"GeometricMachineLearning Library Functions","text":"","category":"section"},{"location":"library/","page":"Library","title":"Library","text":"Modules = [GeometricMachineLearning]","category":"page"},{"location":"library/#GeometricMachineLearning.AbstractCache","page":"Library","title":"GeometricMachineLearning.AbstractCache","text":"AbstractCache has subtypes:  AdamCache MomentumCache GradientCache\n\nAll of them can be initialized with providing an array (also supporting manifold types).\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.AbstractRetraction","page":"Library","title":"GeometricMachineLearning.AbstractRetraction","text":"AbstractRetraction is a type that comprises all retraction methods for manifolds. For every manifold layer one has to specify a retraction method that takes the layer and elements of the (global) tangent space.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.AdamOptimizer","page":"Library","title":"GeometricMachineLearning.AdamOptimizer","text":"Defines the Adam Optimizer. Algorithm and suggested defaults are taken from (Goodfellow et al., 2016, page 301), except for Œ¥, because single precision is used!\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.Attention","page":"Library","title":"GeometricMachineLearning.Attention","text":"MultiHeadAttention (MHA) serves as a preprocessing step in the transformer. It reweights the input vectors bases on correlations within those data. \n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.BlockIdentityLowerMatrix","page":"Library","title":"GeometricMachineLearning.BlockIdentityLowerMatrix","text":"A BlockIdentityLowerMatrix is a matrix with blocks | 1  0 | | S  1 | Currently, it only implements a custom mul! method, exploiting this structure.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.BlockIdentityUpperMatrix","page":"Library","title":"GeometricMachineLearning.BlockIdentityUpperMatrix","text":"A BlockIdentityUpperMatrix is a matrix with blocks | 1  S | | 0  1 | Currently, it only implements a custom mul! method, exploiting this structure.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.DataLoader","page":"Library","title":"GeometricMachineLearning.DataLoader","text":"Data Loader is a struct that creates an instance based on a tensor (or different input format) and is designed to make training convenient.\n\nImplemented:  If the data loader is called with a single tensor, a batchsize and an outputsize, then the batch is drawn randomly in the relevant range and the output is assigned accordingly.\n\nTODO: Implement DataLoader that works well with GeometricEnsembles etc. \n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.Gradient","page":"Library","title":"GeometricMachineLearning.Gradient","text":"The gradient layer from the SympNet paper (https://www.sciencedirect.com/science/article/abs/pii/S0893608020303063).  Its components are of the form:  $ \\begin{pmatrix}         I & \\nabla{}V \\ 0 & I  \\end{pmatrix}, $ with V(p) = sum_ia_iSigma(sum_jk_ijp_j+b_i), where Sigma is the antiderivative of the activation function sigma. Such layers are by construction symplectic.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.GradientOptimizer","page":"Library","title":"GeometricMachineLearning.GradientOptimizer","text":"Define the Gradient optimizer, i.e. W ‚Üê W - Œ∑*‚àáf(W) Or the riemannian manifold equivalent, if applicable.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.GrassmannLayer","page":"Library","title":"GeometricMachineLearning.GrassmannLayer","text":"Defines a layer that performs simple multiplication with an element of the Grassmann manifold.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.GrassmannManifold","page":"Library","title":"GeometricMachineLearning.GrassmannManifold","text":"maybe consider dividing the output in the check functions by n! TODO: Implement sampling procedures!!\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.ManifoldLayer","page":"Library","title":"GeometricMachineLearning.ManifoldLayer","text":"This defines a manifold layer that only has one matrix-valued manifold A associated with it does xmapstoAx. \n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.MomentumOptimizer","page":"Library","title":"GeometricMachineLearning.MomentumOptimizer","text":"Define the Momentum optimizer, i.e.  V ‚Üê Œ±V - ‚àáf(W) W ‚Üê W + Œ∑V Or the riemannian manifold equivalent, if applicable.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.MultiHeadAttention","page":"Library","title":"GeometricMachineLearning.MultiHeadAttention","text":"MultiHeadAttention (MHA) serves as a preprocessing step in the transformer. It reweights the input vectors bases on correlations within those data. \n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.Optimizer","page":"Library","title":"GeometricMachineLearning.Optimizer","text":"Optimizer struct that stores the 'method' (i.e. Adam with corresponding hyperparameters), the cache and the optimization step.\n\nIt takes as input an optimization method and the parameters of a network. \n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.StiefelLayer","page":"Library","title":"GeometricMachineLearning.StiefelLayer","text":"Defines a layer that performs simple multiplication with an element of the Stiefel manifold.\n\n\n\n\n\n","category":"type"},{"location":"library/#GeometricMachineLearning.Transformer-Tuple{Integer, Integer, Integer}","page":"Library","title":"GeometricMachineLearning.Transformer","text":"The architecture for a \"transformer encoder\" is essentially taken from arXiv:2010.11929, but with the difference that ùêßùê® layer normalization is employed.     This is because we still need to find a generalization of layer normalization to manifolds. \n\n\n\n\n\n","category":"method"},{"location":"library/#GeometricMachineLearning.assign_batch_kernel!-Tuple{Any}","page":"Library","title":"GeometricMachineLearning.assign_batch_kernel!","text":"Takes as input a batch tensor (to which the data are assigned) the whole data tensor and two vectorsparams'' and ``time_steps'' that include the specific parameters and time steps we want to assign. \n\nNote that this assigns sequential data! For e.g. being processed by a transformer.\n\n\n\n\n\n","category":"method"},{"location":"library/#GeometricMachineLearning.assign_output_estimate-Union{Tuple{T}, Tuple{AbstractArray{T, 3}, Any}} where T","page":"Library","title":"GeometricMachineLearning.assign_output_estimate","text":"Closely related to the transformer. It takes the last prediction_window columns of the output and uses is for the final prediction.\n\n\n\n\n\n","category":"method"},{"location":"library/#GeometricMachineLearning.assign_output_kernel!-Tuple{Any}","page":"Library","title":"GeometricMachineLearning.assign_output_kernel!","text":"This should be used together with assignbatchkernel!. It assigns the corresponding output (i.e. target).\n\n\n\n\n\n","category":"method"},{"location":"library/#GeometricMachineLearning.augment_zeros_kernel!-Tuple{Any}","page":"Library","title":"GeometricMachineLearning.augment_zeros_kernel!","text":"Used for differentiating assignoutputestimate (this appears in the loss). \n\n\n\n\n\n","category":"method"},{"location":"library/#GeometricMachineLearning.draw_batch!-Union{Tuple{T}, Tuple{AbstractArray{T, 3}, AbstractArray{T, 3}, AbstractArray{T, 3}, Vararg{Any, 5}}} where T","page":"Library","title":"GeometricMachineLearning.draw_batch!","text":"This function draws random time steps and parameters and based on these assign the batch and the output.\n\n\n\n\n\n","category":"method"},{"location":"library/#GeometricMachineLearning.init_optimizer_cache-Tuple{GradientOptimizer, Any}","page":"Library","title":"GeometricMachineLearning.init_optimizer_cache","text":"Wrapper for the functions setupadamcache, setupmomentumcache, setupgradientcache. These appear outside of optimizer_caches.jl because the OptimizerMethods first have to be defined.\n\n\n\n\n\n","category":"method"},{"location":"library/#GeometricMachineLearning.train!","page":"Library","title":"GeometricMachineLearning.train!","text":"train!(...)\n\nPerform a training of a neural networks on data using given method a training Method\n\nDifferent ways of use:\n\ntrain!(neuralnetwork, data, optimizer = GradientOptimizer(1e-2), training_method; nruns = 1000, batch_size = default(data, type), showprogress = false )\n\nArguments\n\nneuralnetwork::LuxNeuralNetwork : the neural net work using LuxBackend\ndata : the data (see TrainingData)\noptimizer = GradientOptimizer: the optimization method (see Optimizer)\ntraining_method : specify the loss function used \nnruns : number of iteration through the process with default value \nbatch_size : size of batch of data used for each step\n\n\n\n\n\n","category":"function"},{"location":"library/#GeometricMachineLearning.train!-Tuple{AbstractNeuralNetworks.AbstractNeuralNetwork{<:AbstractNeuralNetworks.Architecture}, AbstractTrainingData, TrainingParameters}","page":"Library","title":"GeometricMachineLearning.train!","text":"train!(neuralnetwork, data, optimizer, training_method; nruns = 1000, batch_size, showprogress = false )\n\nArguments\n\nneuralnetwork::LuxNeuralNetwork : the neural net work using LuxBackend\ndata::AbstractTrainingData : the data\n``\n\n\n\n\n\n","category":"method"},{"location":"optimizers/adam_optimizer/#The-Adam-Optimizer","page":"Adam Optimizer","title":"The Adam Optimizer","text":"","category":"section"},{"location":"optimizers/adam_optimizer/","page":"Adam Optimizer","title":"Adam Optimizer","text":"The Adam Optimizer is one of the most widely (if not the most widely used) neural network optimizer. Like most modern neural network optimizers it contains a cache that is updated based on first-order gradient information and then, in a second step, the cache is used to compute a velocity estimate for updating the neural networ weights. ","category":"page"},{"location":"optimizers/adam_optimizer/","page":"Adam Optimizer","title":"Adam Optimizer","text":"Here we first describe the Adam algorithm for the case where all the weights are on a vector space and then show how to generalize this to the case where the weights are on a manifold. ","category":"page"},{"location":"optimizers/adam_optimizer/#All-weights-on-a-vector-space","page":"Adam Optimizer","title":"All weights on a vector space","text":"","category":"section"},{"location":"optimizers/adam_optimizer/","page":"Adam Optimizer","title":"Adam Optimizer","text":"The cache of the Adam optimizer consists of first and second moments. The first moments B_1 store linear information about the current and previous gradients, and the second moments B_2 store quadratic information about current and previous gradients (all computed from a first-order gradient). ","category":"page"},{"location":"optimizers/adam_optimizer/","page":"Adam Optimizer","title":"Adam Optimizer","text":"If all the weights are on a vector space, then we directly compute updates for B_1 and B_2:","category":"page"},{"location":"optimizers/adam_optimizer/","page":"Adam Optimizer","title":"Adam Optimizer","text":"B_1 gets ((rho_1 - rho_1^t)(1 - rho_1^t))cdotB_1 + (1 - rho_1)(1 - rho_1^t)cdotnablaL\n,\nB_2 gets ((rho_2 - rho_1^t)(1 - rho_2^t))cdotB_2 + (1 - rho_2)(1 - rho_2^t)cdotnablaLodotnablaL\n,","category":"page"},{"location":"optimizers/adam_optimizer/","page":"Adam Optimizer","title":"Adam Optimizer","text":"where odotmathbbR^ntimesmathbbR^ntomathbbR^n is the Hadamard product: aodotb_i = a_ib_i. rho_1 and rho_2 are hyperparameters. Their defaults, rho_1=09 and rho_2=099, are taken from (Goodfellow et al., 2016, page 301). After having updated the cache (i.e. B_1 and B_2) we compute a velocity (step 3) with which the parameters Y_t are then updated (step 4).","category":"page"},{"location":"optimizers/adam_optimizer/","page":"Adam Optimizer","title":"Adam Optimizer","text":"W_tgets -etaB_1sqrtB_2 + delta\n,\nY_t+1 gets Y_t + W_t\n.","category":"page"},{"location":"optimizers/adam_optimizer/","page":"Adam Optimizer","title":"Adam Optimizer","text":"Here eta (with default 0.01) is the learning rate and delta (with default 3cdot10^-7) is a small constant that is added for stability. The division, square root and addition in step 3 are performed element-wise. ","category":"page"},{"location":"optimizers/adam_optimizer/#Weights-on-manifolds","page":"Adam Optimizer","title":"Weights on manifolds","text":"","category":"section"},{"location":"optimizers/adam_optimizer/","page":"Adam Optimizer","title":"Adam Optimizer","text":"The problem with generalizing Adam to manifolds is that the Hadamard product odot as well as the other element-wise operations (, sqrt and + in step 3 above) lack a clear geometric interpretation. In GeometricMachineLearning we get around this issue by utilizing a so-called global tangent space representation (see the documenation on StiefelLieAlgHorMatrix).  ","category":"page"},{"location":"optimizers/adam_optimizer/","page":"Adam Optimizer","title":"Adam Optimizer","text":"For the manifold case we first have to compute the global tangent space representation nablaLtoBinmathfrakg^mathrmhor.","category":"page"},{"location":"optimizers/adam_optimizer/#References","page":"Adam Optimizer","title":"References","text":"","category":"section"},{"location":"optimizers/adam_optimizer/","page":"Adam Optimizer","title":"Adam Optimizer","text":"Goodfellow I, Bengio Y, Courville A. Deep learning[M]. MIT press, 2016.\nBrantner B. Generalizing Adam To Manifolds For Efficiently Training Transformers[J]. arXiv preprint arXiv:2305.16901, 2023.","category":"page"},{"location":"layers/attention_layer/#The-Attention-Layer","page":"Attention","title":"The Attention Layer","text":"","category":"section"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"The attention layer (and the orthonormal activation function defined for it) was specifically designed to generalize transformers to symplectic data.  Usually a self-attention layer takes the following form: ","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"Z = z^(1) ldots z^(T) mapsto Zmathrmsoftmax((P^QZ)^T(P^KZ))","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"where we left out the linear mapping onto the values P^V. ","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"The idea behind is that we can perform a non-linear re-weighting of the columns of Z by multiplying with a Z-dependent matrix from the right and therefore take the sequential nature of the data into account (which is not possible with normal neural networks). After the attention step the transformer applies a simple ResNet from the left.","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"What the softmax does is a vector-wise operation, i.e. it operates on each column of an input matrix A = a_1 ldots a_T. The result is a sequence of probability vectors p^(1) ldots p^(T) for which ","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"sum_i=1^Tp^(j)_i=1quadforalljin1dotsT","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"What we want to construct is a symplectic transformation that is transformer-like. For this we modify the attention layer the following way: ","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"Z = z^(1) ldots z^(T) mapsto Zsigma((P^QZ)^T(P^KZ))","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"where sigma(A)=exp(mathttupper_triangular_asymmetrize(A)) and ","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"mathttupper_triangular_asymmetrize(A)_ij = begincases a_ij  textif ij   -a_ji  textif ij  0  textelseendcases","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"This has as a consequence that the matrix Lambda(Z) = sigma((P^QZ)^T(P^KZ)) is orthonormal and hence preserves an extended symplectic structure. To make this more clear, consider that the transformer maps sequences of vectors to sequences of vectors, i.e. VtimescdotstimesV ni z^1 ldots z^T mapsto hatz^1 ldots hatz^T. We can define a symplectic structure on VtimescdotstimesV by rearranging z^1 ldots z^T into a vector. We do this in the following way: ","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"tildeZ = beginpmatrix q^(1)_1  q^(2)_1  cdots  q^(T)_1  q^(1)_2  cdots  q^(T)_d  p^(1)_1  p^(2)_1  cdots  p^(T)_1  p^(1)_2  cdots  p^(T)_d endpmatrix","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"The symplectic structure on this big space is then: ","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"mathbbJ=beginpmatrix\n    mathbbO_dT  mathbbI_dT \n    -mathbbI_dT  mathbbO_dT\nendpmatrix","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"Multiplying with the matrix Lambda(Z) from the right onto z^1 ldots z^T corresponds to applying the sparse matrix ","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"tildeLambda(Z)=left\nbeginarrayccc\n   Lambda(Z)  cdots  mathbbO_T \n   vdots  ddots  vdots \n   mathbbO_T  cdots  Lambda(Z) \n   endarray\nright","category":"page"},{"location":"layers/attention_layer/","page":"Attention","title":"Attention","text":"from the left onto the big vector. ","category":"page"},{"location":"optimizers/manifold_related/global_sections/#Global-Sections","page":"Global Sections","title":"Global Sections","text":"","category":"section"},{"location":"optimizers/manifold_related/global_sections/","page":"Global Sections","title":"Global Sections","text":"The set of functions in global_sections.jl is closely related to the ones in retractions.jl. For simplicitly we freely discuss the Stiefel manifold as an example of a homogeneous space. The terminology here can be easily translated to other homogeneous spaces. ","category":"page"},{"location":"optimizers/manifold_related/global_sections/","page":"Global Sections","title":"Global Sections","text":"There are two steps to them: ","category":"page"},{"location":"optimizers/manifold_related/global_sections/","page":"Global Sections","title":"Global Sections","text":"A mapping from the homogeneous space mathcalM=Gsim to the associated Lie group: mathcalMtoG. This is done by a separate struct, called GlobalSection.\nA mapping from T_YmathcalMtomathfrakg^mathrmhor, where frakg^mathrmhor is the horizontal component to the Lie algebra (our global tangent space representation). See the section for retractions.jl for this. ","category":"page"},{"location":"optimizers/manifold_related/global_sections/#Computing-the-global-section","page":"Global Sections","title":"Computing the global section","text":"","category":"section"},{"location":"optimizers/manifold_related/global_sections/","page":"Global Sections","title":"Global Sections","text":"For the Stiefel manifold, GlobalSection takes an element of YinSt(nN)equivStiefelManifold{T} and returns an instance of GlobalSection{T, StiefelManifold{T}}. This represents an element AinO(N) such that AE = Y. The application O(N)timesSt(nN)toSt(nN) is done with the functions apply_section! and apply_section.","category":"page"},{"location":"optimizers/manifold_related/global_sections/#Computing-the-global-tangent-space-representation-based-on-a-global-section","page":"Global Sections","title":"Computing the global tangent space representation based on a global section","text":"","category":"section"},{"location":"optimizers/manifold_related/global_sections/","page":"Global Sections","title":"Global Sections","text":"The function global_rep does the second step, so it takes an instance of GlobalSection and an element of T_YSt(nN) (simply represented through a matrix), and then returns an element of frakg^mathrmhorequivStiefelLieAlgHorMatrix.","category":"page"},{"location":"optimizers/manifold_related/global_sections/#Why-do-we-need-a-GlobalSection-to-compute-the-global-tangent-space-representation?","page":"Global Sections","title":"Why do we need a GlobalSection to compute the global tangent space representation?","text":"","category":"section"},{"location":"optimizers/manifold_related/global_sections/","page":"Global Sections","title":"Global Sections","text":"For each element YinmathcalM we can perform a splitting mathfrakg = mathfrakg^mathrmhor Yoplusmathfrakg^mathrmver Y, where the two subspaces are the horizontal and the vertical component of mathfrakg ate Y respectively. For homogeneous spaces: T_YmathcalM = mathfrakgcdotY, i.e. every tangent space to mathcalM can be expressed through the application of the Lie algebra to the relevant element. The vertical component consists of those elements of mathfrakg which are mapped to the zero element of T_YmathcalM, i.e. mathfrakg^mathrmver Y = mathrmker(mathfrakgtoT_YmathcalM). The orthogonal complement of mathfrakg^mathrmver Y is the horizontal component and is referred to by mathfrakg^mathrmhor Y. This is naturally isomorphic to T_YmathcalM. ","category":"page"},{"location":"optimizers/manifold_related/global_sections/","page":"Global Sections","title":"Global Sections","text":"In GeometricMachineLearning we do not deal with all vector space mathfrakg^mathrmhor Y, but only with mathfrakg^mathrmhor Eequivmathfrakg^mathrmhorequivStiefelLieAlgHorMatrix. This means we need another mapping mathfrakg^mathrmhor Ytomathfrakg^mathrmhor. global_rep is a composition of two mappings: T_YmathcalM to mathfrakg^mathrmhor Y to mathfrakg^mathfrakhor. The second mapping is done through V to lambda(Y)^-1Vlambda(Y), where lambda(Y) is a global section of Y, i.e. lambda(Y)inG and lambda(Y)E = Y.","category":"page"},{"location":"optimizers/manifold_related/global_sections/#Optimization","page":"Global Sections","title":"Optimization","text":"","category":"section"},{"location":"optimizers/manifold_related/global_sections/","page":"Global Sections","title":"Global Sections","text":"The output of global_rep is then used for all the optimization steps. See e.g. the documentation for adam_optimizer.","category":"page"},{"location":"architectures/sympnet/#SympNet","page":"SympNet","title":"SympNet","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"This page documents the SympNet architecture and its implementation in GeometricMachineLearning.jl.","category":"page"},{"location":"architectures/sympnet/#Quick-overview-of-the-theory-of-SympNet","page":"SympNet","title":"Quick overview of the theory of SympNet","text":"","category":"section"},{"location":"architectures/sympnet/#Principle","page":"SympNet","title":"Principle","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"SympNets is a new type of neural network proposing a new approach to compute the trajectory of an Hamiltonian system in phase space. Let us denote by (qp)=(q_1q_dp_1p_d)in mathbbR^2d the phase space with qin mathbbR^d the generalized position and  pin mathbbR^d the generalized momentum. Given a physical problem, SympNets takes a phase space element (qp) and aims to compute the next position (qp) of the trajectory in phase space a time step later while preserving the well known symplectic structure of Hamiltonian systems. The way SympNet preserve the symplectic structure is really specific and characterizes it as this preserving is intrinsic of the neural network. Indeed, SympNet is not made with traditional layers but with symplectic layers (described later) modifying the traditional universal approximation theorem into a symplectic one : SympNet is able to approach any symplectic function providing conditions on an activation function.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"SympNet (noted Phi in the following) is so an integrator from mathbbR^d times mathbbR^d to mathbbR^d times mathbbR^d preserving symplecticity which can compute, from an initial condition (q_0p_0), a sequence of phase space elements of a trajectory (q_np_n)=Phi(q_n-1p_n-1)==Phi^n(q_0p_0). The time step between predictions is not a parameter we can choose but is related to the temporal frequency of the training data. SympNet can handle both  temporally regular data, i.e with a fix time step between data, and temporally irregular data, i.e with variable time step. ","category":"page"},{"location":"architectures/sympnet/#Architecture-of-SympNets","page":"SympNet","title":"Architecture of SympNets","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"With GeometricMachineLearning.jl, it is possible to implement two types of architecture which are LA-SympNet and G-SympNet. ","category":"page"},{"location":"architectures/sympnet/#LA-SympNet","page":"SympNet","title":"LA-SympNet","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"(Image: )","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"LA-SympNets are made of the alternation of two types of layers, symplectic linear layers and symplectic activation layers.  For a given integer n, a symplectic linear layer is defined by","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"mathcalL^nup\nbeginpmatrix\n q \n p \nendpmatrix\n =  \nbeginpmatrix \n I  S^n0 \n 0S^n  I \nendpmatrix\n cdots \nbeginpmatrix \n I  0 \n S^2  I \nendpmatrix\nbeginpmatrix \n I  S^1 \n 0  I \nendpmatrix\nbeginpmatrix\n q \n p \nendpmatrix\n+ b ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"or ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"mathcalL^nlow\nbeginpmatrix  q    \n p  endpmatrix =  \n  beginpmatrix \n I  0S^n   \n S^n0  I\n endpmatrix cdots \n  beginpmatrix \n I  S^2   \n 0  I\n endpmatrix\n beginpmatrix \n I  0   \n S^1  I\n endpmatrix\n beginpmatrix  q    \n p  endpmatrix\n  + b  ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The parameters to learn are the symmetric matrices S^iinmathbbR^dtimes d and the bias binmathbbR^2d. The integer n is the width of the symplectic linear layer. If ngeq9, we know that the symplectic linear layers represent any linear symplectic map so that n need not be larger than 9. We note the set of symplectic linear layers mathcalM^L. This type of layers plays the role of standard linear layers. ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"For a given activation function sigma, a symplectic activation layer is defined by","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":" mathcalA^up  beginpmatrix  q    \n p  endpmatrix =  \n  beginbmatrix \n Ihatsigma^a   \n 0I\n endbmatrix beginpmatrix  q    \n p  endpmatrix =\n beginpmatrix \n  mathrmdiag(a)sigma(p)+q  \n  p\n endpmatrix","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"or","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":" mathcalA^low  beginpmatrix  q    \n p  endpmatrix =  \n  beginbmatrix \n I0   \n hatsigma^aI\n endbmatrix beginpmatrix  q    \n p  endpmatrix\n =\n beginpmatrix \n q  \n mathrmdiag(a)sigma(q)+p\n endpmatrix","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The parameters to learn are the weights ainmathbbR^d. This type of layers plays the role of standard activation layers layers. We note the set of symplectic activation layers mathcalM^A. ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"A LA-SympNet is a function of the form Psi=l_k+1 circ a_k circ v_k circ cdots circ a_1 circ l_1 where (l_i)_1leq ileq k+1 subset (mathcalM^L)^k+1 and  ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"(a_i)_1leq ileq k subset (mathcalM^A)^k","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":".","category":"page"},{"location":"architectures/sympnet/#G-SympNet","page":"SympNet","title":"G-SympNet","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"G-SympNets are an alternative to LA-SympNet. They are constituated with only one kind of layers called gradient layers. For a given activation function sigma and an integer ngeq d, a gradient layers is a symplectic map from mathbbR^2d to mathbbR^2d defined by","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":" mathcalG^up  beginpmatrix  q    \n p  endpmatrix =  \n  beginbmatrix \n Ihatsigma^Kab   \n 0I\n endbmatrix beginpmatrix  q    \n p  endpmatrix =\n beginpmatrix \n  K^T mathrmdiag(a)sigma(Kp+b)+q  \n  p\n endpmatrix","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"or","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":" mathcalG^low  beginpmatrix  q    \n p  endpmatrix =  \n  beginbmatrix \n I0   \n hatsigma^KabI\n endbmatrix beginpmatrix  q    \n p  endpmatrix\n =\n beginpmatrix \n q  \n K^T mathrmdiag(a)sigma(Kq+b)+p\n endpmatrix","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The parameters of this layer are the scale matrix KinmathbbR^ntimes d, the bias binmathbbR^n and the vector of weights ainmathbbR^n. The idea is that hatsigma^Kab can approximate any function of the form nabla V, hence the name of this layer. The integer n is called the width of the gradient layer.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"If we note by mathcalM^G the set of gradient layers, a G-SympNet is a function of the form Psi=g_k circ g_k-1 circ cdots circ g_1 where (g_i)_1leq ileq k subset (mathcalM^G)^k.","category":"page"},{"location":"architectures/sympnet/#Universal-approximation-theorems","page":"SympNet","title":"Universal approximation theorems","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"We give now properly the universal approximation for both architectures. But let us give few definitions before. ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Let U be an open set of mathbbR^2d, and let us note by SP^r(U) the set of C^r smooth symplectic maps on U. Let us give a topology on the  set of C^r smooth maps from a compact K of mathbbR^n to mathbbR^n for any positive integers n through the norm","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"f_C^r(KmathbbR^n) = undersetalphaleq rsum underset1leq i leq nmaxundersetxin Ksup D^alpha f_i(x)","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"where the differential operator D^alpha is defined for any map of C^r(mathbbR^nmathbbR) by ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"D^alpha f = fracpartial^alpha fpartial x_1^alpha_1x_n^alpha_n","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"with alpha = alpha_1 ++ alpha_n. ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Definition Let sigma a real map and rin mathbbN. sigma is r-finite if sigmain C^r(mathbbRmathbbR) and int D^rsigma(x)dx +infty.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Definition Let mnrin mathbbN with mn0 be given, U an open set of mathbbR^m, and IJsubset C^r(UmathbbR^n. We say J is r-uniformly dense on compacta in I if J subset I and for any fin I, epsilon0, and any compact Ksubset U, there exists gin J such that f-g_C^r(KmathbbR^n)  epsilon.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"We can now gives the theorems.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Theorem (Approximation theorem for LA-SympNet) For any positive integer r0 and open set Uin mathbbR^2d, the set of LA-SympNet is r-uniformly dense on compacta in SP^r(U) if the activation function sigma is r-finite.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Theorem (Approximation theorem for G-SympNet) For any positive integer r0 and open set Uin mathbbR^2d, the set of G-SympNet is r-uniformly dense on compacta in SP^r(U) if the activation function sigma is r-finite.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"These two theorems are at odds with the well-foundedness of the SympNets. ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Example of r-finite functions","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"sigmoid sigma(x)=frac11+e^-x for any positive integer r, \ntanh tanh(x)=frace^x-e^-xe^x+e^-x for any positive integer r. ","category":"page"},{"location":"architectures/sympnet/#SympNet-with-GeometricMachineLearning.jl","page":"SympNet","title":"SympNet with GeometricMachineLearning.jl","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"With GeometricMachineLearning.jl, it is really easy to implement and train a SympNet. The steps are the following :","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Create the architecture in one line with the function GSympNet or LASympNet,\nCreate the neural networks depending a backend (e.g. with Lux),\nCreate an optimizer for the training step,\nTrain the neural networks with the train!function.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Both LA-SympNet and G-SympNet architectures can be generated in one line with GeometricMachineLearning.jl.","category":"page"},{"location":"architectures/sympnet/#LA-SympNet-2","page":"SympNet","title":"LA-SympNet","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"To create a LA-SympNet, one needs to write","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"lasympnet = LASympNet(dim; width=9, nhidden=1, activation=tanh, init_uplow_linear=[true,false], \n            init_uplow_act=[true,false],init_sym_matrices=Lux.glorot_uniform, init_bias=Lux.zeros32, \n            init_weight=Lux.glorot_uniform) ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"LASympNet takes one obligatory argument:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"dim : the dimension of the phase space,","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"and several keywords argument :","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"width : the width for all the symplectic linear layers with default value set to 9 (if width>9, width is set to 9),\nnhidden : the number of pairs of symplectic linear and activation layers with default value set to 0 (i.e LA-SympNet is a single symplectic linear layer),\nactivation : the activation function for all the symplectic activations layers with default value set to tanh,\ninituplowlinear : a vector of boolean whose the ith coordinate is true only if all the symplectic linear layers in (i mod length(init_uplow_linear))-th position is up (for example the default value is [true,false] which represents an alternation of up and low symplectic linear layers),\ninituplowact : a vector of boolean whose the ith coordinate is true only if all the symplectic activation layers in (i mod length(init_uplow_act))-th position is up (for example the default value is [true,false] which represents an alternation of up and low symplectic activation layers),\ninitsymmatrices: the function which gives the way to initialize the symmetric matrices S^i of symplectic linear layers,\ninit_bias: the function which gives the way to initialize the vector of bias b,\ninit_weight: the function which gives the way to initialize the weight a.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The default value of the last three keyword arguments uses Lux functions.","category":"page"},{"location":"architectures/sympnet/#G-SympNet-2","page":"SympNet","title":"G-SympNet","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"To create a G-SympNet, one needs to write","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"gsympnet = GSympNet(dim; width=dim, nhidden=1, activation=tanh, init_uplow=[true,false], init_weight=Lux.glorot_uniform, \ninit_bias=Lux.zeros32, init_scale=Lux.glorot_uniform) ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"GSympNet takes one obligatory argument:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"dim : the dimension of the phase space,","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"and severals keywords argument :","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"width : the width for all the gradients layers with default value set to dim to have widthgeqdim,\nnhidden : the number of gradient layers with default value set to 1,\nactivation : the activation function for all the gradients layers with default value set to tanh,\ninit_uplow: a vector of boolean whose the ith coordinate is true only if all the gradient layers in (i mod length(init_uplow))-th position is up (for example the default value is [true,false] which represents an alternation of up and low gradient layers),\ninit_weight: the function which gives the way to initialize the vector of weights a,\ninit_bias: the function which gives the way to initialize the vector of bias b,\ninit_scale: the function which gives the way to initialize the scale matrix K.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The default value of the last three keyword arguments uses Lux functions.","category":"page"},{"location":"architectures/sympnet/#Loss-function","page":"SympNet","title":"Loss function","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"To train the SympNet, one need data along a trajectory such that the model is trained to perform an integration. These data are (QP) where Qij (respectively Pij) is the real number q_j(t_i) (respectively pij) which is the j-th coordinates of the generalized position (respectively momentum) at the i-th time step. One also need a loss function defined as :","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Loss(QP) = undersetisum d(Phi(Qi-Pi-) Qi- Pi-^T)","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"where d is a distance on mathbbR^d.","category":"page"},{"location":"architectures/sympnet/#Examples","page":"SympNet","title":"Examples","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Let us see how to use it on several examples.","category":"page"},{"location":"architectures/sympnet/#Example-of-a-pendulum-with-G-SympNet","page":"SympNet","title":"Example of a pendulum with G-SympNet","text":"","category":"section"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Let us begin with a simple example, the pendulum system, the Hamiltonian of which is ","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"H(qp)inmathbbR^2 mapsto frac12p^2-cos(q) in mathbbR","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The first thing to do is to create an architecture, in this example a G-SympNet.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"# number of inputs/dimension of system\nconst ninput = 2\n# layer dimension for gradient module \nconst ld = 10 \n# hidden layers\nconst ln = 4\n# activation function\nconst act = tanh\n\n# Creation of a G-SympNet architecture \ngsympnet = GSympNet(ninput, width=ld, nhidden=ln, activation=act)\n\n# Creation of a LA-SympNet architecture \nlasympnet = LASympNet(ninput, nhidden=ln, activation=act)","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Then we can create the neural networks depending on the backend. Here we will use Lux:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"# create Lux network\nnn = NeuralNetwork(gsympnet, LuxBackend())","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"We have to define an optimizer which will be use in the training of the SympNet. For more details on optimizer, please see the corresponding documentation Optimizer.md. For example, let us use a momentum optimizer :","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"# Optimiser\nopt = MomentumOptimizer(1e-2, 0.5)","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"We can now perform the training of the neural networks. The syntax is the following :","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"# number of training runs\nconst nruns = 10000\n# Batchsize used to compute the gradient of the loss function with respect to the parameters of the neural networks.\nconst nbatch = 10\n\n# perform training (returns array that contains the total loss for each training step)\ntotal_loss = train!(nn, opt, data_q, data_p; ntraining = nruns, batch_size = nbatch)","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The train function will change the parameters of the neural networks and gives an a vector containing the evolution of the value of the loss function during the training. Default values for the arguments ntraining and batch_size are respectively 1000 and 10.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"The trainings data data_q and data_p must be matrices of mathbbR^ntimes d where n is the length of data and d is the half of the dimension of the system, i.e data_q[i,j] is q_j(t_i) where (t_1t_n) are the corresponding time of the training data.","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"Then we can make prediction. Let's compare the initial data with a prediction starting from the same phase space point using the provided function Iterate_Sympnet:","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"#predictions\nq_learned, p_learned = Iterate_Sympnet(nn, q0, p0; n_points = size(data_q,1))","category":"page"},{"location":"architectures/sympnet/","page":"SympNet","title":"SympNet","text":"(Image: )","category":"page"},{"location":"Optimizer/#Optimizer","page":"Optimizer","title":"Optimizer","text":"","category":"section"},{"location":"manifolds/grassmann_manifold/#Grassmann-Manifold","page":"Grassmann","title":"Grassmann Manifold","text":"","category":"section"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann","title":"Grassmann","text":"(The description of the Grassmann manifold is based on that of the Stiefel manifold, so this should be read first.)","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann","title":"Grassmann","text":"An element of the Grassmann manifold G(nN) is a vector subspace subsetmathbbR^N of dimension n, and each such subspace can be represented by a full-rank matrix AinmathbbR^Ntimesn and the full space takes the form G(nN) = mathbbR^Ntimesnsim where the equivalence relation is AsimB iff existsCinmathbbR^ntimesntext st AC = B. One can find a parametrization of the manifold the following way: Because the matrix A has full rank, there have to be n independent columns in it: i_1 ldots i_n. For simplicity assume that i_1 = 1 i_2=2 ldots i_n=n and call the matrix made up by these columns C. Then the mapping to the coordinate chart is: AC^-1 and the last N-n columns are the coordinates. ","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann","title":"Grassmann","text":"The tangent space for this element can then be represented through matrices: ","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann","title":"Grassmann","text":"beginpmatrix\n    0  cdots  0 \n    cdots  cdots  cdots  \n    0  cdots  0 \n    a_11  cdots  a_1n \n    cdots  cdots  cdots  \n    a_(N-n)1  cdots  a_(N-n)n\nendpmatrix","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann","title":"Grassmann","text":"The Grassmann manifold can also be seen as the Stiefel manifold modulo an equivalence class. This leads to the following (which is used for optimization):","category":"page"},{"location":"manifolds/grassmann_manifold/","page":"Grassmann","title":"Grassmann","text":"mathfrakg^mathrmhor = mathfrakg^mathrmhorE = leftbeginpmatrix 0  -B^T  B  0 endpmatrix textB arbitraryright","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = GeometricMachineLearning","category":"page"},{"location":"#Geometric-Machine-Learning","page":"Home","title":"Geometric Machine Learning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GeometricMachineLearning.jl implements various scientific machine learning models that aim at learning dynamical systems with geometric structure, such as Hamiltonian (symplectic) or Lagrangian (variational) systems.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GeometricMachineLearning.jl and all of its dependencies can be installed via the Julia REPL by typing ","category":"page"},{"location":"","page":"Home","title":"Home","text":"]add GeometricMachineLearning","category":"page"},{"location":"#Architectures","page":"Home","title":"Architectures","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"architectures/sympnet.md\",\n]","category":"page"},{"location":"#Manifolds","page":"Home","title":"Manifolds","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"manifolds/grassmann_manifold.md\",\n    \"manifolds/stiefel_manifold.md\",\n]","category":"page"}]
}
