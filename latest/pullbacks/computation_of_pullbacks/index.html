<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Pullbacks · GeometricMachineLearning.jl</title><meta name="title" content="Pullbacks · GeometricMachineLearning.jl"/><meta property="og:title" content="Pullbacks · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Pullbacks · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/pullbacks/computation_of_pullbacks/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/pullbacks/computation_of_pullbacks/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/pullbacks/computation_of_pullbacks/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li class="is-active"><a class="tocitem" href>Pullbacks</a><ul class="internal"><li><a class="tocitem" href="#How-to-Compute-Pullbacks"><span>How to Compute Pullbacks</span></a></li><li><a class="tocitem" href="#What-is-a-Pullback?"><span>What is a Pullback?</span></a></li><li><a class="tocitem" href="#Motivation-from-a-differential-geometric-perspective"><span>Motivation from a differential-geometric perspective</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li><li class="toplevel"><a class="tocitem" href="#References-2"><span>References</span></a></li></ul></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/hamiltonian_neural_network/">Hamiltonian Neural Network</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../architectures/symplectic_transformer/">Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../../tutorials/hamiltonian_neural_network/">Hamiltonian Neural Network</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/matrix_softmax/">Matrix Attention</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/symplectic_transformer/">Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Special Arrays and AD</a></li><li class="is-active"><a href>Pullbacks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Pullbacks</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/pullbacks/computation_of_pullbacks.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Pullbacks-and-Automatic-Differentiation"><a class="docs-heading-anchor" href="#Pullbacks-and-Automatic-Differentiation">Pullbacks and Automatic Differentiation</a><a id="Pullbacks-and-Automatic-Differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Pullbacks-and-Automatic-Differentiation" title="Permalink"></a></h1><p>Automatic Differentiation is an important part of modern machine learning. It is essentially a tool to compute the gradient of a loss function with respect to its input arguments, i.e. given a function <span>$L:\Theta\to\mathbb{R}$</span> an AD routine computes:</p><p class="math-container">\[    \mathrm{AD}: \theta \mapsto \nabla_\theta{}L.\]</p><p>When we train a neural network the function <span>$L$</span> is the composition of a neural network </p><p class="math-container">\[    \mathcal{NN}_\theta(x) = l^{(n)}_{\theta_n}\circ{}l^{(1)}_{\theta_1}(x)\]</p><p>and a <a href="../../reduced_order_modeling/losses/#Different-Neural-Network-Losses">loss function</a>, so we have:</p><p class="math-container">\[    L(\theta) = \mathtt{loss}(l^{(n)}_{\theta_n}\circ{}l^{(1)}_{\theta_1}(x), \mathtt{minibatch})\]</p><p>where <span>$\theta = (\theta_1, \ldots, \theta_n)$</span> and <span>$L$</span> further depends on a specific mini batch here. So what we need to do is compute the pullback<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> of every single layer <span>$l^{(i)}_{\theta_i}$</span>. For this consider a function<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup> <span>$f: \mathbb{R}^m \to \mathbb{R}^n$</span> and <span>$x\in\mathbb{R}^m$</span>. The pullback of <span>$f$</span> is a function that, depending on <span>$x$</span>, provides a recipe to map an element from <span>$\mathbb{R}^n$</span> to an element of <span>$\mathbb{R}^m$</span>:</p><p class="math-container">\[    \mathrm{pullbak}(f)[x]:\mathbb{R}^m \simeq T^*_{f(x)}\mathbb{R}^m \to T^*_{x}\mathbb{R}^n \simeq \mathbb{R}^n,\]</p><p>where <span>$T^*_x\mathcal{V}$</span> is the cotangent space of <span>$\mathcal{V}$</span> at <span>$x$</span>. </p><h2 id="How-to-Compute-Pullbacks"><a class="docs-heading-anchor" href="#How-to-Compute-Pullbacks">How to Compute Pullbacks</a><a id="How-to-Compute-Pullbacks-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-Compute-Pullbacks" title="Permalink"></a></h2><p><code>GeometricMachineLearning</code> has many pullbacks for custom array types and other operations implemented. The need for this essentially comes from the fact that we cannot trivially differentiate custom GPU kernels<sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup>. Implemented custom pullback comprise <a href="../../arrays/tensors/#Tensors-in-GeometricMachineLearning">parallel multiplications with tensors</a>.</p><h2 id="What-is-a-Pullback?"><a class="docs-heading-anchor" href="#What-is-a-Pullback?">What is a Pullback?</a><a id="What-is-a-Pullback?-1"></a><a class="docs-heading-anchor-permalink" href="#What-is-a-Pullback?" title="Permalink"></a></h2><p>Here we first explain the principle of a pullback with the example of a vector-valued function. The generalization to matrices and higher-order tensors is straight-forward. </p><p>The pullback of a vector-valued function <span>$f:\mathbb{R}^{n}\to\mathbb{R}^m$</span> can be interpreted as the <em>sensitivities in the input space</em> <span>$\mathbb{R}^n$</span> with respect to variations in the output space <span>$\mathbb{R}^m$</span> via the function <span>$f$</span>: </p><p class="math-container">\[\left[\mathrm{pullback}(f)[a\in\mathbb{R}^n, db\in\mathbb{R}^m]\right]_i = \sum_{j=1}^m\frac{\partial{}f_j}{\partial{}a_i}db_j.\]</p><p>This principle can easily be generalized to matrices. For this consider the function <span>$g::\mathbb{R}^{n_1\times{}n_2}\to\mathbb{R}^{m_1\times{}m_2}$</span>. We then have: </p><p class="math-container">\[\left[\mathrm{pullback}(g)[A\in\mathbb{R}^{n_1\times{}n_2}, dB\in\mathbb{R}^{m_1\times{}m_2}]\right]_{(i_1, i_2)} = \sum_{j_1=1}^{m_1}\sum_{j_2=1}^{m_2}\frac{\partial{}f_{(j_1, j_2)}}{\partial{}a_{(i_1, i_2)}}db_{(j_1, j_2)}.\]</p><p>The generalization to higher-order tensors is again straight-forward.</p><h3 id="Illustrative-example"><a class="docs-heading-anchor" href="#Illustrative-example">Illustrative example</a><a id="Illustrative-example-1"></a><a class="docs-heading-anchor-permalink" href="#Illustrative-example" title="Permalink"></a></h3><p>Consider the matrix inverse <span>$\mathrm{inv}: \mathbb{R}^{n\times{}n}\to\mathbb{R}^{n\times{}n}$</span> as an example. This fits into the above framework where <span>$inv$</span> is a matrix-valued function from <span>$\mathbb{R}^{n\times{}n}$</span> to <span>$\mathbb{R}^{n\times{}n}$</span>. We here write <span>$B := A^{-1} = \mathrm{inv}(A)$</span>. We thus have to compute: </p><p class="math-container">\[\left[\mathrm{pullback}(\mathrm{inv})[A\in\mathbb{R}^{n\times{}n}, dB\in\mathbb{R}^{n\times{}n}]\right]_{(i, j)} = \sum_{k=1}^{n}\sum_{\ell=1}^{n}\frac{\partial{}b_{k, \ell}}{\partial{}a_{i, j}}db_{k, \ell}.\]</p><p>For a matrix <span>$A$</span> that depends on a parameter <span>$\varepsilon$</span> we have: </p><p class="math-container">\[\frac{\partial}{\partial\varepsilon}B = -B\left( \frac{\partial}{\partial\varepsilon} A \right) B.\]</p><p>This can easily be checked: </p><p class="math-container">\[\mathbb{O} = \frac{\partial}{\partial\varepsilon}\mathbb{I} = \frac{\partial}{\partial\varepsilon}(AB) = A\frac{\partial}{\partial\varepsilon}B + \left(\frac{\partial}{\partial\varepsilon}A\right)B.\]</p><p>We can then write: </p><p class="math-container">\[\begin{aligned}
\sum_{k,\ell}\left( \frac{\partial}{\partial{}a_{ij}} b_{k\ell} \right) db_{k\ell}  &amp; = \sum_{k\ell}\left[ \frac{\partial}{\partial{}a_{ij}} B \right]_{k\ell} db_{k,\ell} \\ 
&amp; = - \sum_{k,\ell}\left[B \left(\frac{\partial}{\partial{}a_{ij}} A\right) B \right]_{k\ell} db_{k\ell} \\ 
&amp; = - \sum_{k,\ell,m,n}b_{km} \left(\frac{\partial{}a_{mn}}{\partial{}a_{ij}}\right) b_{n\ell} db_{k\ell} \\ 
&amp; = - \sum_{k,\ell,m,n}b_{km} \delta_{im}\delta_{jn} b_{n\ell} db_{k\ell} \\ 
&amp; = - \sum_{k,\ell}b_{ki} b_{j\ell} db_{k\ell} \\ 
&amp; \equiv - B^T\cdot{}dB\cdot{}B^T. 
\end{aligned}\]</p><p>We use this expression to differentiate the <a href="../../layers/attention_layer/#GeometricMachineLearning.VolumePreservingAttention"><code>VolumePreservingAttention</code></a> layer. </p><h2 id="Motivation-from-a-differential-geometric-perspective"><a class="docs-heading-anchor" href="#Motivation-from-a-differential-geometric-perspective">Motivation from a differential-geometric perspective</a><a id="Motivation-from-a-differential-geometric-perspective-1"></a><a class="docs-heading-anchor-permalink" href="#Motivation-from-a-differential-geometric-perspective" title="Permalink"></a></h2><p>The notion of a <em>pullback in automatic differentiation</em> is motivated by the concept of <em>pullback in differential geometry</em> [<a href="../../references/#betancourt2018geometric">25</a>, <a href="../../references/#bolte2020mathematical">26</a>]. In both cases we want to compute, based on a mapping </p><p class="math-container">\[f:\mathcal{V}\to\mathcal{W}, a \mapsto f(a) =: b, \]</p><p>a <em>map of differentials</em> <span>$db \mapsto da$</span>. In the differential geometry case <span>$db$</span> and <span>$da$</span> are part of the associated cotangent spaces, i.e. <span>$db\in{}T^*_b\mathcal{W}$</span> and <span>$da\in{}T^*_a\mathcal{V}$</span>; in AD we (mostly) deal with spaces of arrays, i.e. vector spaces, which means that <span>$T^*_b\mathcal{W} \simeq \mathcal{W}$</span> and <span>$T^*_a\mathcal{V} \simeq \mathcal{V}$</span>. If we have neural network weights on manifolds however, then we have to map weights from <span>$T^*_a\mathcal{V}$</span> (the result of an AD routine) to <span>$T_a\mathcal{V}$</span> before we can apply a <a href="../../optimizers/manifold_related/retractions/#Retractions">retraction</a>. The mapping </p><p class="math-container">\[T^*_a\mathcal{V} \to T_a\mathcal{V}\]</p><p>is equivalent to applying the <a href="../../manifolds/riemannian_manifolds/#The-Riemannian-Gradient">Riemannian gradient</a>.</p><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.ZygotePullback" href="#GeometricMachineLearning.ZygotePullback"><code>GeometricMachineLearning.ZygotePullback</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ZygotePullback &lt;: AbstractPullback</code></pre><p>The pullback based on the <a href="https://github.com/FluxML/Zygote.jl"><code>Zygote</code></a> backend.</p><p><strong>Examples</strong></p><p>For a network that is trained on inputs only:</p><pre><code class="language-julia hljs">using GeometricMachineLearning
using GeometricMachineLearning: _processing

loss = AutoEncoderLoss()
_pullback = ZygotePullback(loss)
nn = NeuralNetwork(Chain(Dense(10, 2, tanh), Dense(2, 10, tanh)))
input = rand(10)
_pullback(nn.params, nn.model, input)[2](1) |&gt; _processing |&gt; typeof

# output

@NamedTuple{L1::@NamedTuple{W::Matrix{Float64}, b::Vector{Float64}}, L2::@NamedTuple{W::Matrix{Float64}, b::Vector{Float64}}}</code></pre><p>In this example <a href="../../architectures/hamiltonian_neural_network/#GeometricMachineLearning._processing"><code>_processing</code></a> is used to get around some <code>Zygote</code> quirks.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/a6e4ae3a1f8de75fde2d488c61ba71f7ce6bb4d5/src/pullbacks/zygote_pullback.jl#LL1-L25">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><!--<h1 id="References-2"><a class="docs-heading-anchor" href="#References-2">References</a><a class="docs-heading-anchor-permalink" href="#References-2" title="Permalink"></a></h1>--><div class="citation noncanonical"><dl><dt>[25]</dt><dd><div>M. Betancourt. <em>A geometric theory of higher-order automatic differentiation</em>, arXiv preprint arXiv:1812.11592 (2018).</div></dd><dt>[26]</dt><dd><div>J. Bolte and E. Pauwels. <em>A mathematical model for automatic differentiation in machine learning</em>. Advances in Neural Information Processing Systems <strong>33</strong>, 10809–10819 (2020).</div></dd></dl></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>The term <em>pullback</em> originally comes from differential geometry [<a href="../../references/#bishop1980tensor">16</a>]. This motivation is discussed below.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a><span>$\mathbb{R}^m$</span> can be interpreted as the space of the neural network parameters <span>$\Theta$</span> here. </li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>This may change in the future if the package <code>Enzyme</code> [<a href="../../references/#moses2021reverse">24</a>] reaches maturity.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../arrays/tensors/">« Tensors</a><a class="docs-footer-nextpage" href="../../structure_preservation/symplecticity/">Symplecticity »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.0 on <span class="colophon-date" title="Wednesday 9 July 2025 11:45">Wednesday 9 July 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
