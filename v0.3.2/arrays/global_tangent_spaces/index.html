<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Global Tangent Spaces · GeometricMachineLearning.jl</title><meta name="title" content="Global Tangent Spaces · GeometricMachineLearning.jl"/><meta property="og:title" content="Global Tangent Spaces · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Global Tangent Spaces · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/arrays/global_tangent_spaces/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/arrays/global_tangent_spaces/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/arrays/global_tangent_spaces/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li class="is-active"><a class="tocitem" href>Global Tangent Spaces</a><ul class="internal"><li><a class="tocitem" href="#Global-Sections"><span>Global Sections</span></a></li><li><a class="tocitem" href="#The-Global-Tangent-Space-for-the-Stiefel-Manifold"><span>The Global Tangent Space for the Stiefel Manifold</span></a></li><li><a class="tocitem" href="#Global-Tangent-Space-for-the-Grassmann-Manifold"><span>Global Tangent Space for the Grassmann Manifold</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Optimizers</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/global_sections/">Global Sections</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modelling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Network Losses</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_autoencoder/">PSD and Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/kolmogorov_n_width/">Kolmogorov n-width</a></li><li><a class="tocitem" href="../../reduced_order_modeling/projection_reduction_errors/">Projection and Reduction Error</a></li></ul></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">Sympnets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../library/">Library</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Special Arrays and AD</a></li><li class="is-active"><a href>Global Tangent Spaces</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Global Tangent Spaces</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/arrays/global_tangent_spaces.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Global-Tangent-Spaces"><a class="docs-heading-anchor" href="#Global-Tangent-Spaces">Global Tangent Spaces</a><a id="Global-Tangent-Spaces-1"></a><a class="docs-heading-anchor-permalink" href="#Global-Tangent-Spaces" title="Permalink"></a></h1><p>In <code>GeometricMachineLearning</code> standard neural network optimizers are generalized to <a href="../../manifolds/homogeneous_spaces/#Homogeneous-Spaces">homogeneous spaces</a> by leveraging the special structure of the tangent spaces of this class of manifolds. When we introduced homogeneous spaces we already talked about that every tangent space to a homogeneous space <span>$T_Y\mathcal{M}$</span> is of the form: </p><p class="math-container">\[    T_Y\mathcal{M} = \mathfrak{g} \cdot Y := \{AY: A\in{}\mathfrak{g}\}.\]</p><p>We then have a decomposition of <span>$\mathfrak{g}$</span> into a vertical part <span>$\mathfrak{g}^{\mathrm{ver}, Y}$</span> and a horizontal part <span>$\mathfrak{g}^{\mathrm{hor}, Y}$</span> and the horizontal part is isomorphic to <span>$T_Y\mathcal{M}$</span>. </p><p>We now identify a special element <span>$E \in \mathcal{M}$</span> and designate the horizontal component <span>$\mathfrak{g}^{\mathrm{hor}, E}$</span> as our <em>global tangent space</em>. We will refer to this global tangent space by <span>$\mathfrak{g}^\mathrm{hor}$</span>. We can now find a transformation from any <span>$\mathfrak{g}^{\mathrm{hor}, Y}$</span> to <span>$\mathfrak{g}^\mathrm{hor}$</span> and vice-versa (these spaces are isomorphic).</p><div class="admonition is-info"><header class="admonition-header">Theorem</header><div class="admonition-body"><p>Let <span>$A\in{}G$</span> an element such that <span>$AE = Y$</span>. Then we have</p><p class="math-container">\[A^{-1}\cdot\mathfrak{g}^{\mathrm{hor},Y}\cdot{}A = \mathfrak{g}^\mathrm{hor},\]</p><p>i.e. for every element <span>$B\in\mathfrak{g}^\mathrm{hor}$</span> we can find a <span>$B^Y \in \mathfrak{g}^{\mathrm{hor},Y}$</span> s.t. <span>$B = A^{-1}B^YA$</span> (and vice-versa).</p></div></div><details class="admonition is-details"><summary class="admonition-header">Proof</summary><div class="admonition-body"><p>We first show that for every <span>$B^Y\in\mathfrak{g}^{\mathrm{hor},Y}$</span> the element <span>$A^{-1}B^YA$</span> is in <span>$\mathfrak{g}^{\mathrm{hor}}$</span>. First not that <span>$A^{-1}B^YA\in\mathfrak{g}$</span> by a fundamental theorem of Lie group theory (closedness of the Lie algebra under adjoint action). Now assume that <span>$A^{-1}B^YA$</span> is not fully contained in <span>$\mathfrak{g}^\mathrm{hor}$</span>, i.e. it also has a vertical component. So we would lose information when performing <span>$A^{-1}B^YA \mapsto A^{-1}B^YAE = A^{-1}B^YY$</span>, but this contradicts the fact that <span>$B^Y\in\mathfrak{g}^{\mathrm{hor},Y}.$</span> We now have to proof that for every <span>$B\in\mathfrak{g}^\mathrm{hor}$</span> we can find an element in <span>$\mathfrak{g}^{\mathrm{hor}, Y}$</span> such that this element is mapped to <span>$B$</span>. By a argument similar to the one above we can show that <span>$ABA^{-1}\in\mathfrak{g}^\mathrm{hor, Y}$</span> and this element maps to <span>$B$</span>. Proofing that the map is injective is now trivial.</p></div></details><p>We should note that we have written all Lie group and Lie algebra actions as simple matrix multiplications, like <span>$AE = Y$</span>. For some Lie groups and Lie algebras we should use different notations [<a href="../../references/#holm2009geometric">9</a>]. These Lie groups are however not relevant for what we use in <code>GeometricMachineLearning</code> and we will stick to regular matrix notation.</p><h2 id="Global-Sections"><a class="docs-heading-anchor" href="#Global-Sections">Global Sections</a><a id="Global-Sections-1"></a><a class="docs-heading-anchor-permalink" href="#Global-Sections" title="Permalink"></a></h2><p>Note that the theorem above requires us to find an element <span>$A\in{}G$</span> such that <span>$AE = Y$</span>. If we can find a mapping <span>$\lambda:\mathcal{M}\to{}G$</span> we call such a mapping a <em>global section</em>. </p><div class="admonition is-info"><header class="admonition-header">Definition</header><div class="admonition-body"><p>We call a mapping from <span>$\lambda:\mathcal{M} \to G$</span> a homogeneous space to its associated Lie group a <strong>global section</strong> if it satisfies:</p><p class="math-container">\[\lambda(Y)E = Y,\]</p><p>where <span>$E$</span> is the distinct element of the homogeneous space.</p></div></div><p>Note that in general global sections are not unique because the rank of <span>$G$</span> is in general greater than that of <span>$\mathcal{M}$</span>. We give an example of how to construct such a global section for the Stiefel and the Grassmann manifolds below. </p><h2 id="The-Global-Tangent-Space-for-the-Stiefel-Manifold"><a class="docs-heading-anchor" href="#The-Global-Tangent-Space-for-the-Stiefel-Manifold">The Global Tangent Space for the Stiefel Manifold</a><a id="The-Global-Tangent-Space-for-the-Stiefel-Manifold-1"></a><a class="docs-heading-anchor-permalink" href="#The-Global-Tangent-Space-for-the-Stiefel-Manifold" title="Permalink"></a></h2><p>We now discuss the specific form of the global tangent space for the <a href="../../manifolds/homogeneous_spaces/#The-Stiefel-Manifold">Stiefel manifold</a>. We choose the distinct element<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> <span>$E$</span> to have an especially simple form (this matrix can be build by calling <a href="../../library/#GeometricMachineLearning.StiefelProjection"><code>StiefelProjection</code></a>):</p><p class="math-container">\[E = \begin{bmatrix}
\mathbb{I}_n \\ 
\mathbb{O}
\end{bmatrix}\in{}St(n, N).\]</p><p>Based on this elements of the vector space <span>$\mathfrak{g}^{\mathrm{hor}, E} =: \mathfrak{g}^{\mathrm{hor}}$</span> are: </p><p class="math-container">\[\begin{pmatrix}
A &amp; B^T \\ B &amp; \mathbb{O}
\end{pmatrix},\]</p><p>where <span>$A$</span> is a skew-symmetric matrix of size <span>$n\times{}n$</span> and <span>$B$</span> is an arbitrary matrix of size <span>$(N - n)\times{}n$</span>.</p><p>Arrays of type <span>$\mathfrak{g}^{\mathrm{hor}, E}$</span> are implemented in <code>GeometricMachineLearning</code> under the name <a href="../../library/#GeometricMachineLearning.StiefelLieAlgHorMatrix"><code>StiefelLieAlgHorMatrix</code></a>.</p><p>We can call this with e.g. a skew-symmetric matrix <span>$A$</span> and an arbitrary matrix <span>$B$</span>:</p><pre><code class="language-julia hljs">N, n = 10, 4

A = rand(SkewSymMatrix, n)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">4×4 SkewSymMatrix{Float64, Vector{Float64}}:
 0.0       -0.311448  -0.121148   -0.38669
 0.311448   0.0       -0.20453    -0.018572
 0.121148   0.20453    0.0        -0.0721807
 0.38669    0.018572   0.0721807   0.0</code></pre><pre><code class="language-julia hljs">B = rand(N - n, n)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">6×4 Matrix{Float64}:
 0.121813   0.160222  0.540529  0.492297
 0.137766   0.904766  0.546787  0.100781
 0.0508083  0.991961  0.783961  0.878875
 0.38644    0.740841  0.813172  0.959667
 0.846458   0.229042  0.83098   0.634652
 0.835334   0.96917   0.656091  0.0648724</code></pre><pre><code class="language-julia hljs">B1 = StiefelLieAlgHorMatrix(A, B, N, n)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10×10 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:
 0.0        -0.311448  -0.121148   …  -0.38644   -0.846458  -0.835334
 0.311448    0.0       -0.20453       -0.740841  -0.229042  -0.96917
 0.121148    0.20453    0.0           -0.813172  -0.83098   -0.656091
 0.38669     0.018572   0.0721807     -0.959667  -0.634652  -0.0648724
 0.121813    0.160222   0.540529       0.0        0.0        0.0
 0.137766    0.904766   0.546787   …   0.0        0.0        0.0
 0.0508083   0.991961   0.783961       0.0        0.0        0.0
 0.38644     0.740841   0.813172       0.0        0.0        0.0
 0.846458    0.229042   0.83098        0.0        0.0        0.0
 0.835334    0.96917    0.656091       0.0        0.0        0.0</code></pre><p>We can also call it with a matrix of shape <span>$N\times{}N$</span>:</p><pre><code class="language-julia hljs">B2 = Matrix(B1) # note that this does not have any special structure

StiefelLieAlgHorMatrix(B2, n)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10×10 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, SubArray{Float64, 2, Matrix{Float64}, Tuple{UnitRange{Int64}, UnitRange{Int64}}, false}}:
 0.0        -0.311448  -0.121148   …  -0.38644   -0.846458  -0.835334
 0.311448    0.0       -0.20453       -0.740841  -0.229042  -0.96917
 0.121148    0.20453    0.0           -0.813172  -0.83098   -0.656091
 0.38669     0.018572   0.0721807     -0.959667  -0.634652  -0.0648724
 0.121813    0.160222   0.540529       0.0        0.0        0.0
 0.137766    0.904766   0.546787   …   0.0        0.0        0.0
 0.0508083   0.991961   0.783961       0.0        0.0        0.0
 0.38644     0.740841   0.813172       0.0        0.0        0.0
 0.846458    0.229042   0.83098        0.0        0.0        0.0
 0.835334    0.96917    0.656091       0.0        0.0        0.0</code></pre><p>Or we can call it a matrix of shape <span>$N\times{}n$</span>:</p><pre><code class="language-julia hljs">E = StiefelProjection(N, n)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10×4 StiefelProjection{Float64, Matrix{Float64}}:
 1.0  0.0  0.0  0.0
 0.0  1.0  0.0  0.0
 0.0  0.0  1.0  0.0
 0.0  0.0  0.0  1.0
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0</code></pre><pre><code class="language-julia hljs">B3 = B1 * E

StiefelLieAlgHorMatrix(B3, n)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10×10 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, SubArray{Float64, 2, Matrix{Float64}, Tuple{UnitRange{Int64}, UnitRange{Int64}}, false}}:
 0.0        -0.311448  -0.121148   …  -0.38644   -0.846458  -0.835334
 0.311448    0.0       -0.20453       -0.740841  -0.229042  -0.96917
 0.121148    0.20453    0.0           -0.813172  -0.83098   -0.656091
 0.38669     0.018572   0.0721807     -0.959667  -0.634652  -0.0648724
 0.121813    0.160222   0.540529       0.0        0.0        0.0
 0.137766    0.904766   0.546787   …   0.0        0.0        0.0
 0.0508083   0.991961   0.783961       0.0        0.0        0.0
 0.38644     0.740841   0.813172       0.0        0.0        0.0
 0.846458    0.229042   0.83098        0.0        0.0        0.0
 0.835334    0.96917    0.656091       0.0        0.0        0.0</code></pre><p>We now demonstrate how to map from an element of <span>$\mathfrak{g}^{\mathrm{hor}, Y}$</span> to an element of <span>$\mathfrak{g}^\mathrm{hor}$</span>:</p><pre><code class="language-julia hljs">N, n = 10, 5

Y = rand(StiefelManifold, N, n)
Δ = rgrad(Y, rand(N, n))
ΩΔ = GeometricMachineLearning.Ω(Y, Δ)
λY = GlobalSection(Y)

λY_mat = Matrix(λY)

round.(λY_mat&#39; * ΩΔ * λY_mat; digits = 3)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10×10 Matrix{Float64}:
  0.0    -0.611   0.421   1.362   1.367  …   0.974   0.115  -0.094   0.587
  0.611   0.0     0.172   1.119   1.055      1.052  -0.054  -0.512   0.904
 -0.421  -0.172   0.0     1.215   0.306      0.84    0.297  -0.112   0.641
 -1.362  -1.119  -1.215  -0.0    -0.427      0.436   0.144  -0.656   0.717
 -1.367  -1.055  -0.306   0.427  -0.0        1.221   0.133  -0.508   0.669
 -0.471  -0.92   -0.318  -0.331  -0.405  …   0.0    -0.0     0.0    -0.0
 -0.974  -1.052  -0.84   -0.436  -1.221      0.0    -0.0     0.0    -0.0
 -0.115   0.054  -0.297  -0.144  -0.133      0.0    -0.0    -0.0     0.0
  0.094   0.512   0.112   0.656   0.508     -0.0     0.0    -0.0    -0.0
 -0.587  -0.904  -0.641  -0.717  -0.669      0.0    -0.0     0.0     0.0</code></pre><p>Performing this computation directly is computationally very inefficient however and the user is strongly discouraged to call <code>Matrix</code> on an instance of <a href="../../library/#GeometricMachineLearning.GlobalSection"><code>GlobalSection</code></a>. The better option is calling <a href="#GeometricMachineLearning.global_rep-arrays-global_tangent_spaces"><code>global_rep</code></a>:</p><pre><code class="language-julia hljs">_round(global_rep(λY, Δ); digits = 3)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10×10 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:
  0.0    -0.611   0.421   1.362   1.367  0.471  0.974   0.115  -0.094  0.587
  0.611   0.0     0.172   1.119   1.055  0.92   1.052  -0.054  -0.512  0.904
 -0.421  -0.172   0.0     1.215   0.306  0.318  0.84    0.297  -0.112  0.641
 -1.362  -1.119  -1.215   0.0    -0.427  0.331  0.436   0.144  -0.656  0.717
 -1.367  -1.055  -0.306   0.427   0.0    0.405  1.221   0.133  -0.508  0.669
 -0.471  -0.92   -0.318  -0.331  -0.405  0.0    0.0     0.0     0.0    0.0
 -0.974  -1.052  -0.84   -0.436  -1.221  0.0    0.0     0.0     0.0    0.0
 -0.115   0.054  -0.297  -0.144  -0.133  0.0    0.0     0.0     0.0    0.0
  0.094   0.512   0.112   0.656   0.508  0.0    0.0     0.0     0.0    0.0
 -0.587  -0.904  -0.641  -0.717  -0.669  0.0    0.0     0.0     0.0    0.0</code></pre><p>Internally <code>GlobalSection</code> calls the function <a href="#GeometricMachineLearning.global_section-arrays-global_tangent_spaces"><code>GeometricMachineLearning.global_section</code></a> which does the following for the Stiefel manifold: </p><pre><code class="language-julia hljs">A = randn(N, N - n) # or the gpu equivalent
A = A - Y * (Y&#39; * A)
Y⟂ = qr(A).Q[1:N, 1:(N - n)]</code></pre><p>So we draw <span>$(N - n)$</span> new columns randomly, subtract the part that is spanned by the columns of <span>$Y$</span> and then perform a <span>$QR$</span> composition on the resulting matrix. The <span>$Q$</span> part of the decomposition is a matrix of <span>$(N - n)$</span> columns that is orthogonal to <span>$Y$</span> and is typically referred to as <span>$Y_\perp$</span>  [<a href="../../references/#absil2004riemannian">6</a>, <a href="../../references/#absil2008optimization">10</a>, <a href="../../references/#bendokat2020grassmann">11</a>]. We can easily check that this <span>$Y_\perp$</span> is indeed orthogonal to <span>$Y$</span>.</p><div class="admonition is-info"><header class="admonition-header">Theorem</header><div class="admonition-body"><p>The matrix <span>$Y_\perp$</span> constructed with the above algorithm satisfies</p><p class="math-container">\[Y^TY_\perp = \mathbb{O},\]</p><p>and</p><p class="math-container">\[(Y_\perp)^TY_\perp = \mathbb{I},\]</p><p>i.e. all the columns in the big matrix <span>$[Y, Y_\perp]\in\mathbb{R}^{N\times{}N}$</span> are mutually orthonormal and it therefore is an element of <span>$SO(N)$</span>.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Proof</summary><div class="admonition-body"><p>The second property is trivially satisfied because the <span>$Q$</span> component of a <span>$QR$</span> decomposition is an orthogonal matrix. For the first property note that <span>$Y^TQR = \mathbb{O}$</span> is zero because we have subtracted the <span>$Y$</span> component from the matrix <span>$QR$</span>. The matrix <span>$R\in\mathbb{R}^{N\times{}(N-n)}$</span> further has the property <span>$[R]_{ij} = 0$</span> for <span>$i &gt; j$</span> and we have that </p><p class="math-container">\[(Y^TQ)R = [r_{11}(Y^TQ)_{1\bullet}, r_{12}(Y^TQ)_{1\bullet} + r_{22}(Y^TQ)_{2\bullet}, \ldots, \sum_{i=1}^{N-n}r_{i(N-n)}(Y^TQ)_{i\bullet}].\]</p><p>Now all the coefficients <span>$r_{ii}$</span> are non-zero because the matrix we performed the <span>$QR$</span> decomposition on has full rank and we can see that if <span>$(Y^TQ)R$</span> is zero <span>$Y^TQ$</span> also has to be zero.</p></div></details><p>We now discuss the global tangent space for the Grassmann manifold. This is similar to the Stiefel case.</p><h2 id="Global-Tangent-Space-for-the-Grassmann-Manifold"><a class="docs-heading-anchor" href="#Global-Tangent-Space-for-the-Grassmann-Manifold">Global Tangent Space for the Grassmann Manifold</a><a id="Global-Tangent-Space-for-the-Grassmann-Manifold-1"></a><a class="docs-heading-anchor-permalink" href="#Global-Tangent-Space-for-the-Grassmann-Manifold" title="Permalink"></a></h2><p>In the case of the Grassmann manifold we construct the global tangent space with respect to the distinct element <span>$\mathcal{E}=\mathrm{span}(E)\in{}Gr(n,N)$</span>, where <span>$E$</span> is again the same matrix.</p><p>The tangent tangent space <span>$T_\mathcal{E}Gr(n,N)$</span> can be represented through matrices: </p><p class="math-container">\[\begin{pmatrix}
    0 &amp; \cdots &amp; 0 \\
    \cdots &amp; \cdots &amp; \cdots \\ 
    0 &amp; \cdots &amp; 0 \\
    b_{11} &amp; \cdots &amp; b_{1n} \\
    \cdots &amp; \cdots &amp; \cdots \\ 
    b_{(N-n)1} &amp; \cdots &amp; b_{(N-n)n}
\end{pmatrix}.\]</p><p>This representation is based on the identification <span>$T_\mathcal{E}Gr(n,N)\to{}T_E\mathcal{S}_E$</span> that was discussed in the section on the <a href="../../manifolds/homogeneous_spaces/#The-Grassmann-Manifold">Grassmann manifold</a><sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>. We use the following notation:</p><p class="math-container">\[\mathfrak{g}^\mathrm{hor} = \mathfrak{g}^{\mathrm{hor},\mathcal{E}} = \left\{\begin{pmatrix} 0 &amp; -B^T \\ B &amp; 0 \end{pmatrix}: \text{$B$ arbitrary}\right\}.\]</p><p>This is equivalent to the horizontal component of <span>$\mathfrak{g}$</span> for the Stiefel manifold for the case when <span>$A$</span> is zero. This is a reflection of the rotational invariance of the Grassmann manifold: the skew-symmetric matrices <span>$A$</span> are connected to the group of rotations <span>$O(n)$</span> which is factored out in the Grassmann manifold <span>$Gr(n,N)\simeq{}St(n,N)/O(n)$</span>. In <code>GeometricMachineLearning</code> we thus treat the Grassmann manifold as being embedded in the Stiefel manifold. In [<a href="../../references/#bendokat2020grassmann">11</a>] viewing the Grassmann manifold as a quotient space of the Stiefel manifold is important for &quot;feasibility&quot; in &quot;practical computations&quot;. </p><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.AbstractLieAlgHorMatrix-arrays-global_tangent_spaces" href="#GeometricMachineLearning.AbstractLieAlgHorMatrix-arrays-global_tangent_spaces"><code>GeometricMachineLearning.AbstractLieAlgHorMatrix</code></a> — <span class="docstring-category">Type</span></header><section><div><p><code>AbstractLieAlgHorMatrix</code> is a supertype for various horizontal components of Lie algebras. We usually call this <span>$\mathfrak{g}^\mathrm{hor}$</span>.</p><p>See <a href="../../library/#GeometricMachineLearning.StiefelLieAlgHorMatrix"><code>StiefelLieAlgHorMatrix</code></a> and <a href="../../library/#GeometricMachineLearning.GrassmannLieAlgHorMatrix"><code>GrassmannLieAlgHorMatrix</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/544dfd9e3e02ed7486f51c61176e2aeeefcdf1a9/src/arrays/abstract_lie_algebra_horizontal.jl#LL1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.StiefelLieAlgHorMatrix-arrays-global_tangent_spaces" href="#GeometricMachineLearning.StiefelLieAlgHorMatrix-arrays-global_tangent_spaces"><code>GeometricMachineLearning.StiefelLieAlgHorMatrix</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">StiefelLieAlgHorMatrix(A::SkewSymMatrix{T}, B::AbstractMatrix{T}, N::Integer, n::Integer) where T</code></pre><p>Build an instance of <code>StiefelLieAlgHorMatrix</code> based on a skew-symmetric matrix <code>A</code> and an arbitrary matrix <code>B</code>.</p><p><code>StiefelLieAlgHorMatrix</code> is the <em>horizontal component of the Lie algebra of skew-symmetric matrices</em> (with respect to the canonical metric). The projection here is: <span>$\pi:S \to SE$</span> where </p><p class="math-container">\[E = \begin{pmatrix} \mathbb{I}_{n} \\ \mathbb{O}_{(N-n)\times{}n}  \end{pmatrix}.\]</p><p>The matrix <span>$E$</span> is implemented under <a href="../../library/#GeometricMachineLearning.StiefelProjection"><code>StiefelProjection</code></a> in <code>GeometricMachineLearning</code>.</p><p>An element of StiefelLieAlgMatrix takes the form: </p><p class="math-container">\[\begin{pmatrix}
A &amp; B^T \\ B &amp; \mathbb{O}
\end{pmatrix},\]</p><p>where <span>$A$</span> is skew-symmetric (this is <a href="../../library/#GeometricMachineLearning.SkewSymMatrix"><code>SkewSymMatrix</code></a> in <code>GeometricMachineLearning</code>).</p><p>Also see <a href="../../library/#GeometricMachineLearning.GrassmannLieAlgHorMatrix"><code>GrassmannLieAlgHorMatrix</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/544dfd9e3e02ed7486f51c61176e2aeeefcdf1a9/src/arrays/stiefel_lie_algebra_horizontal.jl#LL1-L22">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.StiefelLieAlgHorMatrix-Tuple{AbstractMatrix, Int64}-arrays-global_tangent_spaces" href="#GeometricMachineLearning.StiefelLieAlgHorMatrix-Tuple{AbstractMatrix, Int64}-arrays-global_tangent_spaces"><code>GeometricMachineLearning.StiefelLieAlgHorMatrix</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">StiefelLieAlgHorMatrix(D::AbstractMatrix, n::Integer)</code></pre><p>Take a big matrix as input and build an instance of <code>StiefelLieAlgHorMatrix</code> belonging to the StiefelManifold <span>$St(n, N)$</span> where <span>$N$</span> is the number of rows of <code>D</code>.</p><p>If the constructor is called with a big <span>$N\times{}N$</span> matrix, then the projection is performed the following way: </p><p class="math-container">\[\begin{pmatrix}
A &amp; B_1  \\
B_2 &amp; D
\end{pmatrix} \mapsto 
\begin{pmatrix}
\mathrm{skew}(A) &amp; -B_2^T \\ 
B_2 &amp; \mathbb{O}
\end{pmatrix}.\]</p><p>The operation <span>$\mathrm{skew}:\mathbb{R}^{n\times{}n}\to\mathcal{S}_\mathrm{skew}(n)$</span> is the skew-symmetrization operation. This is equivalent to calling of <a href="../../library/#GeometricMachineLearning.SkewSymMatrix"><code>SkewSymMatrix</code></a> with an <span>$n\times{}n$</span> matrix.</p><p>This can also be seen as the operation:</p><p class="math-container">\[D \mapsto \Omega(E, DE) = \mathrm{skew}\left(2 \left(\mathbb{I} - \frac{1}{2} E E^T \right) DE E^T\right).\]</p><p>Also see <a href="../../library/#GeometricMachineLearning.Ω-Union{Tuple{T}, Tuple{GrassmannManifold{T, AT} where AT&lt;:AbstractMatrix{T}, AbstractMatrix{T}}} where T"><code>GeometricMachineLearning.Ω</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/544dfd9e3e02ed7486f51c61176e2aeeefcdf1a9/src/arrays/stiefel_lie_algebra_horizontal.jl#LL38-L64">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.GrassmannLieAlgHorMatrix-arrays-global_tangent_spaces" href="#GeometricMachineLearning.GrassmannLieAlgHorMatrix-arrays-global_tangent_spaces"><code>GeometricMachineLearning.GrassmannLieAlgHorMatrix</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GrassmannLieAlgHorMatrix(B::AbstractMatrix{T}, N::Integer, n::Integer) where T</code></pre><p>Build an instance of <code>GrassmannLieAlgHorMatrix</code> based on an arbitrary matrix <code>B</code> of size <span>$(N-n)\times{}n$</span>.</p><p><code>GrassmannLieAlgHorMatrix</code> is the <em>horizontal component of the Lie algebra of skew-symmetric matrices</em> (with respect to the canonical metric). The projection here is: <span>$\pi:S \to SE/\sim$</span> where </p><p class="math-container">\[E = \begin{pmatrix} \mathbb{I}_{n} \\ \mathbb{O}_{(N-n)\times{}n}  \end{pmatrix},\]</p><p>and the equivalence relation is </p><p class="math-container">\[V_1 \sim V_2 \iff \exists A\in\mathcal{S}_\mathrm{skew}(n) \text{such that } V_2 = V_1 + \begin{pmatrix} A \\ \mathbb{O} \end{pmatrix}\]</p><p>An element of GrassmannLieAlgMatrix takes the form: </p><p class="math-container">\[\begin{pmatrix}
\bar{\mathbb{O}} &amp; B^T \\ B &amp; \mathbb{O}
\end{pmatrix},\]</p><p>where <span>$\bar{\mathbb{O}}\in\mathbb{R}^{n\times{}n}$</span> and <span>$\mathbb{O}\in\mathbb{R}^{(N - n)\times{}n}.$</span></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/544dfd9e3e02ed7486f51c61176e2aeeefcdf1a9/src/arrays/grassmann_lie_algebra_horizontal.jl#LL1-L25">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.GrassmannLieAlgHorMatrix-Tuple{AbstractMatrix, Int64}-arrays-global_tangent_spaces" href="#GeometricMachineLearning.GrassmannLieAlgHorMatrix-Tuple{AbstractMatrix, Int64}-arrays-global_tangent_spaces"><code>GeometricMachineLearning.GrassmannLieAlgHorMatrix</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">GrassmannLieAlgHorMatrix(D::AbstractMatrix, n::Integer)</code></pre><p>Take a big matrix as input and build an instance of <code>GrassmannLieAlgHorMatrix</code> belonging to the GrassmannManifold <span>$Gr(n, N)$</span> where <span>$N$</span> is the number of rows of <code>D</code>.</p><p>If the constructor is called with a big <span>$N\times{}N$</span> matrix, then the projection is performed the following way: </p><p class="math-container">\[\begin{pmatrix}
A &amp; B_1  \\
B_2 &amp; D
\end{pmatrix} \mapsto 
\begin{pmatrix}
\bar{\mathbb{O}} &amp; -B_2^T \\ 
B_2 &amp; \mathbb{O}
\end{pmatrix}.\]</p><p>This can also be seen as the operation:</p><p class="math-container">\[D \mapsto \Omega(E, DE - EE^TDE),\]</p><p>where <span>$\Omega$</span> is the horizontal lift <a href="../../library/#GeometricMachineLearning.Ω-Union{Tuple{T}, Tuple{GrassmannManifold{T, AT} where AT&lt;:AbstractMatrix{T}, AbstractMatrix{T}}} where T"><code>GeometricMachineLearning.Ω</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/544dfd9e3e02ed7486f51c61176e2aeeefcdf1a9/src/arrays/grassmann_lie_algebra_horizontal.jl#LL40-L64">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.GlobalSection-arrays-global_tangent_spaces" href="#GeometricMachineLearning.GlobalSection-arrays-global_tangent_spaces"><code>GeometricMachineLearning.GlobalSection</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GlobalSection(Y::AbstractMatrix)</code></pre><p>Construct a global section for <code>Y</code>.  </p><p>A global section <span>$\lambda$</span> is a mapping from a homogeneous space <span>$\mathcal{M}$</span> to the corresponding Lie group <span>$G$</span> such that </p><p class="math-container">\[\lambda(Y)E = Y,\]</p><p>Also see <a href="../../library/#GeometricMachineLearning.apply_section-Union{Tuple{AT}, Tuple{T}, Tuple{GlobalSection{T, AT}, AT}} where {T, AT&lt;:(StiefelManifold{T, AT} where AT&lt;:AbstractMatrix{T})}"><code>apply_section</code></a> and <a href="#GeometricMachineLearning.global_rep-arrays-global_tangent_spaces"><code>global_rep</code></a>.</p><p><strong>Implementation</strong></p><p>For an implementation of <code>GlobalSection</code> for a custom array (especially manifolds), the function <a href="#GeometricMachineLearning.global_section-arrays-global_tangent_spaces"><code>global_section</code></a> has to be generalized.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/544dfd9e3e02ed7486f51c61176e2aeeefcdf1a9/src/optimizers/manifold_related/global_sections.jl#LL1-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.global_section-arrays-global_tangent_spaces" href="#GeometricMachineLearning.global_section-arrays-global_tangent_spaces"><code>GeometricMachineLearning.global_section</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">global_section(Y::StiefelManifold)</code></pre><p>Compute a matrix of size <span>$N\times(N-n)$</span> whose columns are orthogonal to the columns in <code>Y</code>.</p><p>This matrix is also called <span>$Y_\perp$</span> [<a href="../../references/#absil2004riemannian">6</a>, <a href="../../references/#absil2008optimization">10</a>, <a href="../../references/#bendokat2020grassmann">11</a>].</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using GeometricMachineLearning
using GeometricMachineLearning: global_section
import Random

Random.seed!(123)

Y = StiefelManifold([1. 0.; 0. 1.; 0. 0.; 0. 0.])

round.(Matrix(global_section(Y)); digits = 3)

# output

4×2 Matrix{Float64}:
 0.0    -0.0
 0.0     0.0
 0.936  -0.353
 0.353   0.936</code></pre><p>Further note that we convert the <code>QRCompactWYQ</code> object to a <code>Matrix</code> before we display it.</p><p><strong>Implementation</strong></p><p>The implementation is done with a QR decomposition (<code>LinearAlgebra.qr!</code>). Internally we do: </p><pre><code class="language-julia hljs">A = randn(N, N - n) # or the gpu equivalent
A = A - Y.A * (Y.A&#39; * A)
qr!(A).Q</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/544dfd9e3e02ed7486f51c61176e2aeeefcdf1a9/src/manifolds/stiefel_manifold.jl#LL80-L120">source</a></section><section><div><pre><code class="language-julia hljs">global_section(Y::GrassmannManifold)</code></pre><p>Compute a matrix of size <span>$N\times(N-n)$</span> whose columns are orthogonal to the columns in <code>Y</code>.</p><p>The method <code>global_section</code> for the Grassmann manifold is equivalent to that for the <a href="../../library/#GeometricMachineLearning.StiefelManifold"><code>StiefelManifold</code></a> (we represent the Grassmann manifold as an embedding in the Stiefel manifold). </p><p>See the documentation for <a href="../../library/#GeometricMachineLearning.global_section-Union{Tuple{StiefelManifold{T, AT} where AT&lt;:AbstractMatrix{T}}, Tuple{T}} where T"><code>global_section(Y::StiefelManifold{T}) where T</code></a>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/544dfd9e3e02ed7486f51c61176e2aeeefcdf1a9/src/manifolds/grassmann_manifold.jl#LL63-L71">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.global_rep-arrays-global_tangent_spaces" href="#GeometricMachineLearning.global_rep-arrays-global_tangent_spaces"><code>GeometricMachineLearning.global_rep</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">global_rep(λY::GlobalSection{T, AT}, Δ::AbstractMatrix{T}) where {T, AT&lt;:StiefelManifold{T}}</code></pre><p>Express <code>Δ</code> (an the tangent space of <code>Y</code>) as an instance of <code>StiefelLieAlgHorMatrix</code>.</p><p>This maps an element from <span>$T_Y\mathcal{M}$</span> to an element of <span>$\mathfrak{g}^\mathrm{hor}$</span>. </p><p>These two spaces are isomorphic where the isomorphism where the isomorphism is established through <span>$\lambda(Y)\in{}G$</span> via:</p><p class="math-container">\[T_Y\mathcal{M} \to \mathfrak{g}^{\mathrm{hor}}, \Delta \mapsto \lambda(Y)^{-1}\Omega(Y, \Delta)\lambda(Y).\]</p><p>Also see <a href="../../library/#GeometricMachineLearning.Ω-Union{Tuple{T}, Tuple{GrassmannManifold{T, AT} where AT&lt;:AbstractMatrix{T}, AbstractMatrix{T}}} where T"><code>GeometricMachineLearning.Ω</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using GeometricMachineLearning
using GeometricMachineLearning: _round
import Random 

Random.seed!(123)

Y = rand(StiefelManifold, 6, 3)
Δ = rgrad(Y, randn(6, 3))
λY = GlobalSection(Y)

_round(global_rep(λY, Δ); digits = 3)

# output

6×6 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:
  0.0     0.679   1.925   0.981  -2.058   0.4
 -0.679   0.0     0.298  -0.424   0.733  -0.919
 -1.925  -0.298   0.0    -1.815   1.409   1.085
 -0.981   0.424   1.815   0.0     0.0     0.0
  2.058  -0.733  -1.409   0.0     0.0     0.0
 -0.4     0.919  -1.085   0.0     0.0     0.0</code></pre><p><strong>Implementation</strong></p><p>The function <code>global_rep</code> does in fact not perform the entire map <span>$\lambda(Y)^{-1}\Omega(Y, \Delta)\lambda(Y)$</span> but only</p><p class="math-container">\[\Delta \mapsto \mathrm{skew}(Y^T\Delta),\]</p><p>to get the small skew-symmetric matrix and </p><p class="math-container">\[\Delta \mapsto (\lambda(Y)_{[1:N, n:N]}^T \Delta)_{[1:(N-n), 1:n]},\]</p><p>for the arbitrary matrix.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/544dfd9e3e02ed7486f51c61176e2aeeefcdf1a9/src/optimizers/manifold_related/global_sections.jl#LL127-L183">source</a></section><section><div><pre><code class="language-julia hljs">global_rep(λY::GlobalSection{T, AT}, Δ::AbstractMatrix{T}) where {T, AT&lt;:GrassmannManifold{T}}</code></pre><p>Express <code>Δ</code> (an the tangent space of <code>Y</code>) as an instance of <code>GrassmannLieAlgHorMatrix</code>.</p><p>The method <code>global_rep</code> for <a href="../../library/#GeometricMachineLearning.GrassmannManifold"><code>GrassmannManifold</code></a> is similar to that for <a href="../../library/#GeometricMachineLearning.StiefelManifold"><code>StiefelManifold</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using GeometricMachineLearning
using GeometricMachineLearning: _round
import Random 

Random.seed!(123)

Y = rand(GrassmannManifold, 6, 3)
Δ = rgrad(Y, randn(6, 3))
λY = GlobalSection(Y)

_round(global_rep(λY, Δ); digits = 3)

# output

6×6 GrassmannLieAlgHorMatrix{Float64, Matrix{Float64}}:
  0.0     0.0     0.0     0.981  -2.058   0.4
  0.0     0.0     0.0    -0.424   0.733  -0.919
  0.0     0.0     0.0    -1.815   1.409   1.085
 -0.981   0.424   1.815   0.0     0.0     0.0
  2.058  -0.733  -1.409   0.0     0.0     0.0
 -0.4     0.919  -1.085   0.0     0.0     0.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/544dfd9e3e02ed7486f51c61176e2aeeefcdf1a9/src/optimizers/manifold_related/global_sections.jl#LL194-L226">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[6]</dt><dd><div>P.-A. Absil, R. Mahony and R. Sepulchre. <em>Riemannian geometry of Grassmann manifolds with a view on algorithmic computation</em>. Acta Applicandae Mathematica <strong>80</strong>, 199–220 (2004).</div></dd><dt>[10]</dt><dd><div>P.-A. Absil, R. Mahony and R. Sepulchre. <em>Optimization algorithms on matrix manifolds</em> (Princeton University Press, Princeton, New Jersey, 2008).</div></dd><dt>[11]</dt><dd><div>T. Bendokat, R. Zimmermann and P.-A. Absil. <em>A Grassmann manifold handbook: Basic geometry and computational aspects</em>, arXiv preprint arXiv:2011.13699 (2020).</div></dd><dt>[48]</dt><dd><div>B. Brantner. <em>Generalizing Adam To Manifolds For Efficiently Training Transformers</em>, arXiv preprint arXiv:2305.16901 (2023).</div></dd></dl></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>We already introduced this special matrix together with the Stiefel manifold.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>We derived the following expression for the <a href="../../manifolds/homogeneous_spaces/#The-Riemannian-Gradient-of-the-Grassmann-Manifold">Riemannian gradient of the Grassmann manifold</a>: <span>$\mathrm{grad}_\mathcal{Y}^{Gr}L = \nabla_Y{}L - YY^T\nabla_YL$</span>. The tangent space to the element <span>$\mathcal{E}$</span> can thus be written as <span>$\bar{B} - EE^T\bar{B}$</span> where <span>$B\in\mathbb{R}^{N\times{}n}$</span> and the matrices in this tangent space have the desired form. </li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../skew_symmetric_matrix/">« Symmetric and Skew-Symmetric Matrices</a><a class="docs-footer-nextpage" href="../../pullbacks/computation_of_pullbacks/">Pullbacks »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Tuesday 2 July 2024 09:15">Tuesday 2 July 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
