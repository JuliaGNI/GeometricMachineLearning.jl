<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>BFGS Optimizer · GeometricMachineLearning.jl</title><meta name="title" content="BFGS Optimizer · GeometricMachineLearning.jl"/><meta property="og:title" content="BFGS Optimizer · GeometricMachineLearning.jl"/><meta property="twitter:title" content="BFGS Optimizer · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/bfgs_optimizer/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/bfgs_optimizer/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/bfgs_optimizer/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Optimizers</span><ul><li><a class="tocitem" href="../optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../manifold_related/global_sections/">Global Sections</a></li><li><a class="tocitem" href="../manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../optimizer_methods/">Optimizer Methods</a></li><li class="is-active"><a class="tocitem" href>BFGS Optimizer</a><ul class="internal"><li><a class="tocitem" href="#The-Riemannian-Version-of-the-BFGS-Algorithm"><span>The Riemannian Version of the BFGS Algorithm</span></a></li><li><a class="tocitem" href="#The-Curvature-Condition-and-the-Wolfe-Conditions"><span>The Curvature Condition and the Wolfe Conditions</span></a></li><li><a class="tocitem" href="#Initialization-of-the-BFGS-Algorithm"><span>Initialization of the BFGS Algorithm</span></a></li><li><a class="tocitem" href="#Stability-of-the-Algorithm"><span>Stability of the Algorithm</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modelling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Network Losses</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_autoencoder/">PSD and Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/kolmogorov_n_width/">Kolmogorov n-width</a></li><li><a class="tocitem" href="../../reduced_order_modeling/projection_reduction_errors/">Projection and Reduction Error</a></li></ul></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">Sympnets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../library/">Library</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Optimizers</a></li><li class="is-active"><a href>BFGS Optimizer</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>BFGS Optimizer</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/optimizers/bfgs_optimizer.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="The-BFGS-Optimizer"><a class="docs-heading-anchor" href="#The-BFGS-Optimizer">The BFGS Optimizer</a><a id="The-BFGS-Optimizer-1"></a><a class="docs-heading-anchor-permalink" href="#The-BFGS-Optimizer" title="Permalink"></a></h1><p>The presentation shown here is largely taken from [<a href="../../references/#wright2006numerical">22</a>, chapters 3 and 6] with a derivation based on an online comment [<a href="../../references/#2279304">23</a>]. The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm is a second order optimizer that can be also be used to train a neural network.</p><p>It is a version of a <em>quasi-Newton</em> method and is therefore especially suited for convex problems. As is the case with any other (quasi-)Newton method<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> the BFGS algorithm approximates the objective with a quadratic function in each optimization step:</p><p class="math-container">\[m^{(k)}(x) = L(x^{(k)}) + (\nabla_{x^{(k)}}L)^T(x - x^{(k)}) + \frac{1}{2}(x - x^{(k)})^TR^{(k)}(x - x^{(k)}),\]</p><p>where <span>$R^{(k)}$</span> is referred to as the <em>approximate Hessian</em>. We further require <span>$R^{(k)}$</span> to be symmetric and positive definite. Differentiating the above expression and setting the derivative to zero gives us: </p><p class="math-container">\[\nabla_xm^{(k)} = \nabla_{x^{(k)}}L + R^{(k)}(x - x^{(k)}) = 0,\]</p><p>or written differently: </p><p class="math-container">\[x - x^{(k)} = -(R^{(k)})^{-1}\nabla_{x^{(k)}}L.\]</p><p>This value we will from now on call <span>$p^{(k)} := -H^{(k)}\nabla_{x_k}L$</span> with <span>$H^{(k)} := (R^{(k)})^{-1}$</span> and refer to as the <em>search direction</em>. The new iterate then is: </p><p class="math-container">\[x^{(k+1)} = x^{(k)} + \eta^{(k)}p^{(k)},\]</p><p>where <span>$\eta^{(k)}$</span> is the <em>step length</em>. Techniques that describe how to pick an appropriate <span>$\eta^{(k)}$</span> are called <em>line-search methods</em> and are discussed below. First we discuss what requirements we impose on <span>$R^{(k)}$</span>. A first reasonable condition would be to require the gradient of <span>$m^{(k)}$</span> to be equal to that of <span>$L$</span> at the points <span>$x^{(k-1)}$</span> and <span>$x^{(k)}$</span>: </p><p class="math-container">\[\begin{aligned}
\nabla_{x^{(k)}}m^{(k)}  &amp; = \nabla_{x^{(k)}}L + R^{(k)}(x^{(k)} - x^{(k)})  &amp; \overset{!}{=} &amp; \nabla_{x^{(k)}}L \text{ and } \\
\nabla_{x^{(k-1)}}m^{(k)} &amp; = \nabla_{x^{(k)}}L + R^{(k)}(x^{(k-1)} - x^{(k)}) &amp; \overset{!}{=} &amp; \nabla_{x^{(k-1)}}L.
\end{aligned}\]</p><p>The first one of these conditions is automatically satisfied. The second one can be rewritten as: </p><p class="math-container">\[R^{(k)}(x^{(k)} - x^{(k-1)}) \overset{!}{=} \nabla_{x^{(k)}}L - \nabla_{x^{(k-1)}}L. \]</p><p>The following notations are often used: </p><p class="math-container">\[s^{(k-1)} := \eta^{(k-1)}p^{(k-1)} :=  x^{(k)} - x^{(k-1)} \quad\text{ and }\quad y^{(k-1)} := \nabla_{x^(k)}L - \nabla_{x^{(k-1)}}L.\]</p><p>The condition mentioned above then becomes: </p><p class="math-container">\[R^{(k)}s^{(k-1)} \overset{!}{=} y^{(k-1)},\]</p><p>and we call it the <em>secant equation</em>. </p><p>In order to pick the ideal <span>$R^{(k)}$</span> we solve the following problem: </p><p class="math-container">\[\begin{aligned}
\min_R &amp; ||R &amp; - R^{(k-1)}||_W \\ 
\text{s.t.} &amp; R  &amp; = R^T\quad\text{and}\\
            &amp; Rs^{(k-1)} &amp; = y^{(k-1)},
\end{aligned}\]</p><p>where the first condition is symmetry and the second one is the secant equation. For the norm <span>$||\cdot||_W$</span> we pick the weighted Frobenius norm:</p><p class="math-container">\[||A||_W := ||W^{1/2}AW^{1/2}||_F,\]</p><p>where <span>$||\cdot||_F$</span> is the usual Frobenius norm<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup> and the matrix <span>$W=\tilde{R}^{(k-1)}$</span> is the inverse of the <em>average Hessian</em>:</p><p class="math-container">\[\tilde{R}^{(k-1)} = \int_0^1 \nabla^2f(x^{(k-1)} + \tau\eta^{(k-1)}p^{(k-1)})d\tau.\]</p><p>We now state the solution to this minimization problem:</p><div class="admonition is-info"><header class="admonition-header">Theorem</header><div class="admonition-body"><p>The solution of the minimization problem is:</p><p class="math-container">\[R^{(k)} = (\mathbb{I} - \frac{1}{(y^{(k-1)})^Ts^{(k-1)}}y^{(k-1)}(s^{(k-1)})^T)R^{(k-1)}(\mathbb{I} - \frac{1}{y^({k-1})^Ts^{(k-1)}}s^{(k-1)}(y^{(k-1)})^T) + \\ \frac{1}{(y^{(k-1)})^Ts^{(k-1)}}y^{(k)}(y^{(k)})^T,\]</p><p>with <span>$y^{(k-1)} = \nabla_{x^{(k)}}L - \nabla_{x^{(k-1)}}L$</span> and <span>$s^{(k-1)} = x^{(k)} - x^{(k-1)}$</span> as above.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Proof</summary><div class="admonition-body"><p>In order to find the ideal <span>$R^{(k)}$</span> under the conditions described above, we introduce some notation: </p><ul><li><span>$\tilde{R}^{(k-1)} := W^{1/2}R^{(k-1)}W^{1/2}$</span>,</li><li><span>$\tilde{R} := W^{1/2}RW^{1/2}$</span>, </li><li><span>$\tilde{y}^{(k-1)} := W^{-1/2}y^{(k-1)}$</span>, </li><li><span>$\tilde{s}^{(k-1)} := W^{1/2}s^{(k-1)}$</span>.</li></ul><p>With this notation we can rewrite the problem of finding <span>$R^{(k)}$</span> as: </p><p class="math-container">\[\begin{aligned}
\min_{\tilde{R}} &amp; ||\tilde{R} - \tilde{R}^{(k-1)}||_F &amp; \\ 
\text{s.t.}\quad &amp; \tilde{R} = \tilde{R}^T\quad &amp; \text{and}\\
            &amp;\tilde{R}\tilde{s}^{(k-1)} = \tilde{y}^{(k-1)}&amp;.
\end{aligned}\]</p><p>We further have <span>$y^{(k-1)} = Ws^{(k-1)}$</span> and hence <span>$\tilde{y}^{(k-1)} = \tilde{s}^{(k-1)}$</span> by a corollary of the mean value theorem: <span>$\int_0^1 g&#39;(\xi_1 + \tau(\xi_2 - \xi_1)) d\tau (\xi_2 - \xi_1) = g(\xi_2 - \xi_1)$</span> for a vector-valued function <span>$g$</span>.</p><p>Now we rewrite <span>$R$</span> and <span>$R^{(k-1)}$</span> in a new basis <span>$U = [u|u_\perp]$</span>, where <span>$u := \tilde{s}_{k-1}/||\tilde{s}_{k-1}||$</span> and <span>$u_\perp$</span> is an orthogonal complement of <span>$u$</span> (i.e. we have <span>$u^Tu_\perp=0$</span> and <span>$u_\perp^Tu_\perp=\mathbb{I}$</span>):</p><p class="math-container">\[\begin{aligned}
U^T\tilde{R}^{(k-1)}U - U^T\tilde{R}U = \begin{bmatrix}  u^T \\ u_\perp^T \end{bmatrix}(\tilde{R}^{(k-1)} - \tilde{R})\begin{bmatrix} u &amp; u_\perp \end{bmatrix} = \\
\begin{bmatrix}
    u^T\tilde{R}^{(k-1)}u - 1 &amp; u^T\tilde{R}^{(k-1)}u \\
    u_\perp^T\tilde{R}^{(k-1)}u &amp; u_\perp^T(\tilde{R}^{(k-1)}-\tilde{R}^{(k)})u_\perp
\end{bmatrix}.
\end{aligned}\]</p><p>By a property of the Frobenius norm we can consider the blocks independently: </p><p class="math-container">\[||\tilde{R}^{(k-1)} - \tilde{R}||^2_F = (u^T\tilde{R}^{(k-1)}u -1)^2 + ||u^T\tilde{R}^{(k-1)}u_\perp||_F^2 + ||u_\perp^T\tilde{R}^{(k-1)}u||_F^2 + ||u_\perp^T(\tilde{R}^{(k-1)} - \tilde{R})u_\perp||_F^2\]</p><p>We see that <span>$\tilde{R}$</span> only appears in the last term, which should therefore be made zero, i.e. the projections of <span>$\tilde{R}_{k-1}$</span> and <span>$\tilde{R}$</span> onto the space spanned by <span>$u_\perp$</span> should coincide. With the condition <span>$\tilde{R}u \overset{!}{=} u$</span> w hence get: </p><p class="math-container">\[\tilde{R} = U\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; u^T_\perp\tilde{R}^{(k-1)}u_\perp \end{bmatrix}U^T = uu^T + (\mathbb{I}-uu^T)\tilde{R}^{(k-1)}(\mathbb{I}-uu^T).\]</p><p>If we now map back to the original coordinate system, the ideal solution for <span>$R^{(k)}$</span> is: </p><p class="math-container">\[R^{(k)} = (\mathbb{I} - \frac{1}{(y^{(k-1)})^Ts^{(k-1)}}y^{(k-1)}(s^{(k-1)})^T)R^{(k-1)}(\mathbb{I} - \frac{1}{(y^{k-1})^Ts^{(k-1)}}s^{(k-1)}(y^{(k-1)})^T) + \\ \frac{1}{(y^{(k-1)})^Ts^{(k-1)}}y^{(k)}(y^{(k)})^T,\]</p><p>and the assertion is proved.</p></div></details><p>What we need in practice however is not <span>$R^{(k)}$</span>, but its inverse <span>$H^{(k)}$</span>. This is because we need to find <span>$s^{(k-1)}$</span> based on <span>$y^{(k-1)}$</span>.  To get <span>$H^{(k)}$</span> based on the expression for <span>$R^{(k)}$</span> above we can use the <em>Sherman-Morrison-Woodbury formula</em><sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup> to obtain:</p><p class="math-container">\[H^{(k)} = H^{(k-1)} - \frac{H^{(k-1)}y^{(k-1)}(y^{(k-1)})^TH^{(k-1)}}{(y^{(k-1)})^TH^{(k-1)}y^{(k-1)}} + \frac{s^{(k-1)}(s^{(k-1)})^T}{(y^{(k-1)})^Ts^{(k-1)}}.\]</p><p>The cache and the parameters are updated with:</p><ol><li>Compute the gradient <span>$\nabla_{x^{(k)}}L$</span>,</li><li>obtain a negative search direction <span>$p^{(k)} \gets H^{(k)}\nabla_{x^{(k)}}L$</span>,</li><li>compute <span>$s^{(k)} = -\eta^{(k)}p^{(k)}$</span>,</li><li>update <span>$x^{(k + 1)} \gets x^{(k)} + s^{(k)}$</span>,</li><li>compute <span>$y^{(k)} \gets \nabla_{x^{(k)}}L - \nabla_{x^{(k-1)}}L$</span>,</li><li>update <span>$H^{(k + 1)} \gets H^{(k)} - \frac{H^{(k)}y^{(k)}(y^{(k)})^TH^{(k)}}{(y^{(k)})^TH^{(k)}y^{(k)}} + \frac{s^{(k)}(s^{(k)})^T}{(y^{(k)})^Ts^{(k)}}$</span>.</li></ol><p>The cache of the BFGS algorithm thus consists of the matrix <span>$H^{(\cdot)}$</span> for each vector <span>$x^{(\cdot)}$</span> in the neural network and the gradient for the previous time step <span>$\nabla_{x^{(k-1)}}L$</span>. <span>$s^{(k)}$</span> here is again the <em>velocity</em> that we use to update the neural network weights. </p><h2 id="The-Riemannian-Version-of-the-BFGS-Algorithm"><a class="docs-heading-anchor" href="#The-Riemannian-Version-of-the-BFGS-Algorithm">The Riemannian Version of the BFGS Algorithm</a><a id="The-Riemannian-Version-of-the-BFGS-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#The-Riemannian-Version-of-the-BFGS-Algorithm" title="Permalink"></a></h2><p>Generalizing the BFGS algorithm to the setting of a Riemannian manifold is very straightforward. All we have to do is replace Euclidean gradient by Riemannian ones (composed with a horizontal lift): </p><p class="math-container">\[\nabla_{x^{(k)}}L \implies (\Lambda^{(k)})^{-1}(\Omega\circ\mathrm{grad}_{x^{(k)}}L)\Lambda^{(k)},\]</p><p>and addition by a retraction:</p><p class="math-container">\[    x^{(k+1)} \gets x^{(k)} + s^{(k)} \implies x^{(k+1)} \gets \mathrm{Retraction}(s^{(k)})x^{(k)}.\]</p><p>If we deal with manifolds however we cannot simply take differences. But we do have however:</p><p class="math-container">\[x^{(k+1)} = x\]</p><h2 id="The-Curvature-Condition-and-the-Wolfe-Conditions"><a class="docs-heading-anchor" href="#The-Curvature-Condition-and-the-Wolfe-Conditions">The Curvature Condition and the Wolfe Conditions</a><a id="The-Curvature-Condition-and-the-Wolfe-Conditions-1"></a><a class="docs-heading-anchor-permalink" href="#The-Curvature-Condition-and-the-Wolfe-Conditions" title="Permalink"></a></h2><p>In textbooks [<a href="../../references/#wright2006numerical">22</a>] an application of the BFGS algorithm typically further involves a line search subject to the <em>Wolfe conditions</em>. If these are satisfied the <em>curvature condition</em> usually also is.</p><p>Before we discussed imposing the <em>secant condition</em> on <span>$R^{(k)}$</span>. Another condition is that <span>$R^{(k)}$</span> has to be positive-definite at point <span>$s^{(k-1)}$</span>:</p><p class="math-container">\[(s^{(k-1)})^Ty^{(k-1)} &gt; 0.\]</p><p>This is referred to as the <em>curvature condition</em>. If we impose the <em>Wolfe conditions</em>, the <em>curvature condition</em> holds automatically. The Wolfe conditions are stated with respect to the parameter <span>$\eta^{(k)}$</span>.</p><div class="admonition is-info"><header class="admonition-header">Definition</header><div class="admonition-body"><p>The <strong>Wolfe conditions</strong> are:</p><p class="math-container">\[\begin{aligned}
L(x^{(k)}+\eta^{(k)}p^{(k)}) &amp; \leq{}L(x^{(k)}) + c_1\eta^{(k)}(\nabla_{x^{(k)}}L)^Tp^{(k)} &amp; \text{ for } &amp; c_1\in(0,1) \quad\text{and} \\
(\nabla_{(x^{(k)} + \eta^{(k)}p^{(k)})}L)^Tp^{(k)} &amp; \geq c_2(\nabla_{x^{(k)}}L)^Tp^{(k)} &amp; \text{ for } &amp; c_2\in(c_1,1).
\end{aligned}\]</p><p>The two Wolfe conditions above are respectively called the <em>sufficient decrease condition</em> and the <em>curvature condition</em> respectively.</p></div></div><p>A possible choice for <span>$c_1$</span> and <span>$c_2$</span> are <span>$10^{-4}$</span> and <span>$0.9$</span> [<a href="../../references/#wright2006numerical">22</a>]. We further have:</p><div class="admonition is-info"><header class="admonition-header">Theorem</header><div class="admonition-body"><p>The second Wolfe condition, also called curvature condition, is stronger than the curvature condition mentioned before under the assumption that the first Wolfe condition is true and <span>$L(x^{(k+1)}) &lt; L(^{(x_k)})$</span>.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Proof</summary><div class="admonition-body"><p>We use the second Wolfe condition to write</p><p class="math-container">\[(\nabla_{x^{(k)}}L)^Tp^{(k-1)} - c_2(\nabla_{x^{(k-1)}}L)^Tp^{(k-1)} = (y^{(k-1)})^Tp^{(k-1)} + (1 - c_2)(\nabla_{x^{(k-1)}}L)^Tp^{(k-1)} \geq 0,\]</p><p>and we can apply the first Wolfe condition on the second term in this expression: </p><p class="math-container">\[(1 - c_2)(\nabla_{x^{(k-1)}}L)^Tp^{(k-1)}\geq\frac{1-c_2}{c_1\eta^{(k-1)}}(L(x^{(k)}) - L(x^{(k-1)})),\]</p><p>which is negative if the value of <span>$L$</span> is decreasing.</p></div></details><h2 id="Initialization-of-the-BFGS-Algorithm"><a class="docs-heading-anchor" href="#Initialization-of-the-BFGS-Algorithm">Initialization of the BFGS Algorithm</a><a id="Initialization-of-the-BFGS-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Initialization-of-the-BFGS-Algorithm" title="Permalink"></a></h2><p>We initialize <span>$H^{(0)}$</span> with the identity matrix <span>$\mathbb{I}$</span> and the gradient information <span>$B$</span> with zeros. </p><pre><code class="language-julia hljs">using GeometricMachineLearning

weight = (Y = rand(StiefelManifold, 10, 5), )
method = BFGSOptimizer()
o = Optimizer(method, weight)

o.cache.Y.H</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">35×35 Matrix{Float64}:
 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0
 ⋮                        ⋮              ⋱            ⋮                   
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     1.0  0.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  1.0  0.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  1.0  0.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  1.0  0.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  1.0  0.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  1.0  0.0
 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  1.0</code></pre><p>This is a matrix of size <span>$35\times35.$</span> This is because the skew-symmetric <span>$A$</span>-part of an element of <span>$\mathfrak{g}^\mathrm{hor}$</span> has 10 elements and the <span>$B$</span>-part has 25 elements. </p><p>The <em>previous gradient</em> in <a href="../../library/#GeometricMachineLearning.BFGSCache"><code>BFGSCache</code></a> is stored in the same way as it is in e.g. the <a href="../../library/#GeometricMachineLearning.MomentumOptimizer"><code>MomentumOptimizer</code></a>:</p><pre><code class="language-julia hljs">o.cache.Y.B</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">10×10 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:
 0.0  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0
 0.0   0.0  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0
 0.0   0.0   0.0  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0
 0.0   0.0   0.0   0.0  -0.0  -0.0  -0.0  -0.0  -0.0  -0.0
 0.0   0.0   0.0   0.0   0.0  -0.0  -0.0  -0.0  -0.0  -0.0
 0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0
 0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0
 0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0
 0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0
 0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0</code></pre><h2 id="Stability-of-the-Algorithm"><a class="docs-heading-anchor" href="#Stability-of-the-Algorithm">Stability of the Algorithm</a><a id="Stability-of-the-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Stability-of-the-Algorithm" title="Permalink"></a></h2><p>Similar to the <a href="../optimizer_methods/#The-Adam-Optimizer">Adam optimizer</a> we also add a <span>$\delta$</span> term for stability to two of the terms appearing in the update rule of the BFGS algorithm in practice. </p><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.BFGSOptimizer-optimizers-bfgs_optimizer" href="#GeometricMachineLearning.BFGSOptimizer-optimizers-bfgs_optimizer"><code>GeometricMachineLearning.BFGSOptimizer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BFGSOptimizer(η, δ)</code></pre><p>Make an instance of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) optimizer. </p><p><code>η</code> is the <em>learning rate</em>. <code>δ</code> is a stabilization parameter.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/544dfd9e3e02ed7486f51c61176e2aeeefcdf1a9/src/optimizers/bfgs_optimizer.jl#LL1-L8">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.BFGSCache-optimizers-bfgs_optimizer" href="#GeometricMachineLearning.BFGSCache-optimizers-bfgs_optimizer"><code>GeometricMachineLearning.BFGSCache</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BFGSCache(B)</code></pre><p>Make the cache for the BFGS optimizer based on the array <code>B</code>.</p><p>It stores an array for the gradient of the previous time step <code>B</code> and the inverse of the Hessian matrix <code>H</code>.</p><p>The cache for the inverse of the Hessian is initialized with the idendity. The cache for the previous gradient information is initialized with the zero vector.</p><p>Note that the cache for <code>H</code> is changed iteratively, whereas the cache for <code>B</code> is newly assigned at every time step.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/544dfd9e3e02ed7486f51c61176e2aeeefcdf1a9/src/optimizers/bfgs_cache.jl#LL1-L12">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[22]</dt><dd><div>J. N. Stephen J. Wright. <em>Numerical optimization</em> (Springer Science+Business Media, New York, NY, 2006).</div></dd><dt>[23]</dt><dd><div>A. (https://math.stackexchange.com/users/253273/a-%ce%93). <em>Quasi-newton methods: Understanding DFP updating formula</em>. Mathematics Stack Exchange. URL:https://math.stackexchange.com/q/2279304 (version: 2017-05-13).</div></dd><dt>[53]</dt><dd><div>W. Huang, P.-A. Absil and K. A. Gallivan. <em>A Riemannian BFGS method for nonconvex optimization problems</em>. In: <em>Numerical Mathematics and Advanced Applications ENUMATH 2015</em> (Springer, 2016); pp. 627–634.</div></dd></dl></div><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>Various Newton methods and quasi-Newton methods differ in how they model the <em>approximate Hessian</em>.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>The Frobenius norm is <span>$||A||_F^2 = \sum_{i,j}a_{ij}^2$</span>.</li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>The <em>Sherman-Morrison-Woodbury formula</em> states <span>$(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}$</span>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../optimizer_methods/">« Optimizer Methods</a><a class="docs-footer-nextpage" href="../../layers/sympnet_gradient/">Sympnet Layers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Tuesday 2 July 2024 09:15">Tuesday 2 July 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
