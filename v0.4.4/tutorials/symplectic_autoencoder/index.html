<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Symplectic Autoencoders · GeometricMachineLearning.jl</title><meta name="title" content="Symplectic Autoencoders · GeometricMachineLearning.jl"/><meta property="og:title" content="Symplectic Autoencoders · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Symplectic Autoencoders · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/symplectic_autoencoder/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/symplectic_autoencoder/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/tutorials/symplectic_autoencoder/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/hamiltonian_neural_network/">Hamiltonian Neural Network</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../architectures/symplectic_transformer/">Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../hamiltonian_neural_network/">Hamiltonian Neural Network</a></li><li class="is-active"><a class="tocitem" href>Symplectic Autoencoders</a><ul class="internal"><li><a class="tocitem" href="#The-system"><span>The system</span></a></li><li><a class="tocitem" href="#Get-the-data"><span>Get the data</span></a></li><li><a class="tocitem" href="#Train-the-network"><span>Train the network</span></a></li><li><a class="tocitem" href="#The-online-stage-with-a-standard-integrator"><span>The online stage with a standard integrator</span></a></li><li><a class="tocitem" href="#The-online-stage-with-a-neural-network"><span>The online stage with a neural network</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li><li class="toplevel"><a class="tocitem" href="#References-2"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../matrix_softmax/">Matrix Attention</a></li><li><a class="tocitem" href="../volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../symplectic_transformer/">Symplectic Transformer</a></li><li><a class="tocitem" href="../adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Symplectic Autoencoders</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Symplectic Autoencoders</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/tutorials/symplectic_autoencoder.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Symplectic-Autoencoders-and-the-Toda-Lattice"><a class="docs-heading-anchor" href="#Symplectic-Autoencoders-and-the-Toda-Lattice">Symplectic Autoencoders and the Toda Lattice</a><a id="Symplectic-Autoencoders-and-the-Toda-Lattice-1"></a><a class="docs-heading-anchor-permalink" href="#Symplectic-Autoencoders-and-the-Toda-Lattice" title="Permalink"></a></h1><p>In this tutorial we use a <a href="../../architectures/symplectic_autoencoder/#The-Symplectic-Autoencoder">symplectic autoencoder</a> to approximate the solution of the Toda lattice with a lower-dimensional Hamiltonian model and compare it with standard <a href="../../reduced_order_modeling/symplectic_mor/#Proper-Symplectic-Decomposition">proper symplectic decomposition</a>.</p><div class="admonition is-success" id="Remark-4bbfd86903465d83"><header class="admonition-header">Remark<a class="admonition-anchor" href="#Remark-4bbfd86903465d83" title="Permalink"></a></header><div class="admonition-body"><p>As with any neural network we have to make the following choices:</p><ol><li>specify the <em>architecture</em>,</li><li>specify the <em>type</em> and <em>backend</em>,</li><li>pick an <em>optimizer</em> for training the network,</li><li>specify how you want to perform <em>batching</em>,</li><li>choose a <em>number of epochs</em>,</li></ol><p>where points 1 and 3 depend on a variable number of hyperparameters.</p></div></div><p>For the symplectic autoencoder point 1 is done by calling <a href="../../architectures/symplectic_autoencoder/#GeometricMachineLearning.SymplecticAutoencoder"><code>SymplecticAutoencoder</code></a>, point 2 is done by calling <code>NeuralNetwork</code>, point 3 is done by calling <a href="../../optimizers/optimizer_framework/#GeometricMachineLearning.Optimizer"><code>Optimizer</code></a> and point 4 is done by calling <a href="../../data_loader/data_loader/#GeometricMachineLearning.Batch"><code>Batch</code></a>.</p><h2 id="The-system"><a class="docs-heading-anchor" href="#The-system">The system</a><a id="The-system-1"></a><a class="docs-heading-anchor-permalink" href="#The-system" title="Permalink"></a></h2><p>The Toda lattice [<a href="../../references/#toda1967vibration">89</a>] is a prototypical example of a Hamiltonian PDE. It is described by </p><p class="math-container">\[    H(q, p) = \sum_{n\in\mathbb{Z}}\left(  \frac{p_n^2}{2} + \alpha e^{q_n - q_{n+1}} \right).\]</p><p>Starting from this equation we further assume a finite number of particles <span>$N$</span> and impose periodic boundary conditions: </p><p class="math-container">\[\begin{aligned}
    q_{n+N} &amp;  \equiv q_n \\ 
    p_{n+N} &amp;   \equiv p_n.
\end{aligned}\]</p><p>In this tutorial we want to reduce the dimension of the big system by a significant factor with (i) proper symplectic decomposition (PSD) and (ii) symplectic autoencoders (SAE). The first approach is strictly linear whereas the second one allows for more general mappings. </p><h3 id="Using-the-Toda-lattice-in-numerical-experiments"><a class="docs-heading-anchor" href="#Using-the-Toda-lattice-in-numerical-experiments">Using the Toda lattice in numerical experiments</a><a id="Using-the-Toda-lattice-in-numerical-experiments-1"></a><a class="docs-heading-anchor-permalink" href="#Using-the-Toda-lattice-in-numerical-experiments" title="Permalink"></a></h3><p>In order to use the Toda lattice in numerical experiments we have to pick suitable initial conditions. For this, consider the third-degree spline: </p><p class="math-container">\[h(s)  = \begin{cases}
        1 - \frac{3}{2}s^2 + \frac{3}{4}s^3 &amp; \text{if } 0 \leq s \leq 1 \\ 
        \frac{1}{4}(2 - s)^3 &amp; \text{if } 1 &lt; s \leq 2 \\ 
        0 &amp; \text{else.} 
\end{cases}\]</p><p>Plotted on the relevant domain it looks like this: </p><p><img src="../../tikz/third_degree_spline_light.png" alt/> <img src="../../tikz/third_degree_spline_dark.png" alt/></p><p>We end up with the following choice of parametrized initial conditions: </p><p class="math-container">\[u_0(\mu)(\omega) = h(s(\omega, \mu)), \quad s(\omega, \mu) =  20 \mu  |\omega + \frac{\mu}{2}|,\]</p><p>where the <span>$\omega$</span> is an element of the domain <span>$\Omega = [-0.5, 0.5].$</span> For the purposes of this tutorial we will use the default value for <span>$\mu$</span> provided in <a href="https://github.com/JuliaGNI/GeometricProblems.jl"><code>GeometricProblems</code></a>:</p><pre><code class="language-julia hljs">import GeometricProblems.TodaLattice as tl

tl.μ</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.3</code></pre><p>We thus look at the displacement of <span>$N = 200$</span> particles on the periodic domain <span>$\Omega = [-0.5, 0.5]/~ \simeq S^1$</span> where the equivalence relation <span>$~$</span> indicates that we associate the points <span>$-0.5$</span> and <span>$0.5$</span> with each other.</p><h2 id="Get-the-data"><a class="docs-heading-anchor" href="#Get-the-data">Get the data</a><a id="Get-the-data-1"></a><a class="docs-heading-anchor-permalink" href="#Get-the-data" title="Permalink"></a></h2><p>The training data can very easily be obtained by using the packages <a href="https://github.com/JuliaGNI/GeometricProblems.jl"><code>GeometricProblems</code></a> and <a href="https://github.com/JuliaGNI/GeometricIntegrators.jl"><code>GeometricIntegrators</code></a>:</p><pre><code class="language-julia hljs">using GeometricIntegrators: integrate, ImplicitMidpoint
using GeometricMachineLearning

pr = tl.hodeproblem(; tspan = (0.0, 800.))
sol = integrate(pr, ImplicitMidpoint())</code></pre><p>We then put the format in the correct format by calling <a href="../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a><sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>:</p><pre><code class="language-julia hljs">dl_cpu = DataLoader(sol; autoencoder = true, suppress_info = true)</code></pre><p>Also note that the keyword <code>autoencoder</code> was set to true when calling <a href="../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a>. The keyword argument <code>supress_info</code> determines whether data loader provides some additional information on the data it is called on.</p><h2 id="Train-the-network"><a class="docs-heading-anchor" href="#Train-the-network">Train the network</a><a id="Train-the-network-1"></a><a class="docs-heading-anchor-permalink" href="#Train-the-network" title="Permalink"></a></h2><p>We now want to compare two different approaches: <a href="../../reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.PSDArch"><code>PSDArch</code></a> and <a href="../../architectures/symplectic_autoencoder/#GeometricMachineLearning.SymplecticAutoencoder"><code>SymplecticAutoencoder</code></a>. For this we first have to set up the networks: </p><pre><code class="language-julia hljs">const reduced_dim = 2

psd_arch = PSDArch(dl_cpu.input_dim, reduced_dim)
sae_arch = SymplecticAutoencoder(dl_cpu.input_dim, reduced_dim; n_encoder_blocks = 4,
                                                                n_decoder_blocks = 4,
                                                                n_encoder_layers = 2,
                                                                n_decoder_layers = 2)</code></pre><p>Training a neural network is usually done by calling an instance of <a href="../../optimizers/optimizer_framework/#GeometricMachineLearning.Optimizer"><code>Optimizer</code></a> in <code>GeometricMachineLearning</code>. <a href="../../reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.PSDArch"><code>PSDArch</code></a> however can be solved directly by using singular value decomposition and this is done by calling <a href="../../reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.solve!">solve!</a>:  </p><pre><code class="language-julia hljs">psd_nn_cpu = NeuralNetwork(psd_arch, CPU(), eltype(dl_cpu))

solve!(psd_nn_cpu, dl_cpu)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.6946587819586485</code></pre><p>The <code>SymplecticAutoencoder</code> we train with the <a href="../../optimizers/optimizer_methods/#GeometricMachineLearning.AdamOptimizerWithDecay"><code>AdamOptimizerWithDecay</code></a> however<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>:</p><pre><code class="language-julia hljs">using CUDA

const n_epochs = 262144
const batch_size = 4096

backend = CUDABackend()
dl = DataLoader(dl_cpu, backend, Float32)


sae_nn_gpu = NeuralNetwork(sae_arch, CUDADevice(), Float32)
o = Optimizer(sae_nn_gpu, AdamOptimizerWithDecay(integrator_train_epochs))

# train the network
o(sae_nn_gpu, dl, Batch(batch_size), n_epochs)</code></pre><p>After training we map the network parameters to cpu:</p><pre><code class="language-julia hljs">const mtc = GeometricMachineLearning.map_to_cpu</code></pre><pre><code class="language-julia hljs">sae_nn_cpu = mtc(sae_nn_gpu)</code></pre><h2 id="The-online-stage-with-a-standard-integrator"><a class="docs-heading-anchor" href="#The-online-stage-with-a-standard-integrator">The online stage with a standard integrator</a><a id="The-online-stage-with-a-standard-integrator-1"></a><a class="docs-heading-anchor-permalink" href="#The-online-stage-with-a-standard-integrator" title="Permalink"></a></h2><p>After having trained our neural network we can now evaluate it in the online stage of reduced complexity modeling: </p><pre><code class="language-julia hljs">psd_rs = HRedSys(pr, encoder(psd_nn_cpu), decoder(psd_nn_cpu); integrator = ImplicitMidpoint())
sae_rs = HRedSys(pr, encoder(sae_nn_cpu), decoder(sae_nn_cpu); integrator = ImplicitMidpoint())</code></pre><p>We integrate the full system (again) as well as the two reduced systems<sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup>:</p><pre class="documenter-example-output"><code class="nohighlight hljs ansi">FOM + Implicit Midpoint: 116.611271 seconds (232.87 k allocations: 119.185 MiB, 0.01% gc time)
PSD + Implicit Midpoint: 2.801956 seconds (11.07 M allocations: 7.253 GiB, 21.40% gc time)
SAE + Implicit Midpoint: 237.690133 seconds (95.72 M allocations: 96.268 GiB, 3.54% gc time)</code></pre><p>And plot the solutions for </p><pre><code class="language-julia hljs">time_steps = (0, 300, 800)</code></pre><p><img src="../sae_validation_light.png" alt="Comparison between FOM (blue), PSD with implicit midpoint (orange) and SAE with implicit midpoint (green)."/> <img src="../sae_validation_dark.png" alt="Comparison between FOM (blue), PSD with implicit midpoint (orange) and SAE with implicit midpoint (green)."/></p><p>We can see that the SAE has much more approximation capabilities than the PSD. But even though the SAE reasonably reproduces the full-order model (FOM), we see that the online stage of the SAE takes even longer than evaluating the FOM. In order to solve this problem we have to make the <em>online stage more efficient</em>.</p><h2 id="The-online-stage-with-a-neural-network"><a class="docs-heading-anchor" href="#The-online-stage-with-a-neural-network">The online stage with a neural network</a><a id="The-online-stage-with-a-neural-network-1"></a><a class="docs-heading-anchor-permalink" href="#The-online-stage-with-a-neural-network" title="Permalink"></a></h2><p>Instead of using a standard integrator we can also use a neural network that is trained on the reduced data. For this: </p><pre><code class="language-julia hljs">const integrator_train_epochs = 65536
const integrator_batch_size = 4096
const seq_length = 4

integrator_architecture = StandardTransformerIntegrator(reduced_dim;
                                                                    transformer_dim = 20,
                                                                    n_blocks = 3,
                                                                    n_heads = 5,
                                                                    L = 3,
                                                                    upscaling_activation = tanh)

integrator_nn = NeuralNetwork(integrator_architecture, backend)

integrator_method = AdamOptimizerWithDecay(integrator_train_epochs)

o_integrator = Optimizer(integrator_method, integrator_nn)

# map from autoencoder type to integrator type
dl_integration = DataLoader(dl; autoencoder = false)

integrator_batch = Batch(integrator_batch_size, seq_length)</code></pre><pre><code class="language-julia hljs">loss = GeometricMachineLearning.ReducedLoss(encoder(sae_nn_gpu), decoder(sae_nn_gpu))

train_integrator_loss = o_integrator(   integrator_nn, 
                                        dl_integration, 
                                        integrator_batch, 
                                        integrator_train_epochs, 
                                        loss)</code></pre><p>We can now evaluate the solution:</p><pre><code class="language-julia hljs">@time &quot;time stepping with transformer&quot; time_series = iterate(mtc(integrator_nn), ics; n_points = length(sol.t), prediction_window = seq_length)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">time stepping with transformer: 0.227818 seconds (1.89 M allocations: 322.665 MiB, 24.30% gc time)</code></pre><p><img src="../sae_integrator_validation_light.png" alt="Comparison between FOM (blue), PSD with implicit midpoint (orange), SAE with implicit midpoint (green) and SAE with transformer (purple)."/> <img src="../sae_integrator_validation_dark.png" alt="Comparison between FOM (blue), PSD with implicit midpoint (orange), SAE with implicit midpoint (green) and SAE with transformer (purple)."/></p><p>Note that integration of the system with the transformer is orders of magnitudes faster than any comparable method and also leads to an improvement in accuracy over the case where we build the reduced space with the symplectic autoencoder and use implicit midpoint in the online phase.</p><div class="admonition is-success" id="Remark-f590a71c3d635d3e"><header class="admonition-header">Remark<a class="admonition-anchor" href="#Remark-f590a71c3d635d3e" title="Permalink"></a></header><div class="admonition-body"><p>While training the symplectic autoencoder we completely ignore the online phase, but only aim at finding a good low-dimensional approximation to the solution manifold. This is why we observe that the approximated solution differs somewhat form the actual one when using implicit midpoint for integrating the low-dimensional system (blue line vs. green line).</p></div></div><p>Here we compared PSD with an SAE whith the same reduced dimension. One may argue that this is not entirely fair as the PSD has much fewer parameters than the SAE:</p><pre><code class="language-julia hljs">(parameterlength(psd_nn_cpu), parameterlength(sae_nn_cpu))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(398, 904366)</code></pre><p>and we also saw that evaluating <em>PSD + Implicit Midpoint</em> is much faster than <em>SAE + Implicit Midpoint</em>. We thus model the system with PSDs of higher reduced dimension:</p><pre><code class="language-julia hljs">const reduced_dim2 = 8

psd_arch2 = PSDArch(dl_cpu.input_dim, reduced_dim2)

psd_nn2 = NeuralNetwork(psd_arch2, CPU(), eltype(dl_cpu))

solve!(psd_nn2, dl_cpu)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.03624886679095656</code></pre><p>And we see that the error is a lot lower than for the case <code>reduced_dim = 2</code>. We now proceed with building the reduced Hamiltonian system as before, again using <a href="../../reduced_order_modeling/symplectic_mor/#GeometricMachineLearning.HRedSys"><code>HRedSys</code></a>:</p><pre><code class="language-julia hljs">psd_rs2 = HRedSys(pr, encoder(psd_nn2), decoder(psd_nn2); integrator = ImplicitMidpoint())</code></pre><p>We integrate this PSD to check how big the difference in performance is:</p><pre class="documenter-example-output"><code class="nohighlight hljs ansi">PSD + Implicit Midpoint: 11.635070 seconds (11.23 M allocations: 37.094 GiB, 18.83% gc time)</code></pre><p>We can also plot the comparison with the FOM as before:</p><p><img src="../psd_validation2_light.png" alt="Comparison between the FOM and the PSD with a bigger reduced dimension."/> <img src="../psd_validation2_dark.png" alt="Comparison between the FOM and the PSD with a bigger reduced dimension."/></p><p>We see that for a reduced dimension of <span>$2n = 8$</span> the PSD looks slightly better than the SAE for <span>$2n = 2.$</span> As with the SAE we can also use a transformer to integrate the dynamics on the low-dimensional space:</p><pre><code class="language-julia hljs">const integrator_architecture2 = StandardTransformerIntegrator(reduced_dim2;
                                                                            transformer_dim = 20,
                                                                            n_blocks = 3,
                                                                            n_heads = 5,
                                                                            L = 3,
                                                                            upscaling_activation = tanh)
integrator_nn2 = NeuralNetwork(integrator_architecture2, backend)
const integrator_method2 = AdamOptimizerWithDecay(integrator_train_epochs)
const o_integrator2 = Optimizer(integrator_method2, integrator_nn2)

loss2 = GeometricMachineLearning.ReducedLoss(encoder(psd_nn2), decoder(psd_nn2))</code></pre><p>For training we leave <code>dl_integration</code>, <code>integrator_batch</code> and <code>integrator_train_epochs</code> unchanged:</p><pre><code class="language-julia hljs">train_integrator_loss2 = o_integrator(integrator_nn2, dl_integration, integrator_batch, integrator_train_epochs, loss2)</code></pre><p>We again integrate the system and then plot the result:</p><pre><code class="language-julia hljs">@time &quot;time stepping with transformer&quot; time_series2 = iterate(mtc(integrator_nn2), ics; n_points = length(sol.t), prediction_window = seq_length)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">time stepping with transformer: 0.251973 seconds (1.89 M allocations: 328.647 MiB, 32.56% gc time)</code></pre><p>We see that using the transformer on the six-dimensional PSD-reduced system takes slightly longer than using the transformer on the two-dimensional SAE-reduced system. The accuracy is much worse however. Before we plotted the solution for:</p><pre><code class="language-julia hljs">time_steps</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(0, 300, 800)</code></pre><p>Now we do so with:</p><pre><code class="language-julia hljs">time_steps = (0, 4, 5)</code></pre><p><img src="../psd_integrator_validation_light.png" alt="Comparison between FOM (blue), PSD with implicit midpoint (orange), and PSD with transformer (red)."/> <img src="../psd_integrator_validation_dark.png" alt="Comparison between FOM (blue), PSD with implicit midpoint (orange), and PSD with transformer (red)."/></p><p>Here we however see a dramatic deterioration in the quality of the approximation. We assume that this because the <code>transformer_dim</code> was chosen to be <code>20</code> for the SAE and the PSD, but in the second case the reduced space is of dimension six, whereas it is of dimension two in the first case. This may mean that we need an even bigger transformer to find a good approximation of the reduced space.</p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[68]</dt><dd><div>L. Peng and K. Mohseni. <em>Symplectic model reduction of Hamiltonian systems</em>. SIAM Journal on Scientific Computing <strong>38</strong>, A1–A27 (2016).</div></dd><dt>[65]</dt><dd><div>C. Greif and K. Urban. <em>Decay of the Kolmogorov N-width for wave problems</em>. Applied Mathematics Letters <strong>96</strong>, 216–222 (2019).</div></dd><dt>[70]</dt><dd><div>P. Buchfink, S. Glas and B. Haasdonk. <em>Symplectic model reduction of Hamiltonian systems on nonlinear manifolds and approximation with weakly symplectic autoencoder</em>. SIAM Journal on Scientific Computing <strong>45</strong>, A289–A311 (2023).</div></dd><dt>[3]</dt><dd><div>B. Brantner and M. Kraus. <em>Symplectic autoencoders for Model Reduction of Hamiltonian Systems</em>, arXiv preprint arXiv:2312.10004 (2023).</div></dd></dl></div><!--<h1 id="References-2"><a class="docs-heading-anchor" href="#References-2">References</a><a class="docs-heading-anchor-permalink" href="#References-2" title="Permalink"></a></h1><div class="citation noncanonical"><dl><dt>[68]</dt><dd><div>L. Peng and K. Mohseni. <em>Symplectic model reduction of Hamiltonian systems</em>. SIAM Journal on Scientific Computing <strong>38</strong>, A1–A27 (2016).</div></dd><dt>[65]</dt><dd><div>C. Greif and K. Urban. <em>Decay of the Kolmogorov N-width for wave problems</em>. Applied Mathematics Letters <strong>96</strong>, 216–222 (2019).</div></dd><dt>[70]</dt><dd><div>P. Buchfink, S. Glas and B. Haasdonk. <em>Symplectic model reduction of Hamiltonian systems on nonlinear manifolds and approximation with weakly symplectic autoencoder</em>. SIAM Journal on Scientific Computing <strong>45</strong>, A289–A311 (2023).</div></dd><dt>[3]</dt><dd><div>B. Brantner and M. Kraus. <em>Symplectic autoencoders for Model Reduction of Hamiltonian Systems</em>, arXiv preprint arXiv:2312.10004 (2023).</div></dd></dl></div>--><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>For more information on <a href="../../data_loader/data_loader/#GeometricMachineLearning.DataLoader"><code>DataLoader</code></a> see the <a href="../../data_loader/data_loader/#The-Data-Loader">corresponding section</a>.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>It is not feasible to perform the training on CPU, which is why we use <code>CUDA</code> [<a href="../../references/#besard2018juliagpu">11</a>] here. We further perform the training in single precision.</li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>All of this is done with <code>ImplicitMidpoint</code> as integrator.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../hamiltonian_neural_network/">« Hamiltonian Neural Network</a><a class="docs-footer-nextpage" href="../mnist/mnist_tutorial/">MNIST »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.4 on <span class="colophon-date" title="Thursday 5 June 2025 10:27">Thursday 5 June 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
