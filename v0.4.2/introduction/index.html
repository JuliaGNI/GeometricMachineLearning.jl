<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Introduction and Outline · GeometricMachineLearning.jl</title><meta name="title" content="Introduction and Outline · GeometricMachineLearning.jl"/><meta property="og:title" content="Introduction and Outline · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Introduction and Outline · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/introduction/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/introduction/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/introduction/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../architectures/hamiltonian_neural_network/">Hamiltonian Neural Network</a></li><li><a class="tocitem" href="../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../architectures/symplectic_transformer/">Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../tutorials/hamiltonian_neural_network/">Hamiltonian Neural Network</a></li><li><a class="tocitem" href="../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../tutorials/matrix_softmax/">Matrix Attention</a></li><li><a class="tocitem" href="../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../tutorials/symplectic_transformer/">Symplectic Transformer</a></li><li><a class="tocitem" href="../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../references/">References</a></li><li><a class="tocitem" href="../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Introduction and Outline</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Introduction and Outline</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/introduction.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Introduction-and-Outline"><a class="docs-heading-anchor" href="#Introduction-and-Outline">Introduction and Outline</a><a id="Introduction-and-Outline-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction-and-Outline" title="Permalink"></a></h1><p>One may argue that <em>structure-preserving machine learning</em> is a contradiction. One of the most popular books on neural networks [<a href="../references/#goodfellow2016deep">43</a>] introduces the term <em>machine learning</em> the following way: &quot;The difficulties faced by systems relying on hard-coded knowledge suggest that [artificial intelligence] systems need the ability to acquire their own knowledge, by extracting patterns from raw data. This capability is known as machine learning.&quot; The many success stories of deep neural networks such as ChatGPT [<a href="../references/#achiam2023gpt">95</a>] have shown that abandoning hard-coded knowledge in favour of extracting patterns can yield enormous improvement for many applications. In scientific computing the story is a different one however. Hard coding of certain properties into an algorithm has proved indispensible for many numerical applications. The introductory chapter to one of the canonical references on geometric numerical integration [<a href="../references/#hairer2006geometric">1</a>] contains the sentence: &quot;It turned out that the preservation of geometric properties of the flow not only produces an improved qualitative behaviour, but also allows for a more accurate long-time integration than with general-purpose methods.&quot; Here &quot;preservation of geometric properties&quot; means <em>hard-coding physical information</em> into an algorithm.</p><p>Despite the allure of neglecting hard-coded knowledge in an &quot;era of big data&quot; [<a href="../references/#duan2019artificial">96</a>] many researchers have very early realized that systems that work for image recognition, natural language processing and other purely data-driven tasks may not be suitable to treat problems from physics [<a href="../references/#psichogios1992hybrid">97</a>]. Scientific machine learning [<a href="../references/#baker2019workshop">98</a>], which in this work refers to the application of machine learning techniques for the solution of differential equations from science and engineering, has however much too often neglected the preservation of geometric properties that has proved to be so important in traditional numerics. An ostensible solution is offered by so-called physics-informed neural networks (PINNS), whose eponymous paper [<a href="../references/#raissi2019physics">71</a>] is one of the most-cited in scientific machine learning. The authors write: &quot;Coming to our rescue, for many cases pertaining to the modeling of physical and biological systems, there exists a vast amount of prior knowledge that is currently not being utilized in modern machine learning practice.&quot; It is stated that PINNS &quot;[enrich] deep learning with the longstanding developments in mathematical physics;&quot; one should however add that they also ignore <em>longstanding developments in numerics</em>, like preserving the geometric properties which are observed to be crucial in [<a href="../references/#hairer2006geometric">1</a>].</p><p>What this work aims at doing is not &quot;to set the foundations for a new paradigm&quot; [<a href="../references/#raissi2019physics">71</a>], but rather to show that in many cases it is advantageous to imbue neural networks with specific structure and one should to do this whenever possible. In this regard this work is much more closely related to traditional numerics than to neural network research as we try to design problem-specific algorithms rather than &quot;universal approximators&quot; [<a href="../references/#hornik1989multilayer">30</a>]. The <em>structure-preserving neural networks</em> in this work are never fundamentally new architectures but build on existing neural network designs [<a href="../references/#jin2020sympnets">5</a>, <a href="../references/#vaswani2017attention">54</a>] or more classical methods [<a href="../references/#peng2016symplectic">68</a>]. We design neural networks that have a specific structure encoded in them (modeling part) and then make their behavior reflect information found in data (machine learning part). We refer to this as <em>geometric machine learning</em>.</p><p><img src="../tikz/gml_venn_light.png" alt="Geometric machine learning (GML) like traditional geometric numerical integration (GNI) and other structure-preserving numerical methods aims at building models that share properties with the analytic solution of a differential equation."/> <img src="../tikz/gml_venn_dark.png" alt="Geometric machine learning (GML) like traditional geometric numerical integration (GNI) and other structure-preserving numerical methods aims at building models that share properties with the analytic solution of a differential equation."/></p><p>In the picture above we visualize that geometric machine learning aims at constructing so-called structure-preserving mappings that are ideally close to the analytic solution and perform better than classical methods (e.g. GNI). <em>Structure-preserving</em> here means that the model shares properties with the analytic solution of the underlying differential equation. In this work the most important of these properties are <em>symplecticity</em> and <em>volume preservation</em>, but this may extend to others such as the null space of certain operators [<a href="../references/#arnold2006finite">99</a>] and symmetries encoded into a differential equation [<a href="../references/#lishkova2023discrete">100</a>, <a href="../references/#dierkes2023hamiltonian">101</a>].</p><p>For us the biggest motivation for geometric machine learning comes from <em>data-driven reduced order modeling</em>. There we want to find <em>reduced representations</em> of so-called <em>full order models</em> of which we have data available; such reduced representation ideally have much lower computational complexity then the full order model. Data-driven reduced order modeling is especially useful when solving parametric partial differential equations (PPDEs). In this case we can solve the full order model for a few parameter instances and then build a cheaper representation of the full model (a so-called <em>reduced model</em>) with neural networks. This can bring dramatic speed-ups in performance. </p><p>Closely linked to the research presented here is the development of a software package written in <code>Julia</code> called <code>GeometricMachineLearning</code> [<a href="../references/#brantner2020geometric">102</a>]. Throughout this work we will demonstrate concepts such as neural network architecture and (Riemannian) optimization by using <code>GeometricMachineLearning</code><sup class="footnote-reference"><a id="citeref-0" href="#footnote-0">[0]</a></sup>. Most sections contain a subsection <strong>Library Functions</strong> that explains types and functions in <code>GeometricMachineLearning</code> that pertain to the text in that section (they are generated as so-called docstrings [<a href="../references/#julia2024documentation">103</a>]). We show an example here:</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.GradientCache-introduction" href="#GeometricMachineLearning.GradientCache-introduction"><code>GeometricMachineLearning.GradientCache</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GradientCache(Y)</code></pre><p>Do not store anything.</p><p>The cache for the <a href="../optimizers/optimizer_methods/#GeometricMachineLearning.GradientOptimizer"><code>GradientOptimizer</code></a> does not consider past information.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/469ed208918b50a7470546e59a8cd4cda5e6cdc3/src/optimizers/optimizer_caches.jl#LL79-L85">source</a></section></article><p>So the docstring shows the name of the type or method, in most cases how to call it and then gives some information explaining what it does and potentially hyperlinks to other similar docstrings (<a href="../optimizers/optimizer_methods/#GeometricMachineLearning.GradientOptimizer"><code>GradientOptimizer</code></a> in this case); all of this information is indented by a tab. Docstrings may include other information under subheaders <strong>Arguments</strong> (showing the arguments the method can be supplied with), <strong>Examples</strong> (giving more detailed examples (including results) of how to use the method) and <strong>Implementation</strong> (giving details on how the method is implemented). When we reference a docstring it is always printed in blue (e.g. <a href="../optimizers/optimizer_methods/#GeometricMachineLearning.GradientOptimizer"><code>GradientOptimizer</code></a>), indicating a hyperlink. In addition there is an <em>index of docstrings</em> showing all docstrings in chronological order with the associated page number.</p><p>Similar to <strong>Library Functions</strong>, which is included in most sections, almost every chapter concludes with a section <strong>Chapter Summary</strong> and an additional section <strong>References</strong> that shows further related reading material. The <strong>Chapter Summary</strong> recaps the important aspects of the corresponding chapter, states again what is new (this may be mathematical or software aspects) and gives information to what other parts of the dissertation the contents of the present chapter are relevant.</p><p>All the code necessary to reproduce the results is included in the text and does not have any specific hardware requirements. Except for training some of the neural networks (which was done on an NVIDA Geforce RTX 4090 [<a href="../references/#rtx4090">104</a>]) all the code snippets were run on CPU (via GitHub runners [<a href="../references/#actions">105</a>]). All plots have been generated with <code>Makie</code> [<a href="../references/#DanischKrumbiegel2021">106</a>].</p><p>This dissertation is structures into four main parts: (i) background information, (ii) an explanation of the optimizer framework used for training neural networks, (iii) a detailed explanation of the various neural network layers and architectures that we use and (iv) a number of examples of how to use <code>GeometricMachineLearning</code>. We explain the content of these parts in some detail<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>.</p><h2 id="Background-Information"><a class="docs-heading-anchor" href="#Background-Information">Background Information</a><a id="Background-Information-1"></a><a class="docs-heading-anchor-permalink" href="#Background-Information" title="Permalink"></a></h2><p>The background material, which does not include any original work, covers all the prerequisites for introducing our new optimizers in Part II. In addition it introduces some basic functionality of <code>GeometricMachineLearning</code>. It contains (among others) the following sections:</p><ul><li><a href="../manifolds/basic_topology/#Basic-Concepts-from-General-Topology">Concepts from general topology</a>: here we introduce topological spaces, closedness, compactness, countability and Hausdorffness amongst others. These concepts are prerequisites for defining manifolds.</li><li><a href="../manifolds/manifolds/#(Matrix)-Manifolds">General theory on manifolds</a>: we introduce manifolds, the preimage theorem and submersion theorem. These theorems will be used to construct manifolds; the preimage theorem is used to give structure to the <a href="../manifolds/homogeneous_spaces/#The-Stiefel-Manifold">Stiefel</a> and the <a href="../manifolds/homogeneous_spaces/#The-Grassmann-Manifold">Grassmann manifold</a>, and the immersion theorem gives structure to the <a href="../reduced_order_modeling/reduced_order_modeling/#The-Solution-Manifold">solution manifold</a> which is used in reduced order modeling.</li><li><a href="../manifolds/riemannian_manifolds/#Riemannian-Manifolds">Riemannian manifolds</a>: for optimizing on manifolds we need to define a metric on them, which leads to <em>Riemannian manifolds</em>. We introduce <em>geodesics</em> and the <em>Riemannian gradient</em> here.</li><li><a href="../manifolds/homogeneous_spaces/#Homogeneous-Spaces">Homogeneous spaces</a>: homogeneous spaces are a special class of manifolds to which our <em>generalized optimizer framework</em> can be applied. They trivially include all Lie groups and spaces like the <a href="../manifolds/homogeneous_spaces/#The-Stiefel-Manifold">Stiefel manifold</a>, the <a href="../manifolds/homogeneous_spaces/#The-Grassmann-Manifold">Grassmann manifold</a> and the &quot;homogeneous space of positions and orientations&quot; [<a href="../references/#bon2024optimal">107</a>].</li><li><a href="../arrays/global_tangent_spaces/#Global-Tangent-Spaces">Global tangent spaces</a>: homogeneous spaces allow for identifying for an invariant representation of all tangent spaces which we call <em>global tangent spaces</em><sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup>. We explain this concept in this section.</li><li><a href="../structure_preservation/symplecticity/#Symplectic-Systems">Geometric structure</a>: structure preservation takes a prominent role in this dissertation. In general <em>structure</em> refers to some property that the analytic solution of a differential equation also has and that we want to preserve when modeling the system. Here we discuss <em>symplecticity</em> and <a href="../structure_preservation/volume_preservation/#Divergence-Free-Vector-Fields">volume preservation</a> in detail. We also introduce neural networks in this chapter and give a definition of <a href="../structure_preservation/structure_preserving_neural_networks/#Structure-Preserving-Neural-Networks">geometric neural networks</a>.</li><li><a href="../reduced_order_modeling/reduced_order_modeling/#Basic-Concepts-of-Reduced-Order-Modeling">Reduced order modeling</a>: reduced order modeling serves as a motivation for most of the architectures introduced here. In this section we introduce the basic idea behind reduced order modeling, show a typical workflow and explain what structure preservation looks like <a href="../reduced_order_modeling/symplectic_mor/#Hamiltonian-Model-Order-Reduction">in this context</a>.</li></ul><h2 id="The-Optimizer-Framework"><a class="docs-heading-anchor" href="#The-Optimizer-Framework">The Optimizer Framework</a><a id="The-Optimizer-Framework-1"></a><a class="docs-heading-anchor-permalink" href="#The-Optimizer-Framework" title="Permalink"></a></h2><p>One of the central parts of this dissertation is an <em>optimizer framework</em> that allows the generalization of existing optimizers such as Adam [<a href="../references/#kingma2014adam">108</a>] and BFGS [<a href="../references/#wright2006numerical">44</a>, Chapter 6.1] to homogeneous spaces in a consistent way<sup class="footnote-reference"><a id="citeref-4" href="#footnote-4">[4]</a></sup>. This part contains the following sections:</p><ul><li><a href="../optimizers/optimizer_framework/#Neural-Network-Optimizers">Neural Network Optimizers</a>: here we introduce the concept of a neural network optimizer and discuss the modifications we have to make in order to generalize them to homogeneous spaces.</li><li><a href="../optimizers/manifold_related/retractions/#Retractions">Retractions</a>: an important concept in manifold optimization are retractions [<a href="../references/#absil2008optimization">22</a>]. We introduce them in this section, discuss how they can be constructed for homogeneous spaces and show the two examples of the <em>geodesic retraction</em> and the <em>Cayley retraction</em>.</li><li><a href="../optimizers/manifold_related/parallel_transport/#Parallel-Transport">Parallel Transport</a>: whenever we have an optimizer that contains momentum terms (such as Adam for example) we need to <em>transport</em> these momenta. In this section we explain how this can be done straightforwardly when dealing with homogeneous spaces. </li><li><a href="../optimizers/optimizer_methods/#Standard-Neural-Network-Optimizers">Optimizer methods</a>: in this section we introduce simple optimizers such as the <em>gradient optimizer</em>, the <em>momentum optimizer</em> and <em>Adam</em> and show how to generalize them to our setting. Due to its increased complexity the BFGS optimizer gets <a href="../optimizers/bfgs_optimizer/#The-BFGS-Optimizer">its own section</a>.</li></ul><p><img src="../tikz/tangent_vector_light.png" alt="Weights can be put on manifolds to achieve structure preservation or improved stability."/> <img src="../tikz/tangent_vector_dark.png" alt="Weights can be put on manifolds to achieve structure preservation or improved stability."/></p><h2 id="Special-Neural-Network-Layers-and-Architectures"><a class="docs-heading-anchor" href="#Special-Neural-Network-Layers-and-Architectures">Special Neural Network Layers and Architectures</a><a id="Special-Neural-Network-Layers-and-Architectures-1"></a><a class="docs-heading-anchor-permalink" href="#Special-Neural-Network-Layers-and-Architectures" title="Permalink"></a></h2><p>In here we first discuss specific neural network layers and then architectures. A neural network architecture is always a composition of many neural network layers that is designed for a specific task.</p><p>Special neural network layers include:</p><ul><li><a href="../layers/sympnet_gradient/#SympNet-Layers">SympNet layers</a>: <em>symplectic neural networks</em> (SympNets) [<a href="../references/#jin2020sympnets">5</a>] are special neural networks that are <em>universal approximators in the class of canonical symplectic maps.</em> SympNet layers comprise three different types: linear layers, activation layers and gradient layers. All these are introduced here.</li><li><a href="../layers/volume_preserving_feedforward/#Volume-Preserving-Feedforward-Layer">Volume-preserving layers</a>: the volume-preserving layers presented here are inspired by linear and activation SympNet layers. They slightly differ from other approaches with the same aim [<a href="../references/#bajars2023locally">50</a>].</li><li><a href="../layers/attention_layer/#The-Attention-Layer">Attention layers</a>: many fields in neural network research have seen big improvements due to <em>attention mechanisms</em> [<a href="../references/#vaswani2017attention">54</a>, <a href="../references/#patwardhan2023transformers">87</a>, <a href="../references/#dosovitskiy2020image">88</a>]. Here we introduce this mechanism (which is a neural network layer) and also discuss how to make it volume-preserving [<a href="../references/#brantner2024volume">4</a>]. It serves as a basis for <a href="../layers/multihead_attention_layer/#Multihead-Attention">multihead attention</a> and <a href="../layers/linear_symplectic_attention/#Linear-Symplectic-Attention">linear symplectic attention</a>.</li></ul><p>Special neural network architectures include:</p><ul><li><a href="../architectures/symplectic_autoencoder/#The-Symplectic-Autoencoder">Symplectic autoencoders</a>: the <em>symplectic autoencoder</em> constitutes one of the central elements of this dissertation. It offers a way of flexibly performing nonlinear model order reduction for Hamiltonian systems. In this section we explain its architecture and how it is implemented in <code>GeometricMachineLearning</code> in detail.</li><li><a href="../architectures/sympnet/#SympNet-Architecture">SympNet architectures</a>: based on SympNet layers (which are simple building blocks) one can construct two main types of architectures which are called <span>$LA$</span>-SympNets and <span>$G$</span>-SympNets. We explain both here.</li><li><a href="../architectures/volume_preserving_feedforward/#Volume-Preserving-Feedforward-Neural-Network">Volume-preserving feedforward neural networks</a>: based on <span>$LA$</span>-SympNets we build <em>volume-preserving feedforward neural networks</em> that can learn arbitrary volume-preserving maps. </li><li><a href="../architectures/transformer/#Standard-Transformer">Transformers</a>: transformer neural networks have revolutionized many fields in machine learning like natural language processing [<a href="../references/#vaswani2017attention">54</a>] and image recognition [<a href="../references/#dosovitskiy2020image">88</a>]. We discuss them here and further imbue them with structure-preserving properties to arrive at <a href="../architectures/volume_preserving_transformer/#Volume-Preserving-Transformer">volume-preserving transformers</a> and <a href="../architectures/linear_symplectic_transformer/#Linear-Symplectic-Transformer">linear symplectic transformers</a>.</li></ul><p>We note that SympNets, volume-preserving feedforward neural networks and the three transformer types presented here belong to a category of <a href="../architectures/neural_network_integrators/#Neural-Network-Integrators">neural network integrators</a>, which are used in the <em>online stage</em> of reduced order modeling. That makes them different from symplectic autoencoders which are used in the <em>offline stage</em>.</p><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><p>In this part we demonstrate the neural network architectures implemented in <code>GeometricMachineLearning</code> with a few examples:</p><ul><li><a href="../tutorials/symplectic_autoencoder/#Symplectic-Autoencoders-and-the-Toda-Lattice">Symplectic autoencoders</a>: here we show how to reduce the Toda lattice [<a href="../references/#toda1967vibration">89</a>], which is a 400-dimensional Hamiltonian system in our case, to a two-dimensional Hamiltonian system with symplectic autoencoders.</li><li><a href="../tutorials/sympnet_tutorial/#SympNets-with-GeometricMachineLearning">SympNets</a>: this serves as an introductory example into using <code>GeometricMachineLearning</code> and does not contain any new results. It simply shows how to use SympNets to learn the flow of a harmonic oscillator.</li><li><a href="../tutorials/mnist/mnist_tutorial/#MNIST-Tutorial">Image classification</a>: Here we perform image classification for the MNIST dataset [<a href="../references/#deng2012mnist">90</a>] with vision transformers and show that manifold optimization can enable convergence that would otherwise not be possible.</li><li><a href="../tutorials/grassmann_layer/#Example-of-a-Neural-Network-with-a-Grassmann-Layer">The Grassmann manifold in neural networks</a>: in this example we model a surface embedded in <span>$\mathbb{R}^3$</span> with the help of the Grassmann manifold.</li><li><a href="../tutorials/volume_preserving_attention/#Comparing-Different-VolumePreservingAttention-Mechanisms">Different volume-preserving attention mechanisms</a>: the <a href="../layers/attention_layer/#Volume-Preserving-Attention">volume-preserving attention mechanism</a> in <code>GeometricMachineLearning</code> is based on computing correlations in the input sequence. These correlations can be constructed in two different ways. Here we compare these two.</li><li><a href="../tutorials/linear_symplectic_transformer/#linear_symplectic_transformer_tutorial">Linear Symplectic Transformer</a>: the linear symplectic transformer is used to integrate the four-dimensional Hamiltonian system of the <em>coupled harmonic oscillator</em>. Here we compare the linear symplectic transformer to the standard transformer and SympNets.</li></ul><h2 id="Associated-Papers-and-Contributions"><a class="docs-heading-anchor" href="#Associated-Papers-and-Contributions">Associated Papers and Contributions</a><a id="Associated-Papers-and-Contributions-1"></a><a class="docs-heading-anchor-permalink" href="#Associated-Papers-and-Contributions" title="Permalink"></a></h2><p>The following papers have emerged in connection with the development of <code>GeometricMachineLearning</code>:</p><ol><li>In [<a href="../references/#brantner2023generalizing">7</a>] a new class of optimizers for <em>homogeneous spaces</em>, a category that includes the Stiefel manifold and the Grassmann manifold, is introduced. The results presented in this paper are reproduced in the <a href="../tutorials/mnist/mnist_tutorial/#MNIST-Tutorial">examples</a>.</li><li>In [<a href="../references/#brantner2023symplectic">3</a>] we introduced a new neural network architectures that we call <em>symplectic autoencoders</em>. This is capable of performing non-linear Hamiltonian model reduction. During training of these symplectic autoencoders we use the optimizers introduced in [<a href="../references/#brantner2023generalizing">7</a>]. Similar results to what is presented in the paper are reproduced <a href="../tutorials/symplectic_autoencoder/#Symplectic-Autoencoders-and-the-Toda-Lattice">as an example</a>.</li><li>In [<a href="../references/#brantner2024volume">4</a>] we introduce a new neural network architecture that we call <em>volume-preserving transformers</em>. This is a structure-preserving version of the <em>standard transformer</em> [<a href="../references/#vaswani2017attention">54</a>] for which all components have been made volume preserving. As application we foresee the <em>online phase</em> in reduced order modeling.</li></ol><p>In addition there are new results presented in this work that have not been written up as a separate paper:</p><ol><li>Similar to the volume-preserving transformer [<a href="../references/#brantner2024volume">4</a>] we introduce a <a href="../architectures/linear_symplectic_transformer/#Linear-Symplectic-Transformer">linear symplectic transformer</a> that preserves a symplectic product structure and is also foreseen to be used in reduced order modeling.</li><li>We show how the <a href="../tutorials/grassmann_layer/#Example-of-a-Neural-Network-with-a-Grassmann-Layer">Grassmann manifold can be included into a neural network</a> and construct a loss based on the Wasserstein distance to approximate a nonlinear space from which we can then sample.</li></ol><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-0"><a class="tag is-link" href="#citeref-0">0</a>This document was produced with <code>GeometricMachineLearning</code> <code>v0.3</code>. It may be that the interface will slightly change in future versions, but efforts will be made to keep these changes as small as possible.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>In addition there is also an appendix that provides more implementation details.</li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>These spaces are also discussed in [<a href="../references/#bendokat2020grassmann">23</a>, <a href="../references/#bendokat2021real">37</a>].</li><li class="footnote" id="footnote-4"><a class="tag is-link" href="#citeref-4">4</a>The optimizer framework was introduced in [<a href="../references/#brantner2023generalizing">7</a>].</li></ul></section></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.11.3 on <span class="colophon-date" title="Thursday 15 May 2025 10:29">Thursday 15 May 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
