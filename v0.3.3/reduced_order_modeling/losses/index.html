<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Losses and Errors · GeometricMachineLearning.jl</title><meta name="title" content="Losses and Errors · GeometricMachineLearning.jl"/><meta property="og:title" content="Losses and Errors · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Losses and Errors · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/reduced_order_modeling/losses/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/reduced_order_modeling/losses/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/reduced_order_modeling/losses/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../pod_autoencoders/">POD and Autoencoders</a></li><li class="is-active"><a class="tocitem" href>Losses and Errors</a><ul class="internal"><li><a class="tocitem" href="#Different-Neural-Network-Losses"><span>Different Neural Network Losses</span></a></li><li><a class="tocitem" href="#A-Note-on-Physics-Informed-Neural-Networks"><span>A Note on Physics-Informed Neural Networks</span></a></li><li><a class="tocitem" href="#Projection-and-Reduction-Errors-of-Reduced-Models"><span>Projection and Reduction Errors of Reduced Models</span></a></li><li><a class="tocitem" href="#Projection-Error"><span>Projection Error</span></a></li><li><a class="tocitem" href="#Reduction-Error"><span>Reduction Error</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reduced Order Modeling</a></li><li class="is-active"><a href>Losses and Errors</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Losses and Errors</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/reduced_order_modeling/losses.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Losses-and-Errors"><a class="docs-heading-anchor" href="#Losses-and-Errors">Losses and Errors</a><a id="Losses-and-Errors-1"></a><a class="docs-heading-anchor-permalink" href="#Losses-and-Errors" title="Permalink"></a></h1><p>In general we distinguish between <em>losses</em> that are used during training of a neural network and <em>errors</em> that arise in the context of reduced order modeling. </p><h2 id="Different-Neural-Network-Losses"><a class="docs-heading-anchor" href="#Different-Neural-Network-Losses">Different Neural Network Losses</a><a id="Different-Neural-Network-Losses-1"></a><a class="docs-heading-anchor-permalink" href="#Different-Neural-Network-Losses" title="Permalink"></a></h2><p><code>GeometricMachineLearning</code> has a number of loss functions implemented that can be called <em>standard losses</em>. Those are the <a href="#GeometricMachineLearning.FeedForwardLoss"><code>FeedForwardLoss</code></a>, the <a href="#GeometricMachineLearning.TransformerLoss"><code>TransformerLoss</code></a>, the <a href="#GeometricMachineLearning.AutoEncoderLoss"><code>AutoEncoderLoss</code></a> and the <a href="#GeometricMachineLearning.ReducedLoss"><code>ReducedLoss</code></a>. How to implement custom losses is shown in a <a href="../../tutorials/adjusting_the_loss_function/#Adjusting-the-Loss-Function">tutorial</a>.</p><h2 id="A-Note-on-Physics-Informed-Neural-Networks"><a class="docs-heading-anchor" href="#A-Note-on-Physics-Informed-Neural-Networks">A Note on Physics-Informed Neural Networks</a><a id="A-Note-on-Physics-Informed-Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#A-Note-on-Physics-Informed-Neural-Networks" title="Permalink"></a></h2><p>A popular trend in recent years has been considering known physical properties of the differential equation, or the entire differential equation, through the loss function [<a href="../../references/#raissi2019physics">71</a>]. This is one way of considering physical properties, and <code>GeometricMachineLearning</code> allows for a <a href="../../tutorials/adjusting_the_loss_function/#Adjusting-the-Loss-Function">flexible implementation of custom losses</a>, but this is nonetheless discouraged. In general a neural networks consists of <em>three ingredients</em>:</p><object type="image/svg+xml" class="display-light-only" data=../../tikz/ingredients.png></object><object type="image/svg+xml" class="display-dark-only" data=../../tikz/ingredients_dark.png></object><p>Instead of considering certain properties through the loss function, we instead do so by enforcing them strongly through the network architecture and the optimizer; the latter pertains to <a href="../../optimizers/optimizer_framework/#Generalization-to-Homogeneous-Spaces">manifold optimization</a>. The advantages of this approach are the strong enforcement of properties that we know our network should have and much easier training because we do not have to tune hyperparameters. </p><h2 id="Projection-and-Reduction-Errors-of-Reduced-Models"><a class="docs-heading-anchor" href="#Projection-and-Reduction-Errors-of-Reduced-Models">Projection and Reduction Errors of Reduced Models</a><a id="Projection-and-Reduction-Errors-of-Reduced-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Projection-and-Reduction-Errors-of-Reduced-Models" title="Permalink"></a></h2><p>Two errors that are of very big importance in reduced order modeling are the <em>projection</em> and the <em>reduction error</em>. During training one typically aims at minimizing the projection error, but for the actual application of the model the reduction error is often more important.</p><h2 id="Projection-Error"><a class="docs-heading-anchor" href="#Projection-Error">Projection Error</a><a id="Projection-Error-1"></a><a class="docs-heading-anchor-permalink" href="#Projection-Error" title="Permalink"></a></h2><p>The projection error computes how well a reduced basis, represented by the reduction <span>$\mathcal{P}$</span> and the reconstruction <span>$\mathcal{R}$</span>, can represent the data with which it is build. In mathematical terms: </p><p class="math-container">\[    e_\mathrm{proj}(\mu) := \frac{|| \mathcal{R}\circ\mathcal{P}(M) - M ||}{|| M ||},\]</p><p>where <span>$||\cdot||$</span> is the Frobenius norm (one could also optimize for different norms). The corresponding function in <code>GeometricMachineLearning</code> is <a href="#GeometricMachineLearning.projection_error"><code>projection_error</code></a>. The projection error is equivalent to <a href="#GeometricMachineLearning.AutoEncoderLoss"><code>AutoEncoderLoss</code></a> and is used for training under that name.</p><h2 id="Reduction-Error"><a class="docs-heading-anchor" href="#Reduction-Error">Reduction Error</a><a id="Reduction-Error-1"></a><a class="docs-heading-anchor-permalink" href="#Reduction-Error" title="Permalink"></a></h2><p>The reduction error measures how far the reduced system diverges from the full-order system during integration (online stage). In mathematical terms (and for a single initial condition): </p><p class="math-container">\[e_\mathrm{red}(\mu) := \sqrt{
    \frac{\sum_{t=0}^K|| \mathbf{x}^{(t)}(\mu) - \mathcal{R}(\mathbf{x}^{(t)}_r(\mu)) ||^2}{\sum_{t=0}^K|| \mathbf{x}^{(t)}(\mu) ||^2}
},\]</p><p>where <span>$\mathbf{x}^{(t)}$</span> is the solution of the FOM at point <span>$t$</span> and <span>$\mathbf{x}^{(t)}_r$</span> is the solution of the ROM (in the reduced basis) at point <span>$t$</span>. The reduction error, as opposed to the projection error, not only measures how well the solution manifold is represented by the reduced basis, but also measures how well the FOM dynamics are approximated by the ROM dynamics (via the induced vector field on the reduced basis). The corresponding function in <code>GeometricMachineLearning</code> is <a href="#GeometricMachineLearning.reduction_error"><code>reduction_error</code></a>. The reduction error is, in contract to the projection error, typically not used during training (even though some authors are using a similar error to do so [<a href="../../references/#lee2020model">62</a>]).</p><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.NetworkLoss" href="#GeometricMachineLearning.NetworkLoss"><code>GeometricMachineLearning.NetworkLoss</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NetworkLoss</code></pre><p>An abstract type for all the neural network losses.  If you want to implement <code>CustomLoss &lt;: NetworkLoss</code> you need to define a functor:</p><pre><code class="language-julia hljs">(loss::CustomLoss)(model, ps, input, output)</code></pre><p>where <code>model</code> is an instance of an <code>AbstractExplicitLayer</code> or a <code>Chain</code> and <code>ps</code> the parameters.</p><p>See <a href="#GeometricMachineLearning.FeedForwardLoss"><code>FeedForwardLoss</code></a>, <a href="#GeometricMachineLearning.TransformerLoss"><code>TransformerLoss</code></a>, <a href="#GeometricMachineLearning.AutoEncoderLoss"><code>AutoEncoderLoss</code></a> and <a href="#GeometricMachineLearning.ReducedLoss"><code>ReducedLoss</code></a> for examples.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/388d07b8f9296f31673daee8c52895ed14304c52/src/loss/losses.jl#LL1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.FeedForwardLoss" href="#GeometricMachineLearning.FeedForwardLoss"><code>GeometricMachineLearning.FeedForwardLoss</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">FeedForwardLoss()</code></pre><p>Make an instance of a loss for feedforward neural networks.</p><p>This should be used together with a neural network of type <a href="../../architectures/neural_network_integrators/#GeometricMachineLearning.NeuralNetworkIntegrator"><code>NeuralNetworkIntegrator</code></a>.</p><p><strong>Example</strong></p><p><code>FeedForwardLoss</code> applies a neural network to an input and compares it to the <code>output</code> via an <span>$L_2$</span> norm:</p><pre><code class="language-julia hljs">using GeometricMachineLearning
using LinearAlgebra: norm
import Random
Random.seed!(123)

const d = 2
arch = GSympNet(d)
nn = NeuralNetwork(arch)

input_vec =  [1., 2.]
output_vec = [3., 4.]
loss = FeedForwardLoss()

loss(nn, input_vec, output_vec) ≈ norm(output_vec - nn(input_vec)) / norm(output_vec)

# output

true</code></pre><p>So <code>FeedForwardLoss</code> simply does:</p><p class="math-container">\[    \mathtt{loss}(\mathcal{NN}, \mathtt{input}, \mathtt{output}) = || \mathcal{NN}(\mathtt{input}) - \mathtt{output} || / || \mathtt{output}||,\]</p><p>where <span>$||\cdot||$</span> is the <span>$L_2$</span> norm. </p><p><strong>Parameters</strong></p><p>This loss does not have any parameters.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/388d07b8f9296f31673daee8c52895ed14304c52/src/loss/losses.jl#LL135-L177">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.TransformerLoss" href="#GeometricMachineLearning.TransformerLoss"><code>GeometricMachineLearning.TransformerLoss</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TransformerLoss(seq_length, prediction_window)</code></pre><p>Make an instance of the transformer loss. </p><p>This should be used together with a neural network of type <a href="../../architectures/neural_network_integrators/#GeometricMachineLearning.TransformerIntegrator"><code>TransformerIntegrator</code></a>.</p><p><strong>Example</strong></p><p><code>TransformerLoss</code> applies a neural network to an input and compares it to the <code>output</code> via an <span>$L_2$</span> norm:</p><pre><code class="language-julia hljs">using GeometricMachineLearning
using LinearAlgebra: norm
import Random

const d = 2
const seq_length = 3
const prediction_window = 2

Random.seed!(123)
arch = StandardTransformerIntegrator(d)
nn = NeuralNetwork(arch)

input_mat =  [1. 2. 3.; 4. 5. 6.]
output_mat = [1. 2.; 3. 4.]
loss = TransformerLoss(seq_length, prediction_window)

# start of prediction
const sop = seq_length - prediction_window + 1
loss(nn, input_mat, output_mat) ≈ norm(output_mat - nn(input_mat)[:, sop:end]) / norm(output_mat)

# output

true</code></pre><p>So <code>TransformerLoss</code> simply does:</p><p class="math-container">\[    \mathtt{loss}(\mathcal{NN}, \mathtt{input}, \mathtt{output}) = || \mathcal{NN}(\mathtt{input})[(\mathtt{sl} - \mathtt{pw} + 1):\mathtt{end}] - \mathtt{output} || / || \mathtt{output} ||,\]</p><p>where <span>$||\cdot||$</span> is the <span>$L_2$</span> norm. </p><p><strong>Parameters</strong></p><p>The <code>prediction_window</code> specifies how many time steps are predicted into the future. It defaults to the value specified for <code>seq_length</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/388d07b8f9296f31673daee8c52895ed14304c52/src/loss/losses.jl#LL32-L80">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.AutoEncoderLoss" href="#GeometricMachineLearning.AutoEncoderLoss"><code>GeometricMachineLearning.AutoEncoderLoss</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AutoEncoderLoss()</code></pre><p>Make an instance of <code>AutoEncoderLoss</code>.</p><p>This loss should always be used together with a neural network of type <a href="../pod_autoencoders/#GeometricMachineLearning.AutoEncoder"><code>AutoEncoder</code></a> (and it is also the default for training such a network). </p><p><strong>Example</strong></p><p><code>AutoEncoderLoss</code> applies a neural network to an input and compares it to the <code>output</code> via an <span>$L_2$</span> norm:</p><pre><code class="language-julia hljs">using GeometricMachineLearning
using LinearAlgebra: norm
import Random
Random.seed!(123)

const N = 4
const n = 1
arch = SymplecticAutoencoder(2*N, 2*n)
nn = NeuralNetwork(arch)

input_vec =  [1., 2., 3., 4., 5., 6., 7., 8.]
loss = AutoEncoderLoss()

loss(nn, input_vec) ≈ norm(input_vec - nn(input_vec)) / norm(input_vec)

# output

true</code></pre><p>So <code>AutoEncoderLoss</code> simply does:</p><p class="math-container">\[    \mathtt{loss}(\mathcal{NN}, \mathtt{input}) = || \mathcal{NN}(\mathtt{input}) - \mathtt{input} || / || \mathtt{input} ||,\]</p><p>where <span>$||\cdot||$</span> is the <span>$L_2$</span> norm. </p><p><strong>Parameters</strong></p><p>This loss does not have any parameters.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/388d07b8f9296f31673daee8c52895ed14304c52/src/loss/losses.jl#LL180-L222">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.ReducedLoss" href="#GeometricMachineLearning.ReducedLoss"><code>GeometricMachineLearning.ReducedLoss</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ReducedLoss(encoder, decoder)</code></pre><p>Make an instance of <code>ReducedLoss</code> based on an <a href="../pod_autoencoders/#GeometricMachineLearning.Encoder"><code>Encoder</code></a> and a <a href="../pod_autoencoders/#GeometricMachineLearning.Decoder"><code>Decoder</code></a>.</p><p>This loss should be used together with a <a href="../../architectures/neural_network_integrators/#GeometricMachineLearning.NeuralNetworkIntegrator"><code>NeuralNetworkIntegrator</code></a> or <a href="../../architectures/neural_network_integrators/#GeometricMachineLearning.TransformerIntegrator"><code>TransformerIntegrator</code></a>.</p><p><strong>Example</strong></p><p><code>ReducedLoss</code> applies the <em>encoder</em>, <em>integrator</em> and <em>decoder</em> neural networks in this order to an input and compares it to the <code>output</code> via an <span>$L_2$</span> norm:</p><pre><code class="language-julia hljs">using GeometricMachineLearning
using LinearAlgebra: norm
import Random
Random.seed!(123)

const N = 4
const n = 1

Ψᵉ = NeuralNetwork(Chain(Dense(N, n), Dense(n, n))) |&gt; encoder
Ψᵈ = NeuralNetwork(Chain(Dense(n, n), Dense(n, N))) |&gt; decoder
transformer = NeuralNetwork(StandardTransformerIntegrator(n))

input_mat =  [1.  2.;  3.  4.;  5.  6.;  7.  8.]
output_mat = [9. 10.; 11. 12.; 13. 14.; 15. 16.]
loss = ReducedLoss(Ψᵉ, Ψᵈ)

output_prediction = Ψᵈ(transformer(Ψᵉ(input_mat)))
loss(transformer, input_mat, output_mat) ≈ norm(output_mat - output_prediction) / norm(output_mat)

# output

true</code></pre><p>So the loss computes: </p><p class="math-container">\[\mathrm{loss}_{\mathcal{E}, \mathcal{D}}(\mathcal{NN}, \mathrm{input}, \mathrm{output}) = ||\mathcal{D}(\mathcal{NN}(\mathcal{E}(\mathrm{input}))) - \mathrm{output}||,\]</p><p>where <span>$\mathcal{E}$</span> is the <a href="../pod_autoencoders/#GeometricMachineLearning.Encoder"><code>Encoder</code></a>, <span>$\mathcal{D}$</span> is the <a href="../pod_autoencoders/#GeometricMachineLearning.Decoder"><code>Decoder</code></a>. <span>$\mathcal{NN}$</span> is the neural network we compute the loss of.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/388d07b8f9296f31673daee8c52895ed14304c52/src/loss/losses.jl#LL233-L276">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.projection_error" href="#GeometricMachineLearning.projection_error"><code>GeometricMachineLearning.projection_error</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">projection_error(rs)</code></pre><p>Compute the projection error for a <a href="../symplectic_mor/#GeometricMachineLearning.HRedSys"><code>HRedSys</code></a>.</p><p><strong>Arguments</strong></p><p>If the full system has already been integrated, then the projection error can be computed quicker:</p><pre><code class="language-julia hljs">projection_error(rs, sol_full)</code></pre><p>This saves the cost of again integrating the systems.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/388d07b8f9296f31673daee8c52895ed14304c52/src/reduced_system/reduced_system.jl#LL196-L209">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.reduction_error" href="#GeometricMachineLearning.reduction_error"><code>GeometricMachineLearning.reduction_error</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">reduction_error(rs)</code></pre><p>Compute the reduction error for a <a href="../symplectic_mor/#GeometricMachineLearning.HRedSys"><code>HRedSys</code></a>.</p><p><strong>Arguments</strong></p><p>If the full system and the reduced system have already been integrated, then the reduction error can be computed quicker:</p><pre><code class="language-julia hljs">reduction_error(rs, sol_full, sol_reduced)</code></pre><p>This saves the cost of again integrating the respective systems.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/388d07b8f9296f31673daee8c52895ed14304c52/src/reduced_system/reduced_system.jl#LL176-L189">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[62]</dt><dd><div>K. Lee and K. T. Carlberg. <em>Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders</em>. Journal of Computational Physics <strong>404</strong>, 108973 (2020).</div></dd><dt>[71]</dt><dd><div>M. Raissi, P. Perdikaris and G. E. Karniadakis. <em>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</em>. Journal of Computational physics <strong>378</strong>, 686–707 (2019).</div></dd></dl></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../pod_autoencoders/">« POD and Autoencoders</a><a class="docs-footer-nextpage" href="../symplectic_mor/">Symplectic Model Order Reduction »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Thursday 26 September 2024 18:07">Thursday 26 September 2024</span>. Using Julia version 1.10.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
