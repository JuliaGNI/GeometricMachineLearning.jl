<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Symplectic Model Order Reduction · GeometricMachineLearning.jl</title><meta name="title" content="Symplectic Model Order Reduction · GeometricMachineLearning.jl"/><meta property="og:title" content="Symplectic Model Order Reduction · GeometricMachineLearning.jl"/><meta property="twitter:title" content="Symplectic Model Order Reduction · GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/reduced_order_modeling/symplectic_mor/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/reduced_order_modeling/symplectic_mor/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/reduced_order_modeling/symplectic_mor/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../../optimizers/optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../../optimizers/manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../../optimizers/optimizer_methods/">Optimizer Methods</a></li><li><a class="tocitem" href="../../optimizers/bfgs_optimizer/">BFGS Optimizer</a></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../losses/">Losses and Errors</a></li><li class="is-active"><a class="tocitem" href>Symplectic Model Order Reduction</a><ul class="internal"><li><a class="tocitem" href="#The-Symplectic-Solution-Manifold"><span>The Symplectic Solution Manifold</span></a></li><li><a class="tocitem" href="#Proper-Symplectic-Decomposition"><span>Proper Symplectic Decomposition</span></a></li><li><a class="tocitem" href="#Symplectic-Autoencoders"><span>Symplectic Autoencoders</span></a></li><li><a class="tocitem" href="#Workflow-for-Symplectic-ROM"><span>Workflow for Symplectic ROM</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li><li class="toplevel"><a class="tocitem" href="#References-2"><span>References</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/hamiltonian_neural_network/">Hamiltonian Neural Network</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../architectures/symplectic_transformer/">Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../../tutorials/hamiltonian_neural_network/">Hamiltonian Neural Network</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/matrix_softmax/">Matrix Attention</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/symplectic_transformer/">Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reduced Order Modeling</a></li><li class="is-active"><a href>Symplectic Model Order Reduction</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Symplectic Model Order Reduction</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/reduced_order_modeling/symplectic_mor.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Hamiltonian-Model-Order-Reduction"><a class="docs-heading-anchor" href="#Hamiltonian-Model-Order-Reduction">Hamiltonian Model Order Reduction</a><a id="Hamiltonian-Model-Order-Reduction-1"></a><a class="docs-heading-anchor-permalink" href="#Hamiltonian-Model-Order-Reduction" title="Permalink"></a></h1><p>Hamiltonian PDEs are partial differential equations that, like its ODE counterpart, have a Hamiltonian associated with it. The linear wave equation can be written as such a Hamiltonian PDE with </p><p class="math-container">\[\mathcal{H}(q, p; \mu) := \frac{1}{2}\int_\Omega\mu^2(\partial_\xi{}q(t,\xi;\mu))^2 + p(t,\xi;\mu)^2d\xi.\]</p><p>Note that in contrast to the ODE case where the Hamiltonian is a function <span>$H(\cdot; \mu):\mathbb{R}^{2d}\to\mathbb{R},$</span> we now have a functional <span>$\mathcal{H}(\cdot, \cdot; \mu):\mathcal{C}^\infty(\mathcal{D})\times\mathcal{C}^\infty(\mathcal{D})\to\mathbb{R}.$</span> The PDE for this Hamiltonian can be obtained similarly as in the ODE case:</p><p class="math-container">\[\partial_t{}q(t,\xi;\mu) = \frac{\delta{}\mathcal{H}}{\delta{}p} = p(t,\xi;\mu), \quad \partial_t{}p(t,\xi;\mu) = -\frac{\delta{}\mathcal{H}}{\delta{}q} = \mu^2\partial_{\xi{}\xi}q(t,\xi;\mu)\]</p><p>Neglecting the Hamiltonian structure of a system can have grave consequences on the performance of the reduced order model [<a href="../../references/#peng2016symplectic">68</a>–<a href="../../references/#buchfink2023symplectic">70</a>] which is why all algorithms in <code>GeometricMachineLearning</code> designed for producing reduced order models respect the structure of the system.</p><h2 id="The-Symplectic-Solution-Manifold"><a class="docs-heading-anchor" href="#The-Symplectic-Solution-Manifold">The Symplectic Solution Manifold</a><a id="The-Symplectic-Solution-Manifold-1"></a><a class="docs-heading-anchor-permalink" href="#The-Symplectic-Solution-Manifold" title="Permalink"></a></h2><p>As with regular parametric PDEs, we also associate a solution manifold with Hamiltonian PDEs. This is a finite-dimensional manifold, on which the dynamics can be described through a Hamiltonian ODE. The reduced system, with which we approximate this symplectic solution manifold, is a low dimensional symplectic vector space <span>$\mathbb{R}^{2n}$</span> together with a reduction <span>$\mathcal{P}$</span> and a reconstruction <span>$\mathcal{R}.$</span> If we now take an initial condition on the solution manifold <span>$\hat{u}_0\in\mathcal{M} \approx \mathcal{R}(\mathbb{R}^{2n})$</span> and project it to the reduced space with <span>$\mathcal{P}$</span>, we get <span>$u = \mathcal{P}(\hat{u}_0).$</span> We can now integrate it on the reduced space via the induced differential equation, which is of canonical Hamiltonian form, and obtain an orbit <span>$u(t)$</span> which can then be mapped back to an orbit on the solution manifold<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> via <span>$\mathcal{R}.$</span> The resulting orbit <span>$\mathcal{R}(u(t))$</span> is ideally the unique orbit on the full order model <span>$\hat{u}(t)\in\mathcal{M}$</span>.</p><p>For Hamiltonian model order reduction we additionally require that the reduction <span>$\mathcal{P}$</span> satisfies</p><p class="math-container">\[    \nabla_z\mathcal{P}\mathbb{J}_{2N}(\nabla_z\mathcal{P})^T = \mathbb{J}_{2n} \text{ for $z\in\mathbb{R}^{2N}$}\]</p><p>and the reconstruction <span>$\mathcal{R}$</span> satisfies<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup></p><p class="math-container">\[    (\nabla_z\mathcal{R})^T\mathbb{J}_{2N}\nabla_z\mathcal{R} = \mathbb{J}_{2n}.\]</p><p>With this we have</p><div class="admonition is-info" id="Theorem-9743e8e6bb24963b"><header class="admonition-header">Theorem<a class="admonition-anchor" href="#Theorem-9743e8e6bb24963b" title="Permalink"></a></header><div class="admonition-body"><p>A Hamiltonian system on the reduced space <span>$(\mathbb{R}^{2n}, \mathbb{J}_{2n}^T)$</span> is equivalent to a <em>non-canonical symplectic system</em> <span>$(\mathcal{M}, \mathbb{J}_{2N}^T|_\mathcal{M})$</span> where </p><p class="math-container">\[    \mathcal{M} = \mathcal{R}(\mathbb{R}^{2n})\]</p><p>is an approximation to the solution manifold. We further have</p><p class="math-container">\[    \mathbb{J}_{2N}|_\mathcal{M}(z) = ((\nabla_z\mathcal{R})^+)^T\mathbb{J}_{2n}(\nabla_z\mathcal{R})^+,\]</p><p>so the dynamics on <span>$\mathcal{M}$</span> can be described through a Hamiltonian ODE on <span>$\mathbb{R}^{2n}.$</span></p></div></div><p>For the proof we use the fact that <span>$\mathcal{M} = \mathcal{R}(\mathbb{R}^{2n})$</span> is a manifold <a href="../../manifolds/manifolds/#The-Immersion-Theorem">whose coordinate chart is the local inverse</a> of <span>$\mathcal{R}$</span> which we will call <span>$\psi$</span>, i.e. around a point <span>$y\in\mathcal{M}$</span> we have <span>$\psi\circ\mathcal{R}(y) = y.$</span><sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup> We further define the <em>symplectic inverse</em> of a matrix <span>$A\in\mathbb{R}^{2N\times2n}$</span> as </p><p class="math-container">\[    A^+ = \mathbb{J}_{2n}^TA^T\mathbb{J}_{2N},\]</p><p>which gives:</p><p class="math-container">\[    A^+A = \mathbb{J}_{2n}^TA^T\mathbb{J}_{2N}A = \mathbb{I}_{2n},\]</p><p>iff <span>$A$</span> is symplectic, i.e. <span>$A^T\mathbb{J}_{2N}A = \mathbb{J}_{2n}$</span>.</p><details class="admonition is-details" id="Proof-a7fac6f3dcf78010"><summary class="admonition-header">Proof<a class="admonition-anchor" href="#Proof-a7fac6f3dcf78010" title="Permalink"></a></summary><div class="admonition-body"><p>Note that the tangent space at <span>$y = \mathcal{R}(z)$</span> to <span>$\mathcal{M}$</span> is:</p><p class="math-container">\[    T_y\mathcal{M} = \{(\nabla_z\mathcal{R})v: v\in\mathbb{R}^{2n}\}.\]</p><p>The mapping </p><p class="math-container">\[    \mathcal{M} \to T\mathcal{M}, y \mapsto (\nabla_z\mathcal{R})\mathbb{J}_{2n}\nabla_zH\]</p><p>is clearly a vector field. We now prove that it is symplectic and equal to <span>$\mathbb{J}_{2N}\nabla_y(H\circ\psi).$</span> For this first note that we have <span>$\mathbb{I} = (\nabla_z\mathcal{R})^+\nabla_z\mathcal{R} = (\nabla_{\mathcal{R}(z)}\psi)\nabla_z\mathcal{R}$</span> and that the pseudoinverse is unique. We then have:</p><p class="math-container">\[    \mathbb{J}_{2N}\nabla_yH\circ\psi = \mathbb{J}_{2N}(\nabla_y\psi)^T\nabla_{\psi(y)}H = \mathbb{J}_{2N}\left((\nabla_{\psi(y)}\mathcal{R})^+\right)^T\nabla_{\psi(y)}H = (\nabla_{\psi(y)}\mathcal{R})\mathbb{J}_{2n}\nabla_{\psi(y)}H,\]</p><p>which proves that every Hamiltonian system on <span>$\mathbb{R}^{2n}$</span> induces a Hamiltonian system on <span>$\mathcal{M}$</span>. Conversely assume we are given a Hamiltonian vector field whose flow map evolves on <span>$\mathcal{M}$</span>, which we denote by</p><p class="math-container">\[\hat{X}(z) = \mathbb{J}_{2N}\nabla_{z}\hat{H} = (\nabla_{\psi(z)}\mathcal{R})\bar{X}(\psi(z)),\]</p><p>where <span>$\bar{X}$</span> is a vector field on the reduced space. In the last equality we used that the flow map evolves on <span>$\mathcal{M}$</span>, so the corresponding vector field needs to map to <span>$T\mathcal{M}.$</span> We further have:</p><p class="math-container">\[(\nabla_{\psi(z)}\mathcal{R})^+\hat{X}(z) = \mathbb{J}_{2n}(\nabla_{\psi(z)}\mathcal{R})^T\nabla_z\hat{H} = \mathbb{J}_{2n}\nabla_{\psi(z)}(\hat{H}\circ\mathcal{R}) = \bar{X}(\psi(z)),\]</p><p>and we see that the vector field on the reduced space also has to be Hamiltonian. We can thus express a high-dimensional Hamiltonian system on <span>$\mathcal{M}$</span> with a low-dimensional Hamiltonian system on <span>$\mathbb{R}^{2n}$</span>.</p></div></details><p>In the proof we used that <em>the pseudoinverse is unique</em>. This is not true in general [<a href="../../references/#peng2016symplectic">68</a>], but holds for the architectures discussed here (proper symplectic decomposition and symplectic autoencoders). We will postpone the proof of this until after we introduced <a href="../../architectures/symplectic_autoencoder/#The-Symplectic-Autoencoder">symplectic autoencoders in detail</a>.</p><p>This theorem serves as the basis for Hamiltonian model order reduction via proper symplectic decomposition and symplectic autoencoders. We will now briefly introduce these two approaches<sup class="footnote-reference"><a id="citeref-4" href="#footnote-4">[4]</a></sup>.</p><h2 id="Proper-Symplectic-Decomposition"><a class="docs-heading-anchor" href="#Proper-Symplectic-Decomposition">Proper Symplectic Decomposition</a><a id="Proper-Symplectic-Decomposition-1"></a><a class="docs-heading-anchor-permalink" href="#Proper-Symplectic-Decomposition" title="Permalink"></a></h2><p>For proper symplectic decomposition (PSD) the reduction <span>$\mathcal{P}$</span> and the reconstruction <span>$\mathcal{R}$</span> are constrained to be linear, orthonormal and symplectic. Note that these first two properties are shared with <a href="../pod_autoencoders/#Proper-Orthogonal-Decomposition">POD</a>. The easiest way<sup class="footnote-reference"><a id="citeref-5" href="#footnote-5">[5]</a></sup> to enforce this is through the so-called &quot;cotangent lift&quot; [<a href="../../references/#peng2016symplectic">68</a>]: </p><p class="math-container">\[\mathcal{R} \equiv \Psi_\mathrm{CL} = \begin{bmatrix} \Phi &amp; \mathbb{O} \\ \mathbb{O} &amp; \Phi \end{bmatrix} \text{ where $\Phi\in{}St(n,N)\subset\mathbb{R}^{N\times{}n}$},\]</p><p>i.e. both <span>$\Phi$</span> and <span>$\Psi_\mathrm{CL}$</span> are elements of the <a href="../../manifolds/homogeneous_spaces/#The-Stiefel-Manifold">Stiefel manifold</a> and we furthermore have <span>$\Psi_\mathrm{CL}^T\mathbb{J}_{2N}\Psi_\mathrm{CL} = \mathbb{J}_{2n}$</span>, i.e. <span>$\Psi_\mathrm{CL}$</span> is symplectic. If the <a href="../../data_loader/snapshot_matrix/#Snapshot-Matrix">snapshot matrix</a> is of the form: </p><p class="math-container">\[M = \left[\begin{array}{c:c:c:c}
\hat{q}_1(t_0) &amp;  \hat{q}_1(t_1) &amp; \quad\ldots\quad &amp; \hat{q}_1(t_f) \\
\hat{q}_2(t_0) &amp;  \hat{q}_2(t_1) &amp; \ldots &amp; \hat{q}_2(t_f) \\
\ldots &amp; \ldots &amp; \ldots &amp; \ldots \\
\hat{q}_N(t_0) &amp;  \hat{q}_N(t_1) &amp; \ldots &amp; \hat{q}_N(t_f) \\
\hat{p}_1(t_0) &amp; \hat{p}_1(t_1) &amp; \ldots &amp; \hat{p}_1(t_f) \\
\hat{p}_2(t_0) &amp;  \hat{p}_2(t_1) &amp; \ldots &amp; \hat{p}_2(t_f) \\
\ldots &amp;  \ldots &amp; \ldots &amp; \ldots \\
\hat{p}_{N}(t_0) &amp;  \hat{p}_{N}(t_1) &amp; \ldots &amp; \hat{p}_{N}(t_f) \\
\end{array}\right],\]</p><p>with <span>$\mathtt{nts} := f - 1$</span> is <em>the number of time steps</em>. Then <span>$\Phi_\mathrm{CL}$</span> can be computed in a very straight-forward manner:</p><div class="admonition is-info" id="Theorem-3e638ccc67d23e78"><header class="admonition-header">Theorem<a class="admonition-anchor" href="#Theorem-3e638ccc67d23e78" title="Permalink"></a></header><div class="admonition-body"><p>The ideal cotangent lift <span>$\Psi_\mathrm{CL}$</span> for the snapshot matrix of form</p><p class="math-container">\[    M = \begin{bmatrix} M_q \\ M_p \end{bmatrix},\]</p><p>i.e. the cotangent lift that minimizes the projection error, can be obtained the following way:</p><ol><li>Rearrange the rows of the matrix <span>$M$</span> such that we end up with a <span>$N\times(2\mathtt{nts})$</span> matrix: <span>$\hat{M} := [M_q, M_p]$</span>.</li><li>Perform SVD: <span>$\hat{M} = V\Sigma{}U^T.$</span> </li><li>Set <span>$\Phi\gets{}U\mathtt{[:,1:n]}.$</span></li></ol><p><span>$\Psi_\mathrm{CL}$</span> is then built based on this <span>$\Phi$</span>.</p></div></div><p>For details on the cotangent lift (and other methods for linear symplectic model reduction) consult [<a href="../../references/#peng2016symplectic">68</a>]. In <code>GeometricMachineLearning</code> we use the function <a href="#GeometricMachineLearning.solve!"><code>solve!</code></a> for this task.</p><h2 id="Symplectic-Autoencoders"><a class="docs-heading-anchor" href="#Symplectic-Autoencoders">Symplectic Autoencoders</a><a id="Symplectic-Autoencoders-1"></a><a class="docs-heading-anchor-permalink" href="#Symplectic-Autoencoders" title="Permalink"></a></h2><p>Symplectic Autoencoders are a type of neural network suitable for treating Hamiltonian parametrized PDEs with slowly decaying Kolmogorov <span>$n$</span>-width. It is based on PSD and <a href="../../architectures/sympnet/#SympNet-Architecture">symplectic neural networks</a><sup class="footnote-reference"><a id="citeref-6" href="#footnote-6">[6]</a></sup>.</p><p>Symplectic autoencoders are motivated similarly to standard autoencoders for model order reduction: PSD suffers from similar shortcomings as regular POD. PSD is a linear map and the approximation space <span>$\tilde{\mathcal{M}}= \{\Psi^\mathrm{dec}(z_r)\in\mathbb{R}^{2N}:z_r\in\mathbb{R}^{2n}\}$</span> is therefore also linear. For problems with slowly-decaying Kolmogorov <span>$n$</span>-width this leads to very poor approximations.  </p><p>In order to overcome this difficulty we use neural networks, more specifically <a href="../../architectures/sympnet/#SympNet-Architecture">SympNets</a>, together with cotangent lift-like matrices. The resulting architecture, symplectic autoencoders, are discussed in the <a href="../../architectures/symplectic_autoencoder/#The-Symplectic-Autoencoder">dedicated section on neural network architectures</a>.</p><h2 id="Workflow-for-Symplectic-ROM"><a class="docs-heading-anchor" href="#Workflow-for-Symplectic-ROM">Workflow for Symplectic ROM</a><a id="Workflow-for-Symplectic-ROM-1"></a><a class="docs-heading-anchor-permalink" href="#Workflow-for-Symplectic-ROM" title="Permalink"></a></h2><p>As with any other data-driven <a href="../reduced_order_modeling/#General-Workflow">reduced order modeling technique</a> we first discretize the PDE. This should be done with a structure-preserving scheme, thus yielding a (high-dimensional) Hamiltonian ODE as a result. Going back to the example of the linear wave equation, we can discretize this equation with finite differences to obtain a Hamiltonian ODE: </p><p class="math-container">\[\mathcal{H}_\mathrm{discr}(z(t;\mu);\mu) := \frac{1}{2}x(t;\mu)^T\begin{bmatrix}  -\mu^2D_{\xi{}\xi} &amp; \mathbb{O} \\ \mathbb{O} &amp; \mathbb{I}  \end{bmatrix} x(t;\mu).\]</p><p>In Hamiltonian reduced order modeling we try to find a symplectic submanifold in the solution space<sup class="footnote-reference"><a id="citeref-7" href="#footnote-7">[7]</a></sup> that captures the dynamics of the full system as well as possible.</p><p>Similar to the regular PDE case we again build an encoder <span>$\mathcal{P} \equiv \Psi^\mathrm{enc}$</span> and a decoder <span>$\mathcal{R} \equiv \Psi^\mathrm{dec}$</span>; but now both these mappings are required to be symplectic.</p><p>Concretely this means: </p><ol><li>The encoder is a mapping from a high-dimensional symplectic space to a low-dimensional symplectic space, i.e. <span>$\Psi^\mathrm{enc}:\mathbb{R}^{2N}\to\mathbb{R}^{2n}$</span> such that <span>$\nabla\Psi^\mathrm{enc}\mathbb{J}_{2N}(\nabla\Psi^\mathrm{enc})^T = \mathbb{J}_{2n}$</span>.</li><li>The decoder is a mapping from a low-dimensional symplectic space to a high-dimensional symplectic space, i.e. <span>$\Psi^\mathrm{dec}:\mathbb{R}^{2n}\to\mathbb{R}^{2N}$</span> such that <span>$(\nabla\Psi^\mathrm{dec})^T\mathbb{J}_{2N}\nabla\Psi^\mathrm{dec} = \mathbb{J}_{2n}$</span>.</li></ol><p>If these two maps are constrained to linear maps this amounts to PSD.</p><p>After we obtained <span>$\Psi^\mathrm{enc}$</span> and <span>$\Psi^\mathrm{dec}$</span> we can construct the reduced model. <code>GeometricMachineLearning</code> has a <em>symplectic Galerkin projection</em> implemented. This symplectic Galerkin projection does:</p><p class="math-container">\[    \begin{pmatrix} v_r(q, p) \\ f_r(q, p) \end{pmatrix} = \frac{d}{dt} \begin{pmatrix} q \\ p \end{pmatrix} = \mathbb{J}_{2n}(\nabla_z\mathcal{R})^T\mathbb{J}_{2N}^T \begin{pmatrix} v(\mathcal{R}(q, p)) \\ f(\mathcal{R}(q, p)) \end{pmatrix},\]</p><p>where <span>$v$</span> are the first <span>$n$</span> components of the vector field and <span>$f$</span> are the second <span>$n$</span> components of the vector field. The superscript <span>$r$</span> indicated a <em>reduced</em> vector field. These reduced vector fields are built with <a href="#GeometricMachineLearning.build_v_reduced"><code>GeometricMachineLearning.build_v_reduced</code></a> and <a href="#GeometricMachineLearning.build_f_reduced"><code>GeometricMachineLearning.build_f_reduced</code></a>.</p><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.SymplecticEncoder" href="#GeometricMachineLearning.SymplecticEncoder"><code>GeometricMachineLearning.SymplecticEncoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SymplecticEncoder &lt;: Encoder</code></pre><p>This is the abstract <code>SymplecticEncoder</code> type. </p><p>See <a href="../pod_autoencoders/#GeometricMachineLearning.Encoder"><code>Encoder</code></a> for the super type and <a href="#GeometricMachineLearning.NonLinearSymplecticEncoder"><code>NonLinearSymplecticEncoder</code></a> for a derived <code>struct</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/a9c158961b34a14a5a7a7c6cf2b013f82a057a52/src/architectures/autoencoder.jl#LL52-L58">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.SymplecticDecoder" href="#GeometricMachineLearning.SymplecticDecoder"><code>GeometricMachineLearning.SymplecticDecoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SymplecticDecoder &lt;: Decoder</code></pre><p>This is the abstract <code>SymplecticDecoder</code> type.</p><p>See <a href="../pod_autoencoders/#GeometricMachineLearning.Decoder"><code>Decoder</code></a> for the super type and <a href="#GeometricMachineLearning.NonLinearSymplecticDecoder"><code>NonLinearSymplecticDecoder</code></a> for a derived <code>struct</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/a9c158961b34a14a5a7a7c6cf2b013f82a057a52/src/architectures/autoencoder.jl#LL61-L67">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.NonLinearSymplecticEncoder" href="#GeometricMachineLearning.NonLinearSymplecticEncoder"><code>GeometricMachineLearning.NonLinearSymplecticEncoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NonLinearSymplecticEncoder</code></pre><p>This should not be called direclty. Instad use <a href="../../architectures/symplectic_autoencoder/#GeometricMachineLearning.SymplecticAutoencoder"><code>SymplecticAutoencoder</code></a>.</p><p>An instance of <code>NonLinearSymplecticEncoder</code> can be generated by calling <a href="../pod_autoencoders/#GeometricMachineLearning.encoder"><code>encoder</code></a> on an instance of <a href="../../architectures/symplectic_autoencoder/#GeometricMachineLearning.SymplecticAutoencoder"><code>SymplecticAutoencoder</code></a>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/a9c158961b34a14a5a7a7c6cf2b013f82a057a52/src/architectures/symplectic_autoencoder.jl#LL48-L54">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.NonLinearSymplecticDecoder" href="#GeometricMachineLearning.NonLinearSymplecticDecoder"><code>GeometricMachineLearning.NonLinearSymplecticDecoder</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">NonLinearSymplecticDecoder</code></pre><p>This should not be called direclty. Instad use <a href="../../architectures/symplectic_autoencoder/#GeometricMachineLearning.SymplecticAutoencoder"><code>SymplecticAutoencoder</code></a>.</p><p>An instance of <code>NonLinearSymplecticDecoder</code> can be generated by calling <a href="../pod_autoencoders/#GeometricMachineLearning.decoder"><code>decoder</code></a> on an instance of <a href="../../architectures/symplectic_autoencoder/#GeometricMachineLearning.SymplecticAutoencoder"><code>SymplecticAutoencoder</code></a>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/a9c158961b34a14a5a7a7c6cf2b013f82a057a52/src/architectures/symplectic_autoencoder.jl#LL64-L70">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.HRedSys" href="#GeometricMachineLearning.HRedSys"><code>GeometricMachineLearning.HRedSys</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">HRedSys</code></pre><p><code>HRedSys</code> computes the reconstructed dynamics in the full system based on the reduced one. Optionally it can be compared to the FOM solution.</p><p>It can be called using two different constructors.</p><p><strong>Constructors</strong></p><p>The standard constructor is:</p><pre><code class="language-julia hljs">HRedSys(N, n; encoder, decoder, v_full, f_full, v_reduced, f_reduced, parameters, timespan, timestep, ics, projection_error)</code></pre><p>where </p><ul><li><code>encoder</code>: a function <span>$\mathbb{R}^{2N}\mapsto{}\mathbb{R}^{2n}$</span></li><li><code>decoder</code>: a (differentiable) function <span>$\mathbb{R}^{2n}\mapsto\mathbb{R}^{2N}$</span></li><li><code>v_full</code>: a (differentiable) mapping defined the same way as in GeometricIntegrators.</li><li><code>f_full</code>: a (differentiable) mapping defined the same way as in GeometricIntegrators.</li><li><code>v_reduced</code>: a (differentiable) mapping defined the same way as in GeometricIntegrators.</li><li><code>f_reduced</code>: a (differentiable) mapping defined the same way as in GeometricIntegrators.</li><li><code>integrator</code>: is used for integrating the reduced system.</li><li><code>parameters</code>: a NamedTuple that parametrizes the vector fields (the same for full<em>vector</em>field and reduced<em>vector</em>field)</li><li><code>timespan</code>: a tuple <code>(t₀, tₗ)</code> that specifies start and end point of the time interval over which integration is performed. </li><li><code>timestep</code>: the time step </li><li><code>ics</code>: the initial condition for the big system.</li></ul><p>The other, and more convenient, constructor is:</p><pre><code class="language-julia hljs">HRedSys(odeensemble, encoder, decoder) </code></pre><p>where <code>odeensemble</code> can be a <code>HODEEnsemble</code> or a <code>HODEProblem</code>. <code>encoder</code> and <code>decoder</code> have to be neural networks. With this constructor one can also pass an integrator via the keyword argument <code>integrator</code>. The default is <code>ImplicitMidpoint()</code>.  Internally this calls the functions <a href="#GeometricMachineLearning.build_v_reduced"><code>build_v_reduced</code></a> and <a href="#GeometricMachineLearning.build_f_reduced"><code>build_f_reduced</code></a> in order to build the reduced vector fields.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/a9c158961b34a14a5a7a7c6cf2b013f82a057a52/src/reduced_system/reduced_system.jl#LL1-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.build_v_reduced" href="#GeometricMachineLearning.build_v_reduced"><code>GeometricMachineLearning.build_v_reduced</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">build_v_reduced(v_full, f_full, decoder)</code></pre><p>Builds the reduced vector field (<span>$q$</span> part) based on the full vector field for a Hamiltonian system. </p><p>We derive the reduced vector field via the reduced Hamiltonian: <span>$\tilde{H} := H\circ\Psi^\mathrm{dec}$</span>. </p><p>We then get </p><p class="math-container">\[\begin{aligned}
\mathbb{J}_{2n}\nabla_\xi\tilde{H} = \mathbb{J}_{2n}(\nabla\Psi^\mathrm{dec})^T\mathbb{J}_{2N}^T\mathbb{J}_{2N}\nabla_z{}H &amp; = \mathbb{J}_{2n}(\nabla\Psi^\mathrm{dec})^T\mathbb{J}_{2N}^T \begin{pmatrix} v(z) \\ f(z) \end{pmatrix} \\ &amp; = \begin{pmatrix} - (\nabla_p\Psi_q)^Tf(z) + (\nabla_p\Psi_p)^Tv(z) \\ (\nabla_q\Psi_q)^Tf(z) - (\nabla_q\Psi_p)^Tv(z) \end{pmatrix}.
\end{aligned}\]</p><p><code>build_v_reduced</code> outputs the first half of the entries of this vector field.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/a9c158961b34a14a5a7a7c6cf2b013f82a057a52/src/reduced_system/reduced_system.jl#LL103-L118">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.build_f_reduced" href="#GeometricMachineLearning.build_f_reduced"><code>GeometricMachineLearning.build_f_reduced</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">build_f_reduced(v_full, f_full, decoder)</code></pre><p>Builds the reduced vector field (<span>$p$</span> part) based on the full vector field for a Hamiltonian system. </p><p><code>build_f_reduced</code> outputs the second half of the entries of this vector field.</p><p>See <a href="#GeometricMachineLearning.build_v_reduced"><code>build_v_reduced</code></a> for more information.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/a9c158961b34a14a5a7a7c6cf2b013f82a057a52/src/reduced_system/reduced_system.jl#LL133-L141">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.PSDLayer" href="#GeometricMachineLearning.PSDLayer"><code>GeometricMachineLearning.PSDLayer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PSDLayer(input_dim, output_dim)</code></pre><p>Make an instance of <code>PSDLayer</code>.</p><p>This is a PSD-like layer used for symplectic autoencoders.  One layer has the following shape:</p><p class="math-container">\[A = \begin{bmatrix} \Phi &amp; \mathbb{O} \\ \mathbb{O} &amp; \Phi \end{bmatrix},\]</p><p>where <span>$\Phi$</span> is an element of the Stiefel manifold <span>$St(n, N)$</span>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/a9c158961b34a14a5a7a7c6cf2b013f82a057a52/src/layers/psd_like_layer.jl#LL1-L13">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.PSDArch" href="#GeometricMachineLearning.PSDArch"><code>GeometricMachineLearning.PSDArch</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PSDArch &lt;: SymplecticCompression</code></pre><p><code>PSDArch</code> is the architecture that corresponds to <a href="#GeometricMachineLearning.PSDLayer"><code>PSDLayer</code></a>. </p><p><strong>The architecture</strong></p><p>Proper symplectic decomposition (PSD) can be seen as a <a href="../../architectures/symplectic_autoencoder/#GeometricMachineLearning.SymplecticAutoencoder"><code>SymplecticAutoencoder</code></a> for which the decoder and the encoder are both PSD-like matrices (see the docs for <a href="#GeometricMachineLearning.PSDLayer"><code>PSDLayer</code></a>. </p><p><strong>Training</strong></p><p>For optimizing the parameters in this architecture no neural network training is necessary (see the docs for <a href="#GeometricMachineLearning.solve!"><code>solve!</code></a>).</p><p><strong>The constructor</strong></p><p>The constructor only takes two arguments as input:</p><ul><li><code>full_dim::Integer</code></li><li><code>reduced_dim::Integer</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/a9c158961b34a14a5a7a7c6cf2b013f82a057a52/src/architectures/psd.jl#LL1-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.solve!" href="#GeometricMachineLearning.solve!"><code>GeometricMachineLearning.solve!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">solve!(nn::NeuralNetwork{&lt;:PSDArch}, input)</code></pre><p>Solve the cotangent lift problem for an input.</p><p><a href="#GeometricMachineLearning.PSDArch"><code>PSDArch</code></a> does not require neural network training since it is a strictly linear operation that can be solved with singular value decomposition (SVD).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/a9c158961b34a14a5a7a7c6cf2b013f82a057a52/src/architectures/psd.jl#LL49-L55">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[70]</dt><dd><div>P. Buchfink, S. Glas and B. Haasdonk. <em>Symplectic model reduction of Hamiltonian systems on nonlinear manifolds and approximation with weakly symplectic autoencoder</em>. SIAM Journal on Scientific Computing <strong>45</strong>, A289–A311 (2023).</div></dd><dt>[68]</dt><dd><div>L. Peng and K. Mohseni. <em>Symplectic model reduction of Hamiltonian systems</em>. SIAM Journal on Scientific Computing <strong>38</strong>, A1–A27 (2016).</div></dd></dl></div><!--<h1 id="References-2"><a class="docs-heading-anchor" href="#References-2">References</a><a class="docs-heading-anchor-permalink" href="#References-2" title="Permalink"></a></h1><div class="citation noncanonical"><dl><dt>[62]</dt><dd><div>K. Lee and K. T. Carlberg. <em>Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders</em>. Journal of Computational Physics <strong>404</strong>, 108973 (2020).</div></dd><dt>[61]</dt><dd><div>S. Fresca, L. Dede’ and A. Manzoni. <em>A comprehensive deep learning-based approach to reduced order modeling of nonlinear time-dependent parametrized PDEs</em>. Journal of Scientific Computing <strong>87</strong>, 1–36 (2021).</div></dd><dt>[60]</dt><dd><div>T. Blickhan. <em>A registration method for reduced basis problems using linear optimal transport</em>, arXiv preprint arXiv:2304.14884 (2023).</div></dd><dt>[66]</dt><dd><div>A. Chatterjee. <em>An introduction to the proper orthogonal decomposition</em>. Current science, 808–817 (2000).</div></dd><dt>[68]</dt><dd><div>L. Peng and K. Mohseni. <em>Symplectic model reduction of Hamiltonian systems</em>. SIAM Journal on Scientific Computing <strong>38</strong>, A1–A27 (2016).</div></dd><dt>[70]</dt><dd><div>P. Buchfink, S. Glas and B. Haasdonk. <em>Symplectic model reduction of Hamiltonian systems on nonlinear manifolds and approximation with weakly symplectic autoencoder</em>. SIAM Journal on Scientific Computing <strong>45</strong>, A289–A311 (2023).</div></dd></dl></div>--><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>To be precise, an <em>approximation of the solution manifold</em> <span>$\mathcal{R}(\mathbb{R}^{2n})$</span>, as we are not able to find the solution manifold exactly in practice. </li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>We should note that satisfying this <em>symplecticity condition</em> is much more important for the reconstruction than for the reduction. There is a lack of research on whether the symplecticity condition for the projection is really needed; in [<a href="../../references/#buchfink2023symplectic">70</a>] it is entirely ignored for example.</li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>A similar proof can be found in [<a href="../../references/#yildiz2024data">72</a>]. Further note that, if we enforced the condition <span>$\mathcal{P}\circ\mathcal{R} = \mathrm{id}$</span> exactly, the projection <span>$\mathcal{P}$</span> would be equal to the local inverse <span>$\psi.$</span> For the proof here we however only require the existence of <span>$\psi$</span>, not its explicit construction as <span>$\mathcal{P}.$</span></li><li class="footnote" id="footnote-4"><a class="tag is-link" href="#citeref-4">4</a>We will discuss symplectic autoencoders later in a <a href="../../architectures/symplectic_autoencoder/#The-Symplectic-Autoencoder">dedicated section</a>.</li><li class="footnote" id="footnote-5"><a class="tag is-link" href="#citeref-5">5</a>The original PSD paper [<a href="../../references/#peng2016symplectic">68</a>] proposes another approach to build linear reductions and reconstructions with the so-called &quot;complex SVD.&quot; In practice this only brings minor advantages over the cotangent lift however [<a href="../../references/#tyranowski2023symplectic">69</a>].</li><li class="footnote" id="footnote-6"><a class="tag is-link" href="#citeref-6">6</a>We call these SympNets most of the time. This term was coined in [<a href="../../references/#jin2020sympnets">5</a>].</li><li class="footnote" id="footnote-7"><a class="tag is-link" href="#citeref-7">7</a>The submanifold, that approximates the solution manifold, is <span>$\tilde{\mathcal{M}} = \{\Psi^\mathrm{dec}(z_r)\in\mathbb{R}^{2N}:u_r\in\mathrm{R}^{2n}\}$</span> where <span>$z_r$</span> is the reduced state of the system. By a slight abuse of notation we also denote <span>$\tilde{\mathcal{M}}$</span> by <span>$\mathcal{M}$</span> as we have done previously when showing equivalence between Hamiltonian vector fields on <span>$\mathbb{R}^{2n}$</span> and <span>$\mathcal{M}$</span>. </li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../losses/">« Losses and Errors</a><a class="docs-footer-nextpage" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.0 on <span class="colophon-date" title="Wednesday 9 July 2025 11:50">Wednesday 9 July 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
