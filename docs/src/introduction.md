# Introduction

One of the most popular books on neural networks [goodfellow2016deep](@cite) introduces the term *machine learning* the following way: "The difficulties faced by systems relying on hard-coded knowledge suggest that [artificial intelligence] systems need the ability to acquire their own knowledge, by extracting patterns from raw data. This capability is known as machine learning." The many success stories of deep neural networks such as ChatGPT [achiam2023gpt](@cite) have shown that abandoning hard-coded knowledge in favour of extracting patterns can yield enormous improvement for many applications. In numerics the story is a different one however. Hard coding of certain properties into an algorithm has proved indispensible for many numerical applications. The introduction to one of the canonical references on geometric numerical integration [hairer2006geometric](@cite) contains the sentence: "It turned out that the preservation of geometric properties of the flow not only produces an improved qualitative behaviour, but also allows for a more accurate long-time integration than with general-purpose methods" and "preservation of geometric properties" means *hard-coded physical information* here.

Researchers have realized very eraly that systems that work for image recognition, natural language processing and other purely data-driven tasks may not be suitable to treat problems from physics [psichogios1992hybrid](@cite). Scientific machine learning [baker2019workshop](@cite), which can be roughly defined as an application of machine learning techniques to science and engineering, has however much too often neglected the preservation of geometric properties that has proved to be so important in traditional numerics. An ostensible solution is offered by so-called physics-informed neural networks (PINNS), whose eponymous paper [raissi2019physics](@cite) is one of the most-cited in scientific machine learning. The authors write: "Coming to our rescue, for many cases pertaining to the modeling of physical and biological systems, there exists a vast amount of prior knowledge that is currently not being utilized in modern machine learning practice." It is stated that PINNS "[enrich] deep learning with the longstanding developments in mathematical physics;" one should however add that they also ignore *longstanding developments in numerics*, like preserving the geometric properties which are observed to be crucial in [hairer2006geometric](@cite).

What this work aims at doing is not "to set the foundations for a new paradigm" [raissi2019physics](@cite), but rather to show that it makes sense to imbue neural networks with specific structure and one should to do this whenever possible. In this regard this work is much more closely related to traditional numerics than to neural network research as we try to design problem-specific algorithms rather than "universal approximators" [hornik1989multilayer](@cite). The *structure-preserving neural networks* in this work are never fundamentally new architectures but build on existing neural network designs [vaswani2017attention, jin2020sympnets](@cite) or more classical methods [peng2016symplectic](@cite). We design neural networks that have a specific structure encoded in them (modeling part) and then make their behavior reflect information found in data (machine learning part). We refer to this as *geometric machine learning*.

![Geometric machine learning (GML) like traditional geometric numerical integration (GNI) and other structure-preserving numerical methods aims at building models that share properties with the analytic solution of a differential equation.]("tikz/gml_venn.png")

In this picture we visualize that geometric machine learning aims at constructing so-called structure-preserving mappings that are ideally close to the analytic solution. *Structure-preserving* here means that the model shares properties with the analytic solution. In this work the most important of these properties are *symplecticity* and *volume preservation*, but this may extend to others such as the null space of certain operators [arnold2006finite](@cite).

For us the biggest motivation for geometric machine learning comes from *data-driven reduced order modeling*. There we want to find cheap representations of so-called *full order models* of which we have data available. This is the case when solving parametric partial differential equations (PPDEs) for example. In this case we can solve the full order model for a few parameter instances and then build a cheaper representation of the full model (a so-called *reduced model*) with neural networks. This can bring dramatic speed-ups in performance. 

Closely linked to the research presented here is the development of a software package written in `Julia` called `GeometricMachineLearning`.