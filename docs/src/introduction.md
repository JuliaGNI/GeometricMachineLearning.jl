# Introduction

One of the most popular books on neural networks [goodfellow2016deep](@cite) introduces the term *machine learning* the following way: "The difficulties faced by systems relying on hard-coded knowledge suggest that [artificial intelligence] systems need the ability to acquire their own knowledge, by extracting patterns from raw data. This capability is known as machine learning." The many success stories of deep neural networks such as ChatGPT [achiam2023gpt](@cite) have shown that abandoning hard-coded knowledge in favour of extracting patterns can yield enormous improvement for many applications. On the other hand such hard coding of certain properties into an algorithm has proved indispensible for many applications in physics: "It turned out that the preservation of geometric properties of the flow not only produces an improved qualitative behaviour, but also allows for a more accurate long-time integration than with general-purpose methods." [hairer2006geometric](@cite).

Scientific machine learning [baker2019workshop](@ref), which can be roughly defined as an application of machine learning techniques to science and engineering, has much too often neglected the preservation of geometric properties. An ostensible solution is offered by so-called physics-informed neural networks (PINNS), whose eponymous paper [raissi2019physics](@cite) is one of the most-cited in scientific machine learning. The authors write: "Coming to our rescue, for many cases pertaining to the modeling of physical and biological systems, there a exist a vast amount of prior knowledge that is currently not being utilized in modern machine learning practice." 