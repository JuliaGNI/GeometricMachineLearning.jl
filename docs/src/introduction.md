# Introduction

One of the most popular books on neural networks [goodfellow2016deep](@cite) introduces the term *machine learning* the following way: "The difficulties faced by systems relying on hard-coded knowledge suggest that [artificial intelligence] systems need the ability to acquire their own knowledge, by extracting patterns from raw data. This capability is known as machine learning." The many success stories of deep neural networks such as ChatGPT [achiam2023gpt](@cite) have shown that abandoning hard-coded knowledge in favour of extracting patterns can yield enormous improvement for many applications. On the other hand such hard coding of certain properties into an algorithm has proved indispensible for many applications in physics: "It turned out that the preservation of geometric properties of the flow not only produces an improved qualitative behaviour, but also allows for a more accurate long-time integration than with general-purpose methods." [hairer2006geometric](@cite).

Scientific machine learning [baker2019workshop](@cite), which can be roughly defined as an application of machine learning techniques to science and engineering, has much too often neglected the preservation of geometric properties. An ostensible solution is offered by so-called physics-informed neural networks (PINNS), whose eponymous paper [raissi2019physics](@cite) is one of the most-cited in scientific machine learning. The authors write: "Coming to our rescue, for many cases pertaining to the modeling of physical and biological systems, there exists a vast amount of prior knowledge that is currently not being utilized in modern machine learning practice." It is stated that PINNS "[enrich] deep learning with the longstanding developments in mathematical physics;" one should however add that they also ignore *longstanding developments in numerics*, like preserving the geometric properties which are observed to be crucial in [hairer2006geometric](@cite).

What this work aims at doing is not "to set the foundations for a new paradigm" [raissi2019physics](@cite), but rather to show that it makes sense to imbue neural networks with specific structure and one should to do this whenever necessary. In this regard this work is much more closely related to traditional numerics than to neural network research as we try to design problem-specific algorithms rather than universal approximators. The *structure-preserving neural networks* in this work are never fundamentally new architectures but build on existing neural network designs [vaswani2017attention, jin2020sympnets](@cite) or more classical methods [peng2016symplectic](@cite).