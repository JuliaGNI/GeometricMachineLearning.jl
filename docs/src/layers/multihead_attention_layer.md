# Multihead Attention Layer

In order to arrive from the [attention layer](attention_layer.md) at the **multihead attention layer** we only have to do a simple modification: 


## References 
- Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).