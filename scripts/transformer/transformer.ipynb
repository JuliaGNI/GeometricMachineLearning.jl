{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the packages `GeometricMachineLearning`, `LinearAlgebra`, `ProgressMeter`, `Zygote`, `Random`, `Lux` and `MLDatatasets` or just functions within them. A random number generator (rng) is needed because Lux requires one to initialize the network parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "using GeometricMachineLearning\n",
    "using GeometricMachineLearning: ResNet\n",
    "using LinearAlgebra: norm\n",
    "using ProgressMeter: @showprogress\n",
    "using Zygote: gradient \n",
    "import MLDatasets\n",
    "import Lux\n",
    "import Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the transformer on the MNIST data set. The images within that data set are 28 $\\times$ 28. For training we reshape this matrix to a 49 $\\times$ 16 matrix. The following offers a visualization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fix the constants relating to the data set. `patch_length` describes the size of the image patches. `n_heads` is the number of heads in the multihead attention layers. The number of patches is just the original dimension of the image divided by the patch length and then squared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_dim = 28\n",
    "patch_length = 7\n",
    "n_heads = 7\n",
    "patch_number = (image_dim÷patch_length)^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the data set and perform some preprocessing. The function `split_and_flatten` is part of `GeometricMachineLearning`. It does what was described before. The images are also divided by a factor of 255, leaving this out shouldn't change much, but you can experiment with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49×16×10000 Array{Float32, 3}:\n",
       "[:, :, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0         …  0.0  0.0          0.00390619  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.0          0.00118416  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.0          0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.0          0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.0          0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0         …  0.0  0.000584391  0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.00129181     0.0  0.00390619   0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.0          0.00176855  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.0          1.53787f-5  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.0          0.0         0.0\n",
       " ⋮                               ⋱                                ⋮\n",
       " 0.0  0.0  0.0  0.0  0.0         …  0.0  0.00318339   0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.000276817  0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.0          0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.0          0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.0          0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0         …  0.0  0.0          0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.0          0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.0          0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0            0.0  0.0          0.0         0.0\n",
       "\n",
       "[:, :, 2] =\n",
       " 0.0  0.0          0.0         0.0  …  0.0         0.00389081  0.00379854\n",
       " 0.0  0.0          0.0         0.0     0.00267589  0.00389081  0.00379854\n",
       " 0.0  0.0          0.0         0.0     0.00389081  0.00389081  0.002599\n",
       " 0.0  0.0          0.0         0.0     0.00389081  0.00389081  0.00179931\n",
       " 0.0  0.0          0.0         0.0     0.00389081  0.00389081  0.00179931\n",
       " 0.0  0.0          0.0         0.0  …  0.00389081  0.00389081  0.000876586\n",
       " 0.0  0.0          0.0         0.0     0.00389081  0.0038293   0.0\n",
       " 0.0  0.0          0.0         0.0     0.0         0.00389081  0.0\n",
       " 0.0  0.0          0.0         0.0     0.0         0.00389081  0.0\n",
       " 0.0  0.0          0.0         0.0     0.00181469  0.00389081  0.0\n",
       " ⋮                                  ⋱                          ⋮\n",
       " 0.0  0.00327566   0.0         0.0  …  0.0         0.0         0.0\n",
       " 0.0  0.00218378   0.0         0.0     0.0         0.0         0.0\n",
       " 0.0  0.000799692  9.22722f-5  0.0     0.0         0.0         0.0\n",
       " 0.0  0.00384468   0.00316801  0.0     0.0         0.0         0.0\n",
       " 0.0  0.00389081   0.00389081  0.0     0.0         0.0         0.0\n",
       " 0.0  0.00322953   0.00215302  0.0  …  0.0         0.0         0.0\n",
       " 0.0  0.000492118  0.0         0.0     0.0         0.0         0.0\n",
       " 0.0  0.000184544  0.0         0.0     0.0         0.0         0.0\n",
       " 0.0  0.0          0.0         0.0     0.0         0.0         0.0\n",
       "\n",
       "[:, :, 3] =\n",
       " 0.0  0.0  0.0         0.0  0.0  …  0.0  0.0         4.61361f-5  0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.0021684   0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0  …  0.0  0.00390619  0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.00315263  0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.0         0.0         0.0\n",
       " ⋮                               ⋱                               ⋮\n",
       " 0.0  0.0  0.0         0.0  0.0  …  0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.00207612  0.0  0.0     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.00370627  0.0  0.0  …  0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0         0.0  0.0     0.0  0.0         0.0         0.0\n",
       "\n",
       ";;; … \n",
       "\n",
       "[:, :, 9998] =\n",
       " 0.0  0.0         0.0          …  0.0  0.0          0.00306036  0.0\n",
       " 0.0  0.0         0.0             0.0  0.0          0.00012303  0.0\n",
       " 0.0  0.0         0.0             0.0  0.0          0.0         0.0\n",
       " 0.0  0.0         0.0             0.0  0.000584391  0.0         0.0\n",
       " 0.0  0.0         0.0             0.0  0.00390619   0.0         0.0\n",
       " 0.0  0.0         0.0          …  0.0  0.00390619   0.0         0.0\n",
       " 0.0  0.0         0.0             0.0  0.00390619   0.0         0.0\n",
       " 0.0  0.0         0.0             0.0  0.0          0.00215302  0.0\n",
       " 0.0  0.0         0.0             0.0  0.0          0.0         0.0\n",
       " 0.0  0.0         0.0             0.0  0.0          0.0         0.0\n",
       " ⋮                             ⋱                                ⋮\n",
       " 0.0  0.0         0.0          …  0.0  0.0          0.0         0.0\n",
       " 0.0  0.0         0.0             0.0  0.0          0.0         0.0\n",
       " 0.0  0.0         0.000538255     0.0  0.0          0.0         0.0\n",
       " 0.0  0.0         0.0             0.0  0.0          0.0         0.0\n",
       " 0.0  0.0         0.0             0.0  0.0          0.0         0.0\n",
       " 0.0  0.0         0.0          …  0.0  0.0          0.0         0.0\n",
       " 0.0  0.00035371  0.0             0.0  0.0          0.0         0.0\n",
       " 0.0  0.00333718  0.00173779      0.0  0.0          0.0         0.0\n",
       " 0.0  0.00350634  0.00370627      0.0  0.0          0.0         0.0\n",
       "\n",
       "[:, :, 9999] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0         …  0.0  0.0         0.0038293   0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0            0.0  0.00213764  0.00266052  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0            0.0  0.0038293   0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0            0.0  0.00390619  0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0            0.0  0.00390619  0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0         …  0.0  0.00390619  0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0            0.0  0.00390619  0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0            0.0  0.0         0.00206075  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0            0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0            0.0  0.00149173  0.0         0.0\n",
       " ⋮                        ⋮           ⋱                               ⋮\n",
       " 0.0  0.0  0.0  0.0  0.0  0.00390619  …  0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.00298347     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0            0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0            0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.00127643     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.00390619  …  0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.00390619     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.00390619     0.0  0.0         0.0         0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.00381392     0.0  0.0         0.0         0.0\n",
       "\n",
       "[:, :, 10000] =\n",
       " 0.0  0.0          0.0          0.0  …  0.0  0.000107651  0.00299885  0.0\n",
       " 0.0  0.0          0.0          0.0     0.0  0.000538255  0.00202999  0.0\n",
       " 0.0  0.0          0.0          0.0     0.0  0.00202999   0.00202999  0.0\n",
       " 0.0  0.0          0.0          0.0     0.0  0.00346021   0.00202999  0.0\n",
       " 0.0  0.0          0.0          0.0     0.0  0.00389081   0.00169166  0.0\n",
       " 0.0  0.0          0.0          0.0  …  0.0  0.00389081   6.15148f-5  0.0\n",
       " 0.0  0.0          0.0          0.0     0.0  0.00389081   0.0         0.0\n",
       " 0.0  0.0          0.0          0.0     0.0  0.0          0.0         0.0\n",
       " 0.0  0.0          0.0          0.0     0.0  0.0          0.0         0.0\n",
       " 0.0  0.0          0.0          0.0     0.0  0.0          0.0         0.0\n",
       " ⋮                                   ⋱                                ⋮\n",
       " 0.0  0.00147636   0.000984237  0.0  …  0.0  0.0          0.0         0.0\n",
       " 0.0  0.00316801   0.0          0.0     0.0  0.0          0.0         0.0\n",
       " 0.0  0.0          0.00389081   0.0     0.0  0.0          0.0         0.0\n",
       " 0.0  0.0          0.00390619   0.0     0.0  0.0          0.0         0.0\n",
       " 0.0  0.000676663  0.00389081   0.0     0.0  0.0          0.0         0.0\n",
       " 0.0  0.00279892   0.00304498   0.0  …  0.0  0.0          0.0         0.0\n",
       " 0.0  0.00369089   0.000369089  0.0     0.0  0.0          0.0         0.0\n",
       " 0.0  0.00389081   0.0          0.0     0.0  0.0          0.0         0.0\n",
       " 0.0  0.00389081   0.0          0.0     0.0  0.0          0.0         0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x, train_y = MLDatasets.MNIST(split=:train)[:]\n",
    "test_x, test_y = MLDatasets.MNIST(split=:test)[:]\n",
    "\n",
    "# preprocessing steps (also perform rescaling so that the images have values between 0 and 1)\n",
    "function preprocess_x(x)\n",
    "    x_reshaped = zeros(Float32, patch_length^2, patch_number, size(x, 3))\n",
    "    for i in axes(x, 3)\n",
    "        x_reshaped[:, :, i] = split_and_flatten(x[:, :, i], patch_length)/255\n",
    "    end\n",
    "    x_reshaped\n",
    "end\n",
    "\n",
    "train_x_reshaped = preprocess_x(train_x);\n",
    "test_x_reshaped = preprocess_x(test_x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to perform preprocessing on the target data (`train_y` and `test_y`). This is referred to as **one-hot encoding**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×10000 Matrix{Bool}:\n",
       " 0  0  0  1  0  0  0  0  0  0  1  0  0  …  0  0  0  0  0  1  0  0  0  0  0  0\n",
       " 0  0  1  0  0  1  0  0  0  0  0  0  0     0  0  0  0  0  0  1  0  0  0  0  0\n",
       " 0  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  1  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  0  0  0\n",
       " 0  0  0  0  1  0  1  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  1  0  0\n",
       " 0  0  0  0  0  0  0  0  1  0  0  0  0  …  1  0  0  0  0  0  0  0  0  0  1  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  1  0     0  1  0  0  0  0  0  0  0  0  0  1\n",
       " 1  0  0  0  0  0  0  0  0  0  0  0  0     0  0  1  0  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  1  0  0  0  0  0  0  0  0\n",
       " 0  0  0  0  0  0  0  1  0  1  0  0  1     0  0  0  0  1  0  0  0  0  0  0  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function encode_y(y)\n",
    "    y_encoded = zeros(Bool, 10, length(y))\n",
    "    for i in axes(y,1)\n",
    "        y_encoded[y[i]+1,i] = 1\n",
    "    end\n",
    "    y_encoded\n",
    "end\n",
    "\n",
    "train_y_encoded = encode_y(train_y);\n",
    "test_y_encoded = encode_y(test_y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a bunch of different models for comparison. `Classification` is a neural network layer that takes matrices. The first entry is the number of rows and the second entry is the number of labels (output). The number of layers in the transformer (i.e. multihead attention and resnet) are specified through $L$. The `Classification` layer has the following options: \n",
    "- `use_bias`: you can use a bias in the classification layer\n",
    "- `add_connection`: you can use the residual connection for the multihead attention or not. \n",
    "- `use_average`: if the input to your layer is a matrix (or a tensor) and this is set to `true` it computes the average of the columns $\\frac{1}{\\mathtt{n\\_col}}\\sum_{j = 1\\ldots\\mathtt{n\\_col}}x_{i,j}$ after the linear transformation has been applied, if `false` then it takes the first column.\n",
    "- `use_softmax`: If set to `true` then this uses `softmax` as the nonlinearity, if `false` it uses elementwise sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "    layer_1 = MultiHeadAttention(),     \u001b[90m# 7_203 parameters\u001b[39m\n",
       "    layer_2 = ResNet(49 => 49, tanh_fast),  \u001b[90m# 2_450 parameters\u001b[39m\n",
       "    layer_3 = MultiHeadAttention(),     \u001b[90m# 7_203 parameters\u001b[39m\n",
       "    layer_4 = ResNet(49 => 49, tanh_fast),  \u001b[90m# 2_450 parameters\u001b[39m\n",
       "    layer_5 = Classification(),         \u001b[90m# 490 parameters\u001b[39m\n",
       ") \u001b[90m        # Total: \u001b[39m19_796 parameters,\n",
       "\u001b[90m          #        plus \u001b[39m0 states, \u001b[90msummarysize \u001b[39m200 bytes."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = 2\n",
    "\n",
    "models = (\n",
    "\n",
    "    model₀ = Lux.Chain(Tuple(map(_ -> ResNet(49, tanh), 1:L))..., Classification(patch_length^2, 10, use_bias=false, use_average=false, use_softmax=false)),\n",
    "\n",
    "    model₁ = Lux.Chain( Transformer(patch_length^2, n_heads, L, add_connection=false, Stiefel=false),\n",
    "            Classification(patch_length^2, 10, use_bias=false, use_average=false, use_softmax=false)),\n",
    "\n",
    "    model₂ = Lux.Chain(Transformer(patch_length^2, n_heads, L, add_connection=true, Stiefel=false),\n",
    "                        Classification(patch_length^2, 10, use_bias=false, use_average=false, use_softmax=false)),\n",
    "\n",
    "    model₃ = Lux.Chain(Transformer(patch_length^2, n_heads, L, add_connection=false, Stiefel=true),\n",
    "                        Classification(patch_length^2, 10, use_bias=false, use_average=false, use_softmax=false)),\n",
    "                        \n",
    "    model₄ = Lux.Chain(Transformer(patch_length^2, n_heads, L, add_connection=true, Stiefel=true),\n",
    "                        Classification(patch_length^2, 10, use_bias=false, use_average=false, use_softmax=false))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual training. If you have an NVIDIA gpu (that is supported by `CUDA.jl`) you can use it here. All the functionality in `GeometricMachineLearning` has been adapted for this (using `KernelAbstractions.jl`). This probably also works with `AMDGPU.jl` and `Metal.jl`, but would have to be tested. NOTE: during actual training you may want to suppress evaluating the total loss at each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training (generic function with 5 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const num = 60000\n",
    "function training(model::Lux.Chain, batch_size=32, n_epochs=.01, o=AdamOptimizer(), enable_cuda=false)\n",
    "    enable_cuda ? using CUDA : nothing\n",
    "    ps, st = enable_cuda ? Lux.setup(CUDA.device(), Random.default_rng(), model) : Lux.setup(Random.default_rng(), model)\n",
    "\n",
    "    function loss(ps, x, y)\n",
    "        x_eval = enable_cuda ? Lux.apply(model, x |> cu, ps, st)[1] : x_eval = Lux.apply(model, x, ps, st)[1]\n",
    "        enable_cuda ? norm(x_eval - (y |> cu))/sqrt(size(y, 2)) : norm(x_eval - (y))/sqrt(size(y, 2))\n",
    "    end\n",
    "\n",
    "    #the number of training steps is calculated based on the number of epochs and the batch size\n",
    "    training_steps = Int(ceil(n_epochs*num/batch_size))\n",
    "    #this records the training error\n",
    "    loss_array = zeros(training_steps + 1)\n",
    "    loss_array[1] = enable_cuda ? loss(ps, train_x_reshaped |> cu, train_y_encoded |> cu) : loss_array[1] = loss(ps, train_x_reshaped, train_y_encoded)\n",
    "\n",
    "    println(\"initial loss: \", loss_array[1])\n",
    "\n",
    "    #initialize the optimizer cache\n",
    "    optimizer_instance = enable_cuda ? Optimizer(CUDA.device(), o, model) : Optimizer(o, model)\n",
    "\n",
    "    @showprogress \"Training network ...\" for i in 1:training_steps\n",
    "        #draw a mini batch \n",
    "        indices = Int.(ceil.(rand(batch_size)*num))\n",
    "        x_batch = enable_cuda ? (train_x_reshaped[:, :, indices] |> cu) : train_x_reshaped[:, :, indices]\n",
    "        y_batch = enable_cuda ? (train_y_encoded[:, indices] |> cu) : train_y_encoded[:, indices]\n",
    "\n",
    "        #compute the gradient using Zygote\n",
    "        dp = gradient(ps -> loss(ps, x_batch, y_batch), ps)[1]\n",
    "\n",
    "        #update the cache of the optimizer and the parameter\n",
    "        optimization_step!(optimizer_instance, model, ps, dp)    \n",
    "\n",
    "        #compute the loss at the current step\n",
    "        loss_array[1+i] = enable_cuda ? loss(ps, train_x_reshaped |> cu, train_y_encoded |> cu) : loss(ps, train_x_reshaped, train_y_encoded)\n",
    "\n",
    "    end\n",
    "    println(\"final loss: \", loss_array[end])\n",
    "    enable_cuda ? println(\"final test loss: \", loss(ps, test_x_reshaped |> cu, test_y_encoded |> cu),\"\\n\") : println(\"final test loss: \", loss(ps, test_x_reshaped, test_y_encoded),\"\\n\")\n",
    "\n",
    "    loss_array\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 0.9486927545760359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  11%|███▎                           |  ETA: 0:00:48\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  16%|████▉                          |  ETA: 0:00:45\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  21%|██████▌                        |  ETA: 0:00:42\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  26%|████████▏                      |  ETA: 0:00:39\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  32%|█████████▊                     |  ETA: 0:00:37\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  37%|███████████▍                   |  ETA: 0:00:34\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  42%|█████████████                  |  ETA: 0:00:31\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  47%|██████████████▋                |  ETA: 0:00:28\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  53%|████████████████▍              |  ETA: 0:00:26\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  58%|██████████████████             |  ETA: 0:00:23\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  63%|███████████████████▋           |  ETA: 0:00:20\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  68%|█████████████████████▎         |  ETA: 0:00:17\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  74%|██████████████████████▉        |  ETA: 0:00:14\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  79%|████████████████████████▌      |  ETA: 0:00:11\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  84%|██████████████████████████▏    |  ETA: 0:00:09\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  89%|███████████████████████████▊   |  ETA: 0:00:06\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ...  95%|█████████████████████████████▍ |  ETA: 0:00:03\u001b[39m\u001b[K"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\u001b[32mTraining network ... 100%|███████████████████████████████| Time: 0:00:53\u001b[39m\u001b[K\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 0.9486604864158703\n",
      "final test loss: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9486714172363281\n",
      "\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/文档/GeometricMachineLearning.jl/scripts/manifold_based/transformer.ipynb:7"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 0.01\n",
    "o = AdamOptimizer(0.001f0, 0.9f0, 0.99f0, 1.0f-8)\n",
    "enable_cuda = false\n",
    "\n",
    "NamedTuple{keys(models)}(Tuple(training(model, batch_size, n_epochs, o, enable_cuda, give_training_error) for model in models))\n",
    "\n",
    "function plot_stuff()\n",
    "    p = plot(loss_array₀, label=\"0\")\n",
    "    plot!(p, loss_array₁, label=\"1\")\n",
    "    plot!(p, loss_array₂, label=\"2\")\n",
    "    plot!(p, loss_array₃, label=\"3\")\n",
    "    plot!(p, loss_array₄, label=\"4\")\n",
    "end\n",
    "\n",
    "give_training_error ? plot_stuff : nothing "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
