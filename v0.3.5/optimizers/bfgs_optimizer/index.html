<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>BFGS Optimizer ¬∑ GeometricMachineLearning.jl</title><meta name="title" content="BFGS Optimizer ¬∑ GeometricMachineLearning.jl"/><meta property="og:title" content="BFGS Optimizer ¬∑ GeometricMachineLearning.jl"/><meta property="twitter:title" content="BFGS Optimizer ¬∑ GeometricMachineLearning.jl"/><meta name="description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="twitter:description" content="Documentation for GeometricMachineLearning.jl."/><meta property="og:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/bfgs_optimizer/"/><meta property="twitter:url" content="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/bfgs_optimizer/"/><link rel="canonical" href="https://juliagni.github.io/GeometricMachineLearning.jl/optimizers/bfgs_optimizer/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/extra_styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="GeometricMachineLearning.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="GeometricMachineLearning.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">HOME</a></li><li><span class="tocitem">Manifolds</span><ul><li><a class="tocitem" href="../../manifolds/basic_topology/">Concepts from General Topology</a></li><li><a class="tocitem" href="../../manifolds/metric_and_vector_spaces/">Metric and Vector Spaces</a></li><li><a class="tocitem" href="../../manifolds/inverse_function_theorem/">Foundations of Differential Manifolds</a></li><li><a class="tocitem" href="../../manifolds/manifolds/">General Theory on Manifolds</a></li><li><a class="tocitem" href="../../manifolds/existence_and_uniqueness_theorem/">Differential Equations and the EAU theorem</a></li><li><a class="tocitem" href="../../manifolds/riemannian_manifolds/">Riemannian Manifolds</a></li><li><a class="tocitem" href="../../manifolds/homogeneous_spaces/">Homogeneous Spaces</a></li></ul></li><li><span class="tocitem">Special Arrays and AD</span><ul><li><a class="tocitem" href="../../arrays/skew_symmetric_matrix/">Symmetric and Skew-Symmetric Matrices</a></li><li><a class="tocitem" href="../../arrays/global_tangent_spaces/">Global Tangent Spaces</a></li><li><a class="tocitem" href="../../arrays/tensors/">Tensors</a></li><li><a class="tocitem" href="../../pullbacks/computation_of_pullbacks/">Pullbacks</a></li></ul></li><li><span class="tocitem">Structure-Preservation</span><ul><li><a class="tocitem" href="../../structure_preservation/symplecticity/">Symplecticity</a></li><li><a class="tocitem" href="../../structure_preservation/volume_preservation/">Volume-Preservation</a></li><li><a class="tocitem" href="../../structure_preservation/structure_preserving_neural_networks/">Structure-Preserving Neural Networks</a></li></ul></li><li><span class="tocitem">Optimizer</span><ul><li><a class="tocitem" href="../optimizer_framework/">Optimizers</a></li><li><a class="tocitem" href="../manifold_related/retractions/">Retractions</a></li><li><a class="tocitem" href="../manifold_related/parallel_transport/">Parallel Transport</a></li><li><a class="tocitem" href="../optimizer_methods/">Optimizer Methods</a></li><li class="is-active"><a class="tocitem" href>BFGS Optimizer</a><ul class="internal"><li><a class="tocitem" href="#The-Riemannian-Version-of-the-BFGS-Algorithm"><span>The Riemannian Version of the BFGS Algorithm</span></a></li><li><a class="tocitem" href="#The-Curvature-Condition-and-the-Wolfe-Conditions"><span>The Curvature Condition and the Wolfe Conditions</span></a></li><li><a class="tocitem" href="#Stability-of-the-Algorithm"><span>Stability of the Algorithm</span></a></li><li><a class="tocitem" href="#Library-Functions"><span>Library Functions</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li><li class="toplevel"><a class="tocitem" href="#References-2"><span>References</span></a></li></ul></li></ul></li><li><span class="tocitem">Special Neural Network Layers</span><ul><li><a class="tocitem" href="../../layers/sympnet_gradient/">Sympnet Layers</a></li><li><a class="tocitem" href="../../layers/volume_preserving_feedforward/">Volume-Preserving Layers</a></li><li><a class="tocitem" href="../../layers/attention_layer/">(Volume-Preserving) Attention</a></li><li><a class="tocitem" href="../../layers/multihead_attention_layer/">Multihead Attention</a></li><li><a class="tocitem" href="../../layers/linear_symplectic_attention/">Linear Symplectic Attention</a></li></ul></li><li><span class="tocitem">Reduced Order Modeling</span><ul><li><a class="tocitem" href="../../reduced_order_modeling/reduced_order_modeling/">General Framework</a></li><li><a class="tocitem" href="../../reduced_order_modeling/pod_autoencoders/">POD and Autoencoders</a></li><li><a class="tocitem" href="../../reduced_order_modeling/losses/">Losses and Errors</a></li><li><a class="tocitem" href="../../reduced_order_modeling/symplectic_mor/">Symplectic Model Order Reduction</a></li></ul></li><li><a class="tocitem" href="../../port_hamiltonian_systems/">port-Hamiltonian Systems</a></li><li><span class="tocitem">Architectures</span><ul><li><a class="tocitem" href="../../architectures/abstract_neural_networks/">Using Architectures with <code>NeuralNetwork</code></a></li><li><a class="tocitem" href="../../architectures/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../architectures/neural_network_integrators/">Neural Network Integrators</a></li><li><a class="tocitem" href="../../architectures/sympnet/">SympNet</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_feedforward/">Volume-Preserving FeedForward</a></li><li><a class="tocitem" href="../../architectures/transformer/">Standard Transformer</a></li><li><a class="tocitem" href="../../architectures/volume_preserving_transformer/">Volume-Preserving Transformer</a></li><li><a class="tocitem" href="../../architectures/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li></ul></li><li><span class="tocitem">Data Loader</span><ul><li><a class="tocitem" href="../../data_loader/snapshot_matrix/">Snapshot matrix &amp; tensor</a></li><li><a class="tocitem" href="../../data_loader/data_loader/">Routines</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/sympnet_tutorial/">SympNets</a></li><li><a class="tocitem" href="../../tutorials/symplectic_autoencoder/">Symplectic Autoencoders</a></li><li><a class="tocitem" href="../../tutorials/mnist/mnist_tutorial/">MNIST</a></li><li><a class="tocitem" href="../../tutorials/grassmann_layer/">Grassmann Manifold</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_attention/">Volume-Preserving Attention</a></li><li><a class="tocitem" href="../../tutorials/volume_preserving_transformer_rigid_body/">Volume-Preserving Transformer for the Rigid Body</a></li><li><a class="tocitem" href="../../tutorials/linear_symplectic_transformer/">Linear Symplectic Transformer</a></li><li><a class="tocitem" href="../../tutorials/adjusting_the_loss_function/">Adjusting the Loss Function</a></li><li><a class="tocitem" href="../../tutorials/optimizer_comparison/">Comparing Optimizers</a></li></ul></li><li><a class="tocitem" href="../../references/">References</a></li><li><a class="tocitem" href="../../docstring_index/">Index of Docstrings</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Optimizer</a></li><li class="is-active"><a href>BFGS Optimizer</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>BFGS Optimizer</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/main/docs/src/optimizers/bfgs_optimizer.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="The-BFGS-Optimizer"><a class="docs-heading-anchor" href="#The-BFGS-Optimizer">The BFGS Optimizer</a><a id="The-BFGS-Optimizer-1"></a><a class="docs-heading-anchor-permalink" href="#The-BFGS-Optimizer" title="Permalink"></a></h1><p>The presentation shown here is largely taken from [<a href="../../references/#wright2006numerical">44</a>, chapters 3 and 6] with a derivation based on an online comment [<a href="../../references/#2279304">45</a>]. The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm is a second order optimizer that can be also be used to train a neural network.</p><p>It is a version of a <em>quasi-Newton</em> method and is therefore especially suited for convex problems. As is the case with any other (quasi-)Newton method<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> the BFGS algorithm approximates the objective with a quadratic function in each optimization step:</p><p class="math-container">\[m^{(k)}(x) = L(x^{(k)}) + (\nabla_{x^{(k)}}L)^T(x - x^{(k)}) + \frac{1}{2}(x - x^{(k)})^TR^{(k)}(x - x^{(k)}),\]</p><p>where <span>$R^{(k)}$</span> is referred to as the <em>approximate Hessian</em>. We further require <span>$R^{(k)}$</span> to be symmetric and positive definite. Differentiating the above expression and setting the derivative to zero gives us: </p><p class="math-container">\[\nabla_xm^{(k)} = \nabla_{x^{(k)}}L + R^{(k)}(x - x^{(k)}) = 0,\]</p><p>or written differently: </p><p class="math-container">\[x - x^{(k)} = -(R^{(k)})^{-1}\nabla_{x^{(k)}}L.\]</p><p>This value we will from now on call <span>$p^{(k)} := -H^{(k)}\nabla_{x_k}L$</span> with <span>$H^{(k)} := (R^{(k)})^{-1}$</span> and refer to as the <em>search direction</em>. The new iterate then is: </p><p class="math-container">\[x^{(k+1)} = x^{(k)} + \eta^{(k)}p^{(k)},\]</p><p>where <span>$\eta^{(k)}$</span> is the <em>step length</em>. Techniques that describe how to pick an appropriate <span>$\eta^{(k)}$</span> are called <em>line-search methods</em> and are discussed below. First we discuss what requirements we impose on <span>$R^{(k)}$</span>. A first reasonable condition would be to require the gradient of <span>$m^{(k)}$</span> to be equal to that of <span>$L$</span> at the points <span>$x^{(k-1)}$</span> and <span>$x^{(k)}$</span>: </p><p class="math-container">\[\begin{aligned}
\nabla_{x^{(k)}}m^{(k)}  &amp; = \nabla_{x^{(k)}}L + R^{(k)}(x^{(k)} - x^{(k)})  &amp; \overset{!}{=} &amp; \nabla_{x^{(k)}}L  &amp; \text{ and } \\
\nabla_{x^{(k-1)}}m^{(k)} &amp; = \nabla_{x^{(k)}}L + R^{(k)}(x^{(k-1)} - x^{(k)}) &amp; \overset{!}{=} &amp; \nabla_{x^{(k-1)}}L. &amp; 
\end{aligned}\]</p><p>The first one of these conditions is automatically satisfied. The second one can be rewritten as: </p><p class="math-container">\[x^{(k)} - x^{(k-1)} \overset{!}{=} H^{(k)}(\nabla_{x^{(k)}}L - \nabla_{x^{(k-1)}}L). \]</p><p>The following notations are often used: </p><p class="math-container">\[s^{(k-1)} := \eta^{(k-1)}p^{(k-1)} :=  x^{(k)} - x^{(k-1)} \quad\text{ and }\quad y^{(k-1)} := \nabla_{x^(k)}L - \nabla_{x^{(k-1)}}L.\]</p><p>The condition mentioned above then becomes: </p><p class="math-container">\[s^{(k-1)} \overset{!}{=} H^{(k)}y^{(k-1)},\]</p><p>and we call it the <em>secant equation</em>. </p><p>In order to pick the ideal <span>$H^{(k)}$</span> we solve the following problem: </p><p class="math-container">\[\begin{aligned}
&amp; \min_H &amp; &amp; ||H - H^{(k-1)}||_W \\ 
&amp; \text{s.t.} &amp; &amp; H = H^T\quad\text{and}\\
            &amp; \text{and} &amp; &amp; s^{(k-1)} = Hy^{(k-1)},
\end{aligned}\]</p><p>where the first condition is symmetry and the second one is the secant equation. For the norm <span>$||\cdot||_W$</span> we pick the weighted Frobenius norm:</p><p class="math-container">\[||A||_W := ||W^{1/2}AW^{1/2}||_F,\]</p><p>where <span>$||\cdot||_F$</span> is the usual Frobenius norm<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup> and the matrix <span>$W=\tilde{R}^{(k-1)}$</span> is the <em>average Hessian</em>:</p><p class="math-container">\[\tilde{R}^{(k-1)} = \int_0^1 \nabla^2f(x^{(k-1)} + \tau\eta^{(k-1)}p^{(k-1)})d\tau.\]</p><p>We now state the solution to this minimization problem:</p><div class="admonition is-info"><header class="admonition-header">Theorem</header><div class="admonition-body"><p>The solution of the minimization problem is:</p><p class="math-container">\[H^{(k)} = (\mathbb{I} - \frac{1}{(s^{(k-1)})^Ty^{(k-1)}}s^{(k-1)}(y^{(k-1)})^T)H^{(k-1)}(\mathbb{I} - \frac{1}{(s^{k-1})^Ty^{(k-1)}}y^{(k-1)}(s^{(k-1)})^T) + \\ \frac{1}{(s^{(k-1)})^Ty^{(k-1)}}s^{(k-1)}(s^{(k-1)})^T,\]</p><p>with <span>$y^{(k-1)} = \nabla_{x^{(k)}}L - \nabla_{x^{(k-1)}}L$</span> and <span>$s^{(k-1)} = x^{(k)} - x^{(k-1)}$</span> as above.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Proof</summary><div class="admonition-body"><p>In order to find the ideal <span>$H^{(k)}$</span> under the conditions described above, we introduce some notation: </p><ul><li><span>$\tilde{H}^{(k-1)} := W^{1/2}H^{(k-1)}W^{1/2}$</span>,</li><li><span>$\tilde{H} := W^{1/2}HW^{1/2}$</span>, </li><li><span>$\tilde{y}^{(k-1)} := W^{-1/2}y^{(k-1)}$</span>, </li><li><span>$\tilde{s}^{(k-1)} := W^{1/2}s^{(k-1)}$</span>.</li></ul><p>With this notation we can rewrite the problem of finding <span>$H^{(k)}$</span> as: </p><p class="math-container">\[\begin{aligned}
&amp; \min_{\tilde{H}} &amp; &amp; ||\tilde{H} - \tilde{H}^{(k-1)}||_F  \\ 
&amp; \text{s.t.}\quad &amp; &amp; \tilde{H} = \tilde{H}^T\quad \\
&amp; \text{and} &amp; &amp; \tilde{s}^{(k-1)} = \tilde{H}\tilde{y}^{(k-1)}.
\end{aligned}\]</p><p>We further have <span>$y^{(k-1)} = Ws^{(k-1)}$</span> and hence <span>$\tilde{y}^{(k-1)} = \tilde{s}^{(k-1)}$</span> by a corollary of the mean value theorem: <span>$\int_0^1 g&#39;(\xi_1 + \tau(\xi_2 - \xi_1)) d\tau (\xi_2 - \xi_1) = g(\xi_2) - g(\xi_1)$</span> for a vector-valued function <span>$g$</span>.</p><p>Now we rewrite <span>$H$</span> and <span>$H^{(k-1)}$</span> in a new basis <span>$U = [u|u_\perp]$</span>, where <span>$u := \tilde{y}^{(k-1)}/||\tilde{y}^{(k-1)}||$</span> and <span>$u_\perp$</span> is an orthogonal complement of <span>$u$</span> (i.e. we have <span>$u^Tu_\perp=0$</span> and <span>$u_\perp^Tu_\perp=\mathbb{I}$</span>):</p><p class="math-container">\[\begin{aligned}
U^T\tilde{H}^{(k-1)}U - U^T\tilde{H}U = \begin{bmatrix}  u^T \\ u_\perp^T \end{bmatrix}(\tilde{H}^{(k-1)} - \tilde{H})\begin{bmatrix} u &amp; u_\perp \end{bmatrix} = \\
\begin{bmatrix}
    u^T\tilde{H}^{(k-1)}u - 1 &amp; u^T\tilde{H}^{(k-1)}u_\perp \\
    u_\perp^T\tilde{H}^{(k-1)}u &amp; u_\perp^T(\tilde{H}^{(k-1)}-\tilde{H}^{(k)})u_\perp
\end{bmatrix}.
\end{aligned}\]</p><p>By a property of the Frobenius norm we can consider the blocks independently: </p><p class="math-container">\[\begin{aligned}
||\tilde{H}^{(k-1)} - \tilde{H}||^2_F &amp; = ||U^T(\tilde{H}^{(k-1)} - \tilde{H})U||^2_F \\
&amp; = (u^T\tilde{H}^{(k-1)}u -1)^2 + ||u^T\tilde{H}^{(k-1)}u_\perp||_F^2 + ||u_\perp^T\tilde{H}^{(k-1)}u||_F^2 + ||u_\perp^T(\tilde{H}^{(k-1)} - \tilde{H})u_\perp||_F^2.
\end{aligned}\]</p><p>We see that <span>$\tilde{H}$</span> only appears in the last term, which should therefore be made zero, i.e. the projections of <span>$\tilde{H}_{k-1}$</span> and <span>$\tilde{H}$</span> onto the space spanned by <span>$u_\perp$</span> should coincide. With the condition <span>$\tilde{H}u \overset{!}{=} u$</span> we hence get: </p><p class="math-container">\[\tilde{H} = U\begin{bmatrix} 1 &amp; 0 \\ 0 &amp; u^T_\perp\tilde{H}^{(k-1)}u_\perp \end{bmatrix}U^T = uu^T + (\mathbb{I}-uu^T)\tilde{H}^{(k-1)}(\mathbb{I}-uu^T).\]</p><p>If we now map back to the original coordinate system, the ideal solution for <span>$H^{(k)}$</span> is: </p><p class="math-container">\[H^{(k)} = (\mathbb{I} - \frac{1}{(s^{(k-1)})^Ty^{(k-1)}}s^{(k-1)}(y^{(k-1)})^T)H^{(k-1)}(\mathbb{I} - \frac{1}{(s^{k-1})^Ty^{(k-1)}}y^{(k-1)}(s^{(k-1)})^T) + \\ \frac{1}{(s^{(k-1)})^Ty^{(k-1)}}s^{(k-1)}(s^{(k-1)})^T,\]</p><p>and the assertion is proved.</p></div></details><p>The cache and the parameters are updated with:</p><ol><li>Compute the gradient <span>$\nabla_{x^{(k)}}L$</span>,</li><li>obtain a negative search direction <span>$p^{(k)} \gets -H^{(k)}\nabla_{x^{(k)}}L$</span>,</li><li>compute <span>$s^{(k)} = \eta^{(k)}p^{(k)}$</span>,</li><li>compute <span>$y^{(k)} \gets \nabla_{x^{(k)}}L - \nabla_{x^{(k-1)}}L$</span>,</li><li>update <span>$H^{(k + 1)} \gets (\mathbb{I} - \frac{1}{(s^{(k)})^Ty^{(k)}}s^{(k)}(y^{(k)})^T)H^{(k)}(\mathbb{I} - \frac{1}{s^({k})^Ty^{(k)}}y^{(k)}(s^{(k)})^T) + \\ \frac{1}{(s^{(k)})^Ty^{(k)}}s^{(k)}(s^{(k)})^T$</span>,</li><li>update <span>$x^{(k + 1)} \gets x^{(k)} + s^{(k)}$</span>.</li></ol><p>The cache of the BFGS algorithm thus consists of the matrix <span>$H^{(\cdot)}$</span> for each weight <span>$x^{(\cdot)}$</span> in the neural network and the gradient for the previous time step <span>$\nabla_{x^{(k-1)}}L$</span>. <span>$s^{(k)}$</span> here is again the <em>velocity</em> that we use to update the neural network weights. </p><h2 id="The-Riemannian-Version-of-the-BFGS-Algorithm"><a class="docs-heading-anchor" href="#The-Riemannian-Version-of-the-BFGS-Algorithm">The Riemannian Version of the BFGS Algorithm</a><a id="The-Riemannian-Version-of-the-BFGS-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#The-Riemannian-Version-of-the-BFGS-Algorithm" title="Permalink"></a></h2><p>Generalizing the BFGS algorithm to the setting of a Riemannian manifold is straightforward. All we have to do is replace Euclidean gradient by Riemannian ones (composed with a lift via <a href="../../arrays/global_tangent_spaces/#GeometricMachineLearning.global_rep"><code>global_rep</code></a>): </p><p class="math-container">\[\nabla_{x^{(k)}}L \implies (\Lambda^{(k)})^{-1}(\Omega(x^{(k)}, \mathrm{grad}_{x^{(k)}}L))\Lambda^{(k)} = \mathtt{global\_rep}(\mathrm{grad}_{x^{(k)}}),\]</p><p>and addition by a retraction:</p><p class="math-container">\[    x^{(k+1)} \gets x^{(k)} + s^{(k)} \implies x^{(k+1)} \gets \mathrm{Retraction}(s^{(k)})x^{(k)}.\]</p><p>The Hessian for the manifold BFGS algorithm is of size <span>$\tilde{N}\times\tilde{N}$</span> where <span>$\tilde{N} = \mathrm{dim}(\mathfrak{g}^\mathrm{hor})$</span>. For <a href="../../arrays/global_tangent_spaces/#The-Global-Tangent-Space-for-the-Stiefel-Manifold">the global tangent space belonging to the Stiefel manifold</a> we have <span>$\tilde{N} = (N - n)n + n(n - 1)\div2$</span>.</p><p>In order to multiply the stored weights with the Hessian <span>$H$</span> we use the vectorization operation <a href="../../arrays/skew_symmetric_matrix/#Base.vec-Tuple{GeometricMachineLearning.AbstractTriangular}"><code>vec</code></a> for all weights:</p><pre><code class="language-julia hljs">A = SkewSymMatrix([1, 2, 3], 3)
B = [4 5 6; ]
BÃÑ = StiefelLieAlgHorMatrix(A, B, 4, 3)
BÃÑ |&gt; vec</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">vcat(3-element Vector{Int64}, 3-element Vector{Int64}):
 1
 2
 3
 4
 5
 6</code></pre><p>The <span>$H$</span> matrix in the cache is correspondingly initialized as:</p><pre><code class="language-julia hljs">BFGSCache(BÃÑ)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">&quot;`BFGSCache` that currently stores `B`as  ...&quot;4√ó4 StiefelLieAlgHorMatrix{Int64, SkewSymMatrix{Int64, Vector{Int64}}, Matrix{Int64}}:
 0  0  0  0
 0  0  0  0
 0  0  0  0
 0  0  0  0
... and `H` as
6√ó6 Matrix{Int64}:
 1  0  0  0  0  0
 0  1  0  0  0  0
 0  0  1  0  0  0
 0  0  0  1  0  0
 0  0  0  0  1  0
 0  0  0  0  0  1</code></pre><p>We see that <span>$\bar{B}$</span> is of dimension <span>$\tilde{N} = (N - n)n + n(n - 1)\div2 = 3 + 3 = 6$</span> and <span>$H$</span> is of dimension <span>$\tilde{N}\times\tilde{N} = 6\times6.$</span></p><h2 id="The-Curvature-Condition-and-the-Wolfe-Conditions"><a class="docs-heading-anchor" href="#The-Curvature-Condition-and-the-Wolfe-Conditions">The Curvature Condition and the Wolfe Conditions</a><a id="The-Curvature-Condition-and-the-Wolfe-Conditions-1"></a><a class="docs-heading-anchor-permalink" href="#The-Curvature-Condition-and-the-Wolfe-Conditions" title="Permalink"></a></h2><p>In textbooks [<a href="../../references/#wright2006numerical">44</a>] an application of the BFGS algorithm typically further involves a line search subject to the <em>Wolfe conditions</em>. If these are satisfied the <em>curvature condition</em> usually also is.</p><p>A condition that is similar to the <em>secant condition</em> discussed before is that <span>$R^{(k)}$</span> has to be positive-definite at point <span>$s^{(k-1)}$</span>:</p><p class="math-container">\[(s^{(k-1)})^Ty^{(k-1)} &gt; 0.\]</p><p>This is referred to as the <em>standard curvature condition</em>. If we impose the <em>Wolfe conditions</em>, the <em>standard curvature condition</em> holds automatically. The Wolfe conditions are stated with respect to the parameter <span>$\eta^{(k)}$</span>.</p><div class="admonition is-info"><header class="admonition-header">Definition</header><div class="admonition-body"><p>The <strong>Wolfe conditions</strong> are:</p><p class="math-container">\[\begin{aligned}
L(x^{(k)}+\eta^{(k)}p^{(k)}) &amp; \leq{}L(x^{(k)}) + c_1\eta^{(k)}(\nabla_{x^{(k)}}L)^Tp^{(k)} &amp; \text{ for } &amp; c_1\in(0,1) \quad\text{and} \\
(\nabla_{(x^{(k)} + \eta^{(k)}p^{(k)})}L)^Tp^{(k)} &amp; \geq c_2(\nabla_{x^{(k)}}L)^Tp^{(k)} &amp; \text{ for } &amp; c_2\in(c_1,1).
\end{aligned}\]</p><p>The two Wolfe conditions above are respectively called the <em>sufficient decrease condition</em> and the <em>curvature condition</em>.</p></div></div><p>A possible choice for <span>$c_1$</span> and <span>$c_2$</span> are <span>$10^{-4}$</span> and <span>$0.9$</span> [<a href="../../references/#wright2006numerical">44</a>]. We further have:</p><div class="admonition is-info"><header class="admonition-header">Theorem</header><div class="admonition-body"><p>The second Wolfe condition, also called curvature condition, is stronger than the standard curvature condition under the assumption that the first Wolfe condition is true and <span>$L(x^{(k+1)}) &lt; L(^{(x_k)})$</span>.</p></div></div><details class="admonition is-details"><summary class="admonition-header">Proof</summary><div class="admonition-body"><p>We use the second Wolfe condition to write</p><p class="math-container">\[(\nabla_{x^{(k)}}L)^Tp^{(k-1)} - c_2(\nabla_{x^{(k-1)}}L)^Tp^{(k-1)} = (y^{(k-1)})^Tp^{(k-1)} + (1 - c_2)(\nabla_{x^{(k-1)}}L)^Tp^{(k-1)} \geq 0,\]</p><p>and we can apply the first Wolfe condition on the second term in this expression: </p><p class="math-container">\[(1 - c_2)(\nabla_{x^{(k-1)}}L)^Tp^{(k-1)}\geq\frac{1-c_2}{c_1\eta^{(k-1)}}(L(x^{(k)}) - L(x^{(k-1)})),\]</p><p>which is negative if the value of <span>$L$</span> is decreasing.</p></div></details><p>It is noteworthy that line search has not been used a lot in deep learning in the past. This is beginning to change however [<a href="../../references/#kenneweg2024improving">46</a>, <a href="../../references/#vaswani2019painless">47</a>]. We also note that the BFGS optimizer combined with the <a href="../../arrays/global_tangent_spaces/#Global-Tangent-Spaces">global tangent space representation</a> offers a way of performing second order optimization on manifolds, this is however not the only way to do so [<a href="../../references/#huang2016riemannian">48</a>, <a href="../../references/#gao2024symplectic">49</a>].</p><h2 id="Stability-of-the-Algorithm"><a class="docs-heading-anchor" href="#Stability-of-the-Algorithm">Stability of the Algorithm</a><a id="Stability-of-the-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Stability-of-the-Algorithm" title="Permalink"></a></h2><p>Similar to the <a href="../optimizer_methods/#The-Adam-Optimizer">Adam optimizer</a> we also add a <span>$\delta$</span> term for stability to two of the terms appearing in the update rule of the BFGS algorithm in practice. </p><h2 id="Library-Functions"><a class="docs-heading-anchor" href="#Library-Functions">Library Functions</a><a id="Library-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Library-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.BFGSOptimizer" href="#GeometricMachineLearning.BFGSOptimizer"><code>GeometricMachineLearning.BFGSOptimizer</code></a> ‚Äî <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BFGSOptimizer(Œ∑, Œ¥)</code></pre><p>Make an instance of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) optimizer. </p><p><code>Œ∑</code> is the <em>learning rate</em>. <code>Œ¥</code> is a stabilization parameter.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/66af6a261d853059acec99cdf8faf97d145f9557/src/optimizers/bfgs_optimizer.jl#LL1-L8">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="GeometricMachineLearning.BFGSCache" href="#GeometricMachineLearning.BFGSCache"><code>GeometricMachineLearning.BFGSCache</code></a> ‚Äî <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BFGSCache(B)</code></pre><p>Make the cache for the BFGS optimizer based on the array <code>B</code>.</p><p>It stores an array for the gradient of the previous time step <code>B</code> and the inverse of the Hessian matrix <code>H</code>.</p><p>The cache for the inverse of the Hessian is initialized with the idendity. The cache for the previous gradient information is initialized with the zero vector.</p><p>Note that the cache for <code>H</code> is changed iteratively, whereas the cache for <code>B</code> is newly assigned at every time step.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/66af6a261d853059acec99cdf8faf97d145f9557/src/optimizers/bfgs_cache.jl#LL1-L12">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="AbstractNeuralNetworks.update!-Tuple{Optimizer{&lt;:BFGSOptimizer}, BFGSCache, AbstractArray}" href="#AbstractNeuralNetworks.update!-Tuple{Optimizer{&lt;:BFGSOptimizer}, BFGSCache, AbstractArray}"><code>AbstractNeuralNetworks.update!</code></a> ‚Äî <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">update!(o::Optimizer{&lt;:BFGSOptimizer}, C, B)</code></pre><p>Peform an update with the BFGS optimizer. </p><p><code>C</code> is the cache, <code>B</code> contains the gradient information (the output of <a href="../../arrays/global_tangent_spaces/#GeometricMachineLearning.global_rep"><code>global_rep</code></a> in general).</p><p>First we compute the <em>final velocity</em> with</p><pre><code class="language-julia hljs">vecS = -o.method.Œ∑ * C.H * vec(B)</code></pre><p>and then we update <code>H</code></p><pre><code class="language-julia hljs">C.H .= (ùïÄ - œÅ * SY) * C.H * (ùïÄ - œÅ * SY&#39;) + œÅ * vecS * vecS&#39;</code></pre><p>where <code>SY</code> is <code>vecS * Y&#39;</code> and <code>ùïÄ</code> is the idendity. </p><p><strong>Implementation</strong></p><p>For stability we use <code>Œ¥</code> for computing <code>œÅ</code>:</p><pre><code class="language-julia hljs">œÅ = 1. / (vecS&#39; * Y + o.method.Œ¥)</code></pre><p>This is similar to the <a href="../optimizer_methods/#GeometricMachineLearning.AdamOptimizer"><code>AdamOptimizer</code></a></p><p><strong>Extended help</strong></p><p>If we have weights on a <a href="../../manifolds/manifolds/#GeometricMachineLearning.Manifold"><code>Manifold</code></a> than the updates are slightly more difficult. In this case the <a href="../../arrays/skew_symmetric_matrix/#Base.vec-Tuple{GeometricMachineLearning.AbstractTriangular}"><code>vec</code></a> operation has to be generalized to the corresponding <em>global tangent space</em>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/66af6a261d853059acec99cdf8faf97d145f9557/src/optimizers/bfgs_optimizer.jl#LL18-L48">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Base.vec-Tuple{StiefelLieAlgHorMatrix}" href="#Base.vec-Tuple{StiefelLieAlgHorMatrix}"><code>Base.vec</code></a> ‚Äî <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">vec(A::StiefelLieAlgHorMatrix)</code></pre><p>Vectorize <code>A</code>. </p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using GeometricMachineLearning

A = SkewSymMatrix([1, ], 2)
B = [2 3; ]
BÃÑ = StiefelLieAlgHorMatrix(A, B, 3, 2)
BÃÑ |&gt; vec

# output

vcat(1-element Vector{Int64}, 2-element Vector{Int64}):
 1
 2
 3</code></pre><p><strong>Implementation</strong></p><p>This is using <code>Vcat</code> from the package <code>LazyArrays</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaGNI/GeometricMachineLearning.jl/blob/66af6a261d853059acec99cdf8faf97d145f9557/src/arrays/stiefel_lie_algebra_horizontal.jl#LL222-L248">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation noncanonical"><dl><dt>[44]</dt><dd><div>J.¬†N.¬†Stephen J. Wright. <em>Numerical optimization</em> (Springer Science+Business Media, New York, NY, 2006).</div></dd><dt>[45]</dt><dd><div>A.Œì. (math.stackexchange user 253273). <em>Quasi-newton methods: Understanding DFP updating formula</em>, <a href="https://math.stackexchange.com/q/2279304"><code>https://math.stackexchange.com/q/2279304</code></a> (2017). Accessed on September 19, 2024.</div></dd><dt>[48]</dt><dd><div>W.¬†Huang, P.-A.¬†Absil and K.¬†A.¬†Gallivan. <em>A Riemannian BFGS method for nonconvex optimization problems</em>. In: <em>Numerical Mathematics and Advanced Applications ENUMATH 2015</em> (Springer, 2016); pp.¬†627‚Äì634.</div></dd></dl></div><!--<h1 id="References-2"><a class="docs-heading-anchor" href="#References-2">References</a><a class="docs-heading-anchor-permalink" href="#References-2" title="Permalink"></a></h1><div class="citation noncanonical"><dl><dt>[43]</dt><dd><div>I.¬†Goodfellow, Y.¬†Bengio and A.¬†Courville. <em>Deep learning</em> (MIT press, Cambridge, MA, 2016).</div></dd><dt>[44]</dt><dd><div>J.¬†N.¬†Stephen J. Wright. <em>Numerical optimization</em> (Springer Science+Business Media, New York, NY, 2006).</div></dd><dt>[45]</dt><dd><div>A.Œì. (math.stackexchange user 253273). <em>Quasi-newton methods: Understanding DFP updating formula</em>, <a href="https://math.stackexchange.com/q/2279304"><code>https://math.stackexchange.com/q/2279304</code></a> (2017). Accessed on September 19, 2024.</div></dd><dt>[48]</dt><dd><div>W.¬†Huang, P.-A.¬†Absil and K.¬†A.¬†Gallivan. <em>A Riemannian BFGS method for nonconvex optimization problems</em>. In: <em>Numerical Mathematics and Advanced Applications ENUMATH 2015</em> (Springer, 2016); pp.¬†627‚Äì634.</div></dd><dt>[49]</dt><dd><div>B.¬†Gao, N.¬†T.¬†Son and T.¬†Stykel. <em>Symplectic Stiefel manifold: tractable metrics, second-order geometry and Newton&#39;s methods</em>, arXiv¬†preprint¬†arXiv:2406.14299 (2024).</div></dd></dl></div>--><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>Various Newton methods and quasi-Newton methods differ in how they model the <em>approximate Hessian</em>.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>The Frobenius norm is <span>$||A||_F^2 = \sum_{i,j}a_{ij}^2$</span>.</li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../optimizer_methods/">¬´ Optimizer Methods</a><a class="docs-footer-nextpage" href="../../layers/sympnet_gradient/">Sympnet Layers ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Friday 15 November 2024 15:38">Friday 15 November 2024</span>. Using Julia version 1.11.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
